<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>科学家提出更好解决视觉对话生成问题的双通道多步推理模型 | 极客快訊</title><meta property="og:title" content="科学家提出更好解决视觉对话生成问题的双通道多步推理模型 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p9.pstatp.com/large/pgc-image/Rq1gRT0J7BTPhP"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e6%b8%b8%e6%88%8f/63c8b093.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e6%b8%b8%e6%88%8f/63c8b093.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e6%b8%b8%e6%88%8f/63c8b093.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e6%b8%b8%e6%88%8f/63c8b093.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e6%b8%b8%e6%88%8f/63c8b093.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e6%b8%b8%e6%88%8f/63c8b093.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e6%b8%b8%e6%88%8f/63c8b093.html><link rel=canonical href=https://geekbank.cf/tw/%e6%b8%b8%e6%88%8f/63c8b093.html><meta property="article:published_time" content="2020-11-14T20:54:30+08:00"><meta property="article:modified_time" content="2020-11-14T20:54:30+08:00"><meta name=Keywords content><meta name=description content="科学家提出更好解决视觉对话生成问题的双通道多步推理模型"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E6%B8%B8%E6%88%8F/63c8b093.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>科学家提出更好解决视觉对话生成问题的双通道多步推理模型</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E6%B8%B8%E6%88%8F.html>游戏</a></span></div><div class=post-content><p>为解决目前视觉对话系统中视觉语言两个模态之间的多轮指代、推理以及信息对齐等问题，中国科学院自动化研究所博士陈飞龙、副研究员许家铭和研究员徐波等人与腾讯一起共建了一种双通道多步推理视觉对话生成模型，使得模型从视觉和语言两个方面丰富问题的语义表示，更好地针对问题生成高质量答复。</p><p>由于自然语言处理和计算机视觉技术的快速发展，多模态问题受到了越来越多的关注。视觉对话是一种视觉语言交互任务，需要AI智能体与人类围绕同一个输入图像进行交流。这是一项具有挑战性的任务，要求模型能够充分理解人类当前轮对话的提问，同时有效整合对话历史的语言模态和输入图像的视觉模态，以关注与当前问题相关的语义信息并进行推理，给出高质量答复。研究团队就视觉对话任务提出一种双通道多步推理模型（简称DMRM）。</p><p>DMRM通过利用双通道推理同步地从对话历史和输入图像中捕获信息，以丰富问题的语义表示。具体地说，DMRM维护一个跨模态交互的双通道（如图1所示，Track Module负责从视觉方面丰富问题的语义表示，Locate Module负责从对话历史方面丰富问题的语义表示），通过每个通道中的多步推理过程（如图2所示）获得与当前问题和对话历史相关的视觉特征，以及当前问题和输入图像相关的语言特征。此外，团队还设计了一种多模态注意机制，以进一步增强解码器来生成更准确的答复。</p><p>团队在视觉对话任务中的解码端引入多模态注意机制，有效地缓解了只利用编码端输出多模态信息融合的局限性，在解码过程中能够较好地进行一些错误纠正及语义丰富。</p><p>团队在VisDial v0.9和VisDial v1.0两个公开数据集上进行实验。VisDial v0.9包含了83k训练集，40k测试集，每一幅图像对应10轮对话和图像描述。VisDial v1.0包含了123k训练集，2k验证集和8k测试集。</p><p>表1和表2给出了不同模型在两个数据集上的实验效果。可以看出，在大多数评价指标上，双通道多步推理视觉对话生成模型DMRM都优于其他模型（其中，MRR、R@k越高越好，Mean越低越好）。表3显示了双通道多步推理视觉对话生成模型DMRM的消融实验，分析了各个模块对于视觉对话任务效果的影响，可以看出双通道多步推理以及多模态解码器都起到了重要作用。</p><p>图4则显示了DMRM模型生成的回答结果，融合了多模态解码器的DMRM模型在准确性和语义丰富性上表现更好。</p><p>研究相关成果已被AAAI2020录用。</p><img alt=科学家提出更好解决视觉对话生成问题的双通道多步推理模型 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Rq1gRT0J7BTPhP><p><em>图1 DMRM模型框架</em></p><img alt=科学家提出更好解决视觉对话生成问题的双通道多步推理模型 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Rq1gRTheaIcSK><p><em>图2 多步推理的示意图</em></p><img alt=科学家提出更好解决视觉对话生成问题的双通道多步推理模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rq1gRU5YtNBvQ><p><em>图3 基于多模态注意的解码器</em></p><p><em>表1 不同模型在VisDial v0.9数据集上的实验结果</em></p><img alt=科学家提出更好解决视觉对话生成问题的双通道多步推理模型 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Rq1gRUa6Y6cqIp><p><em>表2 不同模型在VisDial v1.0数据集的实验结果</em></p><img alt=科学家提出更好解决视觉对话生成问题的双通道多步推理模型 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Rq1gRV9DiD0qpH><p><em>表3 DMRM的消融实验</em></p><img alt=科学家提出更好解决视觉对话生成问题的双通道多步推理模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rq1gS25J2eJOG0><img alt=科学家提出更好解决视觉对话生成问题的双通道多步推理模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rq1gS2dF0EgxIO><p><em>图4 视觉对话生成结果样例</em></p><p><strong>来源：</strong><strong>中国科学院自动化研究所</strong></p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'科学家','解决','视觉'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>