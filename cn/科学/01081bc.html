<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>机器爱学习13——支持向量机SVM之预测函数、代价函数 | 极客快訊</title><meta property="og:title" content="机器爱学习13——支持向量机SVM之预测函数、代价函数 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/154043506714200d4a1b95e"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/01081bc.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/01081bc.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/01081bc.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/01081bc.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/01081bc.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/01081bc.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/01081bc.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/01081bc.html><meta property="article:published_time" content="2020-10-29T20:56:37+08:00"><meta property="article:modified_time" content="2020-10-29T20:56:37+08:00"><meta name=Keywords content><meta name=description content="机器爱学习13——支持向量机SVM之预测函数、代价函数"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/01081bc.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>机器爱学习13——支持向量机SVM之预测函数、代价函数</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><div><p>本章开始，将为大家介绍一个比较复杂的机器学习算法：<strong>支持向量机</strong>Support Vector Machine，以下简称SVM。</p><p>网络上有很多关于SVM的讲解，讲的都很详细，内容很完整，本篇及之后关于SVM的文章，权当笔者的学习笔记好了，各位看官若能有所收获，那就再好不过了～</p><h1>什么是SVM</h1><p>SVM的全称为Support Vector Machine，即支持向量机。SVM属于有监督学习算法的一种（前面文章好像没有讲到有监督学习、无监督学习，大家可以自行百度下），主要用来解决数据分类问题，简单的来说，就是解决<strong>二分类问题</strong>。</p><p>等等？二分类问题？二分类问题不是可以通过逻辑回归来解决吗？为什么还要学习SVM？</p><p>相对于逻辑回归，SVM当然有她自己的优势。在大家完全理解SVM之前，我们暂时不介绍两者的异同，待SVM讲解完成之后，我们再做总结。</p><h1>SVM的预测函数</h1><p>当SVM用来解决分类问题时，其hypothesis function和逻辑回归的hypothesis function基本类似。</p><p>我们先回顾下逻辑回归的hypothesis function：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/154043506714200d4a1b95e><p class=pgc-img-caption>逻辑回归的hypothesis function</p></div><p>SVM的hypothesis function中，对f(x)做了点转换，具体如下：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15404351967509e46b4af01><p class=pgc-img-caption>SVM的hypothesis function</p></div><p>其实，SVM和逻辑回归的两个f(x)，说的是一个东西，只不过表达方式不同。</p><p>现在，我们知道了SVM的hypothesis function，接下来，只要求出"最佳"的向量W和实数b就可以了，要求出"最佳"的变量值，就需要借助于cost function，接下来，我们就看看SVM的cost function长什么样。</p><p>注意：</p><ul><li>不管是逻辑回归还是SVM，都将正类记为"+1"，但是逻辑回归将负类记为"0"，而SVM将负类记为"-1"，其实这些都无关紧要，只是一个标记而已，大家记住就好了。</li><li>上面说的"最佳"变量，指的是可以使代价函数最小的变量</li></ul><h1>SVM代价函数的几何意义</h1><p>在前面讲线性回归时，我们的cost functiuon是：样本误差（y-h(x))的平方和，我们的要求是让样本误差的平方和”尽量小“。那么SVM的代价函数是什么？假设我们的要求也是让”变量Z”最小，那”变量Z“到底是什么”</p><p>我们现在就来看看“变量Z”到底是什么！</p><p>讲解之前，先让大家做两道题：</p><p><strong>问题1：下图的二分类问题中，哪条直线（H1、H2、H3）可以作为target function？</strong></p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/15410605392584ce3323453><p class=pgc-img-caption>哪条直线可以作为target function？</p></div><p>我们来一一进行分析：</p><ul><li>对于H1，存在预测错误的情况，肯定是不能作为target function的；</li><li>对于H2，不存在预测错误的情况，可以作为target function；</li><li>对于H3，不存在预测错误的情况，可以作为target function；</li></ul><p>所以，H1不能作为target function；H2、H3都可以作为target function。</p><p>依据training set中有没有出现预测错误的情况，我们很容易得到上面的结论</p><ul><li>有预测错误：不能作为target function</li><li>没有预测错误：可以作为target function</li></ul><p><strong>问题2：上面的两条直线：H2、H3，哪一条更“完美”？</strong></p><p>哪一条更“完美”？？？</p><p>小编你先来告诉我啥叫“完美”？</p><p>OK，换个问法：</p><p><strong>上面两条直线H2、H3，哪一个作为target function时，在test set中更不容易出现预测错误？</strong></p><p>从图中，我们很容易看出来：</p><ul><li>离H2最近的3个点（黑点1、黑点2、白点3），到H2的距离都太小，如果test set中的点正好在这3个点附近，那H2很有可能出现预测错误</li><li>离H3最近的3个点（黑点2、白点3、白点4），到H3的距离都比较大，即使test set中的点在这三个点附近，H3也不太容易出现预测错误</li></ul><p>所以，H2作为target function时，更容易预测错误，而H3更不容易预测错误，因此，H3更“完美”。</p><p>我们再思考下：得到上面结论的依据是什么？</p><p>我们假设离target function最近的点，到target function的距离为d，则：</p><ul><li>当d很小时，target function更容易预测错误</li><li>当d很大时，target function更不容易预测错误</li></ul><p>target function在二维平面中是一条线，在多维空间中就是一个多维平面，SVM称之为<strong>超平面hyperplane</strong>。</p><p>在SVM中，这些离超平面最近的点（向量）有另外一个名字：<strong>支持向量support vertor</strong>。</p><p>SVM要解决的问题，就是去找一个超平面，使支持向量到超平面的距离尽可能的大。</p><p>根据上面的问题2，我们有理由相信：SVM找到的超平面更不容易预测错误。</p><p>然而，讲了这么多，跟代价函数又有什么关系？</p><p>如果我们将支持向量到超平面的距离作为代价函数，通过求代价函数的最大值，找到对应的W和b，不就找到target function了嘛～</p><p>完全正确！<strong>SVM代价函数就是支持向量到超平面的距离，通过最大化这个距离来进行求解</strong>。</p><h1>SVM代价函数的约束：“不偏不倚”</h1><p>前面我们讲到：通过最大化距离d，可以保证target function不容易出现预测错误。在二分类问题中，我们该如何理解这句话？</p><p>二分类问题中，</p><ul><li>正类为了不出现预测错误，当然想让支持向量中正类的点（向量），到超平面的距离越远越好</li><li>同样，负类为了不出现预测错误，当然想让支持向量中负类的点（向量），到超平面的距离越远越好</li></ul><p>两者都想让超平面离自己越远越好，可是超平面离支持向量中正类的点越远，就离负类点越近；离负类点越远，就离正类点越近。</p><p>超平面很无奈，为了保证“不偏不倚”，最后只能呆在中间位置。</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a338343a9f2a4d34902c01b88fa7fea5><p class=pgc-img-caption>图示超平面、支持向量</p></div><p>上图中，有三个超平面H、H1、H2（H、H1、H2互相平行）：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4f111909a4614d40818da989f2be97b5><p class=pgc-img-caption></p></div><p>图中可以看到，H到H1、H2的距离相等，这样H作为target function时，可以保证正类、负类均不容易出现预测错误。</p><p>对于上图，大家可能会有如下疑惑，我们一一解答（其实也是小编自己的疑惑，答案如有纰漏，还望大家不惜赐教）</p><p><strong>问题1：为什么超平面H的表达式中，等号右侧为0，而不是等于其他值？</strong></p><p>假设等号右侧为A(A !=0)，那么我们完全可以在等号两侧同时减去A，得到一个新超平面（此时表达式等号右侧为0），这个新超平面其实和转换前的超平面是同一个超平面。</p><p>所以，我们可以令等号右侧为0，而不会对超平面有任何影响（只不过计算出来的b会有所不同）。</p><p><strong>问题2：为什么超平面H1的表达式中，等号右侧为1，而不是其他值？</strong></p><p>由于H和H1平行，假设H移动到H1后，H1真实表达式如下：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/36839965cd3542a99c851ec04eb3d029><p class=pgc-img-caption></p></div><p>我们完全可以在等式两边同时乘以1/M（成比例的改变W、b），得到一个新超平面（此时表达式等号右侧为1），这个新超平面其实和转换前的超平面是同一个超平面。</p><p>所以，我们可以令等号右侧为1，而不会对超平面有任何影响（只不过计算出来的W、b会有所不同）。</p><p>下面在计算支持向量到超平面距离时，还会遇到成比例缩放W、b的情况，这样做只是让表达式更简单，并不会影响计算结果。</p><p><strong>问题3：为什么H2的表达式中，等号右侧为-1，而不是其他值？</strong></p><p>与上面的问题类似，不再赘述～</p><p><strong>问题4：H到H1、H2的距离1/||w||是如何算出来的？</strong></p><p>关于距离，下文会专门去计算，这里大家可以暂不关注</p><p>下面，我们再介绍两个结论：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1bd97c8a58194789a0827b5161dd8628><p class=pgc-img-caption></p></div><p>为什么有这些结论？这些结论是什么意思？</p><p>对于正类，我们可以认为支持向量到超平面H的距离（函数距离，什么是函数距离，大家不必太关注）为1，其他向量点到超平面H的距离会更远，即大于1。因此所有正类点满足上面的条件。负类也是同样的道理～</p><p>现在大家基本明白上面的两个结论，我们很容易将两个结论合并为如下不等式（正类时，y=+1；负类时，y=-1，大家现在明白为什么负类时y取-1了吧）：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b214ec5a44294e6493b9e5a9b44f2a36><p class=pgc-img-caption></p></div><p>这个不等式有特殊的含义：如果所有的点都满足上面不等式，则SVM就做到了"不偏不倚"（通过上面的讲解，这个应该不难理解）。</p><p>所以，SVM在对代价函数求极值时，需要考虑上面的不等式约束，即在"不偏不倚"的情况下，找到距离最大值对应的参数W、b。</p><p>OK，接下来，我们就来看下如何推导出代价函数，并求解包含不等式约束的极值问题。</p><p>本章剩余内容主要做以下两个工作：</p><ul><li>将代价函数具体化，方便用数学方法求解</li><li>将代价函数变形，方便用简单的数学方法求解</li></ul><h1>代价函数的数学表示</h1><p>前面我们提到：代价函数可以是支持向量到超平面的距离。既然是点到平面的距离，那我们先看看数学上：点到平面的距离公式。</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/154112782540404bf31c143><p class=pgc-img-caption>点到平面的距离公式推导</p></div><p>上图直接截了距离公式的推导过程，大家不必关注过程，只需要关注下面的距离公式即可（其中||W||称为L2范式）：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1541128707397e60735dba9><p class=pgc-img-caption>点到平面距离公式</p></div><p>上面是一个样本点到超平面的距离，我们假设所有样本点中最小距离为γ，则：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/13ecf71f81bc4d37ab26d6aaef6f9440><p class=pgc-img-caption>最小几何间隔</p></div><p>这个最小距离，我们称之为最小几何间隔。我们的代价函数就是这个最小几何间隔，通过<strong>求最小几何间隔的最大值（注意是求某一组数据最小值的最大值），</strong>找到一组W和b，从而得到我们的预测函数。</p><p>据此，我们第一版代价函数格式如下：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/aa4c325540704178981b3e07535bda3f><p class=pgc-img-caption>SVM代价函数版本--1</p></div><h1>代价函数变形－－缩放函数间隔</h1><p>为了方便计算，我们对代价函数做一点缩放（回想一下前面提到的缩放），从而使</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/710ab580a6fc4b8094f56a2f997a59d7><p class=pgc-img-caption>最小函数间隔</p></div><p>缩放后，新的代价函数如下：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bf78c7c494454f8d8fbf820d54e39d6d><p class=pgc-img-caption>SVM代价函数版本--2</p></div><p>缩放后会不会影响W、b？</p><p>缩放相当于在等式两边同时乘以一个正数，使分子变成1，此时等式仍然成立，而且W、b也没有被改变。</p><p>其实上面的数值是点到平面的<strong>函数间隔</strong>，我们通过缩放座标，使函数间隔变为1，但是几何间隔并不会改变，也不会影响W、b。</p><h1>包含约束条件的代价函数</h1><p>还记得我们前面推导的约束条件："不偏不倚"吗？考虑到约束条件，新的代价函数如下：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0240010efcc84845bcdcb6618c1c4453><p class=pgc-img-caption>SVM代价函数版本--3</p></div><p>缩写s.t.表示"Subject to"，是"服从某某条件"的意思。</p><p>我们习惯求代价函数的最小值，因此，这里再对代价函数做一点改动（求1/||W||的最大值，改为求||W||的最小值，为了方便计算，再改为求||W||平方的最小值）。</p><p>改动后，新的代价函数为：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4fcc655aa5f24aa5bea7fd95d5bb3527><p class=pgc-img-caption>SVM代价函数版本--4</p></div><h1>SVM代价函数变形--拉格朗日函数</h1><p>上面的代价函数，是一个包含约束条件的极值问题，这种问题可以用现成的QP (Quadratic Programming) 优化包进行求解。不过我们有更好的求解方法--拉格朗日乘数法。</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7e381d0d18314f8aa6df75e3928a6d4b><p class=pgc-img-caption>拉格朗日乘数法-百科</p></div><p>总结一下，<strong>拉格朗日乘数法可以将有约束的极值问题，转化为无约束的极值问题</strong>。</p><p>针对上面带约束的代价函数，得到对应的拉格朗日函数如下：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/10ac6868a6ff4b109390233eac9f5e63><p class=pgc-img-caption>拉格朗日函数</p></div><p>其中αi称为拉格朗日乘子，并且αi>=0。</p><p>对于上面的拉格朗日函数，有如下结论：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/576fb815fbbf46c88cddc3c295dc81bf><p class=pgc-img-caption></p></div><p>上面的结论是如何得到的呢？</p><p>考虑代价函数中"不偏不倚"的约束条件，则有</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c9e959b763fb441faf6e7353a4e07fe5><p class=pgc-img-caption></p></div><p>故：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/02eec73f40c24436a544fff7c6827c33><p class=pgc-img-caption></p></div><p>简单的来说，就是在满足"不偏不倚"条件、αi>=0时，我们的代价函数就等于拉格朗日函数的最大值。</p><p>借助于拉格朗日乘数法，"不偏不倚"的约束条件就被消除掉了。</p><p>至此，我们的SVM代价函数如下：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fb0d0456f7d1449aa84cf8233aa78bb1><p class=pgc-img-caption>SVM代价函数版本--5</p></div><p>这里的代价函数是min(max)的形式，数学上称为拉格朗日函数的极小极大问题。</p><h1>SVM代价函数变形--拉格朗日对偶问题</h1><p>对于SVM代价函数版本5，我们可以去求解得到W、b。但实际中，我们并不会直接对上面的代价函数进行求解，而是求其对偶问题：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/55aa13602a324b708533bca405c0abda><p class=pgc-img-caption>拉格朗日对偶问题</p></div><p>与原始问题相比，对偶问题只是调整了极大、极小的求解顺序。对偶问题又称为拉格朗日极大极小问题。</p><p>接下来先看两个问题：</p><p><strong>1：我们为什么不直接求原始问题，反而去求其对偶问题？</strong></p><p>原因如下：</p><ul><li>对偶问题中，可以使用SMO算法高效的进行求解</li><li>对偶问题中可以很容易的引入核函数，而通过核函数，可以将线性不可分的数据集转换为线性可分的数据集</li></ul><p>关于SMO算法、核函数，后面会进行讲解。</p><p><strong>2：求解对偶问题得到的W、b，与原始问题的W、b相等吗？</strong></p><p>我们假设原始问题的最优解为p*，对偶问题的最优解为d*，则总是存在如下的关系：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a8d69d9958f940e2960a3d713bcc7093><p class=pgc-img-caption></p></div><p>并且，当<strong>满足强对偶性时，d* == p*。</strong>如果我们的问题满足强对偶性，那通过求对偶问题的最优解，就相当于得到了原始问题的最优解。</p><p>那什么是强对偶性？我们的问题满足强对偶性吗？</p><p>Slater定理告诉我们，当满足以下两个条件时，强对偶性成立：</p><ul><li>Slater条件成立（即数据线性可分）</li><li>原始问题是凸优化问题</li></ul><p>前面我们默认数据集是线性可分的，即存在一个超平面可以将所有数据正确分类，因此Slater条件成立（实际情况中数据可能是线性不可分的，为了满足强对偶性，可以引入核函数，通过核函数的转换，将数据集转换为线性可分）</p><p>另外原始问题中，目标函数1/2||W||**2为凸函数，对应的优化问题即为凸优化问题。</p><p>因此，我们的原始问题是满足强对偶性的，所以对偶问题的最优解即为原始问题的最优解。</p><p>OK，我们来梳理一下思路：</p><ol><li>通过拉格朗日乘数法，我们将带约束的极值问题转化为无约束的原始问题</li><li>我们又证明了原始问题的最优解，等于其对偶问题的最优解（满足强对偶性）</li></ol><p>现在我们的cost function转化成了对偶问题，格式如下：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1abe26e331474e01b2372ac0849be538><p class=pgc-img-caption>SVM代价函数最终版</p></div><p>那么该如何求对偶问题的最优解呢？</p><p>我们先看一个结论（大家感兴趣，可以翻翻高数书，这里直接写出结论）</p><p><strong>如果凸优化问题具有可微的目标函数和约束条件，且满足Slater条件，那么KKT条件是最优解的充要条件。</strong></p><p>我们来分析一下上面的结论：</p><p>首先，我们的原目标函数和约束条件是可微的（具体大家可以查阅可微的充要条件，推导一下），而且满足Slater条件，因此，我们的最优解满足KKT条件。</p><p>既然最优解满足KKT条件，那么从KKT条件中，我们就能得到一些最优解的线索。</p><p>那么什么是KKT条件？KKT条件具体包含如下约束：</p><ol><li>最优解必须满足不等式约束：1-y*(WT*x +b) &lt;=0</li><li>L(W,b,a)对各个变量偏导数为零</li><li>ai>=0</li></ol><p>从KKT条件1、3，我们好像得不到什么线索，我们不妨看看KKT条件2能带给我们什么。</p><p>KKT条件（2），即L(W,b,a)对各个变量偏导数为零，我们来求L（W，b，a）对W、b的偏导数：</p><div class=pgc-img><img alt=机器爱学习13——支持向量机SVM之预测函数、代价函数 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fdb41d5f8e6a464a839b41252ab48236><p class=pgc-img-caption></p></div><p>上面就是我们通过KKT条件得到的最优解的一些线索，我们可以将这些线索代入对偶问题来做简化，如何代入？又该如何求解对偶问题最优解，我们留在下一章进行讲解。</p><p>本章内容就讲到这里，讲了这么多，有用的东西有三个：</p><ul><li>SVM预测函数</li><li>SVM代价函数（拉格朗日对偶问题）</li><li>KKT条件</li></ul></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'函数','机器','爱学习'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>