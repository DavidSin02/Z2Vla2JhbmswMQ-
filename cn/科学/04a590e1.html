<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>常用数据挖掘算法从入门到精通 第十一章 支持向量机算法 | 极客快訊</title><meta property="og:title" content="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/37dd0003bcc2aafbf1d2"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/04a590e1.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/04a590e1.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/04a590e1.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/04a590e1.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/04a590e1.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/04a590e1.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/04a590e1.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/04a590e1.html><meta property="article:published_time" content="2020-10-29T21:13:00+08:00"><meta property="article:modified_time" content="2020-10-29T21:13:00+08:00"><meta name=Keywords content><meta name=description content="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/04a590e1.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>常用数据挖掘算法从入门到精通 第十一章 支持向量机算法</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><div><p>上一章为大家介绍了<strong>支持向量机（Support Vector Machine，SVM）</strong>的理论基础—<strong>统计学习理论</strong>的一些重要知识点，本章正式为大家介绍<strong>支持向量机算法</strong>。</p><p>支持向量机是在<strong>统计学习理论</strong>的<strong>VC维</strong>和<strong>结构风险最小化原理</strong>的基础上发展起来的一种新的<strong>机器学习方法</strong>。SVM根据<strong>有限样本</strong>的信息在<strong>模型的复杂性</strong>（即<strong>对特定样本的学习精度</strong>）和<strong>学习能力</strong>（即<strong>无错误地识别任意样本的能力</strong>）之间寻求最佳<strong>折中</strong>，以期获得最好的<strong>推广能力</strong>。</p><h1>结构风险最小化(Structural Risk Minimization，SRM）</h1><p>统计学习理论从VC维的概念出发，推导出关于<strong>经验风险</strong>和<strong>期望风险（真实风险的期望风险）</strong>之间关系的重要结论，称为<strong>泛化误差界</strong>，统计学习理论给出了以下估计真实风险的不等式。<br></p><p><img alt="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/37dd0003bcc2aafbf1d2></p><p class=pgc-img-caption>估计真实风险的不等式</p><p>其中R(w)是<strong>真实风险</strong>，Remp(w)表示<strong>经验风险</strong>，Φ(n/h)称为<strong>置信风险（置信范围）</strong>；n代表<strong>样本数量</strong>，h是函数集合的<strong>VC维</strong>，Φ是<strong>递减函数</strong>。</p><p>上述不等式（定理）说明，学习机器的<strong>期望风险</strong>由两部分组成：</p><ul class=list-paddingleft-2><li><p>第一部分是<strong>经验风险（学习误差引起的损失）</strong>，依赖于预测函数的选择</p></li><li><p>第二部分称为<strong>置信范围</strong>，是关于函数集<strong>VC维h</strong>的增函数</p></li></ul><p>显然，如果<strong>n/h较大</strong>，则期望风险值由经验风险值决定，此时为了最小化期望风险，我们只需最小化经验风险即可；</p><p>相反，如果<strong>n/h较小</strong>，经验风险最小并不能保证期望风险一定最小，此时我们必须同时考虑<strong>不等式右端的两项之和</strong>，称为<strong>结构风险</strong>。</p><p><img alt="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/37de000044fb10b2449c></p><p class=pgc-img-caption>结构风险最小化</p><ul class=list-paddingleft-2><li><p>一般的学习方法(如神经网络)是<strong>基于 Remp(w) 最小，</strong>满足对<strong>已有训练数据的最佳拟和，</strong>在理论上可以通过增加算法（如神经网络）的<strong>规模</strong>使得Remp(w) 不断降低以至为0</p></li><li><p>但是,这样使得算法（神经网络）的<strong>复杂度增加</strong>，<strong>VC维h增加，</strong>从而<strong>Φ</strong><strong>(n/h)增大，</strong>导致<strong>实际风险R(w)增加</strong>，这就是学习算法的<strong>过拟合(Overfitting)</strong>.</p></li></ul><p>大家如果想更好地理解<strong>结构风险最小化原则</strong>，可以先看一下前一篇文章<strong>《 第十章 支持向量机理论基础》</strong>，里面有一些关于统计学习理论主要知识的比较详细的介绍。</p><h1>分类问题的数学表示</h1><p><strong>2维</strong>空间上的分类问题⇨<strong>n维</strong>空间上的分类问题。</p><p><img alt="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/37e4000069129513b1a6></p><p class=pgc-img-caption>分类问题的数学表示</p><h1>分类问题的学习方法</h1><p><img alt="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/37e300008062bbae99ed></p><p class=pgc-img-caption>分类问题的学习方法</p><p>SVM分类问题大致有三种：<strong>线性可分问题、近似线性可分问题、线性不可分问题</strong>。</p><h1>线性可分情形：最大间隔原理</h1><p>对于线性可分的情况，<strong><em>l</em>_</strong>到<strong><em>l0</em></strong>和<strong><em>l+</em></strong>到<strong><em>l0</em></strong>的距离和，利用两条平行直线间的距离公式很容易得到距离（间隔）=<strong>2/||w||</strong>，最大化间隔就是求w的最小值。<em>S.T.</em>说明必须<strong>在满足约束条件（正确分类）的前提下求w的最小值</strong>。<br></p><p><img alt="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/37e500007683980dd25e></p><p class=pgc-img-caption>线性可分情形</p><blockquote><p><strong>模型求解：</strong></p></blockquote><p>原始问题是一个典型的<strong>线性约束的凸二次规划问题</strong>，模型求解主要用到了<strong>运筹学</strong>里面的方法，在这里就不仔细展开了，求解的思想主要是：</p><ol class=list-paddingleft-2><li><p>第一步，在原始问题中引入<strong>拉格朗日乘子</strong>转化为无约束问题（拉格朗日乘子法）;</p></li><li><p>第二步，根据最优化的<strong>一阶条件</strong>将原始问题转化为<strong>对偶问题</strong>;</p></li><li><p>第三步，根据<strong>KKT条件</strong>得到求得最优解时应满足的条件.</p></li></ol><blockquote><p>KKT条件是拉格朗日乘子法的泛化，在有等式约束时使用拉格朗日乘子法，在有不等约束时使用KKT条件</p></blockquote><ul class=list-paddingleft-2><li><p><strong>支持向量：</strong></p></li></ul><p>在两类样本中<strong>离最优分类超平面最近</strong>且在平行于最优分类超平面的平面<strong><em>l_</em></strong>,<em><strong>l+</strong></em>上的训练样本就叫做<strong>支持向量</strong>，理解为它们支撑起了<strong>超平面</strong><strong><em>l_</em></strong>和<strong><em>l+</em></strong>，所以称为<strong>支持向量</strong>，数学含义如下。</p><p><img alt="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/37dd00040c0b4d16de93></p><p class=pgc-img-caption>支持向量</p><h1>近似线性可分情形</h1><p><img alt="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/37dd000415cde11c2b99></p><p class=pgc-img-caption>近似线性可分情形</p><p><img alt="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/37e60000a245e904203f></p><p class=pgc-img-caption>引入松弛变量</p><p><img alt="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/37de0000b054e48174ff></p><p class=pgc-img-caption>引入惩罚函数</p><p>即，<strong>C代表了经验风险与置信风险的折中。</strong></p><h1>线性不可分情形</h1><p>把寻找<strong>低维空间非线性的“最大超曲面”</strong>问题转化为在<strong>高维空间</strong>中求解<strong>线性的“最大间隔平面”</strong>问题。即，把<strong>非线性可分的样本映射到高维空间，使样本线性可分</strong>。</p><p><img alt="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/37e50000ed267e348feb></p><p class=pgc-img-caption>线性不可分情形</p><ul class=list-paddingleft-2><li><p>线性不可分模型</p></li></ul><p><img alt="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/37e40000c6157e64036d></p><p class=pgc-img-caption>线性不可分模型</p><p>模型（3）的求解，必须知道<strong>非线性映射Ф的具体形式</strong>，但实际工作上，给出Ф的具体形式往往是<strong>非常困难</strong>的。</p><ul class=list-paddingleft-2><li><p>线性不可分问题的求解</p></li></ul><p><img alt="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/37de0000f419a5ac5ea0></p><p class=pgc-img-caption>线性不可分问题的求解</p><h1><strong>核函数K(xi,xj)</strong></h1><p>K(xi,xj)=(Ф(xi),Ф(xj))=Ф(xi)*Ф(xj)是样本xi,xj<strong>在特征空间中的内积</strong>，称为<strong>输入空间</strong>X上的<strong>核函数</strong>。</p><ul class=list-paddingleft-2><li><p>对非线性问题, 可以通过<strong>非线性变换</strong>转化为某个高维空间中的线性问题, 在变换空间求最优分类面. 这种<strong>变换可能比较复杂</strong>, 因此这种思路在一般情况下不易实现</p></li><li><p>为了避免从低维空间到高维空间可能带来的<strong>维数灾难问题</strong>，避免进行<strong>高维的内积运算</strong><br></p></li></ul><p>综上考虑引入核函数。</p><ul class=list-paddingleft-2><li><p>利用<strong>核函数</strong>代替向<strong>高维空间的非线性映射</strong>，并且不需要知道<strong>映射函数</strong></p></li><li><p>在最优分类面中采用适当的<strong>核函数</strong>就可以实现某一<strong>非线性变换后的线性分类</strong>,而<strong>计算复杂度却没有增加</strong></p></li><li><p>通过计算<strong>K(xi,xj)的值</strong>可以<strong>避免高维空间的内积运</strong>算，这种内积运算可通过<strong>定义在原空间中的核函数</strong>来实现, 甚至不必知道<strong>变换的形式</strong><br></p></li></ul><blockquote><p>SVM中不同的核函数将形成不同的算法，主要的核函数有三类：</p></blockquote><p><img alt="常用数据挖掘算法从入门到精通 第十一章 支持向量机算法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/37de000106419f7199b0></p><p class=pgc-img-caption>SVM常见的核函数</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'算法','数据','从入'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>