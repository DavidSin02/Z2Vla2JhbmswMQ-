<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>优化 | 浅谈交替方向乘子法(ADMM)的经典使用 | 极客快訊</title><meta property="og:title" content="优化 | 浅谈交替方向乘子法(ADMM)的经典使用 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/3b2f02e7ff904d8db1eba8ff8b85d881"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/12084d18.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/12084d18.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/12084d18.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/12084d18.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/12084d18.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/12084d18.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/12084d18.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/12084d18.html><meta property="article:published_time" content="2020-11-14T20:49:55+08:00"><meta property="article:modified_time" content="2020-11-14T20:49:55+08:00"><meta name=Keywords content><meta name=description content="优化 | 浅谈交替方向乘子法(ADMM)的经典使用"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/12084d18.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>优化 | 浅谈交替方向乘子法(ADMM)的经典使用</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><div><p>『运筹OR帷幄』原创</p><p class=ql-align-center><strong>作者：门泊东吴</strong></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3b2f02e7ff904d8db1eba8ff8b85d881><p class=pgc-img-caption></p></div><p>编者按</p><p>接之前公众号推送的文章《【优化】交替方向乘子法（ADMM）的基本原理》，本文从所求解的优化问题的形式出发，分成三大类归纳整理ADMM算法的经典使用。</p><p class=ql-align-justify><br></p><p class=ql-align-justify>之前公众号推送了文章《【优化】交替方向乘子法（ADMM）的基本原理》，主要参考的是Stephen Boyd等人写的经典小册子[1]，其中的第三章最为相关。我想接着前面讲的流程和原理，整理一下ADMM算法在求解各种优化问题上的经典使用，学艺不精，一知半(不)解，欢迎大家批评指正。</p><p><strong>1. N-block ADMM的直接推广不trivial</strong></p><p class=ql-align-justify>在讲应用之前，我想先聊聊关于N-block ADMM的直接推广。大家都知道，(naive) ADMM works for 2 blocks，很多人也都提到了，从2-block到N-block的直接推广并不trivial，那什么是N-block的直接推广呢？我们不妨来写一下，假设有这样一个线性约束的N-block凸优化问题：</p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b3fcd63132554b8eae0ef2de1c0ba09a><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d3a587d4eaaa40cca10033e974cc9df2><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/80223a38a0c14b0698959e53be5fb1e9><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p><strong>2. ADMM算法的经典使用</strong></p><p class=ql-align-justify>在实际应用中，有很多(大规模)凸优化问题可以写成上述N-block形式，而N-block ADMM的直接推广并不trivial，那为什么ADMM在求解(分布式)凸优化问题上还有那么广泛的应用呢？是如何被使用的呢？我觉得，关键在于问题的巧妙变形。下面，我想尝试从ADMM所求解的优化问题的形式出发，把它的经典使用分成三大类进行归纳整理，希望能帮助大家更好地理解这个算法，内容主要摘自[1]的4-8章还有平时读到的一些paper。</p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9c27984ca279411d9413712d92cafb04><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p>2.1</p><p>直接使用</p><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e90c4623821445e58eeb7fc2827243a8><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p>2.2</p><p>原问题变形后使用</p><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/27c8fbd259aa417da08471a86886b10c><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3a68ef1650194d64bfd6c4f04a569131><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/29bc020747f34f94aa08938e38d6c0bc><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p><strong>机器学习的标准范式：loss function + regularization([1]第6章)</strong></p><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9022b9081b014002bcef622fd5cb5a92><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p><strong>Distributed Model Fitting([1]第8章)</strong></p><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/fd7f7cf8ab744949aa17b6ff240be4e0><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/df9dc902f7624aadbc48dbd308ef1f3e><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p><strong>网络优化问题：点 + 边</strong></p><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/329b213cbd0d4e33ab02cb8e39085326><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/77d84f45e976429bb0f275f02912be22><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0a0990524e6d4cb4b67dc7ce739da0a7><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-center><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e522c10c2e4f4edaa7a2aacf4c11c8fa><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b92cf59001d4459ba2365b3d7bb5c620><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p><strong>Youth ends when egotism does; maturity begins when one</strong></p><p class=ql-align-justify>Basis Pursuit([1]6.2)</p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5a2541427f894a02bede53264f98a702><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p><strong>Linear/Quadratic Programming([1]5.2)</strong></p><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a39aeb7db1ba4857b0ed8f6cdd9c837c><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p><strong>Consensus Optimization([1]第7章)</strong></p><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/feb935d506684099a751a4f6914b2aff><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/328a69014c2c4887aca5c23d10c4cf31><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p><strong>其他：Graph Form, Conic Programming，Semidefinite Programming ...</strong></p><p class=ql-align-justify>还有一些进阶版的ADMM使用，实在写不动了，就简单提一下吧。基于ADMM的分布式优化算法解graph form of constrained convex optimization：</p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/65961a6511a642dd91a6dd5f5ba1b3a6><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>可以参考[8]。用ADMM求解conic optimization可以参考[9]，开源代码地址：SCS（https://github.com/cvxgrp/scs）。用ADMM求解semidefinite programming可以参考[10]。</p><p class=ql-align-justify>ADMM的直接使用和两类变形使用，都是按我个人的理解划分的，<strong>仅供参考</strong>，希望以上的归纳整理能帮助大家更好地理解ADMM的工作原理，以后再遇到新的应用，能一眼识破其中玄机。</p><p><strong>3. ADMM算法的实际表现和收敛速率</strong></p><p class=ql-align-justify>文章《【优化】交替方向乘子(ADMM)的基本原理》提到了”实际当中你如果写代码跑ADMM会发现它不像那些梯度下降，牛顿法，很容易快速收敛到一个较高的误差精度，ADMM实际的收敛速度往往比那些算法慢得多“，亲手写过代码的人应该都会有同感吧，还有步长的选取，也很tricky，取太小，对约束的满足力度不够，取太大，对目标方程的优化力度又不够；所以实际中，才会考虑取变速率的。插播两个最常用的步长规则(step size rules)：</p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ae11b18bdd1d4290a9a074eeba92fd17><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>其他步长规则，可以参考[11]的2.2节；这里也给读者留个问题，为什么这么强调 not summable？</p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/51010dcc62424d029a134c1da7496dfe><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>为什么ADMM和其他一阶算法在追求高精度解上的收敛表现不佳呢，这就和算法的收敛速率有关了(在物理里面，速率是标量，速度是矢量，所以说速率更好吧)，什么是收敛速率？</p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b34d3e5abd2c43339724a391f2b73f51><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/c389fd5bed0c4bf0829b06121df8e4cd><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><div class=pgc-img><img alt="优化 | 浅谈交替方向乘子法(ADMM)的经典使用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9f7914b1379f460e8043e10b5e2bc209><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>怎么理解这四张图所代表的三种收敛速率呢？一个形象的解释可以是：误差每减小一个数量级(小数点往前移一位，纵座标往下降一个单位长度)所需要的迭代步数(横座标往后移的长度)，和减少上一个数量级所需要的迭代步数相比，线性、超线性、次线性收敛速率分别对应的是一样、更少、更多，或者类比成匀速运动、加速运动、减速运动。从这个角度理解，通俗地讲，当前迭代所得到的解越接近真实解(从误差数量级衡量)，具有线性、超线性、次线性收敛速率的算法分别会趋近得一样快、越来越快、越来越慢。ADMM算法往往能很快收敛到一个低/中精度解，但想进一步提高精度，需要的时间就越来越长了，大家实际编程解大规模问题的时候，最好逐步提高精度。另一个很自然的想法是，在某些使用中，可以把ADMM和一些能从低精度解产生高精度解的算法结合起来使用，参考[12](其实我自己也没仔细看过)。</p><p><strong>4. 写在最后</strong></p><p class=ql-align-justify>关于ADMM的研究还在继续，异步、非凸、深度学习...我都没有讲（没啃完），经典应用也一定还有很多我漏掉或者不知道的，欢迎大家讨论补充。写这篇文章的一个初衷，也是为了回答自己内心一直以来的一个疑问，(分布式)优化的算法明明有很多，为什么ADMM能被用得这么广泛？整理完，也很感慨，能对一个算法发掘出这么多花式使用，功劳大概也不输算法的发明者了，嗯，伯乐吧~</p><p><strong>参考文献：</strong></p><p>[1] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein and others, Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers, Foundations and Trends® in Machine Learning, 3(1):1-122, 2011.</p><p>[2] C. Chen, B. He, Y. Ye and X. Yuan, The Direct Extension of ADMM for Multi-block Convex Minimization Problems is Not Necessarily Convergent, Mathematical Programming, 155(1-2):57-79, 2016.</p><p>[3] H. Wang, A. Banerjee and Z.-Q. Luo, Parallel Direction Method of Multipliers, Advances in Neural Information Processing Systems, pp. 181-189, 2014.</p><p>[4] W. Deng, M.-J. Lai, Z. Peng and W. Yin, Parallel Multi-block ADMM with o(1/k) Convergence, Journal of Scientific Computing, 71(2):712-736, 2017.</p><p>[5] X. Wang, M. Hong, S. Ma and Z.-Q. Luo, Solving Multiple-block Separable Convex Minimization Problems using Two-block Alternating Direction Method of Multipliers, arXiv preprint arXiv:1308.5294, 2013.</p><p>[6] N. Parikh, S. Boyd and others, Proximal Algorithms, Foundations and Trends® in Optimization, 1(3):127-239, 2014.</p><p>[7] D. Hallac, J. Leskovec and S. Boyd, Network Lasso: Clustering and Optimization in Large Graphs, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 387-396, 2015.</p><p>[8] N. Parikh, S. Boyd, Block Splitting for Distributed Optimization, Mathematical Programming Computation, 6(1):77-102, 2014.</p><p>[9] B. O’Donoghue, E. Chu, N. Parikh and S. Boyd, Conic Optimization via Operator Splitting and Homogeneous Self-dual Embedding, Journal of Optimization Theory and Applications, 169(3):1042-1068, 2016.</p><p>[10] Z. Wen, D. Goldfarb and W. Yin, Alternating Direction Augmented Lagrangian Methods for Semidefinite Programming, Mathematical Programming Computation, 2(3-4):203-230, 2010.</p><p>[11] S. Boyd and others, Subgradient Methods, Lecture Notes for EE364b（https://web.stanford.edu/class/ee364b/lectures/subgrad_method_notes.pdf）, Stanford University, Spring 2013-14.</p><p>[12] J. Eckstein and M. C. Ferris, Operator-splitting Methods for Monotone Affine Variational Inequalities, with A Parallel Application to Optimal Control, INFORMS Journal on Computing, 10(2):218-235, 1998.</p><p class=ql-align-justify><br></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'优化','浅谈','乘子法'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>