<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>马尔可夫属性，链，奖励过程和决策过程 | 极客快訊</title><meta property="og:title" content="马尔可夫属性，链，奖励过程和决策过程 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/1526973898625c2e391cdd4"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/29b52a1.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/29b52a1.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/29b52a1.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/29b52a1.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/29b52a1.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/29b52a1.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/29b52a1.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/29b52a1.html><meta property="article:published_time" content="2020-10-29T21:03:51+08:00"><meta property="article:modified_time" content="2020-10-29T21:03:51+08:00"><meta name=Keywords content><meta name=description content="马尔可夫属性，链，奖励过程和决策过程"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/29b52a1.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>马尔可夫属性，链，奖励过程和决策过程</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><div><p><img alt=马尔可夫属性，链，奖励过程和决策过程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1526973898625c2e391cdd4></p><h1>马尔可夫属性</h1><p>如果我们的状态表示和拥有完整的历史一样有效，那么我们说我们的模型满足了Markov属性的需求。</p><p>举个例子来说明这一点，想想玩井字游戏。当我们能够根据当前状态作出决定，而不是需要了解整个历史，那么我们就说我们满足了马尔可夫属性的条件。</p><p>或者更笼统地说:</p><blockquote><p>"未来与过去无关"</p></blockquote><p>我们说，我们可以从一个马尔可夫状态s出发</p><p>通过定义状态转换概率来定义继任状态，这是由</p><p><img alt=马尔可夫属性，链，奖励过程和决策过程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1526974123360ba2ace9db6></p><h1>马尔可夫过程或马尔可夫链</h1><p>马尔科夫过程是一个无记忆的随机过程，我们采用一系列满足马尔可夫属性要求的随机状态。或者定义：</p><p>马尔可夫过程是一个tuple &lt;S, P>，其中：</p><ul class=list-paddingleft-2><li><p>S是（有限的）一组状态</p></li><li><p>P是状态转移概率矩阵，Pss'= P [St + 1 = s'| St = s]</p></li></ul><p>我们的P矩阵写成：</p><p><img alt=马尔可夫属性，链，奖励过程和决策过程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15269743458760227db1647></p><p>矩阵的每一行总和为1。</p><p>我们用一个例子来说明这一点。假设我们想要表示天气状况。我们如何预测接下来几天的天气？</p><p>当我们有这个转换矩阵时：</p><p><img alt=马尔可夫属性，链，奖励过程和决策过程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15269743845972a189b0f48></p><p>然后我们可以看到，在当前晴天，我们将有90％的机会在阳光明媚的日子之后，而当我们有一个下雨天时，有50％的机会在下雨天。</p><p>将此图表示为图表会导致：</p><p><img alt=马尔可夫属性，链，奖励过程和决策过程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1526974430525127c81849a></p><h1>马尔科夫奖励流程（MRP）</h1><p>就像我们在强化学习中所做的那样，做出决定的事实。我们介绍一种叫做“reward”的东西。这将帮助我们根据当前的环境和我们将获得的回报来选择行动。</p><p>马尔科夫奖励过程是原始马尔可夫过程的延伸，但增加了奖励。写在一个定义：</p><p>马尔可夫奖励过程是一个元组&lt;S，P，R，γ>其中：</p><ul class=list-paddingleft-2><li><p>S是（有限的）一组状态</p></li><li><p>P是状态转移概率矩阵，Pss'= P [St + 1 = s'| St = s]</p></li><li><p>R是奖励函数，Rs = E [rt + 1 | St = s]</p></li><li><p>γ是折扣因子，γ∈[0,1]</p></li></ul><p>这意味着我们将增加去某些状态的奖励。当我们将这个映射到我们的雏形示例上时：</p><p><img alt=马尔可夫属性，链，奖励过程和决策过程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15269746421928b1e416807></p><p>通过增加这个奖励，我们可以找到一个最优的路径，在我们处于决定的时候。让我们想象我们可以在这里扮演上帝，你会走哪条路?我们想试着走那条一直都是“阳光”的道路，但是为什么呢?因为这意味着我们会得到尽可能高的回报。</p><p><strong>返回</strong></p><p>但我们如何计算我们将获得的完整回报？那么这是由下面的公式表示：</p><p><img alt=马尔可夫属性，链，奖励过程和决策过程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/152697478526866e842592a></p><p>然而，这会导致一些问题：</p><ul class=list-paddingleft-2><li><p>我们倾向于停止探索（我们每次选择奖励最高的选项）</p></li><li><p>循环马尔可夫过程中的无限回报的可能性</p></li></ul><p>这就是为什么我们添加了一个称为折扣因子的新因素。这个因素会减少我们随着时间的推移采取同样行动的奖励。将这添加到我们的原始公式中会导致：</p><p><img alt=马尔可夫属性，链，奖励过程和决策过程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/152697479341685137e71ab></p><h1>马尔可夫决策过程（MDP）</h1><p>马尔科夫决策过程是一个包含决策的马尔可夫奖励过程。这是一个所有状态都是马尔科夫的环境。</p><p>我们现在可以最终确定我们的定义：</p><p>马尔可夫决策过程是一个元组&lt;S，A，P，R，γ>其中：</p><ul class=list-paddingleft-2><li><p>S是（有限的）一组状态</p></li><li><p>A是一组有限的行为</p></li><li><p>P是状态转移概率矩阵，Pass'= P [St + 1 = s'| St = s，At = a]</p></li><li><p>R是奖励函数，R = E [rt + 1 | St = s，At = a]</p></li><li><p>γ是折扣因子，γ∈[0,1]</p></li></ul><p><img alt=马尔可夫属性，链，奖励过程和决策过程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15269748631454199520031></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'过程','属性','奖励'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>