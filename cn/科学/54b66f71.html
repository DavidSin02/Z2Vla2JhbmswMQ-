<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>为什么说Transformer就是图神经网络？ | 极客快訊</title><meta property="og:title" content="为什么说Transformer就是图神经网络？ - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RqSgmP9HojtMNK"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/54b66f71.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/54b66f71.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/54b66f71.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/54b66f71.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/54b66f71.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/54b66f71.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/54b66f71.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/54b66f71.html><meta property="article:published_time" content="2020-11-14T20:53:56+08:00"><meta property="article:modified_time" content="2020-11-14T20:53:56+08:00"><meta name=Keywords content><meta name=description content="为什么说Transformer就是图神经网络？"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/54b66f71.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>为什么说Transformer就是图神经网络？</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><p><br></p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RqSgmP9HojtMNK></div><p><br></p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Rbc1Htm93uvAQn></div><p>作者 | Chaitanya Joshi</p><p>译者 | Kolen</p><p>出品 | AI科技大本营（ID:rgznai100）</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqLxPCXik3SR></div><h1>前言</h1><p>有些工程师朋友经常问我这样一个问题：“图深度学习听起来很棒，但是现在是否有非常成功的商业案例？是否已经在实际应用中部署？”</p><p>除了那些显而易见的案例，比如Pinterest、阿里巴巴和Twitter公司部署的推荐系统，一个稍有细微差别的成功案例就是Transformer架构的实现，它在NLP行业引起了轩然大波。</p><p>通过这篇文章，我想建立起图神经网络（GNNs）和Transformers之间的联系。具体来说，我将首先介绍NLP和GNN领域中模型架构的基本原理，然后使用公式和图表来阐述两者之间的联系，最后将讨论如何让两者协同运作来推动这方面的研究进展。</p><p>我们先来谈谈模型架构的目的——表示学习。</p><h1><br></h1><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqLy93x95UEx></div><h1><br></h1><h1>NLP的表示学习</h1><p>从一个较高的层次来分析，几乎所有的神经网络结构都将输入数据表示为向量（vectors）或者嵌入（embeddings）的形式，从而对数据中有用的统计和语义信息进行编码。这些潜在或隐藏的表示方法可以用于执行一些有用的任务，例如对图像进行分类或翻译句子。其中，神经网络通过接收反馈（通常是通过误差（error）/损失（loss）函数）来学习如何构建越来越好的表示方法。</p><p>在自然语言处理（NLP）中，按照传统方式，人们习惯将递归神经网络（RNNs）以照序列的方式（即一个时间步对应一个单词）来构建句子中每个单词的表示。直观地说，我们可以把RNN层想象成一个传送带，上面的字从左到右进行自回归处理。最后，我们得到句子中每个单词的一个隐藏特征，并将其传递到下一个RNN层或者用于我们选择的NLP任务。</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RsGVfWTIiUXS3J></div><p>Transformers最初是用于机器翻译领域，但是现在已经逐渐取代了主流NLP中的RNNs。该架构采用了一种全新的表示学习方法：完全抛弃了递归的方法，Transformers使用注意力机制构建每个词的特征，从而找出句子中所有其他单词对上述单词的重要性。理解了这一关键点我们就能明白，单词的更新特征仅仅是所有单词特征的线性变换之和，这些特征是根据它们的重要性进行加权。</p><p>早在2017年，这个想法听起来就非常激进，因为NLP界已经习惯了使用RNN处理文本的序列（每次一个单词）的风格。这篇论文的标题可能是火上浇油！</p><p>Yannic Kilcher为此做了一个出色的视频概述。</p><h1><br></h1><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RqMqMYY7LcGio6></div><h1><br></h1><h1>解析Transformer</h1><p>让我们通过将上一节内容转述成数学符号和向量的语言来加深对这个架构的认识。如下所示，我们将句子</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVfXDDdASpaN></div><p>中第i个词的隐藏特征h从</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RsGVfXV3RJuEHQ></div><p>层更新到</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVfXr6zGlBxj></div><p>层：</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVfsA2NBr9CX></div><p>其中，</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RsGVfsY2DyONdM></div><p>表示句子中的词汇集，而</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVfspAABxz4t></div><p>是可以学习到的线性权重（分别表示注意力计算中的Query, Key 和 Value）。句子中的每个单词并行执行注意力机制，从而可以一次性获得它们已更新的特征——这是Transformer相对RNNs的另一个加分点，它使得模型能够逐字更新特征。</p><p>我们可以通过下面这张流程图来更好地理解注意力机制：</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVft53gMLQqU></div><p>输入词汇特征和句子中其他词汇集</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVftV1kwIFOg></div><p>，我们使用点积运算来计算出每对</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgHk6DdUZwu></div><p>的注意力权重，接着对所有的进行softmax运算。最后，把所有的</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgI9GOuKUOH></div><p>相对应的权重累加得到单词i更新后的词汇特征</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgJ7FOogK5y></div><p>。句子中的每个单词都会并行地经历相同的流程来更新其特征。</p><h1><br></h1><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqMZM85xvsbJ></div><h1><br></h1><h1>多头注意力机制</h1><p>事实证明，要让这种点积注意力机制起作用是很难的——如果随机初始化处理得不好会使得整个学习过程失去稳定性。我们可以通过并行执行多个注意力“头”并将结果连接起来（现在每个注意力头都有单独的可学习权重）来克服这个问题：</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgJP2YiZVMj></div><p>其中，</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RsGVgJfMV2UCu></div><p>是第k个注意力头的可学习的权重，而</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgZz68oKw2z></div><p>是一个向下的投影，用以匹配跨层的</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RsGVgaV2E8bG0H></div><p>和的尺寸。</p><p>通过观察上一层中隐藏特征的不同的变换过程以及方面，多头机制允许注意力机制从本质上“规避风险”。关于这点，我们将在后面详细讨论。</p><h1><br></h1><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqNMhMM9NBY></div><h1><br></h1><h1>尺度问题和前向传播子层</h1><p>促使形成最终形态的Transformer结构的关键问题是，注意机制之后的词的特征可能在不同的尺度或重要性上：（1）这可能是由于某些词在将其他词的特征累加时具有非常集中或非常分散的注意力权重。（2）在单个特征/向量输入级别，跨多个注意力头（每个可能会以不同的比例输出值）进行级联可以导致最终向量的输入具有一个大范围的值。遵循传统的机器学习思路，在上述流程中增加一个归一化层似乎是一个合理的选择。</p><p>Transformers使用LayerNorm克服了问题（2），LayerNorm在特征层级上进行归一化并学习一种仿射变换。此外，通过求特征维度的平方根来缩放点积注意力有助于抵消问题（1）。</p><p>最后，作者提出了控制尺度问题的另一个“技巧”：具有特殊结构的考虑位置的双层MLP。在多头注意力之后，他们通过一个可学习的权重将投影到一个更高的维度，在该维度中，经过ReLU非线性变换，然后投影回其原始维度，然后再进行另一个归一化操作：</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgaq8ijcyFT></div><p>说实话，我不确定超参数化前馈子层背后的确切理由是什么，似乎也没有人对此提出疑问！我认为LayerNorm和缩放的点积不能完全解决突出的问题，因此大型MLP是一种可以相互独立地重新缩放特征向量的手段。</p><p>Transformer层的最终形态如下所示：</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RsGVgbS3tIYwc1></div><p>Transformer架构也非常适合非常深的网络，使NLP界能够在模型参数和扩展数据这两方面进行延伸。每个多头注意力子层和前馈子层的输入和输出之间的残差连接是堆叠Transformer层的关键（但为了清楚起见，在上图中省略了）。</p><h1><br></h1><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RqMqdp52DyYCQP></div><h1><br></h1><h1>GNNs构建图的表示</h1><p>我们暂时不讨论NLP。</p><p>图神经网络（GNNs）或图卷积网络（GCNs）在图数据中建立节点和边的表示。它们是通过邻域聚合（或消息传递）来实现的，在邻域聚合中，每个节点从其邻域收集特征，以更新其周围的局部图结构表示。通过堆叠多个GNN层使得该模型可以将每个节点的特征传播到整个图中，从其邻居传播到邻居的邻居，依此类推。</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgbt6AwFbau></div><p>以这个表情符号社交网络为例：由GNN产生的节点特征可用于预测性任务，例如识别最有影响力的成员或提出潜在的联系。</p><p>在他们最基本的形式中，GNNs通过以下方法来更新节点i在层的隐藏层特征h（例如，），也就是先将节点自身的特征和每个邻居节点</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RsGVgvg3p17fka></div><p>特征</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgvyBsOpGoL></div><p>的聚合相累加，然后再整体做一个非线性变换，如下：</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RsGVgwG4iUkUa8></div><p>其中</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RsGVgwXDgmkIUb></div><p>是GNN层的可学习的权重矩阵，而</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgyP28ki7SS></div><p>是一个非线性变换，例如ReLU。在上述例子中，N () ={ , , , }。</p><p>邻域节点</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVhGQEtS2ERx></div><p>上的求和可以被其他输入大小不变的聚合函数代替，例如简单的 均值/最大值函数或其他更强大的函数（如通过注意机制的加权和）。</p><p>这听起来熟悉吗？</p><p>也许这样一条流程可以帮助建立连接：</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RsGVhH69HVeXmm></div><p>如果我们要执行多个并行的邻域聚合头，并且用注意力机制（即加权和）替换领域 上的求和 ，我们将获得图注意力网络（GAT）。加上归一化和前馈MLP，瞧，我们就有了Graph Transformer！</p><h1><br></h1><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rqqo5387tkSUcT></div><h1><br></h1><h1>句子就是由词全连接而成的图</h1><p>为了使连接更加清晰，可以将一个句子看作一个完全连接的图，其中每个单词都连接到其他每个单词。现在，我们可以使用GNN来为图（句子）中的每个节点（单词）构建特征，然后我们可以使用它来执行NLP任务。</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVhHPHtrJruh></div><p>广义上来讲，这就是Transformers正在做的事情：Transformers是以多头注意力作为邻聚合函数的GNNs。标准GNNs从其局部邻域节点聚合特征，而NLP的Transformers将整个句子视为局部邻域，在每个层聚合来自每个单词</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVhHuFsFlV2v></div><p>的特征。</p><p>重要的是，各种特定于问题的技巧（如位置编码、因果/掩码聚合、学习率表和大量的预训练）对于Transformers的成功至关重要，但在GNN界中却很少出现。同时，从GNN的角度看Transformers可以启发我们摆脱模型结构中的许多花哨的玩意。</p><h1><br></h1><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Rqqo547DNIp69u></div><h1><br></h1><h1>可以从Transformers和GNN中学到什么？</h1><p>现在我们已经在Transformers和GNN之间建立了联系，接着让我们来探讨一些新的问题...</p><p><strong>8.1 全连接图是NLP的最佳输入格式吗？</strong></p><p>在统计NLP和ML之前，Noam Chomsky等语言学家致力于发展语言结构的最新理论，如语法树/图。Tree LSTMs已经尝试过这一点，但是也许Transformers/GNNs是可以让语言理论和统计NLP的领域结合得更加紧密的更好的架构？</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVhfx5XnGv4Q></div><p><strong>8.2 如何学习到长期依赖？</strong></p><p>完全连通图使得学习词与词之间的非常长期的依赖关系变得非常困难，这是完全连通图的另一个问题。这仅仅是因为图中的边数与节点数成二次平方关系，即在n个单词的句子中，Transformer/GNN将在n^2对单词上进行计算。如果n很大，那将会是一个非常棘手的问题。</p><p>NLP界对长序列和依赖性问题的看法很有意思：例如，使注意力机制在输入大小方面稀疏或自适应，在每一层中添加递归或压缩，以及使用对局部性敏感的哈希法进行有效的注意，这些都是优化Transformers有希望的新想法。</p><p>有趣的是，还可以看到一些GNN界的想法被混入其中，例如，用于句子图稀疏化的二进制分区似乎是另一种令人兴奋的方法。</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVhgq51JhEku></div><p><strong>8.3 Transformers在学习神经网络的句法吗？</strong></p><p>NLP界有几篇关于Transformers可能学到什么的有趣论文。其基本前提是，对句子中的所有词对使用注意力机制（目的是确定哪些词对最有趣），可以让Transformers学习特定任务句法之类的东西。</p><p>多头注意力中的不同头也可能“关注”不同的句法属性。</p><p>从图的角度来看，通过在完全图上使用GNN，我们能否从GNN在每一层执行邻域聚合的方法中恢复最重要的边线及其可能带来的影响？我还不太相信这种观点。</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVhi3ANTNTli></div><p><strong>8.4 为什么要用多头注意力？为什么要用注意力机制？</strong></p><p>我更赞同多头机制的优化观点——拥有多个注意力可以改进学习，克服不好的随机初始化。例如，这些论文表明，Transformers头可以在训练后“修剪”或“删除”，并且不会产生重大的性能影响。</p><p>多头邻聚合机制在GNNs中也被证明是有效的，例如在GAT使用相同的多头注意力，MoNet使用多个高斯核来聚合特征。虽然多头技巧是为了稳定注意力机制而发明的，但它能否成为提炼出额外模型性能的标准？</p><p>相反，具有简单聚合函数（如sum或max）的GNNs不需要多个聚合头来维持稳定的训练。如果我们不需要计算句子中每个词对之间的成对兼容性，对Transformers来说不是很好吗？</p><p>Transformers能从抛弃注意力中获益吗？Yann Dauphin和合作者最近的工作提出了另一种ConvNet架构。Transformers也可能最终会做一些类似于ConvNets的事情。</p><div classname=pgc-img><img alt=为什么说Transformer就是图神经网络？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVhic3DR3DK></div><p><strong>8.5 为什么Transformers这么难训练？</strong></p><p>阅读新的Transformer论文让我觉得，在确定最佳学习率表、预热策略和衰减设置时，训练这些模型需要一些类似于黑魔法的东西。这可能仅仅是因为模型太大，而且所研究的NLP任务非常具有挑战性。</p><p>但是最近的结果表明，这也可能是由于结构中归一化和残差连接的特定组合导致的。</p><p>在这一点上我很在意，但是也让我感到怀疑：我们真的需要代价昂贵的成对的多头注意力结构，超参数化的MLP子层以及复杂的学习计划吗？</p><p>我们真的需要具有大量碳足迹的（译者注：有人提出现在训练一个模型相当于5辆汽车一天的排碳量）大规模模型吗？</p><p>具有良好归纳偏差的架构难道不容易训练吗？</p><p>原文链接：</p><p>https://dwz.cn/eE9kZK6q</p><p>（本文由AI科技大本营编译，转载请联系微信1092722531）</p><p>【end】</p><p>在这次疫情防控中，无感人体测温系统发挥了怎样的作用？它的技术原理是什么？无感人体测温系统的应用场景中有哪些关键技术与落地困难？高精准的无感人体测温系统的核心技术武器是什么？对于开发者们来说，大家应该了解哪些技术？</p><ul><li>机器会成为神吗？</li><li>6个步骤，告诉你如何用树莓派和机器学习DIY一个车牌识别器！（附详细分析）</li><li>微信回应钉钉健康码无法访问；谷歌取消年度I/O开发者大会；微软公布Visual Studio最新路线图</li><li>什么是CD管道？一文告诉你如何借助Kubernetes、Ansible和Jenkins创建CD管道！</li><li>智能合约初探：概念与演变</li><li>血亏1.5亿元！微盟耗时145个小时弥补删库</li></ul><p><br></p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'什么','Transformer','图神经'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>