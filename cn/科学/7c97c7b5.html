<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” | 极客快訊</title><meta property="og:title" content="「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/1530358602315057a254449"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/7c97c7b5.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/7c97c7b5.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/7c97c7b5.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/7c97c7b5.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/7c97c7b5.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/7c97c7b5.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/7c97c7b5.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/7c97c7b5.html><meta property="article:published_time" content="2020-11-14T20:56:57+08:00"><meta property="article:modified_time" content="2020-11-14T20:56:57+08:00"><meta name=Keywords content><meta name=description content="「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑”"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/7c97c7b5.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑”</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><div><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358602315057a254449><p class=pgc-img-caption></p></div><p>周末好啊各位同学，我们又见面了。</p><p>科幻名著《三体》里有句犀利的台词——降低维度用于攻击。不过，这个“降维”绝对不只是科幻界的专用名词。</p><p>在机器学习中，你同样得了解它。</p><p>很多初学者往往会把降维（Dimensionality reduction），特征选择（feature selection），以及特征提取（feature extraction）混为一谈，因为这三者都削减了进入模型的变量个数。</p><p>但降维是一个更为宽泛的概念，它包括了特征选择和特征提取。</p><p>虽然降维过后，最终使用的变量个数减少了，但特征选择挑选的是特征子集，也就是说，保留下来的所有特征都在原来的特征集中可以找到；而特征提取所提取的是不再是特征子集，而是原来特征的线性（或者非线性）组合，我们经过特征提取后的变量都是新的变量，它的本质是将原始高维空间向低维空间投影，我们所使用的特征不仅少了，而且不再是原来的特征。</p><p>距离是机器学习中的一个很重要的概念。每个样本可以表示为一个向量，也就是高维空间的一个点，距离可以用来衡量样本之间的相似度。但是在高维空间，距离的计算会变得非常困难，而我们关心的问题可能在低维空间就会得到很好的解决。但这不意味着低维空间只是对高维空间的近似，有些问题中，高维空间会增加很多噪声，而在低维空间中会得到比高维空间更好的性能。</p><p>在上周《如何进行特征选择（理论篇）》的学习中，相信大家已经对特征选择有了足够的认识，所以本文的“降维”特指特征提取。</p><p>对于降维有两种分类方式：其一，根据目标值（target）的参与与否，分为有监督降维和无监督降维；其二，根据高维空间与低维空间的关系，分为线性降维和非线性降维。</p><p>我们对每种方法分举一例：</p><p>线性\监督</p><p>无监督</p><p>监督</p><p>线性</p><p>PCA</p><p>LDA</p><p>非线性</p><p>ISOMAP</p><p>KLDA</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153035860228957e666f787><p class=pgc-img-caption></p></div><p><strong>主成分分析（PCA）</strong></p><blockquote><p>数学准备：</p></blockquote><p><strong>1.协方差矩阵</strong>：随机变量组成的向量，每组随机变量的协方差构成的一个对称矩阵，其对角元是每组随机变量的方差</p><p><strong>2.矩阵的对角化</strong>：对于矩阵M，有可逆矩阵V，使得</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15303586021168851740122><p class=pgc-img-caption></p></div><p>成为对角矩阵，而M的特征值对应的特征向量组成了该可逆矩阵V。（换而言之，矩阵V的每一列对应着M的特征向量）</p><p><strong>3.正交矩阵</strong>：转置矩阵等于其逆矩阵（</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15303586022753d4521984a><p class=pgc-img-caption></p></div><p>），构成矩阵的列向量彼此正交。</p><p><strong>4.数据中心化</strong>：对每组随机变量减去均值，再除以标准差。本质是将每组随机变量变为标准的高斯分布。</p><p>PCA（Principal component analysis）是用投影的方法将高维空间压缩到低维。</p><p>想象一下，此时你站在路灯下面，你本身是三维的（此时此刻除去了时间维度），你的影子却在一个二维平面上。</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358602382319e5a16b0><p class=pgc-img-caption></p></div><p>如图，我们将二维空间的点投影到一条直线上。</p><p>但是，我们有无数个投影的方向，就像上图我们可以找出无数条直线来进行投影，那么哪条直线，哪个方向才是最好的呢？PCA的目标就是，找一条直线，使得投影之后的点尽可能的远离彼此，因为点之间的互相远离而不是相互重叠，就意味着某些距离信息被保留了下来。</p><p>在高维空间（维数D）的所有的样本可以被表示为一个向量:</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/153035860230542764127df><p class=pgc-img-caption></p></div><p>在投影之后的低维空间（维数d），样本也是一个向量：</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15303586023994b26dab1bb><p class=pgc-img-caption></p></div><p>向量的变化可以通过一个矩阵联系起来，这个矩阵我们把它叫做投影矩阵，它的作用是将一个高维向量投影到低维空间得出一个低维向量：</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15303586025856cd0dfce87><p class=pgc-img-caption></p></div><p>此时，中心化数据的优势就体现了出来，因为经过中心化的数据，</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1530358602563be4d704fbf><p class=pgc-img-caption></p></div><p>，这就意味着数据的协方差矩阵就成了</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358602622552ab123d3><p class=pgc-img-caption></p></div><p>，投影之后的协方差矩阵就成为了</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358602576773ead440f><p class=pgc-img-caption></p></div><p>,我们的目标是使其方差最大，而协方差矩阵的对角元正是方差，所以我们只需要对其求迹：</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153035860265690fee200c7><p class=pgc-img-caption></p></div><p>换而言之，我们需要找的投影矩阵W其实是一个使</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15303586026942327f95428><p class=pgc-img-caption></p></div><p>对角化的可逆矩阵，而它的转置等于它的逆</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358602809907d4e3647><p class=pgc-img-caption></p></div><p>。所以我们寻找W的过程，就是寻找</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15303586026942327f95428><p class=pgc-img-caption></p></div><p>的特征向量的过程，而方差最大化的过程，也就是寻找</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15303586026942327f95428><p class=pgc-img-caption></p></div><p>最大特征值的过程。</p><p>所以，我们只需要对</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15303586026942327f95428><p class=pgc-img-caption></p></div><p>做特征值分解，将其特征值排序，取到前面的d个特征向量，彼此正交，构成了投影矩阵W，而它们所张成的低维空间，就是使得投影点方差最大的低维空间。</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603032612f2d08f7><p class=pgc-img-caption></p></div><p>如图，这是对一个二元高斯分布用PCA进行降维后的结果，这个平面就是由两个最大的特征值对应的特征向量所张成，可以看出，特征向量彼此正交，且首先找到的是最大的特征值对应的特征向量，逐步寻找第二个，第三个.....如果我们的目标空间是n维，就会取到前n个。</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153035860228957e666f787><p class=pgc-img-caption></p></div><p><strong>线性判别分析（LDA）</strong></p><blockquote><p>数学准备：</p></blockquote><p><strong>1.均值向量</strong>：由多组随机变量组成的向量，对每一组随机变量取均值所构成的向量。</p><p><strong>2.厄米矩阵（Hermitan ）</strong>：转置等于其本身的矩阵，</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358602884fe82c85c5f><p class=pgc-img-caption></p></div><p>。</p><p><strong>3.广义瑞利熵（Rayleigh quotient </strong>）：若x为非零向量，则</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153035860292016695c7c12><p class=pgc-img-caption></p></div><p>为A,B的广义瑞利熵，它的最大值是</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153035860300923d9ccbe5f><p class=pgc-img-caption></p></div><p>的最大特征值。</p><p><strong>4.矩阵的奇异值分解</strong>：任何实矩阵M都可以被分解成为</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15303586029999a32fff26c><p class=pgc-img-caption></p></div><p>这三个矩阵的乘积。U和V均为正交矩阵。U的列向量是</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603080ea6261244d><p class=pgc-img-caption></p></div><p>的特征向量，V的列向量是</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603129f00892e149><p class=pgc-img-caption></p></div><p>的特征向量，同时奇异值的大小</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603080ea6261244d><p class=pgc-img-caption></p></div><p>是的特征值的平方根。</p><p>LDA（Linear Discriminant Analysis）的基本思想也是将高维空间的样本投影到低维空间，使信息损失最少。</p><p>与PCA不同在于，PCA只针对样本矩阵，希望投影到低维空间之后，样本投影点的方差最大；但LDA不仅针对样本矩阵，还使用了类别信息，它希望投影到低维空间后，相同样本的方差最小（相同样本的集中化），不同样本的距离最大（不同样本离散化）。</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358603310d2274c9a36><p class=pgc-img-caption></p></div><p>如图所示，将二维空间投影到一维空间，即一条直线上。图2相比图1，类间样本距离更大，类内样本方差更小。</p><p>以二分类问题为例，我们用</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153035860327650bee5d102><p class=pgc-img-caption></p></div><p>表示两类样本，用</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15303586033066c48ba37cf><p class=pgc-img-caption></p></div><p>表示两类样本的均值向量，用</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/153035860326875829f0a88><p class=pgc-img-caption></p></div><p>来表示两类样本的协方差矩阵，与PCA一样，我们假设存在一个投影矩阵W，这些量会在低维空间变成：</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358603393b092b23d46><p class=pgc-img-caption></p></div><p>其中</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/15303586034164eecc3de93><p class=pgc-img-caption></p></div><p>分别为低维空间的样本，均值向量和协方差矩阵。在投影空间的相同样本的方差最小，意味着</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358603503135dc71d9e><p class=pgc-img-caption></p></div><p>最小；而不同样本的距离最大，意味着</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603565529a6dc311><p class=pgc-img-caption></p></div><p>最大。</p><p>我们定义原始空间的样本协方差矩阵之和为</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603598396fd70244><p class=pgc-img-caption></p></div><p>,类内散度矩阵（whithin-class scatter matrix），用来刻画原始空间上相同样本的方差：</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15303586035950b8f52f77d><p class=pgc-img-caption></p></div><p>同时定义类间散度矩阵（between-class scatter matrix）</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/15303586036441bedb76911><p class=pgc-img-caption></p></div><p>,用来刻画原始空间上不同样本的距离:</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603713d175837f10><p class=pgc-img-caption></p></div><p>将以上的原则结合起来，我们的目的就变成了：</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603772fffae92969><p class=pgc-img-caption></p></div><p>根据广义瑞利熵的形式，我们寻求最大值就变成了对</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603826720199618e><p class=pgc-img-caption></p></div><p>进行奇异值分解，然后选取最大的奇异值和相应的特征向量。这些特征向量所张成的低维空间，就是我们的目标空间。</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358603866bbeb9d71cc><p class=pgc-img-caption></p></div><p><strong>读芯君开扒</strong></p><p><strong>课堂TIPS</strong></p><p>• 降维在表示论中属于低维表示，本质是将原本空间压缩到更小的空间，在这个过程中保证信息损失的最小化。与之相对的是稀疏表示，它是将原本的空间嵌入到更大的空间，在这过程中保证信息损失的最小化。</p><p>• PCA有多种理解方式，除了在低维空间使得样本方差最大化，也可以理解为最小重构均方误差，将问题转化为所选低维空间重构的数据与实际数据的差。引入贝叶斯视角，还可以将PCA理解为最小化高斯先验误差。如果从流形的角度看，就是把数据看作一个拓扑空间的点集，在高斯概率空间内找到一个对应的线性流形。</p><p>• PCA和LDA的优化目标均可以用拉格朗日乘子法解决。PCA同样也可以通过奇异值分解来解决。奇异值分解方法可以理解为是特征值分解的推广，因为特征值分解要求矩阵为一个方阵，但奇异值分解并无此要求。</p><div class=pgc-img><img alt=「周末AI课堂」线性降维方法（理论）｜机器学习你会遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358603866bbeb9d71cc><p class=pgc-img-caption></p></div><p>作者：唐僧不用海飞丝</p><p>如需转载，请后台留言，遵守转载规范</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'周末','AI','课堂'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>