<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>人工智能中的19 种损失函数，你能认识几个？ | 极客快訊</title><meta property="og:title" content="人工智能中的19 种损失函数，你能认识几个？ - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p9.pstatp.com/large/dfic-imagehandler/6a791d8e-481b-4784-993c-5d039225fede"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/91d9c1e.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/91d9c1e.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/91d9c1e.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/91d9c1e.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/91d9c1e.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/91d9c1e.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/91d9c1e.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/91d9c1e.html><meta property="article:published_time" content="2020-10-29T21:03:51+08:00"><meta property="article:modified_time" content="2020-10-29T21:03:51+08:00"><meta name=Keywords content><meta name=description content="人工智能中的19 种损失函数，你能认识几个？"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/91d9c1e.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>人工智能中的19 种损失函数，你能认识几个？</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><div><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/dfic-imagehandler/6a791d8e-481b-4784-993c-5d039225fede><p class=pgc-img-caption></p></div><blockquote><p>作者：mingo_敏链接：https://blog.csdn.net/shanglianlm/article/details/85019768</p></blockquote><p class=ql-align-justify><br></p><p class=ql-align-justify>tensorflow和pytorch很多都是相似的，这里以pytorch为例。</p><hr><p class=ql-align-justify><br></p><h1><strong>19种损失函数</strong></h1><p class=ql-align-justify><strong>1. L1范数损失 L1Loss</strong></p><p>计算 output 和 target 之差的绝对值。</p><pre>torch.nn.L1Loss(reduction='mean')</pre><p>参数：</p><blockquote><p>reduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean。</p></blockquote><p class=ql-align-justify><br></p><hr><p class=ql-align-justify><br></p><h1><strong>2 均方误差损失 MSELoss</strong></h1><p>计算 output 和 target 之差的均方差。</p><pre>torch.nn.MSELoss(reduction='mean')</pre><p>参数：</p><blockquote><p>reduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean。</p></blockquote><p class=ql-align-justify><br></p><hr><p class=ql-align-justify><br></p><h1><strong>3 交叉熵损失 CrossEntropyLoss</strong></h1><p>当训练有 C 个类别的分类问题时很有效. 可选参数 weight 必须是一个1维 Tensor, 权重将被分配给各个类别. 对于不平衡的训练集非常有效。</p><p>在多分类任务中，经常采用 softmax 激活函数+交叉熵损失函数，因为交叉熵描述了两个概率分布的差异，然而神经网络输出的是向量，并不是概率分布的形式。所以需要 softmax激活函数将一个向量进行“归一化”成概率分布的形式，再采用交叉熵损失函数计算 loss。</p><p class=ql-align-justify><br></p><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/248441f678b64267852966afd7312563><p class=pgc-img-caption></p></div><pre>torch.nn.CrossEntropyLoss(weight=None,ignore_index=-100, reduction='mean')</pre><p>参数：</p><blockquote><p>weight (Tensor, optional) – 自定义的每个类别的权重. 必须是一个长度为 C 的 Tensorignore_index (int, optional) – 设置一个目标值, 该目标值会被忽略, 从而不会影响到 输入的梯度。reduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean。</p></blockquote><p class=ql-align-justify><br></p><hr><p class=ql-align-justify><br></p><h1><strong>4 KL 散度损失 KLDivLoss</strong></h1><p>计算 input 和 target 之间的KL散度。KL散度可用于衡量不同的连续分布之间的距离, 在连续的输出分布的空间上(离散采样)上进行直接回归时很有效.</p><pre>torch.nn.KLDivLoss(reduction='mean')</pre><p>参数：</p><blockquote><p>reduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean。</p></blockquote><p class=ql-align-justify><br></p><hr><h1><strong>5 二进制交叉熵损失 BCELoss</strong></h1><p>二分类任务时的交叉熵计算函数。用于测量重构的误差, 例如自动编码机. 注意目标的值 t[i] 的范围为0到1之间.</p><pre>torch.nn.BCELoss(weight=None, reduction='mean')</pre><p>参数：</p><blockquote><p>weight (Tensor, optional) – 自定义的每个 batch 元素的 loss 的权重. 必须是一个长度为 “nbatch” 的 的 Tensor</p></blockquote><p class=ql-align-justify><br></p><hr><p class=ql-align-justify><br></p><h1><strong>6 BCEWithLogitsLoss</strong></h1><p>BCEWithLogitsLoss损失函数把 Sigmoid 层集成到了 BCELoss 类中. 该版比用一个简单的 Sigmoid 层和 BCELoss 在数值上更稳定, 因为把这两个操作合并为一个层之后, 可以利用 log-sum-exp 的 技巧来实现数值稳定.</p><pre>torch.nn.BCEWithLogitsLoss(weight=None, reduction='mean', pos_weight=None)</pre><p>参数：</p><blockquote><p>weight (Tensor, optional) – 自定义的每个 batch 元素的 loss 的权重. 必须是一个长度 为 “nbatch” 的 Tensor</p></blockquote><p class=ql-align-justify><br></p><hr><p class=ql-align-justify><br></p><h1 class=ql-align-justify><strong>7 MarginRankingLoss</strong></h1><pre>torch.nn.MarginRankingLoss(margin=0.0, reduction='mean')</pre><p>criterion：计算输入x1，x2（2个1D张量）与y（1或-1）的损失</p><p>计算两个向量之间的相似度，当两个向量之间的距离大于 margin,则 loss 为正，小于margin，loss 为 0。这里y是label，当label为1时，表示x1大于x2（x1排在x2前面）。</p><p>那么这个损失表达的函数就是x1至少比x2大margin，那么loss才为0(假设真实标签是x1大于x2)。</p><p>对于 mini-batch(小批量) 中每个实例的损失函数如下:</p><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/4a02d78e699e42cdafd3876e9e07337f><p class=pgc-img-caption>图片描述(最多50字)</p></div><p>参数：</p><blockquote><p>margin:默认值0</p></blockquote><p class=ql-align-justify><br></p><hr><p class=ql-align-justify><br></p><h1 class=ql-align-justify><strong>8 HingeEmbeddingLoss</strong></h1><pre>torch.nn.HingeEmbeddingLoss(margin=1.0, reduction='mean')</pre><p>criterion：给定输入张量x和标签张量y（1或-1）的损失</p><p>通常用于测量两个输入是相似还是不相似，例如， 使用L1成对距离作为x，并且通常用于学习非线性嵌入或半监督学习。</p><p>e.g:x是成对距离，y为1表示，这两个对相似，否则不相似。所以当y为1时，期望x越小越好，y为-1时，x越大越好。也就是下面l_n越小越好。</p><p>对于 mini-batch(小批量) 中每个实例的损失函数如下:</p><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/78a9e7def17f4c2d82f51b9e51709fce><p class=pgc-img-caption></p></div><p>参数：</p><blockquote><p>margin:默认值1</p></blockquote><p class=ql-align-justify><br></p><hr><p class=ql-align-justify><br></p><h1 class=ql-align-justify><strong>9 多标签分类损失 MultiLabelMarginLoss</strong></h1><pre>torch.nn.MultiLabelMarginLoss(reduction='mean')</pre><p>criterion：用于一个样本属于多个类别时的分类任务</p><p>例如一个多分类任务，样本 x 属于第 0类，属于第 1 类，不属于第 2 类，不属于第 3 类</p><p>对于mini-batch(小批量) 中的每个样本按如下公式计算损失:</p><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/16a68b5f8b014ec69380eafc7d911013><p class=pgc-img-caption></p></div><p>要看懂上面这个公式，我们需要先搞明白多分类的MarginLoss：</p><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/acd78680f90c4e4eb675b0fe8f3dcc93><p class=pgc-img-caption></p></div><p>上面公式中，N代表这个问题是N分类问题，p一般取1，x_i表示这个样本的对第i类的评分，x_y表示这个样本的第y类的评分（也就是真实标签上的评分）。上面损失的意思是，模型对其他类别的评分要越小越好，最好是远远小于x_y。</p><p>接下来我们来看多标签的问题，我们把上面原始公式规整成下面的样子：</p><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2461bcf1c63b44d493c102392d84364b><p class=pgc-img-caption></p></div><p>可以看到这个公式是多分类公式的推广，这里结合一个例子帮助理解：</p><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/03543eebf59241cfbedd73481a733ef4><p class=pgc-img-caption></p></div><hr><p class=ql-align-justify><br></p><h1 class=ql-align-justify><strong>10 平滑版L1损失 SmoothL1Loss</strong></h1><p>也被称为 Huber 损失函数。</p><pre>torch.nn.SmoothL1Loss(reduction='mean')</pre><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e53f35a101ed43129327ec0f7db5764b><p class=pgc-img-caption></p></div><p>其中</p><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ea0c0cb950b5407d9c4dba1dab3bf9bc><p class=pgc-img-caption></p></div><p>这个主要是拿来做回归的，结合了mse和mae的优点。</p><hr><p class=ql-align-justify><br></p><h1 class=ql-align-justify><strong>11 2分类的logistic损失 SoftMarginLoss</strong></h1><pre>torch.nn.SoftMarginLoss(reduction='mean')</pre><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f55b592bf0e7499a99ced7d74ee4ab61><p class=pgc-img-caption></p></div><p>criterion：用于优化输入张量x和目标张量y（1或-1）之间的两类分类逻辑损失</p><p>注意：这里解决的是多标签二分类问题，每一个标签是1或者-1。</p><hr><p class=ql-align-justify><br></p><h1 class=ql-align-justify><strong>12 多标签 one-versus-all 损失 MultiLabelSoftMarginLoss</strong></h1><pre>torch.nn.MultiLabelSoftMarginLoss(weight=None, reduction='mean')</pre><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/29b1dc669b244c9696cbb1eeb1bcbb24><p class=pgc-img-caption></p></div><p>这里是多标签多分类问题，是上面公式11的推广。这里的标签是0或者1，所以用交叉熵。多分类用的是ova策略。</p><hr><p class=ql-align-justify><br></p><h1 class=ql-align-justify><strong>13 cosine 损失 CosineEmbeddingLoss</strong></h1><pre>torch.nn.CosineEmbeddingLoss(margin=0.0, reduction='mean')</pre><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b6f23cd124a145a5a62ffc7c1869aef7><p class=pgc-img-caption></p></div><p>参数：</p><blockquote><p>margin:默认值0</p></blockquote><p class=ql-align-justify>criterion：基于余弦距离，利用目标张量y(1或-1)，度量输入张量x1和x2之间相似度</p><hr><p class=ql-align-justify><br></p><h1 class=ql-align-justify><strong>14 多类别分类的hinge损失 MultiMarginLoss</strong></h1><pre>torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, reduction='mean')</pre><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/011cdb52665a42f68680d8c9b257fa3d><p class=pgc-img-caption></p></div><p>参数：</p><blockquote><p>p=1或者2 默认值：1margin:默认值1</p></blockquote><p class=ql-align-justify><br></p><hr><p class=ql-align-justify><br></p><h1 class=ql-align-justify><strong>15 三元组损失 TripletMarginLoss</strong></h1><p>和孪生网络相似，具体例子：给一个A，然后再给B、C，看看B、C谁和A更像。</p><p class=ql-align-justify><br></p><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5cc7457665cb40ae8471514e73b921d4><p class=pgc-img-caption></p></div><pre>torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, reduction='mean')</pre><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/38b41893c69b4d7dbadbbc15e1480a8f><p class=pgc-img-caption></p></div><p>其中：</p><div class=pgc-img><img alt="人工智能中的19 种损失函数，你能认识几个？" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6140b5813c97459e809165470978c898><p class=pgc-img-caption></p></div><p class=ql-align-justify>criterion：3元损失，度量输入x1，x2，x3之间的相似度</p><p>triplet：a（anchor），p（positive），n（negative）</p><p>人脸验证中常常用到，它的目的就是让p与a尽量相似（同一个人不同样本），而n与a尽量不相似（不同人的样本）</p><hr><p class=ql-align-justify><br></p><h1><strong>16 连接时序分类损失 CTCLoss</strong></h1><p>CTC连接时序分类损失，可以对没有对齐的数据进行自动对齐，主要用在没有事先对齐的序列化数据训练上。比如语音识别、ocr识别等等。</p><pre>torch.nn.CTCLoss(blank=0, reduction='mean')</pre><p>参数：</p><p class=ql-align-justify><br></p><blockquote><p>reduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean。</p></blockquote><p class=ql-align-justify><br></p><hr><p class=ql-align-justify><br></p><h1><strong>17 负对数似然损失 NLLLoss</strong></h1><p>负对数似然损失. 用于训练 C 个类别的分类问题.</p><pre>torch.nn.NLLLoss(weight=None, ignore_index=-100, reduction='mean')</pre><p>参数：</p><blockquote><p>weight (Tensor, optional) – 自定义的每个类别的权重. 必须是一个长度为 C 的 Tensorignore_index (int, optional) – 设置一个目标值, 该目标值会被忽略, 从而不会影响到 输入的梯度.</p></blockquote><p class=ql-align-justify><br></p><hr><p class=ql-align-justify><br></p><h1><strong>18 NLLLoss2d</strong></h1><p>对于图片输入的负对数似然损失. 它计算每个像素的负对数似然损失.</p><pre>torch.nn.NLLLoss2d(weight=None, ignore_index=-100, reduction='mean')</pre><p>参数：</p><blockquote><p>weight (Tensor, optional) – 自定义的每个类别的权重. 必须是一个长度为 C 的 Tensorreduction-三个值，none: 不使用约简；mean:返回loss和的平均值；sum:返回loss的和。默认：mean。</p></blockquote><p class=ql-align-justify><br></p><hr><p class=ql-align-justify><br></p><h1><strong>19 PoissonNLLLoss</strong></h1><p>目标值为泊松分布的负对数似然损失</p><pre>torch.nn.PoissonNLLLoss(log_input=True, full=False, eps=1e-08, reduction='mean')</pre><p>参数：</p><blockquote><p>log_input (bool, optional) – 如果设置为 True , loss 将会按照公 式 exp(input) - target * input 来计算, 如果设置为 False , loss 将会按照 input - target * log(input+eps) 计算.full (bool, optional) – 是否计算全部的 loss, i. e. 加上 Stirling 近似项 target * log(target) - target + 0.5 * log(2 * pi * target).eps (float, optional) – 默认值: 1e-8</p></blockquote><p class=ql-align-justify><br></p><blockquote><p>参考资料：pytorch loss function 总结http://www.voidcn.com/article/p-rtzqgqkz-bpg.html</p></blockquote></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'19','种损','失函数'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>