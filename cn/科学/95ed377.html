<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>U-Net:卷积网络用于生物医学图像分割（2015年经典论文） | 极客快訊</title><meta property="og:title" content="U-Net:卷积网络用于生物医学图像分割（2015年经典论文） - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/90096e117f23408880d5dc3a760d72f5"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/95ed377.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/95ed377.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/95ed377.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/95ed377.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/95ed377.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/95ed377.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/95ed377.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/95ed377.html><meta property="article:published_time" content="2020-10-29T21:03:33+08:00"><meta property="article:modified_time" content="2020-10-29T21:03:33+08:00"><meta name=Keywords content><meta name=description content="U-Net:卷积网络用于生物医学图像分割（2015年经典论文）"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/95ed377.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>U-Net:卷积网络用于生物医学图像分割（2015年经典论文）</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><h1 class=pgc-h-arrow-right>U-Net:卷积网络用于生物医学图像分割</h1><p><strong>题目：</strong></p><p>U-Net: Convolutional Networks for Biomedical Image Segmentation</p><p><strong>作者：</strong></p><p>Olaf Ronneberger, Philipp Fischer, Thomas Brox</p><p><strong>来源：</strong></p><p>Machine Learning (cs.LG)</p><p>conditionally accepted at MICCAI 2015</p><p>Submitted on 18 May 2015</p><p><strong>文档链接：</strong></p><p>https://arxiv.org/pdf/1505.04597v1.pdf</p><p><strong>代码链接：</strong></p><p>https://github.com/milesial/Pytorch-UNet</p><p>https://github.com/qubvel/segmentation_models</p><p>https://github.com/divamgupta/image-segmentation-keras</p><h1 class=pgc-h-arrow-right>摘要</h1><p>人们普遍认为，深度网络的成功训练需要数千个带注释的训练样本。在本文中，我们提出了一种网络和训练策略，它依赖于数据扩充的强大使用，以更有效地使用可用的带注释的样本。该体系结构由捕获上下文的收缩路径和支持精确定位的对称扩展路径组成。我们证明这样的网络可以从非常少的图像端到端的训练，并且在ISBI挑战中在电子显微镜栈中神经结构的分割上胜过先前的最佳方法(滑动窗口卷积网络)。我们使用相同的网络训练透射光学显微镜图像(相位对比和DIC)，在2015年ISBI细胞跟踪挑战赛中，我们在这些类别中获得了巨大的优势。此外，网络是快速的。在最近的GPU上，512x512图像的分割需要不到一秒的时间。完整的实现(基于Caffe)和经过培训的网络可以在http://lmb.informatik.uni-reiburg.de/people/ronneber/u-net上找到。</p><p><br></p><h1 class=pgc-h-arrow-right>英文原文</h1><p>There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.</p><p><br></p><h1 class=pgc-h-arrow-right>要点</h1><p><br></p><p><strong>背景</strong></p><p><br></p><p>2012年Ciresan等人[1]在滑动窗口设置中训练一个网络，通过提供一个围绕该像素的局部区域(patch)来预测每个像素的类标签输出。首先，这个网络可以本地化。其次，以patch表示的训练数据远远大于训练图像的数量。由此产生的网络在2012年ISBI会议上以较大优势赢得了EM细分挑战。</p><p><strong>发现问题</strong></p><p>显然，Ciresan等人的[1]策略有<strong>两个缺点</strong>。首先，它非常慢，因为每个补丁都必须单独运行网络，而且由于重叠的补丁存在大量冗余。其次，本地化的准确性和上下文的使用之间存在权衡。更大的补丁需要更多的最大池化层，这会降低定位精度，而小的补丁只允许网络看到很少的上下文。最近的一些方法[11,4]提出了一种考虑多层特征的分类器输出。好的本地化和上下文的使用是可以同时进行的。</p><p><strong>U-Net简介</strong></p><p>在本文中，我们构建了一个更优雅的架构，即所谓的全卷积网络[9]。我们修改和扩展了这个架构，使它工作在非常少的训练图像和产生更精确的分割;参见图1。[9]的主要思想是通过连续层来补充通常的收缩网络，其中池操作符被上采样操作符取代。因此，这些层增加了输出的分辨率。为了进行局部化，收缩路径的高分辨率特征与上采样输出相结合。然后，一个连续的卷积层可以学习根据这些信息组装一个更精确的输出。</p><div class=pgc-img><img alt=U-Net:卷积网络用于生物医学图像分割（2015年经典论文） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/90096e117f23408880d5dc3a760d72f5><p class=pgc-img-caption></p></div><p>我们架构中的一个重要修改是，在上采样部分，我们还有大量的特征通道，这些通道允许网络将上下文信息传播到更高分辨率的层。因此，扩展路径或多或少与收缩路径对称，并产生一个u型架构。网络没有任何完全连接的层，只使用每个卷积的有效部分，即。，则分割地图只包含在输入图像中提供完整上下文的像素。该策略允许通过重叠策略对任意大的图像进行无缝分割(见图2)。为了预测图像边界区域的像素，通过镜像输入图像来推断缺失的上下文。这种平铺策略对于将网络应用于大型图像非常重要，否则分辨率将受到GPU内存的限制。</p><div class=pgc-img><img alt=U-Net:卷积网络用于生物医学图像分割（2015年经典论文） onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/7d76514a63ab4ed3b9330d7466af051a><p class=pgc-img-caption></p></div><p><br></p><h1 class=pgc-h-arrow-right>实验</h1><p><br></p><p><strong>数据扩增处理：</strong></p><p>当只有很少的训练样本可用时，数据扩充是向网络传授所需的不变性和鲁棒性的关键。对于显微镜下的图像，我们首先需要的是<strong>平移</strong>和<strong>旋转</strong>的不变性，以及对<strong>变形</strong>和<strong>灰度值变化</strong>的鲁棒性。特别是训练样本的随机弹性变形似乎是训练带有少量注释图像的分割网络的关键概念。我们使用随机位移矢量在粗糙的3×3网格上生成平滑变形。位移采样的高斯分布与10像素的标准差。然后使用双三次插值计算每个像素的位移。收缩路径末端的退出层执行进一步的隐式数据扩充。</p><p><br></p><p><strong>数据集：</strong></p><p><strong>来源：</strong>数据集是由EM分割挑战提供的，该挑战始于2012年ISBI会议。</p><p><strong>任务：</strong>电子显微记录中神经元结构的分割。训练数据是一组30张果蝇一龄幼虫腹神经索(VNC)连续切片透射电镜图像(512x512像素)。每个图像都配有相应的完全注释的细胞(白色)和细胞膜(黑色)的地面真相分割地图。</p><p><strong>评估：</strong>测试集是公开的，但其分割地图是保密的。通过将预测的膜概率图发送给组织者，可以得到评价。评估是通过在10个不同的水平上对地图进行阈值化，并计算<strong>翘曲误差</strong>、Rand误差和<strong>像素误差</strong>[14]来完成的。。</p><p><br></p><p><strong>实验结果：</strong></p><p>u-net(平均超过7个旋转版本的输入数据)在没有进一步预处理或后处理的情况下获得了0.0003529的翘曲误差(新的最佳值，见表1)和0.0382的随机误差。</p><p><strong>各模型在各数据集上的性能评价指标结果：</strong></p><div class=pgc-img><img alt=U-Net:卷积网络用于生物医学图像分割（2015年经典论文） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3f914552722e40bf9e9712a3f21a0029><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=U-Net:卷积网络用于生物医学图像分割（2015年经典论文） onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/4442c63fa6b14ad483736dca65cc4685><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=U-Net:卷积网络用于生物医学图像分割（2015年经典论文） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cb1fe280c82b43ca8063e974e1023943><p class=pgc-img-caption></p></div><p><br></p><h1 class=pgc-h-arrow-right>结论</h1><p>U-Net架构在非常不同的生物医学分割应用上取得了非常好的性能。由于数据扩充与弹性变形，它只需要非常少的注释图像，并有一个非常合理的训练时间只有10小时，在NVidia Titan GPU (6GB)。我们提供完整的基于[6]的实现和训练有素的网络。我们确信U-Net体系结构可以很容易地应用于更多的任务。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'Net','卷积','网络用'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>