<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>机器学习所需数学知识 | 极客快訊</title><meta property="og:title" content="机器学习所需数学知识 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/851e5e3710fb4825b8bd5ccd5b407b10"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/9a024b6.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/9a024b6.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/9a024b6.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/9a024b6.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/9a024b6.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/9a024b6.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/9a024b6.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/9a024b6.html><meta property="article:published_time" content="2020-10-29T20:56:37+08:00"><meta property="article:modified_time" content="2020-10-29T20:56:37+08:00"><meta name=Keywords content><meta name=description content="机器学习所需数学知识"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/9a024b6.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>机器学习所需数学知识</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><div><p>数学</p><p>1.列举常用的最优化方法</p><p>梯度下降法</p><p>牛顿法，</p><p>拟牛顿法</p><p>座标下降法</p><p>梯度下降法的改进型如AdaDelta，AdaGrad，Adam，NAG等。</p><p>2.梯度下降法的关键点</p><p>梯度下降法沿着梯度的反方向进行搜索，利用了函数的一阶导数信息。梯度下降法的迭代公式为：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/851e5e3710fb4825b8bd5ccd5b407b10><p class=pgc-img-caption></p></div><p>根据函数的一阶泰勒展开，在负梯度方向，函数值是下降的。只要学习率设置的足够小，并且没有到达梯度为0的点处，每次迭代时函数值一定会下降。需要设置学习率为一个非常小的正数的原因是要保证迭代之后的x<em>k</em>+1位于迭代之前的值x<em>k</em>的邻域内，从而可以忽略泰勒展开中的高次项，保证迭代时函数值下降。</p><p>梯度下降法只能保证找到梯度为0的点，不能保证找到极小值点。迭代终止的判定依据是梯度值充分接近于0，或者达到最大指定迭代次数。</p><p>梯度下降法在机器学习中应用广泛，尤其是在深度学习中。AdaDelta，AdaGrad，Adam，NAG等改进的梯度下降法都是用梯度构造更新项，区别在于更新项的构造方式不同。</p><p>3.牛顿法的关键点</p><p>牛顿法利用了函数的一阶和二阶导数信息，直接寻找梯度为0的点。牛顿法的迭代公式为：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bc73219a07244b9fa05484dffd4d5f1d><p class=pgc-img-caption></p></div><p>其中H为Hessian矩阵，g为梯度向量。牛顿法不能保证每次迭代时函数值下降，也不能保证收敛到极小值点。在实现时，也需要设置学习率，原因和梯度下降法相同，是为了能够忽略泰勒展开中的高阶项。学习率的设置通常采用直线搜索（line search）技术。</p><p>在实现时，一般不直接求Hessian矩阵的逆矩阵，而是求解下面的线性方程组：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/0d1909d917d84903a5bda99fffa020af><p class=pgc-img-caption></p></div><p>其解d称为牛顿方向。迭代终止的判定依据是梯度值充分接近于0，或者达到最大指定迭代次数。</p><p>牛顿法比梯度下降法有更快的收敛速度，但每次迭代时需要计算Hessian矩阵，并求解一个线性方程组，运算量大。另外，如果Hessian矩阵不可逆，则这种方法失效。</p><p>4.拉格朗日乘数法</p><p>拉格朗日乘数法是一个理论结果，用于求解带有等式约束的函数极值。对于如下问题：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f93a9e58c6b84da4bfd9032f53462770><p class=pgc-img-caption></p></div><p>构造拉格朗日乘子函数：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/0a5996f85ffb438b87444cccaf73246c><p class=pgc-img-caption></p></div><p>在最优点处对x和乘子变量的导数都必须为0：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/874937df2c94454a84498b58ecf62da1><p class=pgc-img-caption></p></div><p>解这个方程即可得到最优解。对拉格朗日乘数法更详细的讲解可以阅读任何一本高等数学教材。机器学习中用到拉格朗日乘数法的地方有：</p><p>主成分分析</p><p>线性判别分析</p><p>流形学习中的拉普拉斯特征映射</p><p>隐马尔科夫模型</p><p>5.凸优化</p><p>数值优化算法面临两个方面的问题：局部极值，鞍点。前者是梯度为0的点，也是极值点，但不是全局极小值；后者连局部极值都不是，在鞍点处Hessian矩阵不定，即既非正定，也非负定。</p><p>凸优化通过对目标函数，优化变量的可行域进行限定，可以保证不会遇到上面两个问题。凸优化是一类特殊的优化问题，它要求：</p><p>优化变量的可行域是一个凸集目标函数是一个凸函数</p><p>凸优化最好的一个性质是：所有局部最优解一定是全局最优解。</p><p>机器学习中典型的凸优化问题有：</p><p>线性回归</p><p>岭回归</p><p>LASSO回归</p><p>Logistic回归</p><p>支持向量机</p><p>Softamx回归</p><p>6.拉格朗日对偶</p><p>对偶是最优化方法里的一种方法，它将一个最优化问题转换成另外一个问题，二者是等价的。拉格朗日对偶是其中的典型例子。对于如下带等式约束和不等式约束的优化问题：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bb67c5083a684a4cb048522432441984><p class=pgc-img-caption></p></div><p>与拉格朗日乘数法类似，构造广义拉格朗日函数：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9e61b3b9646c4772bfc2d582aa3abb18><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/51e8e51f7c9845eaa0aa9b9859f617b2><p class=pgc-img-caption></p></div><p>必须满足</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e50bcac328b1421cab9ddbc195fff275><p class=pgc-img-caption></p></div><p>的约束。原问题为：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4f600e9a7afc4b6ba286c5ebef65fc68><p class=pgc-img-caption></p></div><p>即先固定住x，调整拉格朗日乘子变量，让函数L取极大值；然后控制变量x，让目标函数取极小值。原问题与我们要优化的原始问题是等价的。</p><p>对偶问题为：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/247c39a6d8ec4e5698c443bde441208b><p class=pgc-img-caption></p></div><p>和原问题相反，这里是先控制变量x，让函数L取极小值；然后控制拉格朗日乘子变量，让函数取极大值。</p><p>一般情况下，原问题的最优解大于等于对偶问题的最优解，这称为弱对偶。在某些情况下，原问题的最优解和对偶问题的最优解相等，这称为强对偶。</p><p>强对偶成立的一种条件是Slater条件：一个凸优化问题如果存在一个候选x使得所有不等式约束都是严格满足的，即对于所有的i都有<em>gi </em>(x)&lt;0，不等式不取等号，则强对偶成立，原问题与对偶问题等价。注意，Slater条件是强对偶成立的充分条件而非必要条件。</p><p>拉格朗日对偶在机器学习中的典型应用是支持向量机。</p><p>7.KKT条件</p><p>KKT条件是拉格朗日乘数法的推广，用于求解既带有等式约束，又带有不等式约束的函数极值。对于如下优化问题：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/411db46a072d4d42ab15dd22c871374a><p class=pgc-img-caption></p></div><p>和拉格朗日对偶的做法类似，KKT条件构如下乘子函数：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/95256d2737584137b6da3dc8edffc6ad><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/33886724dd17446bbe6ebd08ffd04bd5><p class=pgc-img-caption></p></div><p>和</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/213a76ff2d6d460390aa7015bb269491><p class=pgc-img-caption></p></div><p>称为KKT乘子。在最优解处</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/40305fd32b864f00b61ea5051ae497e7><p class=pgc-img-caption></p></div><p>应该满足如下条件：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6bbaf25e27a744e2a66f3d607a265ce4><p class=pgc-img-caption></p></div><p>等式约束</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b6c58964867a45e7b565bd63a93ee546><p class=pgc-img-caption></p></div><p>和不等式约束</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f5578efbe353403f94a3277d9f924bec><p class=pgc-img-caption></p></div><p>是本身应该满足的约束，</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0f194dbdf12349e09eb6fb7949842db3><p class=pgc-img-caption></p></div><p>和之前的拉格朗日乘数法一样。唯一多了关于<em>gi </em>(x)的条件：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0591ffee471342fe872f09fd8c0c8d33><p class=pgc-img-caption></p></div><p>KKT条件只是取得极值的必要条件而不是充分条件。</p><p>8.特征值与特征向量</p><p>对于一个n阶矩阵A，如果存在一个数</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/33886724dd17446bbe6ebd08ffd04bd5><p class=pgc-img-caption></p></div><p>和一个非0向量X，满足：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cdf0debf886b4640ab2f3d2383bdb485><p class=pgc-img-caption></p></div><p>则称</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/33886724dd17446bbe6ebd08ffd04bd5><p class=pgc-img-caption></p></div><p>为矩阵A的特征值，X为该特征值对应的特征向量。根据上面的定义有下面线性方程组成立：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/116283e54f8a4ab49c73f90adda38f39><p class=pgc-img-caption></p></div><p>根据线性方程组的理论，要让齐次方程有非0解，系数矩阵的行列式必须为0，即：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e7634923317c4f509ac2afb51d5bb0c1><p class=pgc-img-caption></p></div><p>上式左边的多项式称为矩阵的特征多项式。矩阵的迹定义为主对角线元素之和：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ec07ebcf642240fd80ecd3d8ad4e1b05><p class=pgc-img-caption></p></div><p>根据韦达定理，矩阵所有特征值的和为矩阵的迹：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/bb88b0004b4e406bb731dbfe06340efe><p class=pgc-img-caption></p></div><p>同样可以证明，矩阵所有特征值的积为矩阵的行列式：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f3463a1ddc864279baea9a551407b176><p class=pgc-img-caption></p></div><p>利用特征值和特征向量，可以将矩阵对角化，即用正交变换将矩阵化为对角阵。实对称矩阵一定可以对角化，半正定矩阵的特征值都大于等于0，在机器学习中，很多矩阵都满足这些条件。特征值和特征向量在机器学习中的应用包括：正态贝叶斯分类器、主成分分析，流形学习，线性判别分析，谱聚类等。</p><p>9.奇异值分解</p><p>矩阵对角化只适用于方阵，如果不是方阵也可以进行类似的分解，这就是奇异值分解，简称SVD。假设A是一个m x n的矩阵，则存在如下分解：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ababeede0fd043e58d48c2fb2982afd4><p class=pgc-img-caption></p></div><p>其中U为m x m的正交矩阵，其列称为矩阵A的左奇异向量；</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/77340550c6484a449a612dff11d89bc9><p class=pgc-img-caption></p></div><p>为m x n的对角矩阵，除了主对角线</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/66a34cff6d7e4da796edc6d001b4e80b><p class=pgc-img-caption></p></div><p>以外，其他元素都是0；V为n x n的正交矩阵，其行称为矩阵A的右奇异向量。U的列为AAT的特征向量，V的列为AT A的特征向量。</p><p>10.最大似然估计</p><p>有些应用中已知样本服从的概率分布，但是要估计分布函数的参数</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6301509f9daa4668a640faf5c222955f><p class=pgc-img-caption></p></div><p>，确定这些参数常用的一种方法是最大似然估计。</p><p>最大似然估计构造一个似然函数，通过让似然函数最大化，求解出</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6301509f9daa4668a640faf5c222955f><p class=pgc-img-caption></p></div><p>。最大似然估计的直观解释是，寻求一组参数，使得给定的样本集出现的概率最大。</p><p>假设样本服从的概率密度函数为</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0858d76dae8546d7a9fd9c85a23aab54><p class=pgc-img-caption></p></div><p>，其中X为随机变量，</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6301509f9daa4668a640faf5c222955f><p class=pgc-img-caption></p></div><p>为要估计的参数。给定一组样本x<em>i</em>,<em>i </em>=1,...,<em>l</em>，它们都服从这种分布，并且相互独立。最大似然估计构造如下似然函数：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e0f1774484ca49e9b8ddc78e5766d2a0><p class=pgc-img-caption></p></div><p>其中x<em>i</em>是已知量，这是一个关于</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6301509f9daa4668a640faf5c222955f><p class=pgc-img-caption></p></div><p>的函数，我们要让该函数的值最大化，这样做的依据是这组样本发生了，因此应该最大化它们发生的概率，即似然函数。这就是求解如下最优化问题：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/92a9ba6b7a2c462fa421486bb3b6dbbe><p class=pgc-img-caption></p></div><p>乘积求导不易处理，因此我们对该函数取对数，得到对数似然函数：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3b08d91b4fa54d75b08be6add4f23bf0><p class=pgc-img-caption></p></div><p>最后要求解的问题为：</p><div class=pgc-img><img alt=机器学习所需数学知识 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7b5e9340d68c4810b9e799ce0241b906><p class=pgc-img-caption></p></div><p>最大似然估计在机器学习中的典型应用包括logistic回归，贝叶斯分类器，隐马尔科夫模型等。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'机器','学习','需数学'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>