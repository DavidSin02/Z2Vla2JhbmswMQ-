<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>支持向量机(SVM)的约束和无约束优化、理论和实现 | 极客快訊</title><meta property="og:title" content="支持向量机(SVM)的约束和无约束优化、理论和实现 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/1d9cd81ef1964685bc9d6f6ef3eba7ee"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/a9a3629.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/a9a3629.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/a9a3629.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/a9a3629.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/a9a3629.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/a9a3629.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/a9a3629.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/a9a3629.html><meta property="article:published_time" content="2020-10-29T21:03:51+08:00"><meta property="article:modified_time" content="2020-10-29T21:03:51+08:00"><meta name=Keywords content><meta name=description content="支持向量机(SVM)的约束和无约束优化、理论和实现"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/a9a3629.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>支持向量机(SVM)的约束和无约束优化、理论和实现</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><div><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d9cd81ef1964685bc9d6f6ef3eba7ee><p class=pgc-img-caption></p></div><p>优化是机器学习领域最有趣的主题之一。我们日常生活中遇到的大多数问题都是通过数值优化方法解决的。在这篇文章中，让我们研究一些基本的数值优化算法，以找到任意给定函数(这对于凸函数最有效)的局部最优解。让我们从简单的凸函数开始，其中局部和全局最小值是相同的，然后转向具有多个局部和全局最小值的高度非线性函数。</p><p>整个优化围绕线性代数和微积分的基本概念展开。最近的深度学习更新引起了数值和随机优化算法领域的巨大兴趣，为深度学习网络所展示的惊人定性结果提供了理论支持。在这些类型的学习算法中，没有任何明确已知的优化函数，但我们只能访问0阶和1阶的Oracles。Oracles是在任何给定点返回函数值（0阶），梯度（1阶）或Hessian（2阶）的黑盒子。本本提供了对无约束和约束优化函数的基本理论和数值理解，还包括它们的python实现。</p><h1>一个点成为局部最小值的必要和充分条件：</h1><p>设f(.)是一个连续的二阶可微函数。对于任一点为极小值，它应满足以下条件:</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d872da398b114a4d8b862632f9badd89><p class=pgc-img-caption></p></div><ul><li>一阶必要条件:</li></ul><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d5839c3c511c44118ea985da5959f11c><p class=pgc-img-caption></p></div><ul><li>二阶必要条件：</li></ul><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/14874649780342a9ad41331725a80368><p class=pgc-img-caption></p></div><ul><li>二阶充足条件：</li></ul><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5d245bec5ab84740a9e5641be27bb027><p class=pgc-img-caption></p></div><p><strong>梯度下降：</strong></p><p>梯度下降是学习算法(机器学习、深度学习或深度强化学习)领域所有进展的支柱。在这一节中，我们将看到为了更快更好的收敛，梯度下降的各种修改。让我们考虑线性回归的情况，我们估计方程的系数:</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/03ff395a68b64226bed5444b6322fab8><p class=pgc-img-caption></p></div><p>假设该函数是所有特征的线性组合。通过最小化损失函数来确定最佳系数集：</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/abf70615613440e1aa50fa22f84af6eb><p class=pgc-img-caption></p></div><p>这是线性回归任务的最大似然估计。最小二乘法(Ordinary Least Square)涉及到求特征矩阵的逆，其表达式为:</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/44b1a5f62fc24386bdf35fa13fc54db5><p class=pgc-img-caption></p></div><p>对于实际问题，数据的维数很大，很容易造成计算量的激增。例如,让我们考虑问题的图像特征分析:一般图像的大小1024 x1024,这意味着特征的数量级为10⁶。由于具有大量的特征，这类优化问题只能通过迭代的方式来解决，这就导致了我们所熟知的梯度下降法和牛顿-拉弗森法。</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c264229e33824cbd9b1c38195c4209a3><p class=pgc-img-caption></p></div><p><strong>梯度下降算法：</strong></p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0e048247a9894de58dd3c66367e21e29><p class=pgc-img-caption></p></div><p>梯度下降算法利用先前指定的学习率（eta）在负梯度的方向（最陡的下降方向）上更新迭代（X）。学习率用于在任何给定的迭代中防止局部最小值的overshoot。</p><p>下图显示了函数f（x）=x²的梯度下降算法的收敛性，其中eta = 0.25</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/19a13d2437864c80b34f71cefe0c4771><p class=pgc-img-caption></p></div><p>找出一个最优eta是目前的任务，这需要事先了解函数的理解和操作域。</p><pre>import matplotlib.pyplot as pltimport numpy as np# assuming function to be x**2def get_gradient(x): return 2*xdef get_value(x): return np.sum(x**2)# python implementation of vanilla gradient descent update ruledef​ ​ def gradient_descent_update​ (x, eta): """ get_gradient is 1st order oracle """ return​ x - eta*get_gradient(x)</pre><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3f1f8d6e5dea42c7af6b722393ffed5b><p class=pgc-img-caption></p></div><p><strong>Armijo Goldstein条件梯度下降:</strong></p><p>为了减少手动设置的工作，应用Armijo Goldstein (AG)条件来查找下一个迭代的(eta)。AG条件的形式推导需要线性逼近、Lipchitz条件和基本微积分的知识。</p><p>我们定义两个函数f1(x)和f2(x)作为两个不同系数和的f(x)的线性逼近，其具有两个不同的系数α和β，由下式给出：</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2e2d07d948d1407eba87d799d7cf3081><p class=pgc-img-caption></p></div><p>在AG条件的每次迭代中，满足以下关系的特定eta：</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/dfdb74e3a7cf40f79a646148d433f978><p class=pgc-img-caption></p></div><p>找到并且相应地更新当前迭代。</p><p>下图显示了下一次迭代的范围，对于函数f（x）=x²的收敛，alpha = 0.25，beta = 0.5：</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/34f69404bf124ad8a9a02db8892b52d3><p class=pgc-img-caption></p></div><p>上图中红色、蓝色和绿色的线对应于绿色显示的下一个可能迭代的范围。</p><pre># python implementation of gradient descent with AG condition update ruledef gradient_descent_update_AG(x, ​ alpha​ =0.5, ​ beta​ =0.25): eta​ =0.5 max_eta​ =np.inf min_eta​ =0. value = get_value(x) grad = get_gradient(x) while​ ​ True​ : x_cand = x - (eta)*grad f = get_value(x_cand) f1 = value - eta​ *a​ lpha*np.sum(np.abs(grad)*​ *2​ ) f2 = value - eta​ *be​ ta *np.sum(np.abs(grad)*​ *2​ ) if​ f&lt;=f2 ​ and​ f&gt;=f1: return x_cand if f &lt;= f1: if eta == max_eta: eta = np.min([2.*eta, eta + max_eta/2.]) else: eta = np.min([2.*eta, (eta + max_eta)/2.]) if​ f&gt;=f2: max_eta = eta eta = (eta+min_eta)/2.0</pre><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3ac855612f08472b934c9907d409b797><p class=pgc-img-caption></p></div><p><strong>完全松弛条件的梯度下降：</strong></p><p>在完全松弛条件的情况下，新的函数g（eta）被最小化以获得随后用于寻找下一次迭代的eta。</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/711ef84ec7794dcda5b8bb85055d77c7><p class=pgc-img-caption></p></div><p>此方法涉及解决查找每个下一个迭代的优化问题，其执行方式如下：</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8aaa174400934f5392b3f7976404067d><p class=pgc-img-caption></p></div><pre># The python implementation for FR is shown below:def​ ​ gradient_descent_update_FR​ (x): eta = ​ 0.5 thresh = ​ 1e-6 max_eta = np.inf min_eta = ​ 0. while​ ​ True​ : x_cand = x - eta*get_gradient(x) g_diff = ​ -1.​ *get_gradient(x)*get_gradient(x_cand) if​ np.sum(np.abs(g_diff)) &lt; thresh ​ and​ eta &gt; ​ 0 ​: return​ x_cand if g_diff &gt; 0: if eta == max_eta: eta = np.min([2.*eta, eta + max_eta/2.]) else: eta = np.min([2.*eta, (eta + max_eta)/2.]) else: max_eta = eta eta = (min_eta + eta)/2.0</pre><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1f4bcc16596f4a908271d29c68baaa7d><p class=pgc-img-caption></p></div><p><strong>随机梯度下降</strong></p><p>我们知道，在实际设置中，数据的维数会非常大，这使得对所有特征进行进一步的梯度计算非常昂贵。在这种情况下，随机选择一批点(特征)并计算期望的梯度。整个过程的收敛只是在预期的意义上。</p><p>在数学上它意味着：随机选择一个点（p）来估计梯度。</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/551f9471f42648b1a8af5c02e292178e><p class=pgc-img-caption></p></div><p>在上面的迭代中，wt可以被视为噪声。只有当E（wt）趋于0时，该迭代才会导致局部最优。</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0b15e35938b44ccb984b7881424e9c15><p class=pgc-img-caption></p></div><p>同样，可以看出wt的方差也是有限的。通过以上证明，保证了SGD的收敛性。</p><pre># SGD implementation in pythondef​ ​ SGD​ (self, X, Y, batch_size, thresh=​ 1 ​ ): loss = ​ 100 step = ​ 0 if​ self.plot: losses = [] while​ loss &gt;= thresh: # mini_batch formation index = np.random.randint(​ 0 ​ , len(X), size = batch_size) trainX, trainY = np.array(X)[index], np.array(Y)[index]  self.forward(trainX) loss = self.loss(trainY) self.gradients(trainY) # update self.w0 -= np.squeeze(self.alpha*self.grad_w0) self.weights -= np.squeeze(self.alpha*self.grad_w)  if self.plot: losses.append(loss) if​ step % ​ 1000​ == ​ 999​ : ​  print​ ​ "Batch number: {}"​ .format(step)+​ " current loss: {}"​ .format(loss) step += ​ 1 if​ self.plot : self.visualization(X, Y, losses) pass</pre><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bba5993dc7c640f185258a08380503c3><p class=pgc-img-caption></p></div><p><strong>AdaGrad</strong></p><p>Adagrad是一个优化器，可帮助自动调整优化问题中涉及的每个特征的学习率。这是通过跟踪所有梯度的历史来实现的。该方法也仅在期望意义上收敛。</p><pre>def​ ​ AdaGrad​ (self, X, Y, batch_size,thresh=0.5​ ,epsilon=1e-6​ ): loss = ​ 100 step = ​ 0 if​ self.plot: losses = [] G = np.zeros((X.shape[​ 1 ​ ], X.shape[​ 1 ​ ])) G0 = ​ 0 while​ loss &gt;= thresh: # mini_batch formation index = np.random.randint(​ 0 ​ , len(X), size = batch_size) trainX, trainY = np.array(X)[index], np.array(Y)[index] self.forward(trainX) loss = self.loss(trainY) self.gradients(trainY) G += self.grad_w.T*self.grad_w G0 += self.grad_w0**​ 2 den = np.sqrt(np.diag(G)+epsilon) delta_w = self.alpha*self.grad_w / den delta_w0 = self.alpha*self.grad_w0 / np.sqrt(G0 + epsilon) # update parameters self.w0 -= np.squeeze(delta_w0) self.weights -= np.squeeze(delta_w) if self.plot: losses.append(loss) if​ step % ​ 500​ == ​ 0 ​ : ​  print​ ​ "Batch number: {}".format (step)+​ " current loss: {}"​.format(loss)  step += ​ 1 if​ self.plot : self.visualization(X, Y, losses) pass</pre><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ec15027957d6444fb83a9b21d946d598><p class=pgc-img-caption></p></div><h1>让我们转向基于Hessian的方法，牛顿和拟牛顿方法：</h1><p>基于hessian的方法是基于梯度的二阶优化方法，几何上涉及到梯度和曲率信息来更新权重，因此收敛速度比仅基于梯度的方法快得多，牛顿方法的更新规则定义为:</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f02e0b37347e404aac353e2bd0978171><p class=pgc-img-caption></p></div><p>该算法的收敛速度远快于基于梯度的方法。数学上,梯度下降法的收敛速度正比于O (1 / t),而对于牛顿法它正比于O (1 / t²)。但是对于高维数据，为每个迭代估计二阶oracle的计算成本很高，这导致使用一阶oracle模拟二阶oracle。这就给出了拟牛顿算法。这类拟牛顿法中最常用的算法是BFGS和LMFGS算法。在这一节中，我们只讨论BFGS算法，它涉及到对函数Hessian的rank one矩阵更新。该方法的总体思想是随机初始化Hessian，并使用rank one更新规则在每次迭代中不断更新Hessian。数学上可以表示为:</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/89b5d6410f504bd0802357d4ba10199f><p class=pgc-img-caption></p></div><pre># python implementation for BFGSdef​ ​ BFGS_update​ (H, s, y): smooth = ​ 1e-3 s = np.expand_dims(s, axis= ​ -1​ ) y = np.expand_dims(y, axis= ​ -1​ ) rho = ​ 1.​ /(s.T.dot(y) + smooth) p = (np.eye(H.shape[​ 0 ​ ]) - rho*s.dot(y.T)) return​ p.dot(H.dot(p.T)) + rho*s.dot(s.T)def​ ​ quasi_Newton​ (x0, H0, num_iter=​ 100​ , eta=​ 1 ​ ): xo, H = x0, H0 for​ _ ​ in​ range(num_iter): xn = xo - eta*H.dot(get_gradient(xo)) y = get_gradient(xn) - get_gradient(xo) s = xn - xo H = BFGS_update(H, s, y) xo = xn return​ xo</pre><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9043876e3ab745af8cc4bb1590844a21><p class=pgc-img-caption></p></div><h1>约束优化</h1><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1fd1f56b2eb149af864a9fb932c9e853><p class=pgc-img-caption></p></div><p>现在，是时候讨论一些围绕约束优化(包括问题的制定和解决策略)的关键概念了。本节还将讨论一种称为SVM(支持向量机)的算法的理论和Python实现。当我们在现实生活中遇到问题时，提出一个理想的优化函数是相当困难的，有时是不可行的，所以我们通过对问题施加额外的约束来放松优化函数，优化这个约束设置将提供一个近似，我们将尽可能接近问题的实际解决方案，但也是可行的。求解约束优化问题的方法有拉格朗日公式法、惩罚法、投影梯度下降法、内点法等。在这一节中，我们将学习拉格朗日公式和投影梯度下降法。本节还将详细介绍用于优化CVXOPT的开源工具箱，并介绍使用此工具箱的SVM实现。</p><p>约束优化问题的一般形式：</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/80a0485f0a1e4530abd25bdd7ce90266><p class=pgc-img-caption></p></div><p>其中f(x)为目标函数，g(x)、h(x)分别为不等式约束和等式约束。如果f(x)是凸的约束条件形成一个凸集，(即g(x)为凸函数h(x)为仿射函数），该优化算法保证收敛于全局最小值。对于其他问题，它收敛于局部极小值。</p><p><strong>投影梯度下降</strong></p><p>求解约束优化设置的第一步(也是最明显的一步)是对约束集进行迭代投影。这是求解约束优化问题中最强大的算法。这包括两个步骤(1)在最小化(下降)方向上找到下一个可能的迭代，(2)在约束集上找到迭代的投影。</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/895fb1bee23441f39388472c3cde5420><p class=pgc-img-caption></p></div><pre># projected gradient descent implementationdef​ ​ projection_oracle_l2​ (w, l2_norm): return​ (l2_norm/np.linalg.norm(w))*wdef​ ​ projection_oracle_l1​ (w, l1_norm): # first remember signs and store them. Modify w signs = np.sign(w) w = w*signs  # project this modified w onto the simplex in first orthant. d=len(w) if​ np.sum(w)&lt;=l1_norm: return​ w*signs for​ i ​ in​ range(d): w_next = w+​ 0 w_next[w&gt;​ 1e-7​ ] = w[w&gt;​ 1e-7​ ] - np.min(w[w&gt;​ 1e-7​ ]) if​ np.sum(w_next)&lt;=l1_norm: w = ((l1_norm - np.sum(w_next))*w + (np.sum(w) - l1_norm)*w_next)/(np.sum(w)-np.sum(w_next)) return​ w*signs else​ : w=w_nextdef​ ​ main​ (): eta=​ 1.0 /smoothness_coeff for​ l1_norm ​ in​ np.arange(​ 0 ​ , ​ 10​ , ​ 0.5​ ): w=np.random.randn(data_dim) for​ i ​ in​ range(​ 1000​ ): w = w - eta*get_gradient(w) w = projection_oracle_l1(w, l1_norm) pass</pre><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/7edf28ca75794435ae74c44046c4b5b1><p class=pgc-img-caption></p></div><p><strong>了解拉格朗日公式</strong></p><p>在大多数优化问题中，找到约束集上迭代的投影是一个困难的问题(特别是在复杂约束集的情况下)。它类似于在每次迭代中解决优化问题，在大多数情况下，优化问题是非凸的。在现实中，人们试图通过解决对偶问题而不是原始问题来摆脱约束。</p><p>在深入拉格朗日对偶和原始公式之前，让我们先了解一下KKT条件及其意义</p><ul><li>对于任意点为具有等式约束的局部/全局最小值:</li></ul><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4fc7f515558843a7882a7f155482108d><p class=pgc-img-caption></p></div><ul><li>类似地，不等式约束:</li></ul><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7a0dc25bf3864a89b9140214dd297712><p class=pgc-img-caption></p></div><p>这两个条件可以很容易地通过将单位圆看作一个约束集来观察。在第一部分中，我们只考虑一个(\mu)符号不重要的边界，这是等式约束的结果。在第二种情况下，考虑一个单位圆的内部集合，其中-ve符号表示(\lambda)，表示可行解区域。</p><p>KKT (Karush-Kuhn-Tucker)条件被认为是一阶必要条件，当一个点被认为是一个平稳点(局部极小点、局部极大点、鞍点)时，该条件必须满足。x ^ * 是局部最小值：</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d5deb7ddb2784994b72a8ff9b4d95263><p class=pgc-img-caption></p></div><p>LICQ条件:所有活动约束</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ba46179089304a7d9d48c42f13d15832><p class=pgc-img-caption></p></div><p>它们应该是线性无关的。</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/013c6c5c165b462586cfb6980f7d4f8b><p class=pgc-img-caption></p></div><p><strong>拉格朗日函数</strong></p><p>对于任何函数f (x)与不等式约束g_i (x)≤0和等式约束h_i (x) = 0,拉格朗日L (x \λ\μ)</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cd3555b8fd794d0ea6fed73e79e84902><p class=pgc-img-caption></p></div><p>优化函数：</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/13054fb19dbb4ac688ee541cd21e6a53><p class=pgc-img-caption></p></div><p>以上优化相当于：</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a6309387c53447019d54c6f7e30a6c1b><p class=pgc-img-caption></p></div><p>上面的公式被游戏理论的主张称为原始问题（p ^ *）（即第二个玩家将总是有更好的优化机会），可以很容易地看出：</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d665aa90bde447e997c9a7005bb88126><p class=pgc-img-caption></p></div><p>这个公式被称为对偶问题(d ^ *)。</p><p>当且仅当目标函数为凸且约束集为凸时，原始公式和对偶公式的最优值相同。这项要求的证明理由如下:</p><p>函数是凸的，这意味着</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1b12aa17ef2744a6ae3f3138ec003d26><p class=pgc-img-caption></p></div><h1>SVM（支持向量机）</h1><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9977d74ad95d4dd0aad4d2153cd0cd02><p class=pgc-img-caption></p></div><p>SVM属于用于分类和回归问题的监督机器学习算法类。SVM易于扩展，可以解决线性和非线性问题（通过使用基于核的技巧）。在大多数问题中，我们无法对两类不同的数据进行单独的区分，因此我们需要在决策边界的构建中提供一点余地，这很容易用SVM来表示。支持向量机的思想是在两组不同的数据点之间创建分离的超平面。一旦获得了分离超平面，对其中一个类中的数据点(测试用例)进行分类就变得相对容易了。支持向量机甚至可以很好地用于高维数据。与其他机器学习（ML）模型相比，svm的优点是内存效率高、准确、快速。我们来看看支持向量机的数学</p><p>SVM原始问题</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ea18a226ae984fd7a8749f2e746c2f76><p class=pgc-img-caption></p></div><p>使用拉格朗日算法的SVM对偶问题</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/acd6f4be852b45ab860174fccca257c0><p class=pgc-img-caption></p></div><p>Derivation of Dual</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e38afdcc638c4def8c5d2443f4aa8ed8><p class=pgc-img-caption></p></div><h1>CVXOPT</h1><p>在本节中，我们将讨论使用CVXOPT库在python中实现上述SVM对偶算法。</p><p>CVXOPT通用格式的问题</p><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a587e128be194c36ae2ca49586689408><p class=pgc-img-caption></p></div><pre># SVM using CVXOPTimport​ numpy ​ as​ npfrom​ cvxopt ​ import​ matrix,solversdef​ ​ solve_SVM_dual_CVXOPT​ (x_train, y_train, x_test): """ Solves the SVM training optimisation problem (the Arguments: x_train: A numpy array with shape (n,d), denoting \R^d. y_train: numpy array with shape (n,) Each element x_train: A numpy array with shape (m,d), denoting dual) using cvxopt.  n training samples in takes +1 or -1 m test samples in \R^d. Limits: n&lt;200, d&lt;100, m&lt;1000  Returns: y_pred_test : A numpy array with shape (m,). This is the result of running the learned model on the test instances x_test. Each element is +1 or -1. """ n, d = x_train.shape c = ​ 10​ ​ # let max \lambda value be 10 y_train = np.expand_dims(y_train, axis=​ 1 ​ )*​ 1. P = (y_train * x_train).dot((y_train * x_train).T) q = -1.​ *np.ones((n, ​ 1 ​ )) G = np.vstack((np.eye(n)*​ -1​ ,np.eye(n))) h = np.hstack((np.zeros(n), np.ones(n) * c)) A = y_train.reshape(​ 1 ​ , ​ -1​ ) b = np.array([​ 0.0​ ]) P = matrix(P); q = matrix(q) G = matrix(G); h = matrix(h) A = matrix(A); b = matrix(b) sol = solvers.qp(P, q, G, h, A, b) lambdas = np.array(sol[​ 'x'​ ]) w = ((y_train * lambdas).T.dot(x_train)).reshape(​ -1​ , ​ 1 ​ ) b = y_train - np.dot(x_train, w) prediction = ​ lambda​ x, w, b: np.sign(np.sum(w.T.dot(x) + b)) y_test = np.array([prediction(x_, w, b) ​ for​ x_ ​ in​ x_test]) return​ y_testif​ __name__ == ​ "__main__"​ : # Example format of input to the functions n=​ 100 m=​ 100 d=​ 10 x_train = np.random.randn(n,d) x_test = np.random.randn(m,d) w = np.random.randn(d) y_train = np.sign(np.dot(x_train, w)) y_test = np.sign(np.dot(x_test, w)) y_pred_test = solve_SVM_dual_CVXOPT(x_train, y_train, x_test) check1 = np.sum(y_pred_test == y_test) print​ (​ "Score: {}/{}"​ .format(check1, len(y_Test)))</pre><div class=pgc-img><img alt=支持向量机(SVM)的约束和无约束优化、理论和实现 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b761e859b46a44af9c4bcc391126e11f><p class=pgc-img-caption></p></div></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'SVM','约束','无约束'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>