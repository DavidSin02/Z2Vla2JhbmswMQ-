<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>IT大数据学习分享：关于机器学习的知识点（1） | 极客快訊</title><meta property="og:title" content="IT大数据学习分享：关于机器学习的知识点（1） - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/dfic-imagehandler/d4954510-db08-4291-9757-377b5c9ae457"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/bab82bac.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/bab82bac.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/bab82bac.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/bab82bac.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/bab82bac.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/bab82bac.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/bab82bac.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/bab82bac.html><meta property="article:published_time" content="2020-11-14T20:56:35+08:00"><meta property="article:modified_time" content="2020-11-14T20:56:35+08:00"><meta name=Keywords content><meta name=description content="IT大数据学习分享：关于机器学习的知识点（1）"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/bab82bac.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>IT大数据学习分享：关于机器学习的知识点（1）</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><blockquote><p>作者用超过1.2万字的篇幅，总结了自己学习机器学习过程中遇到知识点。“入门后，才知道机器学习的魅力与可怕。”希望正在阅读本文的你，也能在机器学习上学有所成。作者 尘恋</p></blockquote><p><br></p><div class=pgc-img><img alt=IT大数据学习分享：关于机器学习的知识点（1） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/dfic-imagehandler/d4954510-db08-4291-9757-377b5c9ae457><p class=pgc-img-caption></p></div><p><br></p><h1 class=pgc-h-arrow-right>准备</h1><p>机器学习是什么，人工智能的子类，深度学习的父类。</p><p><strong>机器学习：</strong>使计算机改进或是适应他们的行为，从而使他们的行为更加准确。也就是通过数据中学习，从而在某项工作上做的更好。</p><p>引用王钰院士在2008年会议的一句话，假定W是给定世界的有限或者无限的所有对象的集合，Q是我们能够或得到的有限数据，Q是W的一个很小的真子集，机器学习就是根据世界的样本集来推算世界的模型，使得模型对于整体世界来说为真。</p><p><strong>机器学习的两个驱动：</strong>神经网络，数据挖掘。</p><p><strong>机器学习的分类：</strong></p><p><strong>监督学习</strong>：提供了包含正确回答的训练集，并以这个训练集为基础，算法进行泛化，直到对所有的可能输入都给出正确回答，这也称在范例中学习。</p><p><strong>无监督学习：</strong>没有提供正确回答，算法试图鉴别出输入之间的相似，从而将同样的输入归为一类，这种方法称密度学习。</p><p><strong>强化学习：</strong>介于监督和无监督之间，当答案不正确时，算法被告知，如何改正则不得而知，算法需要去探索，试验不同情况，直到得到正确答案，强化学习有时称为伴随评论家的学习，因为他只对答案评分，而不给出改进建议。</p><p><strong>进化学习</strong>：将生物学的进化看成一个学习过程，我们研究如何在计算机中对这一过程进行建模，采用适应度的概念，相当于对当前解答方案好坏程度的评分。(不是所有机器学习书籍都包含进化学习)</p><p><strong>优点：</strong>泛化，对于未曾碰到的输入也能给出合理的输出。</p><p><strong>监督学习：</strong>回归、分类。</p><p><strong>机器学习过程：</strong></p><p>数据的收集和准备<br>特征选择<br>算法选择<br>参数和模型选择<br>训练<br>评估</p><p><strong>专业术语：</strong></p><p>输入：输入向量x作为算法输入给出的数据</p><p>突触：wij是节点i和节点j之间的加权连接，类似于大脑中的突触，排列成矩阵W</p><p>输出：输出向量y，可以有n个维度</p><p>目标：目标向量t，有n个维度，监督学习所需要等待额外数据，提供了算法正在学习的“正确答案”</p><p>维度：输入向量的个数</p><p>激活函数：对于神经网络，g(·)是一种数学函数，描述神经元的激发和作为对加权输入的响应</p><p>误差：E是根据y和t计算网络不准确性的函数</p><p>权重空间：当我们的输入数据达到200维时，人类的限制使得我们无法看见，我们最多只能看到三维投影，而对于计算机可以抽象出200个相互正交的轴的超平面进行计算，神经网络的参数是将神经元连接到输入的一组权重值，如将神经元的权重视为一组座标，即所谓的权重空间</p><p>维度灾难：随着维度的增加，单位超球面的体积也在不断增加，2d中，单位超球面为圆，3d中则为求，而更高的维度便称为超球面，Vn = (2π/n)*Vn-2，于是当n>2π时，体积开始缩小，因此可用数据减少，意味着我们需要更多的数据，当数据到达100维以上时，单位数据变得极小，进而需要更多的数据，从而造成维度灾难</p><p><strong>维度和体积的关系：</strong></p><p></p><div class=pgc-img><img alt=IT大数据学习分享：关于机器学习的知识点（1） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/31636b4b28e044fa94df485a4d7dd7ba><p class=pgc-img-caption></p></div><p></p><p><strong>机器学习算法测试：</strong></p><p>算法成功程度是预测和一直目标进行比较，对此我们需要一组新的数据，测试集。</p><p>当对算法进行训练时，过度的训练将会导致过拟合，即拟合曲线与数据完美拟合，但是失去了泛化能力，为检测过拟合我们需要用测试集进行验证，称为统计中的交叉验证，它是模型选择中的一部门：为模型选择正确的参数，以便尽可能的泛化。</p><p>数据的准备，我们需要三组数据集，训练算法的训练集，跟踪算法学习效果的验证集，用于产生最终结果的测试集，数据充足情况便执行50:25:25或60:20:20的划分，数据集分配应随机处理，当数据请核实板块，则采用流出方法或多折交叉验证。</p><p>混淆矩阵是检测结果是否良好的分类，制作一个方阵，其包含水平和垂直方向上所有可能的类，在(i，j)处的矩阵元素告诉我们在目标中有多少模式被放入类i中，主对角线上任何东西都是正确答案，主对角线元素之和除以所有元素的和，从而得到的百分比便是精度。</p><p>精度指标：真正例是被正确放入类1，假正例是被错误放入类1，而真反例是被正确放入类2，假反例是被错误放入类2。</p><p></p><div class=pgc-img><img alt=IT大数据学习分享：关于机器学习的知识点（1） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5ad591c98bf24639b1397e53f83ff858><p class=pgc-img-caption></p></div><p></p><p>敏感率=#TP/(#TP+#FN) 特异率=#TN/(#TN+#FP)<br>查准率=#TP/(#TP+#FP) 查全率=#TP/(#TP+#FN)<br>F1 = 2*(查准率*查全率)/(查准率+查全率)</p><p><strong>受试者工作曲线：</strong>y轴真正例率，x轴假正例率，线下区面积：AUC。</p><p>数<strong>据与概率的转换：</strong>通过贝叶斯法则处理联合概率P(C,X)和条件概率P(X|C)得出P(C|X)，MAP问题是训练数据中最可能的类是什么。将所有类的最终结果考虑在内的方法称为贝叶斯最优分类。</p><p><strong>损失矩阵：</strong>指定类Ci被分为类Cj所涉及的风险。</p><p><strong>基本统计概念：</strong>协方差，度量两个变量的依赖程度。</p><p>Cov({xi},{yi})=E({xi} – u)E({yi} – v)</p><p><strong>权衡偏差与方差：</strong>偏差-方差困境：更复杂的模型不一定能产生更好的结果;模型糟糕可能由于两个原因，模型不准确而与数据不匹配，或者不精确而有极大的不稳定性。第一种情况称为偏差，第二种情况称为方差。</p><h1 class=pgc-h-arrow-right>01 神经元、神经网络和线性判别</h1><p><strong>1. 鲁棒性</strong></p><p>鲁棒是Robust的音译，也就是健壮和强壮的意思。它是在异常和危险情况下系统生存的关键。比如说，计算机软件在输入错误、磁盘故障、网络过载或有意攻击情况下，能否不死机、不崩溃，就是该软件的鲁棒性。</p><p><strong>2. 神经网络</strong></p><p>神经网络模仿的便是生物学中的神经网络，通过输入进而判定神经元激活否。</p><p>将一系列的神经元放置在一起，假设数据存在模式。通过神经元一些已知的样例，我们希望他能够发现这种模式，并且正确预测其他样例，称为模式识别。为了让神经网络能够学习，我们需要改变神经元的权重和阈值进而得到正确的结果，历史上的第一个神经网络——感知器。</p><p><strong>3. Hebb法则</strong></p><p>突触连接强度的变化和两个相连神经元激活得相关性成比例，如果两个神经元始终同时激活，那么他们之间连接的强度会变大，反之，如果两个神经元从来不同时激活，那么他们之间的连接会消失。也被成为长时效增强法则和神经可塑性。</p><p><strong>4. McCulloch和Pitts神经元</strong></p><p>建模，一组输入加权wi相当于突触，一个加法器把输入信号相加(等价于收集电荷的细胞膜)，一个激活函数，决定细胞对于当前的输入是否激活，输入乘于权重的和与阈值进行判断，大於则激活，否则抑制。局限性：现实中的神经元不给出单一的输出相应，而是给出一个点位序列，一种连续的方式给出分等级的输出。神经元不会根据电脑的时钟脉冲去顺序更新，而是随机的异步更新。</p><p><strong>5. 感知器</strong></p><p></p><div class=pgc-img><img alt=IT大数据学习分享：关于机器学习的知识点（1） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a43c2a6725b14426b1741b7baa19612b><p class=pgc-img-caption></p></div><p></p><p>▲感知器神经网络</p><p><strong>权重更新规则</strong></p><p>Wij &lt;- Wij – n(yi – ti)*xi</p><p>N为学习效率，过大会造成网络不稳定，过小会造成学习时间久;yi为神经元输出，ti为神经元目标，xi为神经元输入，Wij为权重。</p><p><strong>感知器学习算法</strong></p><p>分为两部分，训练阶段和再现阶段。</p><p></p><div class=pgc-img><img alt=IT大数据学习分享：关于机器学习的知识点（1） onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/38fdd6235d314ab48650b257610ccd79><p class=pgc-img-caption></p></div><p></p><p><strong>6. 线性可分性</strong></p><p>一条直线将神经元激活的和不激活的神经元划分开来，这条直线称为决策边界，也称为判别函数，在三维空间该决策边界为平面，更高维则为超平面。</p><p><strong>7. 感知器收敛定理</strong></p><p>感知器以1/γ*γ为界，其中γ为分离超平面与最接近的数据点之间的距离。</p><p>只要把数据映射到正确的维度空间，那么总是可以用一个线性函数来把两个类别区分开，为了较有效率的解决这个问题，有一整类的方法称为核分类器，也是支持向量机的基础。</p><p><strong>8. 数据项预处理</strong></p><p>特征选择，我们每次去掉一个不同的特征，然后试着在所得的输入子集上训练分类器，看结果是否有所提高，如果去掉某一个特征能使得结果有所改进，那么久彻底去掉他，在尝试能否去掉其他的特征，这是一个测试输出与每一个特征的相关性的过于简单方法。</p><p><strong>9. 线性回归</strong></p><p>回归问题是用一条线去拟合数据，而分类问题是寻找一条线来划分不同类别。回归方法，引入一个指示变量，它简单的标识每一个数据点所属的类别。现在问题就变成了用数据去预测指示变量，第二种方法是进行重复的回归，每一次对其中的一个类别，指示值为1代表样本属于该类别，0代表属于其他类别。</p><h1 class=pgc-h-arrow-right>02 维度简约</h1><p><strong>1. 降维的三种算法</strong></p><p>特征选择法：仔细查找可见的并可以利用的特征而无论他们是否有用，把它与输出变量关联起来</p><p>特征推导法：通过应用数据迁移，即通过可以用矩阵来描述的平移和旋转来改变图标的座标系，从而用旧的特征推导出新的特征，因为他允许联合特征，并且坚定哪一个是有用的，哪一个没用</p><p>聚类法：把相似的数据点放一起，看能不能有更少的特征</p><p><strong>2. 特征选择方法</strong></p><p>建设性方法：通过迭代不断加入，测试每一个阶段的错误以了解某个特征加入时是否会发生变化。破坏性方法是去掉应用在决策树上的特征。</p><p><strong>主成分分析(PCA)</strong></p><p>主成分的概念是数据中变化最大的方向。算法首先通过减去平均值来把数据集中， 选择变化最大的方向并把它设为座标轴，然后检查余下的变化并且找一个座标轴使得它垂直于第一个并且覆盖尽可能多的变化。</p><p>不断重复这个方法直到找到所有可能的座标轴。这样的结果就是所有的变量都是沿着直角座标系的轴，并且协方差矩阵是对角的——每个新变量都与其他变量无关，而只与自己有关。一些变化非常小的轴可以去掉不影响数据的变化性。</p><p><strong>具体算法</strong></p><p></p><div class=pgc-img><img alt=IT大数据学习分享：关于机器学习的知识点（1） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e7faabb2768240f88703c3f17b718ed5><p class=pgc-img-caption></p></div><p></p><p><strong>3. 基于核的PCA算法</strong></p><p></p><div class=pgc-img><img alt=IT大数据学习分享：关于机器学习的知识点（1） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2170eccabb3d47f7b6250933a6ad9989><p class=pgc-img-caption></p></div><p></p><p><strong>4. 因素分析</strong></p><p>观察数据是否可以被少量不相关的因素或潜在的变量解释，目的用于发现独立因素和测量每一个因素固有的误差。</p><p><strong>5. 独立成分分析(ICA)</strong></p><p>统计成分是独立的，即对于E[bi,bj] = E[bi]E[bj]与及bi是不相关的。</p><p><strong>6. 局部线性嵌入算法</strong></p><p>找出每个点的邻近点(即前k个近的点):计算每对点间的距离。找到前k个小的距离。对于其他点，令Wij=0.对每个点xi:创建一个邻近点的位置表z,计算zi=zi-xi。</p><p>根据约束条件计算令等式(6.31)最小的权矩阵W:计算局部协方差C=ZZ^T，其中Z是zi组成的矩阵。利用CW=I计算W,其中I是N*N单位矩阵。对于非邻近点，令Wij=0。</p><p>对W/∑W设置其他元素计算使得等式(6.32)最小的低维向量 yi:创建M=(I-W)T(I-W).计算M的特征值和特征向量。根据特征值的大小给特征向量排序。对应于第q小的特征值，将向量y的第q行设置为第q+1 个特征向量(忽略特征值为0)</p><p></p><div class=pgc-img><img alt=IT大数据学习分享：关于机器学习的知识点（1） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8feca6c276a5410ba683efebce30da54><p class=pgc-img-caption></p></div><p></p><p><strong>7. 多维标度算法</strong></p><p></p><div class=pgc-img><img alt=IT大数据学习分享：关于机器学习的知识点（1） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/86a9c06adaa845289f105842c88b2c53><p class=pgc-img-caption></p></div><p></p><p><strong>8. ISOMAP算法</strong></p><p></p><div class=pgc-img><img alt=IT大数据学习分享：关于机器学习的知识点（1） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6de99dbc9b754a04ac38601539f72a7d><p class=pgc-img-caption></p></div><p></p><h1 class=pgc-h-arrow-right>03 概率学习</h1><p><strong>1. 期望最大算法(EM)</strong></p><p>额外加入位置变量，通过这些变量最大化函数。</p><p><strong>2. 高斯混合模型的期望最大算法</strong></p><p></p><div class=pgc-img><img alt=IT大数据学习分享：关于机器学习的知识点（1） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d2704059656e428bbb4cdd1c9c2023e4><p class=pgc-img-caption></p></div><p></p><p><strong>3. 通常的期望最大化算法</strong></p><p></p><div class=pgc-img><img alt=IT大数据学习分享：关于机器学习的知识点（1） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ce3cde331597406096ad4aa61474a06f><p class=pgc-img-caption></p></div><p></p><p><strong>4. 信息准则</strong></p><p>除了通过模型选择确定停止学习的时间，前期采用验证集思想，而信息准则则是确定一些方法从而期待这个训练过的模型可以表现的多好。</p><p>艾卡信息准则：AIC = ln(C)-k<br>贝叶斯信息准则：BIC = 2ln(C)-klnN</p><p>K是模型中参数的数目，N是训练样本的数量，C是模型的最大似然。以上两种方法都是奥卡姆剃刀的一种形式。</p><p><strong>5. 奥卡姆剃刀</strong></p><p>如无必要，勿增实体，即简单有效原理。</p><p><strong>6. 最近邻法</strong></p><p>如果没有一个描述数据的模型，那么最好的事情就是观察相似的数据并且把他们选择成同一类。</p><p><strong>7. 核平滑法</strong></p><p>用一个和(一堆点的权重函数)来根据输入的距离来决定每一个数据点有多少权重。当两个核都会对离当前输入更近的点给出更高的权重，而当他们离当前输入点越远时，权重会光滑的减少为0，权重通过λ来具体化。</p><p><strong>8. KD-Tree</strong></p><p>在一个时刻选择一个维度并且将它分裂成两个，从而创建一颗二进制树，并且让一条直线通过这个维度里点的座标的中位数。这与决策树的差别不大。数据点作为树的树叶。</p><p>制作树与通常的二进制树的方法基本相同：我们定义一个地方来分裂成两种选择——左边和右边， 然后沿着它们向下。可以很自然地想到用递归的方法来写算法。</p><p>选择在哪分裂和如何分裂使得KD-Tree是不同的。在每一步只有一个维度分裂，分裂的地方是通过计算那一维度的点的中位数得到的，并且在那画一条直线。通常，选择哪一个维度分裂要么通过不同的选择要么随机选择。</p><p>算法向下搜索可能的维度是基于到目前为止树的深度，所以在二维里，它要么是水平的要么是垂直的分裂。组成这个方法的核心是简单地选代选取分裂的函数，找到那个座标的中位数的值，并且根据那个值来分裂点。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'学习','大数据','关于机器'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>