<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>矩阵与张量的区别 | 极客快訊</title><meta property="og:title" content="矩阵与张量的区别 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/1ff82d7f948943e08f28939b58ea6d2f"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/cfa93bbc.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/cfa93bbc.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/cfa93bbc.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/cfa93bbc.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/cfa93bbc.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/cfa93bbc.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/cfa93bbc.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/cfa93bbc.html><meta property="article:published_time" content="2020-11-14T20:52:19+08:00"><meta property="article:modified_time" content="2020-11-14T20:52:19+08:00"><meta name=Keywords content><meta name=description content="矩阵与张量的区别"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/cfa93bbc.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>矩阵与张量的区别</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><div><p><em>摘要：</em> 关于矩阵和张量的区别有些人可能不太清楚，看了这篇文章相信你会很明白了</p><p>这个问题有一个简短的答案，让我们从那里开始吧。然后，我们可以查看一个应用程序以获得更深入的了解。</p><p>矩阵是由括号括起的n×m（例如，3×3）个数字的网格。我们可以加上和减去相同大小的矩阵，只要大小兼容（（n×m）×（m×p）= n×p），就将一个矩阵与另一个矩阵相乘，以及可以将整个矩阵乘以常数。向量是一个只有一行或一列的矩阵（但见下文）。因此，我们可以对任何矩阵进行一系列数学运算。</p><p>不过，基本的思想是，矩阵只是一个二维的数字网格。</p><p>张量通常被认为是一个广义矩阵。也就是说，它可以是1-D矩阵（一个向量实际上就是一个张量），3-D矩阵（类似于一个数字的立方），甚至是0-D矩阵（单个数字），或者一个更难形象化的高维结构。张量的维数叫做它的秩。</p><p>但是这个描述忽略了张量最重要的性质!</p><p>张量是一个数学实体，它存在于一个结构中并与其他数学实体相互作用。如果以常规方式转换结构中的其他实体，那么张量必须服从一个相关的变换规则。</p><p>张量的这种“动态”特性是将其与单纯矩阵区分开来的关键。它是一个团队成员，当一个影响到所有成员的转换被引入时，它的数值会随着队友的数值而变化。</p><p>任何秩-2张量都可以表示为一个矩阵，但并不是每个矩阵都是秩-2张量。张量矩阵表示的数值取决于整个系统应用了什么变换规则。</p><p>对于您的目的，这个答案可能已经足够了，但是我们可以通过一个小例子来说明它是如何工作的。这个问题是在一个深度学习研讨会上提出的，所以让我们看一下该领域的一个简单例子。</p><p>假设我在神经网络中有一个隐藏的3个节点层。数据流入它们，通过它们的ReLU函数，然后弹出一些值。对于确定性，我们分别得到2.5,4和1.2。 （别担心，图表即将出现。）我们可以将这些节点的输出表示为向量，</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=矩阵与张量的区别 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1ff82d7f948943e08f28939b58ea6d2f><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>假设有另外一层3个节点。第一层的3个节点中的每个节点都有一个权重，该权重与其对接下来3个节点的输入相关联。那么，将这些权重写为3×3矩阵的条目将是非常方便的。假设我们已经对网络进行了多次更新，并得到了权重（本例中半随机选择）。</p><p>在这里，一行的权值都到下一层的同一个节点，而某一列的权值都来自第一层的同一个节点。例如，输入节点1对输出节点3的权值是0.2（第3行，第1列）。 我们可以通过将权重矩阵乘以输入向量来计算馈入下一层节点的总值，</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=矩阵与张量的区别 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7251acac0ae04854a1bb376994569dd2><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>不喜欢矩阵?这里有一个图。数据从左到右流动。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=矩阵与张量的区别 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b88bc4019e46462b8e01e21f9d9d6e72><p class=pgc-img-caption></p></div><p class=ql-align-center>太棒了!到目前为止，我们所看到的只是矩阵和向量的一些简单操作。</p><p>但是，假设我想对每个神经元进行干预并使用自定义激活函数。一种简单的方法是从第一层重新缩放每个ReLU函数。在本例中，假设我将第一个节点向上扩展2倍，保留第二个节点，将第三个节点向下扩展1/5。这将改变这些函数的图形如下图所示:</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=矩阵与张量的区别 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8b617edac5904a4282dc5b1887968910><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>这种修改的效果是将第一层生成的值分别乘以2、1和1/5。等于L1乘以一个矩阵A，</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=矩阵与张量的区别 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f97b937912cb4200bfdd1aab9d21db9d><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>现在，如果这些新值通过原来的权值网络被输入，我们得到完全不同的输出值，如图所示:</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=矩阵与张量的区别 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dad4138193c946d1adc2cc43ee50cd45><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>如果神经网络之前运作正常，我们现在就把它破坏了。我们必须重新进行训练以恢复正确的重量。</p><p>或者我们会吗？</p><p>第一个节点的值是之前的两倍。 如果我们将所有输出权值减少1/2，则它对下一层的净贡献不变。我们没有对第二个节点做任何处理，所以我们可以不考虑它的权值。最后，我们需要将最后一组权值乘以5，以补偿该节点上的1/5因子。从数学上讲，这相当于使用一组新的权值，我们通过将原权矩阵乘以A的逆矩阵得到:</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=矩阵与张量的区别 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/83e7e74c6fde406f87a97b40b09dd08c><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>如果我们将第一层的修改后的输出与修改后的权值结合起来，我们最终会得到到达第二层的正确值：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=矩阵与张量的区别 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/95a4bd4b4dc04c2fa5b48eb74746a6d0><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>太好了！尽管我们做出了最大努力，但网络仍在重新运作！</p><p>好了，我们已经学了很多数学了，让我们回顾一下。</p><p>当我们把节点的输入，输出和权值看作固定的量时，我们称它们为向量和矩阵，并用它完成。</p><p>但是，一旦我们开始用其中一个向量进行修复，以常规方式对其进行转换，我们就必须通过相反的方式转换权值来进行补偿。这个附加的、集成的结构将单纯的数字矩阵提升为一个真正的张量对象。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=矩阵与张量的区别 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3bb2637c56cc46c4b2845b546efd3b6e><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>事实上，我们可以进一步描述它的张量性质。如果我们把对节点的变化称为协变(即，随着节点的变化而乘以A)，那么权值就变成了一个逆变张量(具体来说，对节点变化，乘以A的倒数而不是A本身)。张量可以在一个维度上是协变的，在另一个维度上是逆变的，但那是另外的事了。</p><p>现在你知道了矩阵和张量之间的区别了吧。</p><p>文章原标题《 What ’ s the difference between a matrix and a tensor?》</p><p>作者：Steven Steinke 译者：Viola，审校：。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'矩阵','张量','区别'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>