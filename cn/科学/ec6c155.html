<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>ECCV 2020附代码论文合集(目标检测） | 极客快訊</title><meta property="og:title" content="ECCV 2020附代码论文合集(目标检测） - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/31e617a451a4464484945f89ca231e68"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/ec6c155.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/ec6c155.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/ec6c155.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/ec6c155.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/ec6c155.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/ec6c155.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/ec6c155.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/ec6c155.html><meta property="article:published_time" content="2020-10-29T20:56:20+08:00"><meta property="article:modified_time" content="2020-10-29T20:56:20+08:00"><meta name=Keywords content><meta name=description content="ECCV 2020附代码论文合集(目标检测）"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/ec6c155.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>ECCV 2020附代码论文合集(目标检测）</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><div class=pgc-img><img alt="ECCV 2020附代码论文合集(目标检测）" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/31e617a451a4464484945f89ca231e68><p class=pgc-img-caption></p></div><p style=text-align:center><br></p><p>上次我们给大家带来了关于CNN与图像分割主题的论文，本次的主题是目标检测，包含2D、3D的目标检测，旋转目标检测，视频目标检测，同样每篇论文都附带论文代码，大家在阅读论文的同时也可亲自动手实践，快来看看吧！</p><p><br></p><h1 class=pgc-h-arrow-right>目标检测</h1><p style=text-align:center><strong><br></strong></p><p>目标检测是与计算机视觉与图像处理的计算机技术，处理的是在数字图像和视频中检测出的特定类别的语义对象（如人类、建筑物或汽车）的实例。目标检测的研究领域包括人脸检测和行人检测。目标检测在计算机视觉的许多领域都有应用，包括图像检索和视频监控 。</p><p><br></p><h1 class=pgc-h-arrow-right>2D目标检测</h1><p><strong><br></strong></p><p><em><strong>1 Dense RepPoints: Representing Visual Objects with Dense Point Sets</strong></em><br><strong>作者：</strong>Yang Ze,Xu Yinghao,Xue Han,Zhang Zheng,Urtasun Raquel,Wang Liwei,Lin Stephen,Hu Han<br><strong>机构：</strong>北京大学，香港中文大学<br><strong>简介：</strong>本文提出了一种对象表示法，称为dense Rep Points，用于灵活而详细地建模对象外观和几何体。与边界框的粗几何定位和特征提取不同，DenseRepPoints自适应地将一组密集的点分布到对象上具有重要几何意义的位置，为对象分析提供信息提示。技术的发展是为了解决与监督训练从图像片段和符号密集点集相关的挑战，并使这种广泛的表示在理论上是可行的。此外，该表示的多功能性被用于在多个粒度级别上建模对象结构。稠密的表示点显著提高了面向几何的可视化理解任务的性能，包括在具有挑战性的COCO基准测试中对象检测的1:6AP增益。</p><p style=text-align:justify><br></p><div class=pgc-img><img alt="ECCV 2020附代码论文合集(目标检测）" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3475fd6f304f4b7daf14ccb1641adde3><p class=pgc-img-caption></p></div><p style=text-align:center><br></p><p><strong>论文链接：</strong>https://www.aminer.cn/pub/5e0333623a55aca24ec3eeed/<br><strong>代码地址：</strong>https://github.com/justimyhxu/Dense-RepPoints</p><p style=text-align:justify><br></p><p style=text-align:justify><br><em><strong>2 Corner Proposal Network for Anchor-free, Two-stage Object Detection</strong></em><br><strong>作者：</strong>Kaiwen Duan,Lingxi Xie,Honggang Qi,Song Bai,Qingming Huang,Qi Tian<br><strong>机构：</strong>中国科学院大学，华为<br><strong>简介：</strong>目标检测的目标是确定目标在图像中的类别和位置。本文提出了一种新的无锚的两阶段框架，该框架首先通过寻找潜在的角点组合来提取多个目标方案，然后通过独立的分类阶段为每个方案分配一个类别标签。作者证明这两个阶段分别是提高查全率和查准率的有效解决方案，并且可以集成到一个端到端网络中。他们的方法被称为角点建议网络（Corner proposition Network，CPN），它具有检测不同尺度对象的能力，并且避免了被大量的误报建议所迷惑。在MS-COCO数据集上，CPN达到了49.2%的AP，这在现有的目标检测方法中具有竞争力。CPN同样适用于计算效率的场景，在26.2/43.3fps时，CPN的AP达到41.6%/39.7%，超过了大多数具有相同推理速度的竞争对手。</p><p style=text-align:justify><br></p><div class=pgc-img><img alt="ECCV 2020附代码论文合集(目标检测）" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3197ba7fc7674e0983b65a605b5cc130><p class=pgc-img-caption></p></div><p style=text-align:center><br></p><p><strong>论文链接：</strong>https://www.aminer.cn/pub/5f213ebe91e011f62007af97/<br><strong>代码地址：</strong>https://github.com/Duankaiwen/CPNDet</p><p><br></p><p><br><em><strong>3 BorderDet: Border Feature for Dense Object Detection</strong></em><br><strong>作者：</strong>Han Qiu,Yuchen Ma,Zeming Li,Songtao Liu,Jian Sun<br><strong>机构：</strong>旷视科技，西安交通大学<br><strong>简介：</strong>密集型目标探测器依赖于滑动窗口模式，它可以在规则的图像网格上预测目标。同时，采用网格点上的特征映射生成边界盒预测。点特征使用方便，但可能缺少精确定位的明确边界信息。本文提出了一种简单高效的边界对齐算子，从边界的极值点提取“边界特征”，以增强点特征。在BorderAlign的基础上，作者设计了一种新的检测体系结构BorderDet，它明确地利用了边界信息来实现更强的分类和更精确的定位。使用ResNet-50主干，他们的方法将单级探测器FCOS提高了2.8 AP增益（38.6 v.s.41.4）。通过ResNeXt-101-DCN主干，他们的BorderDet获得了50.3 AP，优于现有的最新方法。</p><p><br></p><div class=pgc-img><img alt="ECCV 2020附代码论文合集(目标检测）" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/488d7e1c896f47e5943ca21f2bf929c9><p class=pgc-img-caption></p></div><p><br></p><p><strong>论文链接：</strong>https://www.aminer.cn/pub/5f19565a91e01182befeea14/<br><strong>代码地址：</strong>https://github.com/Megvii-BaseDetection/BorderDet</p><p style=text-align:justify><br></p><p style=text-align:justify><br><em><strong>4 Multi-Scale Positive Sample Refinement for Few-Shot Object Detection</strong></em><br><strong>作者：</strong>Jiaxi Wu,Songtao Liu,Di Huang,Yunhong Wang<br><strong>机构：</strong>北京航空航天大学<br><strong>简介：</strong>少镜头目标检测（FSOD）有助于检测器适应训练实例较少的看不见的类，在手动标注耗时或数据采集受限的情况下非常有用。与以往利用少量镜头分类技术来促进FSOD的尝试不同，本研究强调了处理尺度变化问题的必要性，该问题由于样本分布的独特性而具有挑战性。为此，作者提出了一种多尺度正样本优化（MPSR）方法来丰富FSOD中的目标尺度。它生成多尺度正样本作为目标金字塔，并在不同尺度上对预测进行细化。作者通过将其作为一个辅助分支集成到流行的快速R-CNN和FPN架构中，展示了它的优势，提供了一个强大的FSOD解决方案。在PASCAL-VOC和MS-COCO上进行了多个实验，结果表明，该方法取得了最新的结果，显著优于其他同类方法，显示了其有效性。</p><p style=text-align:justify><br></p><div class=pgc-img><img alt="ECCV 2020附代码论文合集(目标检测）" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6aaad78223894789a4ac18c46de86848><p class=pgc-img-caption></p></div><p style=text-align:center><br></p><p><strong>论文链接：</strong>https://www.aminer.cn/pub/5f16b7ca91e011b48ae9413e/<br><strong>代码地址：</strong>https://github.com/jiaxi-wu/MPSR</p><p><br></p><p><br><em><strong>5 PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments</strong></em><br><strong>作者：</strong>Zhiming Chen,Kean Chen,Weiyao Lin,John See,Hui Yu,Yan Ke,Cong Yang<br><strong>机构：</strong>扩博智能，上海交通大学<br><strong>简介：</strong>使用定向包围盒（OBB）进行目标检测，可以减少与背景区域的重叠，从而更好地定位旋转对象。现有的OBB方法大多建立在水平包围盒探测器上，通过引入一个额外的角度维度，通过距离损失来优化。然而，由于距离损失只会最小化OBB的角度误差，并且它与IoU松散相关，因此它对高宽高比的对象不敏感。因此，提出了一种新的损失，像素IoU（PIoU）损失，利用角度和IoU进行精确的OBB回归。PIoU损失由IoU度量导出，采用像素形式，简单易行，适用于水平和定向包围盒。为了证明其有效性，作者评估了基于锚定和无锚框架的PIoU损失。实验结果表明，PIoU损耗可以显著提高OBB探测器的性能，特别是对于高宽高比和复杂背景的目标。此外，以前的评估数据集不包括对象具有高宽高比的场景，因此引入了一个新的数据集Retail50K，以鼓励社区采用OBB检测器来适应更复杂的环境。</p><p><br></p><div class=pgc-img><img alt="ECCV 2020附代码论文合集(目标检测）" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e2c86c881e234017875f12c8c1eb766d><p class=pgc-img-caption></p></div><p><br></p><p><strong>论文链接：</strong>https://www.aminer.cn/pub/5f16be7c91e011b48ae94206/<br><strong>代码地址：</strong>https://github.com/clobotics/piou</p><p><br></p><p><br><em><strong>6 Boosting Weakly Supervised Object Detection with Progressive Knowledge Transfer</strong></em><br><strong>论文链接：</strong>https://www.aminer.cn/pub/5f116cbb91e011264d4475a6/<br><strong>代码地址：</strong>https://github.com/mikuhatsune/wsod_transfer</p><p><br></p><p><br><em><strong>7 Probabilistic Anchor Assignment with IoU Prediction for Object Detection</strong></em><strong><br>论文链接：</strong>https://www.aminer.cn/pub/5f11708491e011264d44761b/<br><strong>代码地址：</strong>https://github.com/kkhoot/PAA</p><p><br></p><p><br><em><strong>8 HoughNet: Integrating near and long-range evidence for bottom-up object detection</strong></em><br><strong>论文链接：</strong>https://www.aminer.cn/pub/5f044d8d91e0114d4aaa49dc/<br><strong>代码地址：</strong>https://github.com/nerminsamet/houghnet<br></p><p><em><strong>9 OS2D: One-Stage One-Shot Object Detection by Matching Anchor Features</strong></em><br><strong>论文链接：</strong>https://www.aminer.cn/pub/5e71f49891e0115656f5cfcb/<br><strong>代码地址：</strong>https://github.com/aosokin/os2d<br></p><p><em><strong>10 End-to-End Object Detection with Transformers</strong></em><br><strong>论文链接：</strong>https://www.aminer.cn/pub/5ece3bcb91e011dc23c22581/<br><strong>代码地址：</strong>https://github.com/facebookresearch/detr<br></p><p><em><strong>11 Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training</strong></em><br><strong>论文链接：</strong>https://www.aminer.cn/pub/5e96db3891e01129d1a04120/<br><strong>代码地址：</strong>https://github.com/hkzhang95/DynamicRCNN</p><p><br></p><h1 class=pgc-h-arrow-right>遥感旋转目标检测</h1><p style=text-align:justify><strong><br></strong></p><p style=text-align:justify><em><strong>1 Arbitrary-Oriented Object Detection with Circular Smooth Label</strong></em><strong><br>作者：</strong>Yang Xue,Yan Junchi<br><strong>机构：</strong>上海交通大学<br><strong>简介：</strong>任意方向的目标检测由于在航空图像、场景文本、人脸等方面的重要性，近年来在视觉领域引起了越来越多的关注。本文研究了现有的基于回归的旋转检测器存在边界不连续的问题，这是由角周期性或角点排序直接引起的。通过仔细研究，作者发现其根本原因是理想的预测超出了规定的范围。作者设计了一个新的旋转检测基线，通过将角度预测从回归问题转化为一个精度损失很小的分类任务来解决边界问题，与以往使用粗粒度旋转检测的工作相比，设计了高精度的角度分类。他们还提出了一种圆形平滑标签（CSL）技术来处理角度的周期性，并增加了对相邻角的误差容限。进一步介绍了CSL中的四个窗口函数，并探讨了不同窗口半径对检测性能的影响。对DOTA、HRSC2016以及场景文本数据集ICDAR2015和MLT进行了大量的实验和可视化分析，证明了该方法的有效性。</p><p style=text-align:justify><br></p><div class=pgc-img><img alt="ECCV 2020附代码论文合集(目标检测）" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4cf2835d14aa4186a00156b52f1d3356><p class=pgc-img-caption></p></div><p><br></p><p><strong>论文链接：</strong>https://www.aminer.cn/pub/5e6cacc991e01145573c766b/<br><strong>代码地址：</strong>https://github.com/Thinklab-SJTU/CSL_RetinaNet_Tensorflow</p><p style=text-align:justify><br></p><h1 class=pgc-h-arrow-right>3D目标检测</h1><p><br></p><p><br><em><strong>1 Rethinking Pseudo-LiDAR Representation</strong></em><br><strong>作者：</strong>Xinzhu Ma,Shinan Liu,Zhiyi Xia,Hongwen Zhang,Xingyu Zeng,Wanli Ouyang<br><strong>机构：</strong>悉尼大学，商汤科技<br><strong>简介：</strong>最近提出的基于伪激光雷达的三维探测器大大提高了单目/立体三维探测任务的基准。然而，研究界对其潜在的机制仍不甚清楚。本文对伪激光雷达的数据表示进行了深入研究，发现伪激光雷达表示的有效性来自于座标变换，而不是数据表示本身。在此基础上，作者设计了一种基于图像的CNN探测器Patch-Net，它是一种更通用的、可以实例化为基于伪激光雷达的3D探测器。此外，本文的PatchNet中的伪激光雷达数据被组织为图像表示，这意味着现有的2D CNN设计可以很容易地用于从输入数据中提取深层特征并提高3D检测性能。作者在具有挑战性的KITTI数据集上进行了大量的实验，其中提出的PatchNet优于所有现有的基于伪激光雷达的同类产品。</p><p><br></p><div class=pgc-img><img alt="ECCV 2020附代码论文合集(目标检测）" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/28b5bc7a9beb4f2cac3ef98e6e87ad7d><p class=pgc-img-caption></p></div><p><br></p><p><strong>论文链接：</strong>https://www.aminer.cn/pub/5f33bd4591e011861cfa0fe6/<br><strong>论文地址：</strong>https://github.com/xinzhuma/patchnet</p><p style=text-align:justify><br></p><p style=text-align:justify><br><em><strong>2 Pillar-based Object Detection for Autonomous Driving</strong></em><br><strong>作者：</strong>Yue Wang,Alireza Fathi,Abhijit Kundu,David Ross,Caroline Pantofaru,Tom Funkhouser,Justin Solomon<br><strong>机构：</strong>MIT，Google<br><strong>简介：</strong>本文提出了一种简单灵活的自动驾驶目标检测框架。在观察到该应用中的点云非常稀疏的基础上，提出了一种实用的基于柱的方法来解决锚定引起的不平衡问题。特别地，本文的算法在多视点特征学习中加入了柱面投影，预测了每个柱而不是每个点或每个锚点的边界盒参数，并且包含了一个对齐的柱到点投影模块来提高最终预测。本文的无锚方法避免了与以往方法相关的超参数搜索，简化了三维目标检测，同时显著提高了最先进的水平。</p><p style=text-align:justify><br></p><div class=pgc-img><img alt="ECCV 2020附代码论文合集(目标检测）" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4d00911d73c745c78cf7d7b09f610959><p class=pgc-img-caption></p></div><p><br></p><p><strong>论文链接:</strong>https://www.aminer.cn/pub/5f16d62b91e011b48ae944e9/<br><strong>代码地址：</strong>https://github.com/WangYueFt/pillar-od</p><p><br></p><p><br><em><strong>3 EPNet: Enhancing Point Features with Image Semantics for 3D Object Detection</strong></em><br><strong>作者：</strong>Tengteng Huang,Zhe Liu,Xiwu Chen,Xiang Bai<br><strong>机构：</strong>华中科技大学<br><strong>简介：</strong>本文针对三维检测任务中的两个关键问题，即多传感器（即LiDAR点云和相机图像）的开发以及定位和分类置信度之间的不一致性。为此，作者提出了一种新的融合模块，在不需要任何图像注释的情况下，对具有语义特征的点特征进行逐点增强。此外，使用一致性强制损失来明确鼓励本地化和分类可信度的一致性。作者设计了一个端到端的可学习框架EPNet来集成这两个组件。在KITTI和SUN-RGBD数据集上进行的大量实验证明了EPNet优于最先进的方法。</p><p><br></p><div class=pgc-img><img alt="ECCV 2020附代码论文合集(目标检测）" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b5e8d146e40643b8a9dbbd7322c7db1b><p class=pgc-img-caption></p></div><p style=text-align:center><br></p><p><strong>论文链接：</strong>https://www.aminer.cn/pub/5f156bfa91e011d7db223ac2/<br><strong>代码地址：</strong>https://github.com/happinesslz/EPNet</p><p><br></p><h1 class=pgc-h-arrow-right>视频目标检测</h1><p style=text-align:justify><strong><br></strong></p><p style=text-align:justify><em><strong>1 Learning Where to Focus for Efficient Video Object Detection</strong></em><br><strong>作者：</strong>Zhengkai Jiang,Y. Liu,Ceyuan Yang,Jihao Liu, Peng Gao,Qian Zhang,Shiming Xiang,C. Pan<br><strong>机构：</strong>腾讯<br><strong>简介：</strong>将现有的基于图像的检测器转移到视频中是非常重要的，因为部分遮挡、罕见姿势和运动模糊会导致帧质量下降。以前的方法利用光流翘曲在视频帧间传播和聚集特征。然而，直接将图像级光流应用于高层特征可能无法建立精确的空间对应关系。为此，提出了一种新的可学习时空采样（LSTS）模块来准确地学习相邻帧特征之间的语义级对应关系。首先对采样点进行随机初始化，然后迭代更新，在检测监督的指导下逐步寻找更好的空间对应关系。此外，还分别引入稀疏递归特征更新（SRFU）模块和密集特征聚合（DFA）模块来建模时间关系和增强每帧特征。该方法在imagenetvid数据集上实现了最先进的性能，计算复杂度和实时速度都很低。</p><p style=text-align:justify><br></p><div class=pgc-img><img alt="ECCV 2020附代码论文合集(目标检测）" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/669adf852afe488592aa151c7a4f9087><p class=pgc-img-caption></p></div><p style=text-align:center><br></p><p><strong>论文链接：</strong>https://arxiv.org/pdf/1911.05253.pdf<br><strong>代码地址：</strong>https://github.com/jiangzhengkai/LSTS</p><p><br></p><p><br>AMiner 会议智图开放平台 ECCV 2020 专题了解会议的精彩内容，其内容包括论文、作者、华人学者、一作华人学生、论文 PPT 和视频等多维分析服务，是参会学者的会议智能助理。</p><p><br></p><p><br>顶会专题链接：https://www.aminer.cn/conf/eccv2020</p><p><br></p><div class=pgc-img><img alt="ECCV 2020附代码论文合集(目标检测）" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/589f2bac19d24f36a335895555259828><p class=pgc-img-caption></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'ECCV','2020','附代码'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>