<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>在Python中计算神经网络的雅可比矩阵 | 极客快訊</title><meta property="og:title" content="在Python中计算神经网络的雅可比矩阵 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/acc2a9f4315f4fe8a3c6e03201f66725"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/f380664.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/f380664.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/f380664.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/f380664.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/f380664.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/f380664.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/f380664.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/f380664.html><meta property="article:published_time" content="2020-10-29T21:01:39+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:39+08:00"><meta name=Keywords content><meta name=description content="在Python中计算神经网络的雅可比矩阵"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/f380664.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>在Python中计算神经网络的雅可比矩阵</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><div><p>通常，神经网络是一个多变量，矢量值函数，如下所示：</p><div class=pgc-img><img alt=在Python中计算神经网络的雅可比矩阵 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/acc2a9f4315f4fe8a3c6e03201f66725><p class=pgc-img-caption></p></div><p>函数f有一些参数θ(神经网络的权重),它将一个N维向量x(即猫图片的N像素)映射到一个m维矢量(例如，x属于M个不同类别中的每个类别的概率):</p><div class=pgc-img><img alt=在Python中计算神经网络的雅可比矩阵 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a6165c8d00564d0384036323611be3e3><p class=pgc-img-caption></p></div><p>在训练过程中，通常会给输出附加一个标量损失值——分类的一个典型例子是预测类概率的交叉熵损失。当使用这样的标量损失时，M = 1，然后通过执行（随机）梯度下降来学习参数，在此期间重复计算相对于θ的损失函数的梯度。因此，在训练期间计算标量输出值相对于网络参数的梯度是很常见的，并且所有常见的机器学习库都可以这样做，通常使用自动微分非常有效。</p><p>然而,在推理时网络的输出通常是一个向量(例如,类概率)。在这种情况下，查看网络的雅可比矩阵可能会很有趣。在这篇文章中,我解释一下什么是雅可比矩阵,然后我探索和比较一些可能实现用Python来完成。</p><h1>什么是雅可比矩阵，为什么我们会关心？</h1><p>我们称y为f的输出向量。f的雅可比矩阵包含y的每个元素的偏导数，相对于输入x的每个元素：</p><div class=pgc-img><img alt=在Python中计算神经网络的雅可比矩阵 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fd76cd21e4384391884f264d0daadde3><p class=pgc-img-caption></p></div><p>该矩阵告诉我们神经网络输入的local perturbations将如何影响输出。在某些情况下，这些信息可能很有价值。例如，在用于创造性任务的机器学习（ML）系统中，让系统为用户提供一些交互式反馈可以很方便，告诉他们修改每个输入维度将如何影响每个输出类。</p><h1>Tensorflow</h1><p>让我们一起尝试使用Tensorflow。首先，我们需要一个示例网络来玩。在这里，我只对在测试时计算现有网络f的雅可比行列式感兴趣，所以我不专注于训练。假设我们有一个简单的网络[affine → ReLU → affine → softmax]。我们首先定义一些随机参数：</p><pre>import numpy as npN = 500 # Input sizeH = 100 # Hidden layer sizeM = 10 # Output sizew1 = np.random.randn(N, H) # first affine layer weightsb1 = np.random.randn(H) # first affine layer biasw2 = np.random.randn(H, M) # second affine layer weightsb2 = np.random.randn(M) # second affine layer bias</pre><div class=pgc-img><img alt=在Python中计算神经网络的雅可比矩阵 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d38e258a3cad4144812929a9e776a9b8><p class=pgc-img-caption></p></div><p>使用Keras，我们按如下方式实施我们的神经网络：</p><pre>import tensorflow as tffrom tensorflow.keras.layers import Densesess = tf.InteractiveSession()sess.run(tf.initialize_all_variables())model = tf.keras.Sequential()model.add(Dense(H, activation='relu', use_bias=True, input_dim=N))model.add(Dense(O, activation='softmax', use_bias=True, input_dim=O))model.get_layer(index=0).set_weights([w1, b1])model.get_layer(index=1).set_weights([w2, b2])</pre><div class=pgc-img><img alt=在Python中计算神经网络的雅可比矩阵 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5c47bad01b124d9eb92de1f7a0c6201d><p class=pgc-img-caption></p></div><p>现在让我们尝试计算这个神经网络模型的雅可比行列式。不幸的是，Tensorflow目前没有提供一种开箱即用的雅可比矩阵计算方法。方法tf.gradients(y,xs)为xs的每一个x都返回sum(dy / dx)，在我们的例子中，是n维的矢量，不是我们想要的。然而，通过计算每个yi的梯度矢量，我们仍然可以计算出雅可比矩阵，并且将输出分组为矩阵:</p><pre>def jacobian_tensorflow(x):  jacobian_matrix = [] for m in range(M): # We iterate over the M elements of the output vector grad_func = tf.gradients(model.output[:, m], model.input) gradients = sess.run(grad_func, feed_dict={model.input: x.reshape((1, x.size))}) jacobian_matrix.append(gradients[0][0,:])  return np.array(jacobian_matrix)</pre><div class=pgc-img><img alt=在Python中计算神经网络的雅可比矩阵 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c23921834983444fa69c05e3db2ed801><p class=pgc-img-caption></p></div><p>我们用数值微分来检验雅可比矩阵的正确性。下面的函数is_jacobian_correct()采用一个函数来计算雅可比矩阵和前馈函数f:</p><pre>def is_jacobian_correct(jacobian_fn, ffpass_fn): """ Check of the Jacobian using numerical differentiation """ x = np.random.random((N,)) epsilon = 1e-5 """ Check a few columns at random """ for idx in np.random.choice(N, 5, replace=False): x2 = x.copy() x2[idx] += epsilon num_jacobian = (ffpass_fn(x2) - ffpass_fn(x)) / epsilon computed_jacobian = jacobian_fn(x)  if not all(abs(computed_jacobian[:, idx] - num_jacobian) &lt; 1e-3):  return False return Truedef ffpass_tf(x): """ The feedforward function of our neural net """  xr = x.reshape((1, x.size)) return model.predict(xr)[0]is_jacobian_correct(jacobian_tensorflow, ffpass_tf)&gt;&gt; True</pre><div class=pgc-img><img alt=在Python中计算神经网络的雅可比矩阵 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5ded31e5cad14e7cbeba98cb65c42a97><p class=pgc-img-caption></p></div><p>非常好，这是正确的。让我们看看这个计算需要多长时间：</p><pre>tic = time.time()jacobian_tf = jacobian_tensorflow(x0, verbose=False)tac = time.time()print('It took %.3f s. to compute the Jacobian matrix' % (tac-tic))&gt;&gt; It took 0.658 s. to compute the Jacobian matrix</pre><div class=pgc-img><img alt=在Python中计算神经网络的雅可比矩阵 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/da38007bbf3a42aaa39f60f0a30826bd><p class=pgc-img-caption></p></div><p>用CPU需要大约650毫秒。650 ms对于这样的示例来说太慢了，特别是如果我们在测试时考虑到交互式使用，那么是否可以做得更好呢？</p><h1>Autograd</h1><p>Autograd是一个非常好的Python库。要使用它，我们首先必须使用Autograd的封装Numpy 指定我们的前馈函数f：</p><pre>import autograd.numpy as anpdef ffpass_anp(x): a1 = anp.dot(x, w1) + b1 # affine a1 = anp.maximum(0, a1) # ReLU a2 = anp.dot(a1, w2) + b2 # affine  exps = anp.exp(a2 - anp.max(a2)) # softmax out = exps / exps.sum() return out</pre><div class=pgc-img><img alt=在Python中计算神经网络的雅可比矩阵 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ea4426a673a2480caeeea755fb77c959><p class=pgc-img-caption></p></div><p>首先，让我们通过比较之前的Tensorflow前馈函数ffpass_tf()来检查这个函数是否正确。</p><pre>out_anp = ffpass_anp(x0)out_keras = ffpass_tf(x0)np.allclose(out_anp, out_keras, 1e-4)&gt;&gt; True</pre><p>好的,我们有相同的函数f。现在让我们计算雅可比矩阵。Autograd很简单：</p><pre>from autograd import jacobiandef jacobian_autograd(x): return jacobian(ffpass_anp)(x)is_jacobian_correct(jacobian_autograd, ffpass_np)&gt;&gt; True</pre><div class=pgc-img><img alt=在Python中计算神经网络的雅可比矩阵 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f63400bd5683474f9128b9fbcc831361><p class=pgc-img-caption></p></div><p>看起来很正确。需要多长时间呢？</p><pre>%timeit jacobian_autograd(x0)&gt;&gt; 3.69 ms ± 135 µs</pre><p>我们的Tensorflow实现大约需要650毫秒，Autograd需要3.7毫秒，在这种情况下我们的速度提高了约170倍。当然，使用Numpy指定一个模型并不是很方便，因为Tensorflow和Keras提供了许多有用的函数和开箱即用的训练设施......，但是现在我们越过这一步并使用Numpy编写我们的网络，我们是不是会让它更快呢？如果你看一下Autograd的jacobian()函数的实现，它仍然是在函数输出的维度上映射的。这是一个提示，我们可以通过直接依靠Numpy更好的矢量化来改善我们的结果。</p><h1>NumPy</h1><p>如果我们想要一个适当的Numpy实现，我们必须指定每个层的forward 和backward passes，以便自己实现backprop。下面的示例网络包含 - affine，ReLU和softmax。这里的层的实现是非常通用的。</p><p>基本上，backward pass现在传播包含每个网络输出的梯度的矩阵（或者，在一般情况下，张量），我们使用Numpy高效矢量化操作：</p><pre>def affine_forward(x, w, b): """ Forward pass of an affine layer :param x: input of dimension (I, ) :param w: weights matrix of dimension (I, O) :param b: biais vector of dimension (O, ) :return output of dimension (O, ), and cache needed for backprop """ out = np.dot(x, w) + b cache = (x, w) return out, cachedef affine_backward(dout, cache): """ Backward pass for an affine layer. :param dout: Upstream Jacobian, of shape (M, O) :param cache: Tuple of: - x: Input data, of shape (I, ) - w: Weights, of shape (I, O) :return the jacobian matrix containing derivatives of the M neural network outputs with respect to this layer's inputs, evaluated at x, of shape (M, I) """ x, w = cache dx = np.dot(dout, w.T) return dxdef relu_forward(x): """ Forward ReLU """ out = np.maximum(np.zeros(x.shape), x) cache = x return out, cachedef relu_backward(dout, cache): """ Backward pass of ReLU :param dout: Upstream Jacobian :param cache: the cached input for this layer :return: the jacobian matrix containing derivatives of the M neural network outputs with respect to this layer's inputs, evaluated at x. """ x = cache dx = dout * np.where(x &gt; 0, np.ones(x.shape), np.zeros(x.shape)) return dxdef softmax_forward(x): """ Forward softmax """ exps = np.exp(x - np.max(x)) s = exps / exps.sum() return s, s def softmax_backward(dout, cache): """ Backward pass for softmax :param dout: Upstream Jacobian :param cache: contains the cache (in this case the output) for this layer """ s = cache ds = np.diag(s) - np.outer(s, s.T) dx = np.dot(dout, ds) return dx</pre><div class=pgc-img><img alt=在Python中计算神经网络的雅可比矩阵 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5d153fa3c9504db490a91599efb921d7><p class=pgc-img-caption></p></div><p>现在我们已经定义了层，让我们在forward 和backward passes中使用它们：</p><pre>def forward_backward(x): layer_to_cache = dict() # for each layer, we store the cache needed for backward pass # Forward pass a1, cache_a1 = affine_forward(x, w1, b1) r1, cache_r1 = relu_forward(a1) a2, cache_a2 = affine_forward(r1, w2, b2) out, cache_out = softmax_forward(a2) # backward pass dout = np.diag(np.ones(out.size, )) # the derivatives of each output w.r.t. each output. dout = softmax_backward(dout, cache_out) dout = affine_backward(dout, cache_a2) dout = relu_backward(dout, cache_r1) dx = affine_backward(dout, cache_a1)  return out, dx</pre><div class=pgc-img><img alt=在Python中计算神经网络的雅可比矩阵 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/94fbf45f00314a49a6cb5c7c2d485d92><p class=pgc-img-caption></p></div><p>前馈输出是否正确？</p><pre>out_fb = forward_backward(x0)[0]out_tf = ffpass_tf(x0)np.allclose(out_fb, out_tf, 1e-4)&gt;&gt; True</pre><p>我们的雅可比矩阵是否正确？</p><pre>is_jacobian_correct(lambda x: forward_backward(x)[1], ffpass_tf)&gt;&gt; True</pre><p>需要多长时间呢？</p><pre>%timeit forward_backward(x0)&gt;&gt; 115 µs ± 2.38 µs</pre><p>在Autograd需要3.7 ms的情况下，我们现在只需要115μs。</p><h1>结论</h1><p>我已经探索了几种可能的方法来计算雅可比矩阵，在CPU上使用Tensorflow，Autograd和Numpy。每种方法都有不同的优缺点。如果您已准备好指定层的forward 和backward passes，您可以直接使用Numpy获得更好性能 。</p><p>我发现这很有用，因为网络需要在测试时有效运行是很常见的。在这些情况下，花一点时间来确保它们的实现是有效的是值得的。当需要以交互方式计算雅可比矩阵时，这更是如此。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'Python','中计算','神经'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>