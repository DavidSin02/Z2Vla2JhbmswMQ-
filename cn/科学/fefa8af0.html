<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 | 极客快訊</title><meta property="og:title" content="谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p9.pstatp.com/large/pgc-image/15836ed03ab44577b6f20b3a0556ef0c"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/fefa8af0.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/fefa8af0.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/fefa8af0.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/fefa8af0.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/fefa8af0.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/fefa8af0.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%a6/fefa8af0.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%a6/fefa8af0.html><meta property="article:published_time" content="2020-11-14T20:56:31+08:00"><meta property="article:modified_time" content="2020-11-14T20:56:31+08:00"><meta name=Keywords content><meta name=description content="谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E5%AD%A6/fefa8af0.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E5%AD%A6.html>科学</a></span></div><div class=post-content><div><p>栗子 晓查 发自 凹非寺</p><p>量子位 报道 | 公众号 QbitAI</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/15836ed03ab44577b6f20b3a0556ef0c><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>机器学习全靠调参？这个思路已经过时了。</p><p>谷歌大脑团队发布了一项新研究：</p><p>只靠<strong>神经网络架构</strong>搜索出的网络，不训练，不调参，就能直接执行任务。</p><p>这样的网络叫做<strong>WANN</strong>，权重不可知神经网络。</p><p>它在MNIST数字分类任务上，未经训练和权重调整，就达到了92%的准确率，和训练后的线性分类器表现相当。</p><p>除了<strong>监督学习</strong>，WANN还能胜任许多强化<strong>学习任务</strong>。</p><p>团队成员之一的大佬David Ha，把成果发上了推特，已经获得了1300多赞：</p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7643beec3a9f43d8911eb425e8dc811b><p class=pgc-img-caption></p></div><p>那么，先来看看效果吧。</p><p><strong>效果</strong></p><p>谷歌大脑用WANN处理了3种强化学习任务。</p><p>（给每一组神经元，共享同一个权重。)</p><p>第一项任务，<strong>Cart-Pole Swing-Up</strong>。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/aaf5587b5fc142bcab968d58806dae1a><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>这是经典的控制任务，一条滑轨，一台小车，车上一根杆子。</p><p>小车在滑轨的范围里跑，要把杆子从自然下垂的状态摇上来，保持在直立的位置不掉下来。</p><p>(这个任务比单纯的Cart-Pole要难一些：</p><p>Cart-Pole杆子的初始位置就是向上直立，不需要小车把它摇上来，只要保持就可以。)</p><p>难度体现在，没有办法用线性控制器 (Linear Controller) 来解决。每一个时间步的奖励，都是基于小车到滑轨一头的<strong>距离</strong>，以及杆子摆动的<strong>角度</strong>。</p><p>WANN的最佳网络 (Champion Network) 长这样：</p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/83d1b035f75844d0adeda2c5b9013c5c><p class=pgc-img-caption></p></div><p>它在没有训练的状态下，已经表现优异：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/62447b81904c40879b7920c0c6b6a25f><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>表现最好的<strong>共享权重</strong>，给了团队十分满意的结果：只用几次摆动便达到了平衡状态。</p><p>第二项任务，<strong>Bipedal Waker-v2</strong>。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/fb68baaf17364fbba5376bb3d4b84563><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>一只两足“生物”，要在随机生成的道路上往前走，越过凸起，跨过陷坑。奖励多少，就看它从出发到挂掉走了<strong>多长的路</strong>，以及<strong>电机扭矩的成本</strong> (为了鼓励高效运动) 。</p><p>每条腿的运动，都是由一个髋关节、和一个膝关节来控制的。有24个输入，会指导它的运动：包括“激光雷达”探测的前方地形数据，本体感受到的关节运动速度等等。</p><p>比起第一项任务中的<strong>低维输入</strong>，这里可能的网络连接就更多样了：</p><p>所以，需要WANN对从输入到输出的布线方式，有所选择。</p><p>这个高维任务，WANN也优质完成了。</p><p>你看，这是搜索出的最佳架构，比刚才的低维任务复杂了许多：</p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f534d4aca0164667867645e4a61ba7c0><p class=pgc-img-caption></p></div><p>它在-1.5的权重下奔跑，长这样：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/453ec068beb34e1195234308a5747049><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>第三项任务，<strong>CarRacing-v0</strong>。</p><p>这是一个自上而下的 (Top-Down) 、像素环境里的赛车游戏。</p><p>一辆车，由三个连续命令来控制：油门、转向、制动。目标是在规定的时间里，经过尽可能多的砖块。赛道是随机生成的。</p><p>研究人员把解释每个像素 (Pixel Interpretation) 的工作交给了一个预训练的变分自编码器 (VAE) ，它可以把像素表征压缩到16个潜在维度。</p><p>这16维就是网络输入的维度。学到的特征是用来检测WANN学习抽象关联 (Abstract Associations) 的能力，而不是编码不同输入之间<span>显式</span>的几何关系。</p><p>这是WANN最佳网络，在-1.4共享权重下、未经训练的赛车成果：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d722d5707cbf4e2d9bc363dc3c0e56c7><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>虽然路走得有些蜿蜒，但很少偏离跑到。</p><p>而把最佳网络<strong>微调</strong>一下，不用训练，便更加顺滑了：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3cc80408cf7846ac8a693c6174fee98d><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>总结一下，在<strong>简单程度</strong>和<strong>模块化程度</strong>上，第二、三项任务都表现得优秀，两足控制器只用了25个可能输入中的17个，忽略了许多LIDAR传感器和膝关节的速度。</p><p>WANN架构不止能在不训练单个权重的情况下完成任务，而且只用了<strong>210个网络连接</strong> (Connections) ，比当前State-of-the-Art模型用到的2804个连接，少了一个数量级。</p><p>做完强化学习，团队又瞄准了<strong>MNIST</strong>，把WANN拓展到了监督学习的分类任务上。</p><p>一个普通的网络，在参数随机初始化的情况下，MNIST上面的准确率可能只有<strong>10%</strong>左右。</p><p>而新方法搜索到的网络架构WANN，用随机权重去跑，准确率已经超过了<strong>80%</strong>；</p><p>如果像刚刚提到的那样，喂给它<strong>多个权值的合集</strong>，准确率就达到了<strong>91.6%</strong>。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ee4525d2973f4755b41f577cc1dfebc1><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>对比一下，经过微调的权重，带来的准确率是91.9%，训练过的权重，可以带来94.2%的准确率。</p><p>再对比一下，拥有几千个权重的线性分类器：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/7237235e346f43ccb4a27d2e0357e6b2><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>也只是和WANN完全没训练、没微调、仅仅喂食了一些随机权重时的准确率相当。</p><p>论文里强调，MINST手写数字分类是<strong>高维分类任务</strong>。WANN表现得非常出色。</p><p>并且没有哪个权值，显得比其他值更优秀，大家表现得十分均衡：<strong>所以随机权重是可行的</strong>。</p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/97e345afb0a144228076a31b8dd68933><p class=pgc-img-caption></p></div><p>不过，每个不同的权重形成的不同网络，有各自擅长分辨的数字，所以可以把一个拥有多个权值的WANN，用作一个自给自足的合集 (Self-Contained Ensemble) 。</p><p><strong>实现原理</strong></p><p>不训练权重参数获得极高准确度，WANN是如何做到的呢？</p><p>神经网络不仅有权重偏置这些参数，网络的拓扑结构、激活函数的选择都会影响最终结果。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/e4bfe0eb38ce4ef7a7bff8453dabc308><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>谷歌大脑的研究人员在论文开头就提出质疑：神经网络的权重参数与其架构相比有多重要？在没有学习任何权重参数的情况下，神经网络架构可以在多大程度上影响给定任务的解决方案。</p><p>为此，研究人员提出了一种神经网络架构的搜索方法，无需训练权重找到执行强化学习任务的最小神经网络架构。</p><p>谷歌研究人员还把这种方法用在监督学习领域，仅使用随机权重，就能在MNIST上实现就比随机猜测高得多的准确率。</p><p>论文从<strong>架构搜索、贝叶斯神经网络、算法信息论、网络剪枝、神经科学</strong>这些理论中获得启发。</p><p>为了生成WANN，必须将权重对网络的影响最小化，用权重随机采样可以保证最终的网络是架构优化的产物，但是在高维空间进行权重随机采样的难度太大。</p><p>研究人员采取了“简单粗暴”的方法，对所有权重强制进行权重共享（weight-sharing），让权重值的数量减少到一个。这种高效的近似可以推动对更好架构的搜索。</p><p><strong>操作步骤</strong></p><p>解决了权重初始化的问题，接下来的问题就是如何收搜索权重不可知神经网络。它分为四个步骤：</p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/8a9f66ee7daf41d3a632b65373fba643><p class=pgc-img-caption></p></div><p>1、创建初始的最小神经网络拓扑群。</p><p>2、通过多个rollout评估每个网络，并对每个rollout分配不同的共享权重值。</p><p>3、根据性能和复杂程度对网络进行排序。</p><p>4、根据排名最高的网络拓扑来创建新的群，通过竞争结果进行概率性的选择。</p><p>然后，算法从第2步开始重复，在连续迭代中，产生复杂度逐渐增加的权重不可知拓扑（weight agnostic topologies ）。</p><p><strong>拓扑搜索</strong></p><p>用于搜索神经网络拓扑的操作受到<strong>神经进化算法</strong>（NEAT）的启发。在NEAT中，拓扑和权重值同时优化，研究人员忽略权重，只进行拓扑搜索操作。</p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bcd3e98ca4f74fe0918d303da2b45d98><p class=pgc-img-caption></p></div><p>上图展示了网络拓扑空间搜索的具体操作：</p><p>一开始网络上是最左侧的最小拓扑结构，仅有部分输入和输出是相连的。</p><p>然后，网络按以下三种方式进行更改：</p><p>1、<strong>插入节点</strong>：拆分现有连接插入新节点。</p><p>2、<strong>添加连接</strong>：连接两个之前未连接的节点，添加新连接。</p><p>3、<strong>更改激活函数</strong>：重新分配隐藏节点的激活函数。</p><p>图的最右侧展示了权重在[2,2]取值范围内可能的激活函数，如线性函数、阶跃函数、正弦余弦函数、ReLU等等。</p><p><strong>权重依然重要</strong></p><p>WANN与传统的固定拓扑网络相比，可以使用单个的随机共享权重也能获得更好的结果。</p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/8a9f66ee7daf41d3a632b65373fba643><p class=pgc-img-caption></p></div><p>虽然WANN在多项任务中取得了最佳结果，但WANN并不完全独立于权重值，当随机分配单个权重值时，有时也会失败。</p><p>WANN通过编码输入和输出之间的关系起作用，虽然权重的大小的重要性并不高，但它们的一致性，尤其是符号的一致性才是关键。</p><p>随机共享权重的另一个好处是，调整单个参数的影响变得不重要，无需使用基于梯度的方法。</p><p>强化学习任务中的结果让作者考虑推广WANN方法的应用范围。他们又测试了WANN在图像分类基础任务MNIST上的表现，结果在权重接近0时效果不佳。</p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/fb421eb269a8497b9df89bc66c3205bb><p class=pgc-img-caption></p></div><p>有Reddit网友质疑WANN的结果，对于随机权重接近于0的情况，该网络的性能并不好，先强化学习实验中的具体表现就是，小车会跑出限定范围。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=谷歌发布颠覆性研究：不训练不调参，从此神经网络告别炼丹一大步 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/fa27a4bf68454a09bea23c63f39ca9ad><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>对此，作者给出解释，在权重趋于0的情况下，网络的输出也会趋于0，所以后期的优化很难达到较好的性能。</p><p><strong>传送门</strong></p><p>原文链接：</p><p>https://weightagnostic.github.io/</p><p>源代码：</p><p>https://github.com/weightagnostic/weightagnostic.github.io</p><p>— 完 —</p><p>诚挚招聘</p><p>量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。</p><p>量子位 QbitAI · 头条号签约作者</p><p>վ'ᴗ' ի 追踪AI技术和产品新动态</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'谷歌发','颠覆性','不训练'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>