<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>Transformers åº“ä¸­å¸¸è§çš„ç”¨ä¾‹ | æå®¢å¿«è¨Š</title><meta property="og:title" content="Transformers åº“ä¸­å¸¸è§çš„ç”¨ä¾‹ - æå®¢å¿«è¨Š"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><meta property="article:published_time" content="2020-11-14T21:01:26+08:00"><meta property="article:modified_time" content="2020-11-14T21:01:26+08:00"><meta name=Keywords content><meta name=description content="Transformers åº“ä¸­å¸¸è§çš„ç”¨ä¾‹"><meta name=author content="æå®¢å¿«è¨Š"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/012fbe91.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>ğŸ¤“ æå®¢å¿«è®¯ Geek Bank</a></h1><p class=description>ä¸ºä½ å¸¦æ¥æœ€å…¨çš„ç§‘æŠ€çŸ¥è¯† ğŸ§¡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>çŒœä½ å–œæ­¡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=ç§‘æŠ€>ç§‘æŠ€</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=éŠæˆ²>éŠæˆ²</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=ç§‘å­¸>ç§‘å­¸</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>Transformers åº“ä¸­å¸¸è§çš„ç”¨ä¾‹</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>ç§‘æŠ€</a></span></div><div class=post-content><p>æœ¬ç« ä»‹ç»ä½¿ç”¨Transformersåº“æ—¶æœ€å¸¸è§çš„ç”¨ä¾‹ã€‚å¯ç”¨çš„æ¨¡å‹å…è®¸è®¸å¤šä¸åŒçš„é…ç½®ï¼Œå¹¶ä¸”åœ¨ç”¨ä¾‹ä¸­å…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§ã€‚è¿™é‡Œä»‹ç»äº†æœ€ç®€å•çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†è¯¸å¦‚é—®ç­”ã€åºåˆ—åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ç­‰ä»»åŠ¡çš„ç”¨æ³•ã€‚</p><p>è¿™äº›ç¤ºä¾‹åˆ©ç”¨Auto Modelï¼Œè¿™äº›ç±»å°†æ ¹æ®ç»™å®šçš„checkpointå®ä¾‹åŒ–æ¨¡å‹ï¼Œå¹¶è‡ªåŠ¨é€‰æ‹©æ­£ç¡®çš„æ¨¡å‹ä½“ç³»ç»“æ„ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ï¼šAutoModelæ–‡æ¡£ã€‚è¯·éšæ„ä¿®æ”¹ä»£ç ï¼Œä½¿å…¶æ›´å…·ä½“ï¼Œå¹¶ä½¿å…¶é€‚åº”ä½ çš„ç‰¹å®šç”¨ä¾‹ã€‚</p><ul><li>ä¸ºäº†ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä»»åŠ¡ä¸Šè‰¯å¥½åœ°æ‰§è¡Œï¼Œå¿…é¡»ä»ä¸è¯¥ä»»åŠ¡å¯¹åº”çš„checkpointåŠ è½½æ¨¡å‹ã€‚è¿™äº›checkpointé€šå¸¸æ˜¯åœ¨å¤§é‡æ•°æ®ä¸Šé¢„å…ˆè®­ç»ƒçš„ï¼Œå¹¶é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚è¿™æ„å‘³ç€ï¼šå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½é’ˆå¯¹æ‰€æœ‰ä»»åŠ¡è¿›è¡Œäº†å¾®è°ƒã€‚å¦‚æœè¦å¯¹ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥åˆ©ç”¨examplesç›®å½•ä¸­çš„run\$task.pyè„šæœ¬ã€‚</li><li>å¾®è°ƒæ¨¡å‹æ˜¯åœ¨ç‰¹å®šçš„æ•°æ®é›†ä¸Šå¾®è°ƒçš„ã€‚æ­¤æ•°æ®é›†å¯èƒ½ä¸ä½ çš„ç”¨ä¾‹å’ŒåŸŸé‡å ï¼Œä¹Ÿå¯èƒ½ä¸é‡å ã€‚å¦‚å‰æ‰€è¿°ï¼Œä½ å¯ä»¥åˆ©ç”¨ç¤ºä¾‹è„šæœ¬æ¥å¾®è°ƒæ¨¡å‹ï¼Œä¹Ÿå¯ä»¥åˆ›å»ºè‡ªå·±çš„è®­ç»ƒè„šæœ¬ã€‚</li></ul><p>ä¸ºäº†å¯¹ä»»åŠ¡è¿›è¡Œæ¨ç†ï¼Œåº“æä¾›äº†å‡ ç§æœºåˆ¶ï¼š</p><ul><li>ç®¡é“æ˜¯éå¸¸æ˜“äºä½¿ç”¨çš„æŠ½è±¡ï¼Œåªéœ€è¦ä¸¤è¡Œä»£ç ã€‚</li><li>ç›´æ¥å°†æ¨¡å‹ä¸Tokenizer(PyTorch/TensorFlow)ç»“åˆä½¿ç”¨æ¥ä½¿ç”¨æ¨¡å‹çš„å®Œæ•´æ¨ç†ã€‚è¿™ç§æœºåˆ¶ç¨å¾®å¤æ‚ï¼Œä½†æ˜¯æ›´å¼ºå¤§ã€‚</li></ul><p>è¿™é‡Œå±•ç¤ºäº†ä¸¤ç§æ–¹æ³•ã€‚</p><blockquote><p>è¯·æ³¨æ„ï¼Œè¿™é‡Œä»‹ç»çš„æ‰€æœ‰ä»»åŠ¡éƒ½åˆ©ç”¨äº†åœ¨é¢„è®­ç»ƒæ¨¡å‹é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒåçš„æ¨¡å‹ã€‚åŠ è½½æœªé’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒçš„checkpointæ—¶ï¼Œå°†åªåŠ è½½transformerå±‚ï¼Œè€Œä¸ä¼šåŠ è½½ç”¨äºè¯¥ä»»åŠ¡çš„é™„åŠ å±‚ï¼Œä»è€Œéšæœºåˆå§‹åŒ–è¯¥é™„åŠ å±‚çš„æƒé‡ã€‚è¿™å°†äº§ç”Ÿéšæœºè¾“å‡ºã€‚</p></blockquote><h1 class=pgc-h-arrow-right>åºåˆ—åˆ†ç±»</h1><p>åºåˆ—åˆ†ç±»æ˜¯æ ¹æ®å·²ç»ç»™å®šçš„ç±»åˆ«ç„¶åå¯¹åºåˆ—è¿›è¡Œåˆ†ç±»çš„ä»»åŠ¡ã€‚åºåˆ—åˆ†ç±»çš„ä¸€ä¸ªä¾‹å­æ˜¯GLUEæ•°æ®é›†ï¼Œå®ƒå°±æ˜¯å®Œå…¨åŸºäºè¯¥ä»»åŠ¡çš„ã€‚å¦‚æœä½ æƒ³åœ¨GLUEåºåˆ—åˆ†ç±»ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œå¯ä»¥åˆ©ç”¨run_GLUE.pyæˆ–run_tf_GLUE.pyè„šæœ¬ã€‚</p><p>ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ç®¡é“è¿›è¡Œæƒ…ç»ªåˆ†æçš„ä¾‹å­ï¼šè¯†åˆ«è¯¥åºåˆ—æ˜¯ç§¯æçš„è¿˜æ˜¯æ¶ˆæçš„ã€‚å®ƒåˆ©ç”¨sst2ä¸Šçš„å¾®è°ƒæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªGLUEä»»åŠ¡ã€‚</p><pre><code>from transformers import pipelinenlp = pipeline(&#34;sentiment-analysis&#34;)print(nlp(&#34;I hate you&#34;))print(nlp(&#34;I love you&#34;))</code></pre><p>è¿™å°†è¿”å›ä¸€ä¸ªæ ‡ç­¾(â€œç§¯æâ€æˆ–â€œæ¶ˆæâ€)å’Œä¸€ä¸ªåˆ†æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š</p><pre><code>[{&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.9991129}][{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.99986565}]</code></pre><p>ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨æ¨¡å‹è¿›è¡Œåºåˆ—åˆ†ç±»çš„ç¤ºä¾‹ï¼Œä»¥ç¡®å®šä¸¤ä¸ªåºåˆ—æ˜¯å¦æ˜¯å½¼æ­¤çš„è§£é‡Šã€‚è¯¥è¿‡ç¨‹å¦‚ä¸‹ï¼š</p><ul><li>ä»checkpointåç§°å®ä¾‹åŒ–ä¸€ä¸ªtokenizerå’Œä¸€ä¸ªæ¨¡å‹ã€‚è¯¥æ¨¡å‹è¢«è¯†åˆ«ä¸ºä¸€ä¸ªBERTæ¨¡å‹ï¼Œå¹¶ç”¨å­˜å‚¨åœ¨checkpointä¸­çš„æƒé‡åŠ è½½å®ƒã€‚</li><li>ä»è¿™ä¸¤å¥è¯ä¸­æ„å»ºä¸€ä¸ªåºåˆ—ï¼Œä½¿ç”¨æ­£ç¡®çš„ç‰¹å®šäºæ¨¡å‹çš„åˆ†éš”ç¬¦æ ‡è®°ç±»å‹idå’Œæ³¨æ„åŠ›æ©ç (encode()å’Œencode_plus()å¤„ç†è¿™ä¸ªé—®é¢˜)</li><li>å°†è¿™ä¸ªåºåˆ—ä¼ é€’åˆ°æ¨¡å‹ä¸­ï¼Œä»¥ä¾¿å°†å…¶åˆ†ç±»åˆ°ä¸¤ä¸ªå¯ç”¨çš„ç±»ä¸­çš„ä¸€ä¸ªï¼š0(ä¸æ˜¯è§£é‡Š)å’Œ1(æ˜¯è§£é‡Š)</li><li>è®¡ç®—ç»“æœçš„softmaxè·å–ç±»çš„æ¦‚ç‡</li><li>æ‰“å°ç»“æœ</li></ul><p>Pytorchä»£ç </p><pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassificationimport torchtokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased-finetuned-mrpc&#34;)model = AutoModelForSequenceClassification.from_pretrained(&#34;bert-base-cased-finetuned-mrpc&#34;)classes = [&#34;not paraphrase&#34;, &#34;is paraphrase&#34;]sequence_0 = &#34;The company HuggingFace is based in New York City&#34;sequence_1 = &#34;Apples are especially bad for your health&#34;sequence_2 = &#34;HuggingFace&#39;s headquarters are situated in Manhattan&#34;paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, return_tensors=&#34;pt&#34;)not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=&#34;pt&#34;)paraphrase_classification_logits = model(**paraphrase)[0]not_paraphrase_classification_logits = model(**not_paraphrase)[0]paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]print(&#34;Should be paraphrase&#34;)for i in range(len(classes)):    print(f&#34;{classes[i]}: {round(paraphrase_results[i] * 100)}%&#34;)print(&#34;\nShould not be paraphrase&#34;)for i in range(len(classes)):    print(f&#34;{classes[i]}: {round(not_paraphrase_results[i] * 100)}%&#34;)</code></pre><p>TensorFlowä»£ç </p><pre><code>from transformers import AutoTokenizer, TFAutoModelForSequenceClassificationimport tensorflow as tftokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased-finetuned-mrpc&#34;)model = TFAutoModelForSequenceClassification.from_pretrained(&#34;bert-base-cased-finetuned-mrpc&#34;)classes = [&#34;not paraphrase&#34;, &#34;is paraphrase&#34;]sequence_0 = &#34;The company HuggingFace is based in New York City&#34;sequence_1 = &#34;Apples are especially bad for your health&#34;sequence_2 = &#34;HuggingFace&#39;s headquarters are situated in Manhattan&#34;paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, return_tensors=&#34;tf&#34;)not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=&#34;tf&#34;)paraphrase_classification_logits = model(paraphrase)[0]not_paraphrase_classification_logits = model(not_paraphrase)[0]paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]print(&#34;Should be paraphrase&#34;)for i in range(len(classes)):    print(f&#34;{classes[i]}: {round(paraphrase_results[i] * 100)}%&#34;)print(&#34;\nShould not be paraphrase&#34;)for i in range(len(classes)):    print(f&#34;{classes[i]}: {round(not_paraphrase_results[i] * 100)}%&#34;)</code></pre><p>è¿™å°†è¾“å‡ºä»¥ä¸‹ç»“æœï¼š</p><pre><code>Should be paraphrasenot paraphrase: 10%is paraphrase: 90%Should not be paraphrasenot paraphrase: 94%is paraphrase: 6%</code></pre><h1 class=pgc-h-arrow-right>æŠ½å–å¼é—®ç­”</h1><p>æŠ½å–å¼é—®ç­”æ˜¯ä»ç»™å®šé—®é¢˜çš„æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆçš„ä»»åŠ¡ã€‚é—®ç­”æ•°æ®é›†çš„ä¸€ä¸ªä¾‹å­æ˜¯SQuADæ•°æ®é›†ï¼Œå®ƒå®Œå…¨åŸºäºè¯¥ä»»åŠ¡ã€‚å¦‚æœä½ æƒ³åœ¨å›¢é˜Ÿä»»åŠ¡ä¸­å¾®è°ƒæ¨¡å‹ï¼Œå¯ä»¥åˆ©ç”¨run_SQuAD.pyã€‚</p><p>ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ç®¡é“è¿›è¡Œé—®ç­”çš„ç¤ºä¾‹ï¼šä»ç»™å®šé—®é¢˜çš„æ–‡æœ¬ä¸­æå–ç­”æ¡ˆã€‚å®ƒåˆ©ç”¨äº†ä¸€ä¸ªå°é˜Ÿçš„å¾®è°ƒæ¨¡å‹ã€‚</p><pre><code>from transformers import pipelinenlp = pipeline(&#34;question-answering&#34;)context = r&#34;&#34;&#34;Extractive Question Answering is the task of extracting an answer from a text given a question. An example of aquestion answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tunea model on a SQuAD task, you may leverage the `run_squad.py`.&#34;&#34;&#34;print(nlp(question=&#34;What is extractive question answering?&#34;, context=context))print(nlp(question=&#34;What is a good example of a question answering dataset?&#34;, context=context))</code></pre><p>è¿™å°†è¿”å›ä»æ–‡æœ¬ä¸­æå–çš„ç­”æ¡ˆï¼Œä¸€ä¸ªç½®ä¿¡åº¦ï¼Œä»¥åŠâ€œå¼€å§‹â€å’Œâ€œç»“æŸâ€å€¼ï¼Œè¿™äº›å€¼æ˜¯æå–çš„ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­çš„ä½ç½®ã€‚</p><pre><code>{&#39;score&#39;: 0.622232091629833, &#39;start&#39;: 34, &#39;end&#39;: 96, &#39;answer&#39;: &#39;the task of extracting an answer from a text given a question.&#39;}{&#39;score&#39;: 0.5115299158662765, &#39;start&#39;: 147, &#39;end&#39;: 161, &#39;answer&#39;: &#39;SQuAD dataset,&#39;}</code></pre><p>ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨æ¨¡å‹å’ŒTokenizerå›ç­”é—®é¢˜çš„ç¤ºä¾‹ã€‚è¯¥è¿‡ç¨‹å¦‚ä¸‹ï¼š</p><ul><li>ä»checkpointåç§°å®ä¾‹åŒ–ä¸€ä¸ªtokenizerå’Œä¸€ä¸ªæ¨¡å‹ã€‚è¯¥æ¨¡å‹è¢«è¯†åˆ«ä¸ºä¸€ä¸ªBERTæ¨¡å‹ï¼Œå¹¶ç”¨å­˜å‚¨åœ¨checkpointä¸­çš„æƒé‡åŠ è½½å®ƒã€‚</li><li>å®šä¹‰ä¸€æ®µæ–‡æœ¬å’Œå‡ ä¸ªé—®é¢˜ã€‚</li><li>éå†é—®é¢˜å¹¶æ ¹æ®æ–‡æœ¬å’Œå½“å‰é—®é¢˜æ„å»ºä¸€ä¸ªåºåˆ—ï¼Œä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹ç‰¹å®šåˆ†éš”ç¬¦æ ‡è®°ç±»å‹idå’Œæ³¨æ„åŠ›æ©ç å°†æ­¤åºåˆ—ä¼ é€’åˆ°æ¨¡å‹ä¸­ã€‚è¿™å°†è¾“å‡ºæ•´ä¸ªåºåˆ—æ ‡è®°(é—®é¢˜å’Œæ–‡æœ¬)çš„å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„ä¸€ç³»åˆ—åˆ†æ•°ã€‚</li><li>è®¡ç®—ç»“æœçš„softmaxä»¥è·å¾—ä»æ ‡è®°çš„å¼€å§‹ä½ç½®å’Œåœæ­¢ä½ç½®å¯¹åº”çš„æ¦‚ç‡</li><li>å°†è¿™äº›æ ‡è®°è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚</li><li>æ‰“å°ç»“æœ</li></ul><p>Pytorchä»£ç </p><pre><code>from transformers import AutoTokenizer, AutoModelForQuestionAnsweringimport torchtokenizer = AutoTokenizer.from_pretrained(&#34;bert-large-uncased-whole-word-masking-finetuned-squad&#34;)model = AutoModelForQuestionAnswering.from_pretrained(&#34;bert-large-uncased-whole-word-masking-finetuned-squad&#34;)text = r&#34;&#34;&#34; Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purposearchitectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and NaturalLanguage Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability betweenTensorFlow 2.0 and PyTorch.&#34;&#34;&#34;questions = [    &#34;How many pretrained models are available in Transformers?&#34;,    &#34;What does Transformers provide?&#34;,    &#34;Transformers provides interoperability between which frameworks?&#34;,]for question in questions:    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=&#34;pt&#34;)    input_ids = inputs[&#34;input_ids&#34;].tolist()[0]    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)    answer_start_scores, answer_end_scores = model(**inputs)    answer_start = torch.argmax(        answer_start_scores    )  # Get the most likely beginning of answer with the argmax of the score    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))    print(f&#34;Question: {question}&#34;)    print(f&#34;Answer: {answer}\n&#34;)</code></pre><p>TensorFlowä»£ç </p><pre><code>from transformers import AutoTokenizer, TFAutoModelForQuestionAnsweringimport tensorflow as tftokenizer = AutoTokenizer.from_pretrained(&#34;bert-large-uncased-whole-word-masking-finetuned-squad&#34;)model = TFAutoModelForQuestionAnswering.from_pretrained(&#34;bert-large-uncased-whole-word-masking-finetuned-squad&#34;)text = r&#34;&#34;&#34; Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purposearchitectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and NaturalLanguage Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability betweenTensorFlow 2.0 and PyTorch.&#34;&#34;&#34;questions = [    &#34;How many pretrained models are available in Transformers?&#34;,    &#34;What does Transformers provide?&#34;,    &#34;Transformers provides interoperability between which frameworks?&#34;,]for question in questions:    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=&#34;tf&#34;)    input_ids = inputs[&#34;input_ids&#34;].numpy()[0]    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)    answer_start_scores, answer_end_scores = model(inputs)    answer_start = tf.argmax(        answer_start_scores, axis=1    ).numpy()[0]  # Get the most likely beginning of answer with the argmax of the score    answer_end = (        tf.argmax(answer_end_scores, axis=1) + 1    ).numpy()[0]  # Get the most likely end of answer with the argmax of the score    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))    print(f&#34;Question: {question}&#34;)    print(f&#34;Answer: {answer}\n&#34;)</code></pre><p>è¿™å°†è¾“å‡ºé¢„æµ‹ç­”æ¡ˆåçš„é—®é¢˜ï¼š</p><pre><code>Question: How many pretrained models are available in Transformers?Answer: over 32 +Question: What does Transformers provide?Answer: general - purpose architecturesQuestion: Transformers provides interoperability between which frameworks?Answer: tensorflow 2 . 0 and pytorch</code></pre><h1 class=pgc-h-arrow-right>è¯­è¨€å»ºæ¨¡</h1><p>è¯­è¨€å»ºæ¨¡æ˜¯å°†ä¸€ä¸ªæ¨¡å‹ä¸ä¸€ä¸ªç‰¹å®šé¢†åŸŸçš„è¯­æ–™åº“ç›¸åŒ¹é…çš„ä»»åŠ¡ã€‚æ‰€æœ‰æµè¡Œçš„åŸºäºtransformerçš„æ¨¡å‹éƒ½æ˜¯ä½¿ç”¨è¯­è¨€å»ºæ¨¡çš„å˜ä½“æ¥è®­ç»ƒçš„ï¼Œä¾‹å¦‚æ©ç è¯­è¨€å»ºæ¨¡çš„BERTã€å› æœè¯­è¨€å»ºæ¨¡çš„GPT-2ã€‚</p><p>è¯­è¨€å»ºæ¨¡åœ¨é¢„è®­ç»ƒä¹‹å¤–ä¹Ÿå¾ˆæœ‰ç”¨ï¼Œä¾‹å¦‚å°†æ¨¡å‹åˆ†å¸ƒè½¬æ¢ä¸ºç‰¹å®šé¢†åŸŸï¼šä½¿ç”¨åœ¨éå¸¸å¤§çš„è¯­æ–™åº“ä¸Šè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œç„¶åå°†å…¶å¾®è°ƒåˆ°æ–°é—»æ•°æ®é›†æˆ–ç§‘å­¦è®ºæ–‡ä¸Šï¼Œä¾‹å¦‚LysandreJik/arxiv nlp(https://huggingface.co/lysandre/arxiv-nlp)ã€‚</p><h1 class=pgc-h-arrow-right>æ©ç è¯­è¨€å»ºæ¨¡</h1><p>æ©ç è¯­è¨€å»ºæ¨¡æ˜¯ç”¨æ©ç æ ‡è®°å¯¹åºåˆ—ä¸­çš„æ ‡è®°è¿›è¡Œæ©ç ï¼Œå¹¶æç¤ºæ¨¡å‹ç”¨é€‚å½“çš„æ ‡è®°å¡«å……è¯¥æ©ç çš„ä»»åŠ¡ã€‚è¿™å…è®¸æ¨¡å‹åŒæ—¶å¤„ç†å³ä¸Šä¸‹æ–‡(æ©ç å³ä¾§çš„æ ‡è®°)å’Œå·¦ä¸Šä¸‹æ–‡(æ©ç å·¦ä¾§çš„æ ‡è®°)ã€‚è¿™æ ·çš„è®­ç»ƒä¸ºéœ€è¦åŒå‘èƒŒæ™¯çš„ä¸‹æ¸¸ä»»åŠ¡(å¦‚SQuAD)å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚</p><p>ä¸‹é¢æ˜¯ä½¿ç”¨ç®¡é“æ¥æ›¿æ¢åºåˆ—ä¸­çš„æ©ç çš„ç¤ºä¾‹ï¼š</p><pre><code>from transformers import pipelinenlp = pipeline(&#34;fill-mask&#34;)print(nlp(f&#34;HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.&#34;))</code></pre><p>è¿™å°†åœ¨Tokenizerè¯æ±‡è¡¨ä¸­è¾“å‡ºå¡«å……äº†æ©ç çš„åºåˆ—ã€ç½®ä¿¡åº¦å¾—åˆ†ä»¥åŠæ ‡è®°idï¼š</p><pre><code>[    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a tool that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.15627853572368622, &#39;token&#39;: 3944},    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a framework that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.11690319329500198, &#39;token&#39;: 7208},    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a library that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.058063216507434845, &#39;token&#39;: 5560},    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a database that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.04211743175983429, &#39;token&#39;: 8503},    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a prototype that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.024718601256608963, &#39;token&#39;: 17715}]</code></pre><p>ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨æ¨¡å‹å’ŒTokenizerè¿›è¡Œæ©ç è¯­è¨€å»ºæ¨¡çš„ç¤ºä¾‹ã€‚è¯¥è¿‡ç¨‹å¦‚ä¸‹ï¼š</p><ul><li>ä»checkpointåç§°å®ä¾‹åŒ–ä¸€ä¸ªtokenizerå’Œä¸€ä¸ªæ¨¡å‹ã€‚è¯¥æ¨¡å‹è¢«è¯†åˆ«ä¸ºä¸€ä¸ªDistilBERTæ¨¡å‹ï¼Œå¹¶ç”¨å­˜å‚¨åœ¨checkpointä¸­çš„æƒé‡åŠ è½½å®ƒã€‚</li><li>å®šä¹‰ä¸€ä¸ªå¸¦æ©ç æ ‡è®°çš„åºåˆ—ï¼Œä¸ä½¿ç”¨å•è¯è€Œæ˜¯é€‰æ‹©tokenizer.mask_tokenè¿›è¡Œæ”¾ç½®(è¿›è¡Œæ©ç )ã€‚</li><li>å°†è¯¥åºåˆ—ç¼–ç ä¸ºidï¼Œå¹¶åœ¨è¯¥idåˆ—è¡¨ä¸­æ‰¾åˆ°æ©ç æ ‡è®°çš„ä½ç½®ã€‚</li><li>åœ¨æ©ç æ ‡è®°çš„ç´¢å¼•å¤„æ£€ç´¢é¢„æµ‹ï¼šæ­¤å¼ é‡ä¸è¯æ±‡è¡¨çš„å¤§å°ç›¸åŒï¼Œå€¼æ˜¯æ¯ä¸ªæ ‡è®°çš„åˆ†æ•°ã€‚æ¨¡å‹å¯¹ä»–è®¤ä¸ºåœ¨è¿™ç§æƒ…å†µä¸‹å¯èƒ½å‡ºç°çš„æ ‡è®°ä¼šç»™å‡ºæ›´é«˜çš„åˆ†æ•°ã€‚</li><li>ä½¿ç”¨PyTorch topkæˆ–TensorFlow top_kæ–¹æ³•æ£€ç´¢å‰5ä¸ªæ ‡è®°ã€‚</li><li>ç”¨é¢„æµ‹çš„æ ‡è®°æ›¿æ¢æ©ç æ ‡è®°å¹¶æ‰“å°ç»“æœ</li></ul><p>Pytorchä»£ç </p><pre><code>from transformers import AutoModelWithLMHead, AutoTokenizerimport torchtokenizer = AutoTokenizer.from_pretrained(&#34;distilbert-base-cased&#34;)model = AutoModelWithLMHead.from_pretrained(&#34;distilbert-base-cased&#34;)sequence = f&#34;Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.&#34;input = tokenizer.encode(sequence, return_tensors=&#34;pt&#34;)mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]token_logits = model(input)[0]mask_token_logits = token_logits[0, mask_token_index, :]top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()for token in top_5_tokens:    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))</code></pre><p>TensorFlowä»£ç </p><pre><code>from transformers import TFAutoModelWithLMHead, AutoTokenizerimport tensorflow as tftokenizer = AutoTokenizer.from_pretrained(&#34;distilbert-base-cased&#34;)model = TFAutoModelWithLMHead.from_pretrained(&#34;distilbert-base-cased&#34;)sequence = f&#34;Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.&#34;input = tokenizer.encode(sequence, return_tensors=&#34;tf&#34;)mask_token_index = tf.where(input == tokenizer.mask_token_id)[0, 1]token_logits = model(input)[0]mask_token_logits = token_logits[0, mask_token_index, :]top_5_tokens = tf.math.top_k(mask_token_logits, 5).indices.numpy()for token in top_5_tokens:    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))</code></pre><p>è¿™å°†æ‰“å°äº”ä¸ªåºåˆ—ï¼Œå…¶ä¸­å‰äº”ä¸ªæ ‡è®°ç”±æ¨¡å‹é¢„æµ‹ï¼š</p><pre><code>Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.</code></pre><h1 class=pgc-h-arrow-right>å› æœè¯­è¨€å»ºæ¨¡</h1><p>å› æœè¯­è¨€å»ºæ¨¡æ˜¯æ ¹æ®ä¸€ç³»åˆ—çš„æ ‡è®°æ¥é¢„æµ‹æ ‡è®°çš„ä»»åŠ¡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹åªå…³æ³¨å·¦è¾¹çš„ä¸Šä¸‹æ–‡(æ©ç å·¦è¾¹çš„æ ‡è®°)ã€‚è¿™æ ·çš„è®­ç»ƒå¯¹äºç”Ÿæˆä»»åŠ¡æ¥è¯´æ˜¯æœ‰ä½œç”¨çš„ã€‚</p><p>ç›®å‰è¿˜æ²¡æœ‰è¿›è¡Œå› æœè¯­è¨€å»ºæ¨¡/ç”Ÿæˆçš„ç®¡é“ã€‚ ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨Tokenizerå’Œæ¨¡å‹çš„ç¤ºä¾‹ã€‚åˆ©ç”¨generate()æ–¹æ³•æŒ‰ç…§PyTorchä¸­çš„åˆå§‹åºåˆ—ç”Ÿæˆæ ‡è®°ï¼Œå¹¶åœ¨TensorFlowä¸­åˆ›å»ºä¸€ä¸ªç®€å•çš„å¾ªç¯ã€‚</p><p>Pytorchä»£ç </p><pre><code>from transformers import AutoModelWithLMHead, AutoTokenizertokenizer = AutoTokenizer.from_pretrained(&#34;gpt2&#34;)model = AutoModelWithLMHead.from_pretrained(&#34;gpt2&#34;)sequence = f&#34;Hugging Face is based in DUMBO, New York City, and is&#34;input = tokenizer.encode(sequence, return_tensors=&#34;pt&#34;)generated = model.generate(input, max_length=50)resulting_string = tokenizer.decode(generated.tolist()[0])print(resulting_string)</code></pre><p>TensorFlowä»£ç </p><pre><code>from transformers import TFAutoModelWithLMHead, AutoTokenizerimport tensorflow as tftokenizer = AutoTokenizer.from_pretrained(&#34;gpt2&#34;)model = TFAutoModelWithLMHead.from_pretrained(&#34;gpt2&#34;)sequence = f&#34;Hugging Face is based in DUMBO, New York City, and is&#34;generated = tokenizer.encode(sequence)for i in range(50):    predictions = model(tf.constant([generated]))[0]    token = tf.argmax(predictions[0], axis=1)[-1].numpy()    generated += [token]resulting_string = tokenizer.decode(generated)print(resulting_string)</code></pre><p>è¿™å°†ä»åŸå§‹åºåˆ—è¾“å‡º(å¸Œæœ›)çš„å¯¹åº”å­—ç¬¦ä¸²ï¼Œä½¿ç”¨top_p/tok_kåˆ†å¸ƒè·å–generate()é‡‡æ ·çš„ç»“æœï¼š</p><pre><code>Hugging Face is based in DUMBO, New York City, and is a live-action TV series based on the novel by JohnCarpenter, and its producers, David Kustlin and Steve Pichar. The film is directed by!</code></pre><h1 class=pgc-h-arrow-right>å‘½åå®ä½“è¯†åˆ«</h1><p>å‘½åå®ä½“è¯†åˆ«(NER)æ˜¯æ ¹æ®ç±»åˆ«å¯¹æ ‡è®°è¿›è¡Œåˆ†ç±»çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å°†æ ‡è®°æ ‡è¯†ä¸ºä¸ªäººã€ç»„ç»‡æˆ–ä½ç½®ã€‚å‘½åå®ä½“è¯†åˆ«æ•°æ®é›†çš„ä¸€ä¸ªä¾‹å­æ˜¯CoNLL-2003æ•°æ®é›†ï¼Œå®ƒå®Œå…¨åŸºäºè¯¥ä»»åŠ¡ã€‚å¦‚æœä½ æƒ³å¯¹NERä»»åŠ¡çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥åˆ©ç”¨ner/run_ner.py(PyTorch)ã€ner/run_pl_ner.py(åˆ©ç”¨PyTorch lightning)æˆ–ner/run_tf_ner.py(TensorFlow)è„šæœ¬ã€‚</p><p>ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ç®¡é“è¿›è¡Œå‘½åå®ä½“è¯†åˆ«çš„ç¤ºä¾‹ï¼Œè¯•å›¾å°†æ ‡è®°æ ‡è¯†ä¸ºå±äº9ä¸ªç±»ä¹‹ä¸€ï¼š</p><ul><li>O, ä¸æ˜¯å‘½åå®ä½“</li><li>B-MIS, ä¸€ä¸ªæ‚é¡¹å®ä½“çš„å¼€å¤´</li><li>I-MIS, æ‚é¡¹å®ä½“</li><li>B-PER, ä¸€ä¸ªäººåçš„å¼€å¤´</li><li>I-PER, äººå</li><li>B-ORG, ä¸€ä¸ªç»„ç»‡çš„å¼€å¤´</li><li>I-ORG, ç»„ç»‡</li><li>B-LOC, ä¸€ä¸ªåœ°ç‚¹çš„å¼€å¤´</li><li>I-LOC, åœ°ç‚¹</li></ul><p>å®ƒåˆ©ç”¨CoNLL-2003ä¸Šä¸€ä¸ªç»è¿‡å¾®è°ƒçš„æ¨¡å‹ï¼Œç”±dbmdzçš„@stefan-itè¿›è¡Œäº†å¾®è°ƒã€‚</p><pre><code>from transformers import pipelinenlp = pipeline(&#34;ner&#34;)sequence = &#34;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&#34; \           &#34;close to the Manhattan Bridge which is visible from the window.&#34;print(nlp(sequence))</code></pre><p>è¿™å°†è¾“å‡ºä¸Šé¢å®šä¹‰çš„9ä¸ªç±»ä¸­æ ‡è¯†ä¸ºå®ä½“çš„æ‰€æœ‰å•è¯çš„åˆ—è¡¨ã€‚ä»¥ä¸‹æ˜¯é¢„æœŸç»“æœï¼š</p><pre><code>[    {&#39;word&#39;: &#39;Hu&#39;, &#39;score&#39;: 0.9995632767677307, &#39;entity&#39;: &#39;I-ORG&#39;},    {&#39;word&#39;: &#39;##gging&#39;, &#39;score&#39;: 0.9915938973426819, &#39;entity&#39;: &#39;I-ORG&#39;},    {&#39;word&#39;: &#39;Face&#39;, &#39;score&#39;: 0.9982671737670898, &#39;entity&#39;: &#39;I-ORG&#39;},    {&#39;word&#39;: &#39;Inc&#39;, &#39;score&#39;: 0.9994403719902039, &#39;entity&#39;: &#39;I-ORG&#39;},    {&#39;word&#39;: &#39;New&#39;, &#39;score&#39;: 0.9994346499443054, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;York&#39;, &#39;score&#39;: 0.9993270635604858, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;City&#39;, &#39;score&#39;: 0.9993864893913269, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;D&#39;, &#39;score&#39;: 0.9825621843338013, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;##UM&#39;, &#39;score&#39;: 0.936983048915863, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;##BO&#39;, &#39;score&#39;: 0.8987102508544922, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;Manhattan&#39;, &#39;score&#39;: 0.9758241176605225, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;Bridge&#39;, &#39;score&#39;: 0.990249514579773, &#39;entity&#39;: &#39;I-LOC&#39;}]</code></pre><p>æ³¨æ„â€œHugging Faceâ€æ˜¯å¦‚ä½•è¢«ç¡®å®šä¸ºä¸€ä¸ªç»„ç»‡ï¼Œâ€œNew York Cityâ€ï¼Œâ€œDUMBOâ€å’Œâ€œManhattan Bridgeâ€æ˜¯å¦‚ä½•è¢«ç¡®å®šä¸ºåœ°ç‚¹çš„ã€‚</p><p>ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨æ¨¡å‹å’ŒTokenizerè¿›è¡Œå‘½åå®ä½“è¯†åˆ«çš„ç¤ºä¾‹ã€‚ è¯¥è¿‡ç¨‹å¦‚ä¸‹ï¼š</p><ul><li>ä»checkpointåç§°å®ä¾‹åŒ–ä¸€ä¸ªtokenizerå’Œä¸€ä¸ªæ¨¡å‹ã€‚è¯¥æ¨¡å‹è¢«è¯†åˆ«ä¸ºä¸€ä¸ªBERTæ¨¡å‹ï¼Œå¹¶ç”¨å­˜å‚¨åœ¨checkpointä¸­çš„æƒé‡åŠ è½½å®ƒã€‚</li><li>å®šä¹‰ç”¨äºè®­ç»ƒæ¨¡å‹çš„æ ‡ç­¾åˆ—è¡¨ã€‚</li><li>å®šä¹‰ä¸€ä¸ªåŒ…å«å·²çŸ¥å®ä½“çš„åºåˆ—ï¼Œä¾‹å¦‚â€œHugging Faceâ€ä½œä¸ºä¸€ä¸ªç»„ç»‡ï¼Œâ€œNew York Cityâ€ä½œä¸ºä¸€ä¸ªä½ç½®ã€‚</li><li>å°†å•è¯æ‹†åˆ†ä¸ºæ ‡è®°ï¼Œä»¥ä¾¿å®ƒä»¬å¯ä»¥æ˜ å°„åˆ°é¢„æµ‹ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå°æŠ€å·§ï¼Œé¦–å…ˆå¯¹åºåˆ—è¿›è¡Œå®Œå…¨çš„ç¼–ç å’Œè§£ç ï¼Œè¿™æ ·å°±ç•™ä¸‹äº†ä¸€ä¸ªåŒ…å«ç‰¹æ®Šæ ‡è®°çš„å­—ç¬¦ä¸²ã€‚</li><li>å°†è¯¥åºåˆ—ç¼–ç ä¸ºid(è‡ªåŠ¨æ·»åŠ ç‰¹æ®Šæ ‡è®°)ã€‚</li><li>é€šè¿‡å°†è¾“å…¥ä¼ é€’åˆ°æ¨¡å‹å¹¶è·å¾—ç¬¬ä¸€ä¸ªè¾“å‡ºæ¥æ£€ç´¢é¢„æµ‹ã€‚è¿™å°†å¯¼è‡´æ¯ä¸ªæ ‡è®°åœ¨9ä¸ªå¯èƒ½çš„ç±»ä¸Šåˆ†å¸ƒã€‚æˆ‘ä»¬ä½¿ç”¨argmaxæ¥æ£€ç´¢æ¯ä¸ªæ ‡è®°æœ€å¯èƒ½çš„ç±»ã€‚</li><li>å°†æ¯ä¸ªæ ‡è®°åŠå…¶é¢„æµ‹åˆ°ä¸€èµ·å¹¶æ‰“å°å‡ºæ¥ã€‚</li></ul><p>Pytorchä»£ç </p><pre><code>from transformers import AutoModelForTokenClassification, AutoTokenizerimport torchmodel = AutoModelForTokenClassification.from_pretrained(&#34;dbmdz/bert-large-cased-finetuned-conll03-english&#34;)tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased&#34;)label_list = [    &#34;O&#34;,       # ä¸æ˜¯å‘½åå®ä½“    &#34;B-MISC&#34;,  # ä¸€ä¸ªæ‚é¡¹å®ä½“çš„å¼€å¤´    &#34;I-MISC&#34;,  # æ‚é¡¹    &#34;B-PER&#34;,   # ä¸€ä¸ªäººåçš„å¼€å¤´    &#34;I-PER&#34;,   # äººå    &#34;B-ORG&#34;,   # ä¸€ä¸ªç»„ç»‡çš„å¼€å¤´    &#34;I-ORG&#34;,   # ç»„ç»‡    &#34;B-LOC&#34;,   # ä¸€ä¸ªåœ°ç‚¹çš„å¼€å¤´    &#34;I-LOC&#34;    # åœ°ç‚¹]sequence = &#34;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&#34; \           &#34;close to the Manhattan Bridge.&#34;# Bit of a hack to get the tokens with the special tokenstokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))inputs = tokenizer.encode(sequence, return_tensors=&#34;pt&#34;)outputs = model(inputs)[0]predictions = torch.argmax(outputs, dim=2)print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].tolist())])</code></pre><p>TensorFlowä»£ç </p><pre><code>from transformers import TFAutoModelForTokenClassification, AutoTokenizerimport tensorflow as tfmodel = TFAutoModelForTokenClassification.from_pretrained(&#34;dbmdz/bert-large-cased-finetuned-conll03-english&#34;)tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased&#34;)label_list = [    &#34;O&#34;,       # ä¸æ˜¯å‘½åå®ä½“    &#34;B-MISC&#34;,  # ä¸€ä¸ªæ‚é¡¹å®ä½“çš„å¼€å¤´    &#34;I-MISC&#34;,  # æ‚é¡¹    &#34;B-PER&#34;,   # ä¸€ä¸ªäººåçš„å¼€å¤´    &#34;I-PER&#34;,   # äººå    &#34;B-ORG&#34;,   # ä¸€ä¸ªç»„ç»‡çš„å¼€å¤´    &#34;I-ORG&#34;,   # ç»„ç»‡    &#34;B-LOC&#34;,   # ä¸€ä¸ªåœ°ç‚¹çš„å¼€å¤´    &#34;I-LOC&#34;    # åœ°ç‚¹]sequence = &#34;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&#34; \           &#34;close to the Manhattan Bridge.&#34;#ç”¨ç‰¹æ®Šçš„æ ‡è®°æ¥è·å–æ ‡è®°çš„ä¸€ç‚¹æŠ€å·§tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))inputs = tokenizer.encode(sequence, return_tensors=&#34;tf&#34;)outputs = model(inputs)[0]predictions = tf.argmax(outputs, axis=2)print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())])</code></pre><p>è¿™å°†è¾“å‡ºæ˜ å°„åˆ°å…¶é¢„æµ‹çš„æ¯ä¸ªæ ‡è®°çš„åˆ—è¡¨ã€‚ä¸ç®¡é“ä¸åŒçš„æ˜¯ï¼Œè¿™é‡Œæ¯ä¸ªæ ‡è®°éƒ½æœ‰ä¸€ä¸ªé¢„æµ‹ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰åˆ é™¤â€œOâ€ç±»ï¼Œè¿™æ„å‘³ç€åœ¨è¯¥æ ‡è®°ä¸Šæ‰¾ä¸åˆ°ç‰¹å®šçš„å®ä½“ã€‚ä»¥ä¸‹æ•°ç»„åº”ä¸ºè¾“å‡ºï¼š</p><pre><code>[(&#39;[CLS]&#39;, &#39;O&#39;), (&#39;Hu&#39;, &#39;I-ORG&#39;), (&#39;##gging&#39;, &#39;I-ORG&#39;), (&#39;Face&#39;, &#39;I-ORG&#39;), (&#39;Inc&#39;, &#39;I-ORG&#39;), (&#39;.&#39;, &#39;O&#39;), (&#39;is&#39;, &#39;O&#39;), (&#39;a&#39;, &#39;O&#39;), (&#39;company&#39;, &#39;O&#39;), (&#39;based&#39;, &#39;O&#39;), (&#39;in&#39;, &#39;O&#39;), (&#39;New&#39;, &#39;I-LOC&#39;), (&#39;York&#39;, &#39;I-LOC&#39;), (&#39;City&#39;, &#39;I-LOC&#39;), (&#39;.&#39;, &#39;O&#39;), (&#39;Its&#39;, &#39;O&#39;), (&#39;headquarters&#39;, &#39;O&#39;), (&#39;are&#39;, &#39;O&#39;), (&#39;in&#39;, &#39;O&#39;), (&#39;D&#39;, &#39;I-LOC&#39;), (&#39;##UM&#39;, &#39;I-LOC&#39;), (&#39;##BO&#39;, &#39;I-LOC&#39;), (&#39;,&#39;, &#39;O&#39;), (&#39;therefore&#39;, &#39;O&#39;), (&#39;very&#39;, &#39;O&#39;), (&#39;##c&#39;, &#39;O&#39;), (&#39;##lose&#39;, &#39;O&#39;), (&#39;to&#39;, &#39;O&#39;), (&#39;the&#39;, &#39;O&#39;), (&#39;Manhattan&#39;, &#39;I-LOC&#39;), (&#39;Bridge&#39;, &#39;I-LOC&#39;), (&#39;.&#39;, &#39;O&#39;), (&#39;[SEP]&#39;, &#39;O&#39;)]</code></pre></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'Transformers','ç”¨ä¾‹','ä¸­å¸¸'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=æœç´¢>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>ğŸ”</button></form></section><section class=widget><h3 class=widget-title>æœ€æ–°æ–‡ç«  âš¡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>å…¶ä»–</h3><ul class=widget-list><li><a href=TOS.html>ä½¿ç”¨æ¢æ¬¾</a></li><li><a href=CommentPolicy.html>ç•™è¨€æ”¿ç­–</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>è¯çµ¡æˆ‘å€‘</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>æå®¢å¿«è¨Š</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>