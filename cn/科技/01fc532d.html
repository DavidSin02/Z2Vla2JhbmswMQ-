<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>快速理解kafka端到端的延迟 | 极客快訊</title><meta property="og:title" content="快速理解kafka端到端的延迟 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p9.pstatp.com/large/pgc-image/c9eba460f46e4874bf959581369bd364"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/01fc532d.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/01fc532d.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/01fc532d.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/01fc532d.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/01fc532d.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/01fc532d.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/01fc532d.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/01fc532d.html><meta property="article:published_time" content="2020-11-14T21:03:40+08:00"><meta property="article:modified_time" content="2020-11-14T21:03:40+08:00"><meta name=Keywords content><meta name=description content="快速理解kafka端到端的延迟"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/01fc532d.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>快速理解kafka端到端的延迟</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>目录</p><p>欺诈检测、支付系统和股票交易平台只是许多Apache Kafka用例中的一小部分，这些用例需要快速且可预测的数据交付。例如，在线银行交易的欺诈检测必须实时发生，以交付业务价值，而不需要为每个交易增加超过50 100毫秒的开销，以保持良好的客户体验。</p><p>在Kafka术语中，数据交付时间(data delivery time)是由 端到端延迟(end-to-end latency) 定义的，即 消费者获取一条向Kafka生成的记录所需的时间 。延迟目标表示为目标延迟和满足此目标的重要性。例如，您的延迟目标可以表示为:我希望99%的情况下从Kafka获得端到端延迟为50 ms。</p><p>这将增加 可用性、持久性和吞吐量 目标。实现高持久性和高吞吐量两个目标，我们需要进行一定的权衡，挑战在于在保持延迟界限的同时扩展应用程序的吞吐量，并调整Kafka集群的大小以使用可接受的Broker延迟来处理客户端和复制的请求。延迟也取决于您对硬件或云提供商的选择，所以您需要能够监视和调优您的客户端，以在您独特的环境中实现您的特定延迟目标。</p><p>注意 : 通常情况下，Broker所在的网络区域其实也会对延迟造成很大影响，当然这仍然取决于您对可用性和延迟的权衡。</p><p>之前，我们有写过白皮书 <strong>optimizing-your-apache-kafka-deployment</strong> ，其中列出了配置Kafka部署以优化各种目标的指导原则。</p><p>这篇文章将帮助您进一步获得更好的直觉和对端到端延迟的理解，并配置和扩展您的应用程序的吞吐量，同时保持延迟的界限。</p><h4 class=pgc-h-arrow-right>理解到端的延迟(end-to-end latency)</h4><p>端到端延时是指应用逻辑调用 KafkaProducer.send() 生产消息到该消息被应用逻辑通过 KafkaConsumer.poll() 消费到之间的时间。</p><p>下图显示了一条记录在系统中的路径，从Kafkas生产者到Kafka的Broker节点，副本的复制，以及消费者最终在其主体分区日志中获取到具体的消息。</p><p>因此，端到端的延迟主要会由以下几个部分组成:</p><pre><code>Produce timePublish timeCommit timeCatch-up timeFetch time</code></pre><div class=pgc-img><img alt=快速理解kafka端到端的延迟 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/c9eba460f46e4874bf959581369bd364><p class=pgc-img-caption></p></div><p>kafka端到端的延迟</p><p>在接下来的内容中，我们将分别解释这五个延迟阶段的具体含义，特定的客户端配置或应用逻辑设计通常会极大地影响端到端延时，因此我们有必要精准定位哪个因素对延时的影响最大。</p><h4 class=pgc-h-arrow-right>Produce time</h4><p>Produce time 指的是从应用程序通过 KafkaProducer.send() 生产一条记录到包含该消息的生产者请求被发往leader副本所在的broker之间的时间。(因此，生产者所处的网络环境以及对应topic分区leader副本所在的broker的网络可能会影响到produce time的延迟)</p><p>Kafka producer会将 相同topic分区 下的一组消息打包在一起形成一个批次（batch）以提升网络I/O性能。(在必要情况下，我们可以对生产者的batch size进行一定的调整)</p><p>默认情况下，producer会立即发送batch，这样一个batch中通常不会包含太多的消息。为了提高batch的效率，生产者通常会对 linger.ms 来人为设置一个较小的延迟来保证有足够多的消息记录能封装在一个batch中。一旦过了 linger.ms 设置的事件，或者batch size已经达到最大值( batch.size 的参数值)，这个batch将被认为已经完成。</p><p>如果生产者也开启了压缩( compression.type )，kafka的生产者会将已完成的batch进行压缩。在batch完成之前，它的大小时根据生产者指定的压缩类型和之前观测到的压缩比率估算出来的。</p><p>如果发送给leader副本的未确认的生产者请求数量已经达到最大( max.inflight.requests.per.connection=5 )，则在生产者的批处理可能需要等待更长的时间。因此，broker响应生产者请求越快，生产者的等待时间也将会变得更小。</p><h4 class=pgc-h-arrow-right>Publish time</h4><p>Publish time是指内部kafka生产者发送生产者请求到一个broker节点，并且对应的消息到达leader副本日志之间的时间。当请求到达Broker节点时，负责连接的网络线程将获取该请求并将其放入请求队列中。其中一个请求处理程序线程从队列中获取请求并处理它们。(对应broker节点的num.thread 和num.io.thread两个相关参数)</p><p>因此，Publish time包含生产者请求的网络时间，broker上的排队时间，以及将消息追加到日志所消耗的时间(通常也是page cache 的访问时间)。当Broker端负载比较低，网络和日志的追加写入时间会影响publish time，随着broker负载变高，队列延迟的增加</p><p>将会更多的影响publish time。</p><h4 class=pgc-h-arrow-right>Commit time</h4><p>Commit time是指从leader副本中复制消息到全部的同步副本(all in-sync replicas)中所消耗的时间。kafka只会将已提交(committed)的消息暴露给consumer，也就是该消息必须在全部的ISR中包含。follower副本中的消息会从leader副本中并行的拉取，在一个正常的集群中，我们通常不希望副本处于不同步状态(当然有的业务场景可能会导致短暂的不同步现象)。这意味着消息被提交的时间等于ISR中最慢的follower副本所在broker去从ledaer broker节点获取记录并写入到follower副本日志的时间。</p><p>为了复制数据，follower所在的broker会想leader节点发送fetch请求，准确的来讲消费者也是使用fetch请求来获取消息。但是，官方在副本复制的fetch请求中，broker端优化了默认配置: leader副本会尽早的发送请求，只要有一个字节可用，就会发起fetch请求(由 replica.fetch.min.bytes 参数控制)或者当 replica.fetch.wait.max.ms 满足条件。Commit time主要受副本因此配置参数的影响以及集群的当前负载情况。</p><h4 class=pgc-h-arrow-right>Catch-up time</h4><p>Kafka中消息是按照其生产的顺序被消费的，除非显示的声明了一个新的offset或者有一个新的消费者从最新的offset进行消费。同一个分区下，consumer必须要消费完之前发布的消息后才能读取后面的消息。假设在提交消息时，消费者的偏移量是提交消息后面的N条消息，那么，Catch-up time就是消费者消费者N条消息的总时间.</p><div class=pgc-img><img alt=快速理解kafka端到端的延迟 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7995356be1e84c629f3e06af03e5c956><p class=pgc-img-caption></p></div><p>Catch-up time</p><p>当我们在构建实时处理应用的时候，最好让catch-up时间为0，即一旦消息被提交，消费者可以立马读取到消息。如果消费者总是落后，端到端延迟可能会变得无限大。因此，catch-up 时间通常依赖于消费者的能力是否能够追赶上生产者的吞吐量。</p><h4 class=pgc-h-arrow-right>Fetch time</h4><p>订阅主题分区的消费者会不断轮询去从leader副本中获取更多的数据，Fetch time是从leader副本所在broker节点获取消息记录的s时间，可能需要等待足够的数据来形成对fetch请求的响应，并从 KafkaConsumer.poll() 的响应中返回记录。在默认的配置下，已经对于消费者的fetch延迟做了优化( fetch.min.bytes =1)，即及时只有一个字节可用的时候，fetch请求也会响应数据，或者在一个短暂超时之后 fetch.max.wait.ms</p><h4 class=pgc-h-arrow-right>End-to-end latency VS producer and consumer latencies</h4><p>下图显示了Kafka客户端观察到的延迟(通常称为生产者延迟和消费者延迟)与端到端延迟之间的关系。</p><p>生产者延迟是指 KafkaProducer.send() 发送和生产的消息被确认间的事件。消息的确认依赖于 acks 的配置，该参数可以控制消息的持久性(durability):</p><pre><code>acks=0acks=1acks=all</code></pre><p>所以，生产者延迟包含 produce time , publich time(如果acks >= 1) , commit time(如果acks=all) 以及生产者响应从broker返回到生产者的时间。</p><div class=pgc-img><img alt=快速理解kafka端到端的延迟 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5258b9ee54384117b4e7bb9763cb3b96><p class=pgc-img-caption></p></div><p>end-to-end-latency and producer time</p><p>上图清晰的向我们展示了为何改变 acks 参数能够减少生产者延迟(其实是通过从生产者延迟中移除几个延迟概念来减少的 publish和commit )。不过，无论我们如何配置生产者的 acks 参数，publish和commit 时间总是端到端延迟的一部分。</p><p>消费者延迟(Consumer latency)是指消费者发起一个fetch请求到broker节点，以及broker节点向consumer返回响应的时间。计算方法是 KafkaConsumer.poll() 返回的时间。Consumer的延迟主要包含了上图中的fetch time</p><h4 class=pgc-h-arrow-right>控制end-to-end latency</h4><p>如果我们思考一条消息的生命周期，控制端到端延时其实就是控制消息在系统中流转的时间总和。很多Kafka clients端和broker端参数的默认值已然对延时做了优化：比如减少人为等待时间来提高批处理的能力(通过 linger.ms , fetch.min.bytes , replica.fetch.min.bytes 参数来适当调优)。其他的延时可能来自于broker端上的队列等候时间，控制这种延时就要涉及控制broker的负载（CPU或吞吐量），通常情况下我们要时刻关注broker节点的各项基础监控指标。</p><p>如果我们将系统视为一个整体，那么整个端到端的延迟还要求系统中的每一个部分(生产者,broker,消费者)都能够可靠的维持应用程序逻辑所需的吞吐量。</p><p>例如，如果您的应用程序逻辑以100 MB/s发送数据，但是由于某种原因，您的Kafka消费者吞吐量在几秒钟内下降到10 MB/s，那么在此之前产生的大多数消息都需要在系统中等待更长的时间，直到消费者赶上了。此时，你需要一种高效的方式来扩展你的Kafka clients程序以提升吞吐量——高效地利用broker端资源来减少队列等候时间和偶发的网络拥塞。</p><p>理想情况下，限制延迟意味着确保所有延迟都低于目标。但实际生产环境中，由于意外故障和峰值负载，这种严格的保证是不可能的。不过，可以设计应用程序并对系统进行调优，以实现95%的延迟目标，控制所有的消息延迟在95~99%低于目标延迟时间。高百分位延迟也称为尾部延迟，因为它们是延迟频谱的尾部。</p><p>目标延时所用的百分位越大，你需要降低或容忍应用最差表现所做的努力就越多。比如，偶尔的大请求可能会阻塞全部的请求，从而增加整体的延迟，这也就是所谓的 head-of-line 队首阻塞。同时，大量低速率客户端可能偶尔会同时向kafka发送生产或消费请求，或全部刷新集群元数据，也会导致请求队列比平常更长，从而引发比平时更严重的尾延迟。这种行为就是所谓的 micro-bursting (微型冲击，可能就是水滴石穿的意思吧)</p><h4 class=pgc-h-arrow-right>不同客户端配置的延迟测试</h4><p>在这接下来的内容中，我们使用实验结果来说明Kafka客户端配置和吞吐量扩展技术对性能的影响。我们使用kafka内置的 <strong>Trogdor</strong> 测试框架以及生产者和消费者的基准测试， ProduceBench 和 ConsumeBench 来进行我们的生产者和消费者实验测试。</p><p>我们所有的测试都在一个包含9个代理的Kafka集群上运行，该集群的复制因子为3，这保证了在出现最多两个同时发生的节点故障时不会丢失消息。</p><p>Kafka集群运行在AWS的 r5.xlarge 实例上，使用有2T的EBS(弹性块存储)。Kafka的broker节点分布在同一区域内的三个可用性区域(AZ)，以获得更强的容错性，其中每个主题分区副本被放置在一个不同的AZ上，并且kafka客户端配置使用SASL认证和SSL加密，Broker之间使用 PLAINTEXT 进行通信。</p><p>主题: 需要注意的是，分布式集群中节点如果在不同可用区也可能导致延迟的增加，当然这要在延迟和容错性角度进行权衡，也需要考虑到云厂商的可用区之间本身的延迟。</p><p>我们的实验使用了以下非默认客户端配置和其他规范:</p><p><strong>参数项参数值</strong>副本数3topic的分区数108security.protocolSASL_SSLsasl.mechanismPLAINacksalllinger.ms5compression.typelz4Producer record sizevalue=521bytes/key=4bytesTrogdor record value generatoruniformRandomTrogdor record key generatorsequentialNumber of Trogdor agents9</p><p>这个测试场景会产生额外的延迟: 多可用区可能增加commit时间，由于是跨可用区的副本。无论是clients端还是broker端，SSL加密也是有开销的。同时由于SSL无法利用Zero Copy特性进行数据传输，因为consumer获取消息时也会增加额外的开销。</p><p>虽然这些因素都会影响延迟，但是通常情况下企业内部可能还是需要这种架构上的考虑，因此采用该部署结构进行测试。</p><h4 class=pgc-h-arrow-right>持久性设置对延迟的影响</h4><p>当将延迟目标与其他需求叠加在一起时，首先考虑持久性需求是很有用的。由于数据的重要性，通常需要一定程度的持久性。</p><p>优化持久性会增加端到端延迟，因为这会增加延迟的复制开销(提交时间)，并向Broker添加复制负载，从而增加排队延迟。</p><h4 class=pgc-h-arrow-right>Replication factor</h4><p>Replication factor 是Kafka持久化保证的核心，它定义了Kafka集群上保存的topic副本数。Replication factor = N 表示我们最多能够容忍N-1台broker宕机而不必数据丢失。N=1能够令端到端延时最小化，但却是最低的持久化保证。</p><p>增加副本数会增加备份开销并给broker额外增加负载。如果clients端带宽在broker端均匀分布，那么每个broker都会使用 N * w写带宽 和 r + (N - 1) * w读带宽 ，其中w是clients端在broker上的写入带宽占用，r是读带宽占用。</p><p>由此，降低N 对端到端延时影响的最佳方法就是确保每个broker上的负载是均匀的。这会降低commit time，因为commit time是由最慢的那个follower副本决定的。</p><p>如果你的Kafka broker使用了过多的磁盘带宽或CPU，follower就会开始出现追不上leader的情况从而推高了commit time。(其实还需要注意的是，当最小的ISR默认为副本的数量个数时，在出现follower和leader不同步时恰巧leader节点宕机，会导致topic本身不可用)</p><p>我们建议为副本同步消息流量设置成使用不同的listener来减少与正常clients流量的干扰。你也可以在follower broker上增加I/O并行度，并增加副本拉取线程数量 number.replica.fetchers 来改善备份性能。</p><h4 class=pgc-h-arrow-right>Acks</h4><p>纵然我们配置了多个副本，producer还是必须通过acks参数来配置可靠性水平。设置acks=all能够提供最强的可靠性保证，但同时也会增加broker应答PRODUCE请求的时间，就像我们之前讨论的那样。</p><p>Broker端应答的速度变慢通常会降低单个producer的吞吐量，进而增加producer的等待时间。这是因为producer端会限制未应答请求的数量( max.inflight.requests.per.connection )。</p><p>举个例子，在我们的环境中acks=1，我们启动了9个producer（同时也跑了9个consumer），吞吐量达到了195MB/秒。当acks切换成all时，吞吐量下降到161MB/秒。设置更高级别的acks通常要求我们扩展producer程序才能维持之前的吞吐量水平以及最小化producer内部的等待时间。</p><h4 class=pgc-h-arrow-right>Min.insync.replicas</h4><p>min.insync.replicas 是一个重要的持久化参数，因为它定义了broker端ISR副本中最少要有多少个副本写入消息才算PRODUCE请求成功。这个参数会影响可用性，但是不会影响端到端的延时。因此，选择一个小一点的值并不能减少commit time并减少延迟。</p><h4 class=pgc-h-arrow-right>在满足延迟目标的前提下扩展吞吐</h4><h4 class=pgc-h-arrow-right>延迟和吞吐的权衡</h4><p>优化Kafka clients端吞吐量意味着优化batching的效果。Kafka producer内部会执行一类batching，即收集多条消息到一个batch中。</p><p>每个batch被统一压缩然后作为一个整体被写入日志或从日志中读取。这说明消息备份也是以batch为单位进行的。</p><p>Batching会减少每条消息的成本，因为它将这些成本摊还到clients端和broker端。通常来说，batch越大这种开销降低的效果就越高，减少的网络和磁盘I/O就越多。</p><p>另一类batching就是在单个网络请求/响应中收集多个batch以减少网络数据传输量。这能降低clients端和broker端的请求处理开销。这类batching能够提升吞吐量和降低延时，因为batch越大，网络传输I/O量越小，CPU和磁盘使用率越低，故最终能够优化吞吐量。另外batch越大还能减低端到端延时，因为每条消息的成本降低了，使得系统处理相同数量消息的总时间变少了。</p><p>这里的延时-吞吐量权衡是指通过人为增加等待时间来提升打包消息的能力。但过了某个程度，人为等待时间的增加可能会抵消或覆盖你从打包机制获得的延时收益。因此你的延时目标有可能会限制你能实施打包化的水平，进而减少所能达到的吞吐量并增加延时。如果拉低了本能达到的吞吐量或端到端延时水平，你可以通过扩展集群来换取或“购买”更多的吞吐量或处理能力。</p><h4 class=pgc-h-arrow-right>配置kafka的生产者和消费者以实现batching</h4><p>对于producer而言，batching由两个参数进行控制: batch.size(16KB) 和 linger.ms(0) ，前者控制batch的大小，后者限制延迟量。如果使用场景中，应用会频繁的像kafka集群发送数据，及时设置了 linger.ms=0 ，整个batch也会被尽快填满。如果应用生产数据的频率较低，可以通过增加 linger.ms 来增加batch。</p><p>对于consumer而言，可以调整 fetch.min.bytes(1) 来限制每个消费者在每个fetch响应中接收的数据量，该参数指定了broker应该在一个fetch响应中返回的最小数据，以及 fetch.max.wait.ms(500) 来设置等待数据的超时时间。在fetch响应中的数据越多，就会有更少的fetch请求。</p><p>在生产者端的batching也会间接影响produce和fetch的请求数量，因为batch定义了数据能够被获取的最小数据量。</p><p>值得注意的是，默认情况下，Kafka producer和consumer设置的是无人为等待时间，这么做的目的是为了降低延时。但是，即使你的目标就是了使延时最小化，我们依然推荐你设置一个不为0的linger.ms值，比如5~10ms。当然，这么做是有前提的：</p><pre><code>linger.mslinger.ms</code></pre><p>下面这个实验说明了以上两种场景。我们启动了90个producer，向一个有108个分区的topic发送消息。生产端整体的吞吐量峰值在90MB/秒。我们跑了3次测试，每一次对应一种不同的producer配置。</p><div class=pgc-img><img alt=快速理解kafka端到端的延迟 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/55b336893a704ec492a3363796649e6d><p class=pgc-img-caption></p></div><p>因为在给定的总吞吐下，我们有相对大量的生产者，因此 linger.ms = 0 导致在生产者端机会没有batch操作。将 linger.ms 从0调整到5可以增加batching能力: 向kafka发起的生产者请求从2800降低到了1100。这减少了50%和99%的生产者延迟。</p><p>增加 batch.size 不会直接影响生产者的等待时间，因为生产者在填满batch的时间不会超过 linger.ms 的限制。在我们的实验中，增加 batch.size 从16KB到128KB没有增加bacth的效果，因为每个生产者的吞吐量非常低。正如预期的那样，生产者延迟在两种配置之间没有变化。</p><p>总之，如果您的目标是最小化延迟，我们建议保留默认的客户端批处理配置，并尽可能增加 linger.ms 。如果你在意尾延时，最好调优下打包水平来减少请求发送率以及大请求冲击的概率。</p><h4 class=pgc-h-arrow-right>不增加人为延迟以提高batching效率</h4><p>batching效果不好的另一个原因是producer发送消息给大量分区。 如果消息不是发往同一个分区的，它们就无法聚集在一个batch下 。因此，通常最好设计成让每个producer都只向有限的几个分区发送消息。</p><p>另外，可以考虑升级到Kafka 2.4 producer。这个版本引入了一个全新的Sticky分区器。该分区器能够改善non-keyed topic的打包效果，同时还无需引入人为等待。</p><h4 class=pgc-h-arrow-right>clients的数量对尾延迟(tail-latency)的影响</h4><p>即使整体的生产和消费的吞吐量保持不变，通常也是Clients数越多，broker上负载越大。这是因为clients数量多会导致更多的METADATA请求发到Kafka，继而要维护更多的连接，故给broker带来更大的开销。</p><p>相对于50%或平均延时的影响，Clients数量增加对尾延时的影响更大。</p><p>每个producer最多发送 max.inflight.requests.per.connection 个PRODUCE请求给单个broker，而每个consumer一次最多只会给一个broker发送FETCH请求。Clients越多，同一时刻发送到broker的PRODUCE和FETCH请求也就越多，这就增加了形成请求瞬时冲击的概率，进而推高了尾延时。</p><p>Consumer数量通常由topic分区数量以及期望consumer没有较大lag的目标共同决定。但是，我们却很容易为了扩展吞吐量而引入大量的producer。</p><p>基于吞吐量的考量增加producer实例数可能有相反的效果，因为producer会导致更少的消息被打包，毕竟每个producer处理了更少的消息，因而发送速率会变慢。同时producer还必须等待更长的时间来积累相同数量的消息进到batch里面。</p><p>在我们的实验中，我们将producer的数量从90增加到900，发现吞吐量没有他打变化：90MB/秒。</p><p>我们使用 batch.size=16KB , linger.ms=5 , acks=all 的生产者配置，实验结果如下:</p><div class=pgc-img><img alt=快速理解kafka端到端的延迟 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b3d60a71eded48f88f54519cf848eb28><p class=pgc-img-caption></p></div><p>结果显示增加producer数量(90->900)增加了60%的中位数延时值，而99%延时值几乎增加了3倍。</p><p>延时的增加是因为producer端打包效果变差导致的。</p><p>尾延时的增加是因为更大的请求瞬时冲击，这会拉升broker端延时，同时producer端会等待更长的时间来接收应答。</p><p>在900个producer的测试中，broker完全被PRODUCE请求压垮了。用于处理请求的时间几乎占到了broker端CPU使用率的100%。另外由于我们使用了SSL，它也会进一步引入请求级的开销。</p><p>如果你通过添加producer来提升吞吐量，那么可以考虑增加单个proudcer的吞吐量，即 改善batching的效果 。不管怎样，你最终可能会有很多producer实例。比如，大公司收集设备上的统计指标，而设备数可能有成千上万。此时，你可以考虑使用一个Broker收集来自多个clieints的请求，然后把它们转换成更高效的PRODUCE请求再发给Kafka。你也可以增加broker数来降低单个broker上的请求负载。</p><h4 class=pgc-h-arrow-right>关于增加消费者数量的说明</h4><p>当扩展消费者时需要注意，在同一个消费者组的消费者会提交offset信息和心跳到broker节点上(controller节点)。如果按时间间隔执行偏移提交( auto.commit.interval.ms )，则消费者组中的更多消费者者会增加偏移提交率。偏移量提交本质上是向内部 __consumer_offsets 产生请求，因此增加consumer数量会导致broker上的请求负载增加，特别是 auto.commit.interval.ms 值很小的时候。</p><h4 class=pgc-h-arrow-right>压缩配置的影响</h4><p>默认情况下，Kafka producer不做压缩。 compression.type 参数可以决定要不要做压缩。</p><p>压缩会在producer端引入额外的开销来压缩消息，在broker端做校验时解压缩从而引入额外的开销，另外在consumer端解压缩也是开销。</p><p>注意 :通常情况下broker端的压缩参数需要设置成 producer ，以避免压缩方式冲突导致数据无法正常消费，这样broker只需要直接将压缩后的日志写入</p><p>虽然压缩会增加CPU开销，但它还是可能减少端到端延时的，因为它能显著地降低处理数据所需的带宽占用，进而减少broker端的负载。压缩是在batch级别上完成的，故打包效果越好，压缩效果也就越好。</p><h4 class=pgc-h-arrow-right>更多的分区可能增加延迟</h4><p>一个主题的分区是kafka中的并行单元。发送到不同分区的消息可以由生产者并行发送，由不同的Broker并行写入，并可以由不同的消费者并行读取。因此，更多的分区通常会导致更高的吞吐量，不过单从吞吐量的角度来看，我们能够从每个Broker有10个分区的kafka集群，就已经能够达到最大的吞吐量了。您可能需要更多的主题分区来支持您的应用程序逻辑。</p><p>但是，太多的分区可能导致更多的端到端的延迟。每个主题的分区越多，对生产者的批处理就越少。每个Broker的主题分区越多，每个follow副本获取请求的开销就越大。每个fetch请求必须去枚举自己感兴趣的分区，并且每个leader副本必须去检查状态，同时从请求的每个分区中去fetch数据，这通常会导致较小的磁盘I/O。因此，越多的分区，可能会导致更长的commit time和更高的cpu使用，最终导致较长的排队延迟。</p><p>提交时间的增加和更高的CPU负载会导致所有共享同一个Kafka集群的客户端端到端延迟的增加，即使是那些只生产和使用少量主题分区的客户端来说，也是如此。</p><p>我们使用两个topic来做此次测试。一个Topic有9个生产者生产5MB/s的数据，然后有一个对应9个消费者的消费者组。这个实验持续了几天，我们将这个主题中的分区数量从108个逐步增加到7200个(每个Broker8000个)，每个步骤运行一个小时。第二个主题在整个实验运行期间有9个分区和9个生产者，每个生产者向一个分区和一个对应的消费者组(每个分区一个)生产消息，该主题每秒生产一个512bytes的数据。</p><p>下图显示了分区数量对客户端访问9分区主题的99%的端到端延迟的影响，随着每个broker上分区数的增加，clients的端到端延时大致呈线性增加趋势。分区数的增加会推高broker上的CPU负载同时拖慢所有clients的备份，即使是对那些只与固定分区数量交互的clients而言，也会抬高端到端延迟。</p><div class=pgc-img><img alt=快速理解kafka端到端的延迟 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5b63bf5b19b341569d254beed6885b33><p class=pgc-img-caption></p></div><p>mutli-partition-latency</p><p>为了减少延时，最好还是限制每个broker上的分区数，方法是减少总的分区数或扩展集群。你还可以通过增加fetcher线程数量的方式来改善commit time。</p><h4 class=pgc-h-arrow-right>broker节点负载对延迟的影响</h4><p>我们已经讨论了Broker上的负载导致增加排队延迟，从而增加了端到端的延迟，很容易看出为什么请求速率的增加会加剧排队延迟，因为更多的请求会导致更大的队列。</p><p>broker节点上高资源利用率(磁盘或cpu)可能导致更高的队列的延迟，并且延迟的增长会随着资源利用率的增长呈指数级增长。这是一个可以有排队理论解释的已知属性: Kingman公式证明等待某种资源的时间正比于资源繁忙程度/资源空闲程度 (% of time resource is busy)/(% of time resource is idle) .</p><div class=pgc-img><img alt=快速理解kafka端到端的延迟 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e8f0516a463c4c87a55f0bd8221b4d41><p class=pgc-img-caption></p></div><p>由于延迟随资源利用率呈指数增长，如果broker中的任何资源的利用率接近100%，您可能会看到很高的延迟。通过减少每个Broker的资源使用(比如减少每个broker的链接数，请求以及分区数)或扩展集群来整体降低每个broker节点的资源使用率，在这种情况可以显著降低延迟。保持负载在broker之间平均通常情况下是非常有用的，同时也可以 均匀地或基于负载分布分区副本 也能降低尾部延迟。</p><p>因此，通常情况下，负责kafka集群的SRE团队需要自动检测Broker节点上的高资源利用率(磁盘和CPU)，然后重新平衡集群上的分区，以便更均匀地在Broker之间重新分配负载，或者在需要时扩展集群。而如果使用云厂商提供的kafka服务，则可以适当避免这类事情的发生，因为云服务会去做相关的事情。</p><h4 class=pgc-h-arrow-right>总结</h4><p>我们已经演示了，在为吞吐量扩展客户机和分区时的边界延迟要求时可以通过限制每个broker的连接数、分区数和请求速率来限制每个broker的使用。</p><p>边界尾延迟需要额外注意减少来自客户机的任何突发(连接和请求)或应用程序行为中的差异。</p><p>均匀加载的broker节点对于最小化尾部延迟也很重要，因为它受到最慢broker的影响。</p><p>如下是一些相关扩展的文章，可能有助于我们控制整体延迟:</p><ul><li><strong>Optimizing Your Apache Kafka Deployment</strong> 提供配置Kafka部署的指导方针，以优化各种目标:吞吐量、延迟、持久性和可用性(公众号回复: kafka 可获取资料)</li><li><strong>How to Choose the Number of Topics/Partitions in a Kafka Cluster?</strong> (2016) 和 <strong>follow-updates</strong> 有一些kafka 2.0版本的优化</li><li><strong>Improving batching with the new sticky partitioner</strong></li><li><strong>KIP-345</strong> : 引入静态成员协议来减少消费者的再平衡</li></ul></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'kafka','端到','延迟'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>