<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>图解十大 CNN 架构 | 极客快訊</title><meta property="og:title" content="图解十大 CNN 架构 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RY01Bu1Hn58Ql"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0274b1cd.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0274b1cd.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0274b1cd.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0274b1cd.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0274b1cd.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0274b1cd.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0274b1cd.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0274b1cd.html><meta property="article:published_time" content="2020-10-29T21:12:41+08:00"><meta property="article:modified_time" content="2020-10-29T21:12:41+08:00"><meta name=Keywords content><meta name=description content="图解十大 CNN 架构"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/0274b1cd.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>图解十大 CNN 架构</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RY01Bu1Hn58Ql><p><strong>CNN 取得的大多数进展并非源自更强大的硬件、更多的数据集和更大的模型，而主要是由新的想法和算法以及优化的网络结构共同带来的结果。</strong></p><p>原标题 | Illustrated: 10 CNN Architectures</p><p>翻译 | 廖颖、had_in（电子科技大学）、爱曼纽•西蒙（东南大学）</p><blockquote><p>所谓“常见”，我指的是那些深度学习库(如TensorFlow、Keras和PyTorch)共享的有预训练权重的模型，以及通常在课堂上所讲的模型。其中一些模型在ImageNet大规模视觉识别挑战赛(ILSVRC)等竞赛中取得了成功。</p></blockquote><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RY01BuL2AhIXPa><p>将讨论的10个架构及对应论文的年份</p><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RY01BubFd2pXBG><p>我们将讨论在Keras中具有预训练模型的6种架构。上图改编自Keras文档中的一个表。</p><p>写这篇文章的初心是考虑到目前没有太多图解网络结构的博客和文章（如果你知道相关的文章，请分享给我吧）。所以我决定写一篇文章来作为参考。出于这样的目的，我阅读了许多论文和代码（大多来自TensorFlow和Keras）来完成这篇文章。</p><p>补充一点，我们平时看到的卷积神经网络架构是很多因素的结果——升级的计算机硬件、ImageNet比赛、处理特定的任务、新的想法等等。Google 研究员 Christian Szegedy曾提到：</p><blockquote><p>CNN 取得的大多数进展并非源自更强大的硬件、更多的数据集和更大的模型，而主要是由新的想法和算法以及优化的网络结构共同带来的结果。（Christian Szegedy等人，2014）</p></blockquote><p>现在我们继续介绍，看看网络结构是如何慢慢优化起来的。</p><blockquote><p>关于可视化图的说明：可视化图中没有再标注卷积核数量、padding、stride、dropout和拉平操作。</p></blockquote><p></p><h3><strong>目录 (按发表年份排序)</strong></h3><ol><li><p>LeNet-5</p></li><li><p>AlexNet</p></li><li><p>VGG-16</p></li><li><p>Inception-v1</p></li><li><p>Inception-v3</p></li><li><p>ResNet-50</p></li><li><p>Xception</p></li><li><p>Inception-v4</p></li><li><p>Inception-ResNets</p></li><li><p>ResNeXt-50</p></li></ol><p></p><h3><strong>图例</strong></h3><p></p><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RY01But9Yxh4n8><p></p><h2><strong>1. LeNet-5 (1998)</strong></h2><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RY01Bv74Lx75Pu><p>图1：LeNet-5架构，引自他们的论文</p><p>LeNet-5是最简单的架构之一。它有2个卷积层和3个全连接层(因此是“5”——神经网络的名称通常是由它们拥有的卷积层和全连接层的数量派生出来的)。我们现在所知道的平均池化层被称为子采样层，它具有可训练的权重(和当前设计CNNs不同)。这个架构有大约60,000个参数。</p><p></p><h3><strong>⭐️创新点：</strong></h3><p>这种架构已经成为标准的“模板”:叠加卷积层和池化层，并以一个或多个全连接层结束网络。</p><p></p><h3><strong>发表：</strong></h3><ul><li><p>论文：Gradient-Based Learning <i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">App</i>lied to Document Recognition</p></li><li><p>作者：Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner</p></li><li><p>发表于：Proceedings of the IEEE (1998)</p></li></ul><p></p><h3><strong>2. AlexNet</strong></h3><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RY01CHRFMrNfHt><p>图2：AlexNet结构，引自他们的论文</p><p>AlexNet网络有6千万个参数，8个网络层——5个卷积层和3个全连接层。相比于LeNet-5，AlexNet只是堆了更多的网络层。</p><p>在论文发表时，作者指出AlexNet是“在ImageNet子集上训练的最大的卷积神经网络之一。”</p><p><strong>⭐️创新点：</strong></p><p>1.他们首次实现将线性整流函数（ReLus）作为激活函数。</p><p>2.使用卷积神经网络的重叠池化。</p><p><strong>发表：</strong></p><ul><li><p>论文：深度卷积神经网络用于ImageNet分类</p></li><li><p>作者：Alex Krizhevsky, IIya Sutskever, Geoffrey Hinton. 加拿大，多伦多大学</p></li><li><p>发表于：2012年神经信息处理系统<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">会议</i>（NeurIPS 2012）</p></li></ul><p></p><h2><strong>3. VGG-16 (2014)</strong></h2><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RY01CHlUU4TJ6><p>图3：VGG-16架构，引自他们的论文</p><p>你现在应该已经注意到CNNs开始变得越来越深了。这是因为提高深度神经网络性能最直接的方法是增加它们的大小(Szegedy et. al)。Visual Geometry Group (VGG)的工作人员提出了VGG-16，它有13个卷积层和3个全连接层，继续采用了AlexNet的ReLU激活函数。同样，这个网络只是在AlexNet上堆叠了更多的层。它有138M的参数，占用大约500mb的磁盘空间。他们还设计了一种更深的变型，VGG-19。</p><p><strong>⭐️创新点：</strong></p><p>正如他们在摘要中提到的，本文的贡献在于设计了更深层次的网络(大约是AlexNet的两倍)。</p><p><strong>发表：</strong></p><ul><li><p>论文：Very Deep Convolutional Networks for Large-Scale Image Recognition</p></li><li><p>作者：Karen Simonyan, Andrew Zisserman. University of Oxford, UK.</p></li><li><p>arXiv 印本, 2014</p></li></ul><p></p><h2><strong>4. Inception-v1 (2014)</strong></h2><h2><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RY01CI04dckmbm></h2><p>图4：Inception-v1架构。这个CNN有两个辅助网络(在推断时被丢弃)。体系结构引自论文中的图3。</p><p>这个22层网络架构具有5M的参数，被称为 Inception-v1 网络 。这个架构，如论文中所述，大量使用了Network In Network(参见附录)方法。这是通过“Inception 模块”实现的。Inception模块的架构设计是对稀疏结构近似研究的产物(更多信息请阅读论文)。每个模块有3个点改进：</p><p>1.使用不同卷积的并行拓扑结构，然后进行连接，获得1×1、3×3和5×5卷积提取的不同特征，从而对它们进行“归并”。这一想法的灵感来自Arora等人在论文Provable bounds for learning some deep representations，改论文提出了一种逐层构建的方法，即分析最后一层的相关统计数据，并将其归并成具有高相关性的单元组。</p><p>2.采用1×1卷积进行降维，消除计算瓶颈。</p><p>3.1×1卷积在卷积层中加入非线性(基于Network In Network论文)。</p><p>作者还引入了两个辅助分类器，以使分类器在较浅层的网络部分也进行识别，以增加反向传播的梯度信息，并提供额外的正则化。辅助网络(连接到辅助分类器的分支)在推断时被丢弃。</p><p><strong>⭐️创新点：</strong></p><p>使用稠密modules/blocks构建网络。我们并非堆叠卷积层，而是堆叠modules或blocks，其中包含卷积层。Inception得名于2010年由莱昂纳多·迪卡普里奥主演的科幻电影《盗梦空间》。</p><p><strong>发表：</strong></p><ul><li><p>论文：Going Deeper with Convolutions</p></li><li><p>作者：Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich. Google, University of Michigan, University of North Carolina</p></li><li><p>发表于：2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p></li></ul><p></p><h2><strong>5. Inception-v3 (2015)</strong></h2><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RY01CIE1kA4dOq><p><strong>⭐️创新点：</strong></p><p>引入BN层(为了简单起见，没有反映在上面的图中 )。</p><p><strong>✨与之前的版本 Inception-v1 相比，有什么改进?</strong></p><p>将7×7卷积替换为一系列3×3个卷积</p><p><strong>发表：</strong></p><ul><li><p>论文：Rethinking the Inception Architecture for Computer Vision</p></li><li><p>作者：Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna. Google, University College London</p></li><li><p>发表于：2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p></li></ul><p></p><h2><strong>6. ResNet-50 (2015)</strong></h2><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RY01CIVHcA1M2q><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RY01Cha4LMGoRG><p><strong>⭐️创新点：</strong>推广跳连接结构skip connections (</p><p><strong>发表：</strong></p><ul><li><p>论文：Deep Residual Learning for Image Recognition</p></li><li><p>作者：Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Microsoft</p></li><li><p>发表于：2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p></li></ul><p></p><h3><strong>7. Xception(2016)</strong></h3><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RY01Chw3HC6zym><p>图7：Xception 架构，基于keras-team在GitHub上的代码。通道独立卷积层被记作'conv sep'。</p><p>Xception是从Inception上改进，Inception模块用通道独立卷积层替换。它与Inception-v1的参数数量大致相同（23M）。</p><p>Xception将Inception假设引入eXtreme（因此而得名）。那么什么是Inception假设？谢天谢地，文章中明确提到了这一点（感谢François!)</p><ul><li><p>首先，通过1x1卷积核捕获跨通道（或交叉特征映射）相关性。</p></li><li><p>其次，通过常规3x3或5x5卷积捕获每个通道内的空间相关性。</p></li></ul><p>将这个想法运用到极致意味着对每个通道执行1x1卷积，然后对每个输出执行3x3。这与用通道独立卷积替换初始模块相同。</p><p></p><h4><strong>⭐️创新点：</strong></h4><p>引入完全基于通道独立卷积层的CNN。</p><p></p><h4><strong>发表：</strong></h4><p></p><ul><li><p>论文：Xception: Deep Learning with Depthwise Separable Convolutions</p></li><li><p>作者：François Chollet. Google.</p></li><li><p>发表于：2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)</p></li></ul><p></p><h2><strong>8. Inception-v4 (2016)</strong></h2><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RY01CiCEf7ir6g><p></p><h4><strong>发表：</strong></h4><ul><li><p>论文：Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</p></li><li><p>作者：Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi. Google.</p></li><li><p>发表于：Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</p></li></ul><p></p><h2><strong>9. Inception-ResNet-V2 (2016)</strong></h2><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RY01CiREs05XdN><p><strong>✨与前一个版本 Inception-v3 相比，有什么改进?</strong></p><p>1.将 Inception模块转化为Residual Inception模块。</p><p>2.加入更多的Inception模块。</p><p>3.在Stem模块之后添加一个新的Inception模块(Inception-A)。</p><p><strong>发表：</strong></p><ul><li><p>论文：Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</p></li><li><p>作者：Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi. Google</p></li><li><p>发表于：Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</p></li></ul><p></p><h2><strong>10. ResNeXt-50 (2017)</strong></h2><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RY01Cij2ZExHNw><p>Fig. 10: ResNeXt架构, 引自对应论文.</p><p><strong>发表：</strong></p><ul><li><p>论文：Aggregated Residual Transformations for Deep Neural Networks</p></li><li><p>作者：Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He. University of California San Diego, Facebook Research</p></li><li><p>发表于：2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p></li></ul><p></p><h3><strong>附录：Network In Network (2014)</strong></h3><strong>⭐️创新点：</strong><p>1.MLP卷积层, 1×1卷积</p><p>2.全局平均池化(取每个特征map的平均值，并将结果向量输入softmax层)</p><p><strong>发表：</strong></p><ul><li><p>论文: Network In Network</p></li><li><p>作者: Min Lin, Qiang Chen, Shuicheng Yan. National University of Singapore</p></li><li><p>arXiv印本, <i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">2013</i></p></li></ul><p><strong>这里把10个网络结构的可视化图再罗列一下，作简单的回顾：</strong></p><h3><strong>LeNet-5</strong></h3><h3><strong>AlexNet</strong></h3><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RY01D91FmBjKMY><p></p><h3><strong>VGG-16</strong></h3><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RY01D9O5aun3Uo><p></p><h2><strong>Inception-v1</strong></h2><p></p><h2><strong>Inception-v3</strong></h2><p></p><h2><strong>Inception-v4</strong></h2><p></p><h2><strong>Inception-ResNet-V2</strong></h2><p></p><h2><strong>Xception</strong></h2><p></p><h2><strong>ResNet-50</strong></h2><p></p><h2><strong>ResNeXt-50</strong></h2><p></p><h3><strong>神经网络可视化资源</strong></h3><p>这里有一些资源可以让你可视化你的神经网络:</p><ul><li><p>Netron （https://lutzroeder.github.io/netron/）</p></li><li><p>TensorBoard API by TensorFlow（https://<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">www.</i>tensorflow.org/tensorboard/r1/overview）</p></li><li><p>plot_model API by Keras（https://keras.io/visualization/）</p></li><li><p>pytorchviz package（https://github<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-4">.com</i>/szagoruyko/pytorchviz）</p></li></ul><p></p><h3><strong>类似文章</strong></h3><ul><li><p>CNN Architectures: LeNet, AlexNet, VGG, GoogLeNet, ResNet and more ….</p></li><li><p>A Simple Guide to the Versions of the Inception Network</p></li></ul><p></p><h3><strong>参考</strong></h3><p>我使用了提出了上述网络体系结构的论文作为参考。除此之外，这里还有一些我在本文中引用的文章:</p><ul><li><p>https://github<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-4">.com</i>/tensorflow/models/tree/master/research/slim/nets(github<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-4">.com</i>/tensorflow)</p></li><li><p>Implementation of deep learning models from the Keras team(github<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-4">.com</i>/keras-team)</p></li><li><p>Lecture Notes on Convolutional Neural Network Architectures: from LeNet to ResNet (slazebni.cs.illinois.edu)</p></li><li><p>Review: NIN — Network In Network (Image Classification)(towardsdatascience<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-4">.com</i>)</p></li></ul><p>via https://towardsdatascience<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-4">.com</i>/illustrated-10-cnn-architectures-95d78ace614d</p><img alt="图解十大 CNN 架构" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RXhalWP3tXXfbO></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'图解','CNN','架构'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>