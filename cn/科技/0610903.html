<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>深度学习中的优化器对比 | 极客快訊</title><meta property="og:title" content="深度学习中的优化器对比 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/e6c889ca1bde4454985ccf697dd54117"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0610903.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0610903.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0610903.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0610903.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0610903.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0610903.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0610903.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0610903.html><meta property="article:published_time" content="2020-10-29T21:01:05+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:05+08:00"><meta name=Keywords content><meta name=description content="深度学习中的优化器对比"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/0610903.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>深度学习中的优化器对比</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p class=pgc-end-literature>介绍了神经网络训练过程中的常见优化策略，并进行了分析和对比，包括梯度下降、小批量梯度下降、动量梯度下降、RMSProp、Adam 以及最新的RAdam和Ranger</p><h1 class=pgc-h-arrow-right>批量梯度下降法BGD</h1><p>假设训练样本总数为n，样本为 {(x1,y1),(x2,y2),,,,(xn,yn)}，模型参数θ， ，损失函数为fuhao J(θ)，在第i对样本 (xi,yi)上损失函数关于参数的梯度为 ▽J(θ,xi,yi), 学习率为α，，则使用BGD更新参数为：</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e6c889ca1bde4454985ccf697dd54117><p class=pgc-img-caption></p></div><p>由上式可以看出，每进行一次参数更新，需要计算整个数据样本集，因此导致批量梯度下降法的速度会比较慢，尤其是数据集非常大的情况下，收敛速度就会非常慢，但是由于每次的下降方向为总体平均梯度，它得到的会是一个全局最优解。</p><h1 class=pgc-h-arrow-right>SGD</h1><p>随机梯度下降法，不像BGD每一次参数更新，需要计算整个数据样本集的梯度，而是每次参数更新时，仅仅选取一个样本(xi,yi) 计算其梯度，参数更新公式为：</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/37a922b335674ffa830fd2904c954270><p class=pgc-img-caption></p></div><p>可以看到BGD和SGD是两个极端，SGD由于每次参数更新仅仅需要计算一个样本的梯度，训练速度很快，即使在样本量很大的情况下，可能只需要其中一部分样本就能迭代到最优解，由于每次迭代并不是都向着整体最优化方向，导致梯度下降的波动非常大，更容易从一个局部最优跳到另一个局部最优，准确度下降。</p><h1 class=pgc-h-arrow-right>Momentum</h1><h2 class=pgc-h-arrow-right>动量优化法</h2><p>动量优化方法引入物理学中的动量思想，<strong>加速梯度下降</strong>，有Momentum和Nesterov两种算法。当我们将一个小球从山上滚下来，没有阻力时，它的动量会越来越大，但是如果遇到了阻力，速度就会变小，动量优化法就是借鉴此思想，使得梯度方向在不变的维度上，参数更新变快，梯度有所改变时，更新参数变慢，这样就能够加快收敛并且减少动荡。</p><h2 class=pgc-h-arrow-right>Momentum</h2><p>momentum算法思想：参数更新时在一定程度上保留之前更新的方向，同时又利用当前batch的梯度微调最终的更新方向，简言之就是通过积累之前的动量来加速当前的梯度。假设 mt表示t时刻的动量，</p><p>μ 表示动量因子，通常取值0.9或者近似值，在SGD的基础上增加动量，则参数更新公式如下：</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/76d46a8f05e847769f4a8ea49d96d078><p class=pgc-img-caption></p></div><p>在梯度方向改变时，momentum能够降低参数更新速度，从而减少震荡；在梯度方向相同时，momentum可以加速参数更新， 从而加速收敛。总而言之，momentum能够加速SGD收敛，抑制震荡。</p><h1 class=pgc-h-arrow-right>自适应学习率优化算法</h1><p>在机器学习中，学习率是一个非常重要的超参数，但是学习率是非常难确定的，虽然可以通过多次训练来确定合适的学习率，但是一般也不太确定多少次训练能够得到最优的学习率，玄学事件，对人为的经验要求比较高，所以是否存在一些策略自适应地调节学习率的大小，从而提高训练速度。 目前的自适应学习率优化算法主要有：AdaGrad算法，RMSProp算法，Adam算法以及AdaDelta算法。</p><h1 class=pgc-h-arrow-right>AdaGradfuhao</h1><p>定义参数：全局学习率δ，一般会选择 δ=0.01 ; 一个极小的常量 ε ，通常取值10e-8,目的是为了分母为0; 梯度加速变量(gradient accumulation variable) r。</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/10c4cdd95f784ce1935357aec66d0777><p class=pgc-img-caption></p></div><p>从上式可以看出，梯度加速变量r为t时刻前梯度的平方和</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6467d743ee2e4eb58c1cf9c835401cc6><p class=pgc-img-caption></p></div><p>, 那么参数更新量</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ea6be94a2c874a748acdb44cc9fac33b><p class=pgc-img-caption></p></div><p>，将</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/01b32742246d4fb7ad64774680e8492a><p class=pgc-img-caption></p></div><p>看成一个约束项regularizer. 在前期，梯度累计平方和比较小，也就是r相对较小，则约束项较大，这样就能够放大梯度, 参数更新量变大; 随着迭代次数增多，梯度累计平方和也越来越大，即r也相对较大，则约束项变小，这样能够缩小梯度，参数更新量变小。</p><p><strong>缺点：</strong></p><ul><li>仍需要手工设置一个全局学习率δ , 如果δ 设置过大的话，会使regularizer过于敏感，对梯度的调节太大</li><li>中后期，分母上梯度累加的平方和会越来越大，使得参数更新量趋近于0，使得训练提前结束，无法学习</li></ul><h1 class=pgc-h-arrow-right>Adadelta</h1><p>Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/e6e6a1d638d4470b8d7666bd71bb85ff><p class=pgc-img-caption></p></div><p>从上式中可以看出，Adadelta其实还是依赖于全局学习率ŋ，但是作者做了一定处理，经过近似牛顿迭代法之后</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/34d109049a324dc0bbc02220aac8352b><p class=pgc-img-caption></p></div><p>此时可以看出Adadelta已经不依赖全局learning rate了。</p><p><strong>特点：</strong></p><ul><li>训练初中期，加速效果不错，很快。</li><li>训练后期，反复在局部最小值附近抖动。</li></ul><h1 class=pgc-h-arrow-right>RMSprop</h1><p>RMSProp算法修改了AdaGrad的梯度平方和累加为指数加权的移动平均，使得其在非凸设定下效果更好。设定参数：全局初始率 δ , 默认设为0.001; decay rate ρ,默认设置为0.9,一个极小的常量 ε ，通常为10e-6</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5cb1b7814d2846e199d4e0e2971b2d9d><p class=pgc-img-caption></p></div><p><strong>特点：</strong></p><ul><li>其实RMSprop依然依赖于全局学习率 δ</li><li>RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间</li><li>适合处理非平稳目标——对于RNN效果很好</li></ul><h1 class=pgc-h-arrow-right>Adam: Adaptive Moment Estimation</h1><p>Adam中动量直接并入了梯度一阶矩（指数加权）的估计。其次，相比于缺少修正因子导致二阶矩估计可能在训练初期具有很高偏置的RMSProp，Adam包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩估计。 默认参数值设定为： β₁=0.9，β₂=0.999，ε =10-8</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/171f3c5f4d044b08a60c622528b1676b><p class=pgc-img-caption></p></div><p>其中， mt， nt分别是对梯度的一阶矩估计和二阶矩估计；</p><p>m^t ，n^t 是对 mt，nt 的偏差校正，这样可以近似为对期望的无偏估计</p><p><strong>特点：</strong></p><ul><li>Adam梯度经过偏置校正后，每一次迭代学习率都有一个固定范围，使得参数比较平稳。</li><li>结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点</li><li>为不同的参数计算不同的自适应学习率</li><li>也适用于大多非凸优化问题——适用于大数据集和高维空间。</li></ul><h1 class=pgc-h-arrow-right>算法的表现</h1><p>下图是各个算法在等高线的表现，它们都从相同的点出发，走不同的路线达到最小值点。可以看到，Adagrad，Adadelta和RMSprop在正确的方向上很快地转移方向，并且快速地收敛，然而Momentum和NAG先被领到一个偏远的地方，然后才确定正确的方向，NAG比momentum率先更正方向。SGD则是缓缓地朝着最小值点前进。</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a085268bdb1e46e69c2826ab4f91e0dc><p class=pgc-img-caption></p></div><p><br></p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5dd74dd150b84d08a6f618579615199b><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>RAdam</h1><p><br></p><p>新的优化器：Rectified Adam（RAdam），相比adam，可以稳定提高准确率.</p><p>这是经典Adam优化器的一个新变种，Adam用到的warmup是一种方差衰减器，但所需的warmup程度是未知的，而且数据集之间是不同的，因此，用数学算法来作为一种动态方差衰减器，即构建了一个整流器项，这允许自适应动量作为一个潜在的方差的函数缓慢但稳定地得到充分表达。完整模型：</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6f32824146b64a1eab5417679530b5aa><p class=pgc-img-caption></p></div><p><br>RAdam根据方差的潜在散度动态地打开或关闭自适应学习率。实际上，它提供了不需要可调参数的动态warmup。</p><p><br></p><h1 class=pgc-h-arrow-right>Ranger</h1><p>新的state of the art优化器：Ranger</p><p>Ranger是RAdam 与 LookAhead 的互补</p><p>RAdam 可以说是优化器在开始训练时的最佳基础。RAdam 利用动态整流器根据方差调整 Adam 的自适应动量，并有效提供能够根据当前数据集定制的自动预热机制，能够确保训练以扎实的基础顺利迈出第一步。</p><p>LookAhead 则受到深度神经网络损失面的最新理解进展启发，能够在整个训练期间提供健壮且稳定的突破。</p><p>引用 LookAhead 团队的说法——LookAhead“减少了对广泛超参数调整的需求”，同时实现了“以最小计算开销确保不同深度学习任务实现更快收敛速度。”</p><p>因此，二者都在深度学习优化的不同方面带来了突破，而且这种组合具有高度协同性，有望为大家的深度学习结果提供两项最佳改进。如此一来，通过将两项最新突破 (RAdam + LookAhead) 加以结合，Ranger 的整合成果有望为深度学习带来新的发展驱动力，帮助我们进一步追求更稳定且强大的优化方法。</p><p>Hinton 等人曾表示：“我们凭经验证明，Lookahead 能够显著提高 SGD 与 Adam 的性能，包括在 ImageNet、CIFAR-10/100、神经机器翻译以及 Penn Treebank 上的默认超参数设置场景之下。”</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e9a12e6525c048839bb149ac07c4dc78><p class=pgc-img-caption></p></div><p>Lookahead：探索损失面的辅助系统，带来更快且更稳定的探索与收敛效果</p><div class=pgc-img><img alt=深度学习中的优化器对比 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/34f2c2c9a56146c788ad0b43d9744202><p class=pgc-img-caption></p></div><p><br></p><p>转载：</p><p class=pgc-end-literature><br></p><p class=pgc-end-literature>优化算法Optimizer比较和总结https://zhuanlan.zhihu.com/p/55150256</p><p class=pgc-end-literature>https://zhuanlan.zhihu.com/p/22252270</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'化器','学习','深度'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>