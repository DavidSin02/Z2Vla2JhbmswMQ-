<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>腾讯开源TurboTransformers,推理加速性能超TensorRT主流优化引擎 | 极客快訊</title><meta property="og:title" content="腾讯开源TurboTransformers,推理加速性能超TensorRT主流优化引擎 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/42922a5c526a463294f0a5f08a2ee281"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/083e83e.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/083e83e.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/083e83e.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/083e83e.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/083e83e.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/083e83e.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/083e83e.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/083e83e.html><meta property="article:published_time" content="2020-10-29T21:01:27+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:27+08:00"><meta name=Keywords content><meta name=description content="腾讯开源TurboTransformers,推理加速性能超TensorRT主流优化引擎"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/083e83e.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>腾讯开源TurboTransformers,推理加速性能超TensorRT主流优化引擎</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>机器之心报道</p><p><strong>机器之心编辑部</strong></p><blockquote><p>4月24日，腾讯发布了在 GitHub 上的第 100 个开源项目「TurboTransformers」，在多种 CPU 和 GPU 硬件测试中，这款 Transformer 推理加速工具获得了超越 PyTorch/TensorFlow 和目前主流优化引擎的性能表现。</p></blockquote><p><br></p><div class=pgc-img><img alt=腾讯开源TurboTransformers,推理加速性能超TensorRT主流优化引擎 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/42922a5c526a463294f0a5f08a2ee281><p class=pgc-img-caption></p></div><p>在自然语言处理领域，以 BERT 为代表的 Transformer 神经网络模型是近年来最重要的模型创新，为诸如阅读理解、文章摘要、语义分类、同义改写等 NLP 任务带了显著的效果提升。但 Transformer 在提高模型精度的同时，也引入了更多的计算量，这导致 Transformer 的线上 NLP 服务在部署方面面临着巨大挑战。</p><div class=pgc-img><img alt=腾讯开源TurboTransformers,推理加速性能超TensorRT主流优化引擎 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/76dbdb7d1a2e47cbb65ba2a504c0321d><p class=pgc-img-caption></p></div><p><br></p><p>业界普遍采用 TensorFlow 或者 Pytorch 来完成 Transformer 的训练部分，但由于深度学习的训练和推理任务存在差异，训练框架直接应用于线上推理并不能得到极致的性能。</p><p>4月24日，腾讯宣布开源 Transformer 推理加速工具「TurboTransformers」。Turbo 的意思是「涡轮」，一般用来增加发动机氧气含量，带来更大动力，TurboTransformers 则意味着可使推理引擎更加强劲。</p><p>项目地址：https://github.com/Tencent/TurboTransformers</p><p><strong>从业界痛点出发：高速、实用、简单</strong></p><p>具体而言，TurboTransformers 具备高速、实用、简单三个特点：</p><p><strong>1. 优异的CPU/GPU 性能表现</strong></p><p>面向 Intel 多核 CPU 和 NVIDIA GPU 硬件平台，通过核心融合和并行算法优化，TurboTransformers 充发挥硬件的各层级并行计算的能力。在多种 CPU 和 GPU 硬件上获得了超过 PyTorch/TensorFlow 和目前主流优化引擎（如 onnxruntime-mkldnn/onnxruntime-gpu, torch JIT, NVIDIA faster transformers）的性能表现。</p><p><strong>2. 为NLP推理任务特点量身定制</strong></p><p>TurboTransformers 可以支持变长输入序列处理，无需序列补零、截断或者分桶带来的无用计算，也无需任何针对计算图在推理前进行预调优的过程。</p><p><strong>3. 使用方式简单</strong></p><p>TurboTransformers 支持 python 和 C++接口进行调用。TurboTransformers 支持 TensorFlow 和 PyTorch 预训练模型的载入。它可以作为 huggingface/transformers 的推理加速插件，通过加入几行 python 代码获得 BERT 模型的端对端加速效果。</p><p>和 ONNX-runtime、TensorRT、Torchlib 等推理优化引擎相比，TurboTransformers 在性能和使用方式上都具备优势。</p><p><br></p><div class=pgc-img><img alt=腾讯开源TurboTransformers,推理加速性能超TensorRT主流优化引擎 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5fec29499c22488a81013c061070dc16><p class=pgc-img-caption></p></div><p><br></p><p>此前，TurboTransformers 已应用在腾讯内部多个线上 BERT 服务服务场景，微信常用问题回复服务获得 1.88x 加速，公有云情感分析服务获得 2.11x 加速，QQ 看点推荐服务获得 13.6x 加速。</p><p><strong>整体架构</strong></p><p>TurboTransformers 在算子优化、框架优化和接口部署方式简化三个方面做了改进。</p><div class=pgc-img><img alt=腾讯开源TurboTransformers,推理加速性能超TensorRT主流优化引擎 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/50f9462e4e1a4e8985b11e011e9196c8><p class=pgc-img-caption></p></div><p>TurboTransformers 软件架构图。</p><p><strong>算子层优化</strong></p><div class=pgc-img><img alt=腾讯开源TurboTransformers,推理加速性能超TensorRT主流优化引擎 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c24309bc32b04531ad7aed82eca15f84><p class=pgc-img-caption></p></div><p><br></p><p>上图 (a) 展示了论文 Transformer 结构示意图，灰色方框内的结构称为一个 Transformer Cell，BERT encoder 堆叠了 Nx 个这样的 Transformer Cell。图 (b) 为 Cell 的展开细节，每一个矩形都是一个独立的计算核心。</p><p>Transformer Cell 计算包含了 8 个 GEMM（通用矩阵乘法，General Matrix Multiplication）运算，通过调优 Intel MKL 和 cuBLAS 的 GEMM 调用方式来获得最佳 GEMM 性能，并在硬件允许条件下，在 GPU 上使用 tensor core 方式进行 GEMM 运算。</p><p>类似于 NVIDIA FasterTransformers 方案，TurboTransformers 将所有 GEMM 运算之间的计算融合成一个调用核心。这样有两个好处，一是减少了内存访问开销，二是减少多线程启动开销。</p><p>对于这些核心，TurboTransformers 在 CPU 上采用 openmp 进行并行，在 GPU 上使用 CUDA 进行优化实现。对于比较复杂的 LayerNorm 和 Softmax 算子，它们包含了不适合 GPU 上并行的规约操作，TurboTransformers 为它们设计了创新并行算法，极大降低了这些算子的延迟。</p><p><strong>框架层优化</strong></p><p>由于 NLP 的采用变长输入特性，每次运算中间结果的大小其实并不相同。为了避免每次都分配释放内存，TurboTransformers 通过 Caching 方式管理显存。</p><p>此外，团队为 TurboTransformers 提供了一些脚本，将二者的预训练模型转化为 npz 格式供其读入，以无缝支持 pytorch/tensorflow 训练好的序列化模型。考虑到 pytorch huggingface/transformers 是目前最流行的 transformer 训练方法，TurboTransformers 支持直接读入 huggingface/transformers 预训练模型。</p><p><strong>应用部署</strong></p><p>为了减少用户开发难度，TurboTransformers 提供了 C++和 Python 调用接口，可以嵌入到 C++多线程后台服务流程中，也可加入到 pytorch 服务流程中，增加几行代码即可获得端到端 BERT 加速。现阶段更建议通过 docker 部署 TurboTransformers，一方面保证了编译的可移植性，另一方面也可以无缝应用于 K8S 等线上部署平台。</p><p><strong>性能测试</strong></p><p>团队首先在三个 CPU 硬件平台上测试了 TurboTransformers 的性能，下图显示了在 Intel Xeon 6133 CPU 上的性能测试结果（150 次迭代的均值）：</p><div class=pgc-img><img alt=腾讯开源TurboTransformers,推理加速性能超TensorRT主流优化引擎 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f5c706652c73481ca5a305db00739880><p class=pgc-img-caption></p></div><p><br></p><p>接下来在四个 GPU 硬件平台上进行测试，下图显示了在 NVIDIA RTX 2060 GPU 和 NVIDIA V100 GPU 上的性能测试结果（150 次迭代的均值）：</p><div class=pgc-img><img alt=腾讯开源TurboTransformers,推理加速性能超TensorRT主流优化引擎 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e70d053b2efc4a8bbad89a4c55c0e57a><p class=pgc-img-caption></p></div><p>NVIDIA RTX 2060 GPU 测试结果。</p><div class=pgc-img><img alt=腾讯开源TurboTransformers,推理加速性能超TensorRT主流优化引擎 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/fb12a16a8eab481380b750fcd916878c><p class=pgc-img-caption></p></div><p>NVIDIA V100 GPU 测试结果。</p><p>此外，团队还在多个CPU和GPU等平台上测试了 TurboTransformers，更多性能测试结果可见项目主页。</p><p>目前，TurboTransformers 暂时只支持 FP32 的计算，未来腾讯将对其进一步改进，包括对 GPU FP16 的支持等能力等。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'腾讯','开源','TurboTransformers'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>