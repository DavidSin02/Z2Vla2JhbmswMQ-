<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>分布式存储Ceph可靠性的量化分析 | 极客快訊</title><meta property="og:title" content="分布式存储Ceph可靠性的量化分析 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/c4bbbd3f6cdc4f2ab77a1580836190f5"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0e65b3f7.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0e65b3f7.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0e65b3f7.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0e65b3f7.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0e65b3f7.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0e65b3f7.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0e65b3f7.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0e65b3f7.html><meta property="article:published_time" content="2020-11-14T21:07:11+08:00"><meta property="article:modified_time" content="2020-11-14T21:07:11+08:00"><meta name=Keywords content><meta name=description content="分布式存储Ceph可靠性的量化分析"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/0e65b3f7.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>分布式存储Ceph可靠性的量化分析</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><h2 class=pgc-h-arrow-right>什么情况下数据会丢失？</h2><p>这个话题的另外一种提法就是存储的可靠性，所谓存储的可靠性最基本的一点就是数据不要丢失，也就是我们俗称的“找不回来了”。所以，要分析Ceph的可靠性我们只需要搞清楚，到底在什么情况下我们的数据会丢失，并且再也无法恢复，基于此我们便可以创建我们的计算模型。</p><p>我们先来假定一套简单的Ceph环境，3个OSD节点，每个OSD节点对应一块物理硬盘，副本数为3。那么我们排除MON的因素影响Ceph集群的运行的问题，显而易见，当三个OSD对应的物理硬盘全部损坏时，数据必然无法恢复。所以此时集群的可靠性是与硬盘本身的可靠性直接相关。</p><p>我们再来假定一套更大的Ceph环境，30个OSD节点，分3个机架摆放，每一个机架有10个OSD节点，每个OSD节点仍然对应一块物理硬盘，副本数为3，并且通过CRUSH MAP，将每一份副本均匀分布在三个机架上，不会出现两份副本同时出现在一个机架的情况。此时，什么时候会出现数据丢失的情况呢？当三个机架上都有一块硬盘损坏，而恰恰这三块硬盘又保存了同一个Object的全部副本，此时数据就会出现丢失的情况。</p><p>所以根据以上的分析，我们认为，Ceph的可靠性的计算是与OSD的数量(N)、副本数(R)、每一个服务节点的OSD数量(S)、硬盘的年失败概率(AFR)。这里我们使用UnitedStack相关参数进行计算。</p><p>具体取值如下图所示：</p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c4bbbd3f6cdc4f2ab77a1580836190f5><p class=pgc-img-caption></p></div><p></p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8b73c092d0f3423a9af50d4646082bb1><p class=pgc-img-caption></p></div><h2 class=pgc-h-arrow-right>硬盘年失败概率</h2><p>根据维基百科的计算方法(http://en.wikipedia.org/wiki/Annualized_failure_rate)，AFR的计算方法如下：</p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/30aa18ab23d74f5c9a88be5ebfeddf9c><p class=pgc-img-caption></p></div><p>例如，计算Seagate某企业级硬盘的AFR，根据文档查到MTBF为1,200,000小时，则AFR为0.73%，计算过程如下：</p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bda7bfe16d554c8487d6c0665ae8b72f><p class=pgc-img-caption></p></div><p>但是，根据Google的相关计算，在一个大规模集群环境下，往往AFR的值并没有硬盘厂商那样乐观，下面的统计告诉了我们在真实环境下AFR变化的情况：</p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/85eb6a465c134bec978c832ec3c24e1c><p class=pgc-img-caption></p></div><p>所以我们可以看到实际的AFR的变化范围随着年份而变化，取值范围在1.7%-8%左右，所以本文中AFR为1.7%。</p><h2 class=pgc-h-arrow-right>硬盘在一年之内损坏的概率</h2><p>有了AFR，我们就可以尝试计算硬盘在一年中出现故障的概率，根据相关研究，硬盘在一定时间内的失败概率符合Possion分布(已经把知识还给老师的同学请移步：http://en.wikipedia.org/wiki/Poisson_distribution)。具体的计算公式为：</p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c3c8d9f197fb45fd9b23990268f44ded><p class=pgc-img-caption></p></div><p>当我最初拿到这个计算公式时，一下子懵了，到底该如何确定数学期望值lamda呢？</p><h2 class=pgc-h-arrow-right>lamda的计算过程</h2><p>根据相关的研究资料，单块的硬盘损坏的期望值(Failures in Time)是指每10亿小时硬盘的失败率(Failure Rate λ)，计算过程如下：</p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/03b741d933374daa93a6d628c36cc96c><p class=pgc-img-caption></p></div><p>这里的Af(Acceleration Factor)是由测试时间乘以阿伦尼乌斯方程的值得出来的结果，好吧，我承认，我也是现学现卖，这个方程式是化学反应的速率常数与温度之间的关系式，适用于基元反应和非基元反应，甚至某些非均相反应。不过可以看出Failure Rate的计算过程实质主要是计算环境因素引起的物理变化，最终导致失败的数学期望值。所以根据相关研究，最终FIT的计算方法为：</p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d66a857cb1f49f1971028c9e94a3a11><p class=pgc-img-caption></p></div><p>有了这些参数后，我们就可以开始正式计算Ceph集群中，不同机架上有三块硬盘同时出现损坏的概率啦。</p><h2 class=pgc-h-arrow-right>任意一个OSD出现损坏的概率P1(any)</h2><p>我们不太容易直接去计算任意一个OSD出现损坏的概率，但是我们很容易计算没有OSD出现问题的概率，方法如下，用一减去无OSD节点出现问题的概率，得到P1(any)。</p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4d0f5f6c86e145ac9a43c4159951799e><p class=pgc-img-caption></p></div><h2 class=pgc-h-arrow-right>在恢复时间内第二个节点出现故障的概率P2(any)</h2><p>我们知道当Ceph发现一个有问题的OSD节点时，会自动的将节点OUT出去，这个时间大约为10min，同时Ceph的自我修复机制会自动平衡数据，将故障节点的数据重新分配在其他的OSD节点上。</p><p>我们假设我们单盘的容量为1000 GB，使用率为75%，也就是此时将有750 GB的数据需要同步。我们的数据只在本机架平衡，节点写入速度为50 MB/s，计算方法如下：</p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/69f57222373040a5ae671b2711af8382><p class=pgc-img-caption></p></div><p>注意：由于每个节点有三个OSD，所以要求每台物理机所承受的节点带宽至少要大于150 MB/s。并且在这个计算模型下，并没有计算元数据、请求数据、IP包头等额外的信息的大小。</p><p>有了Recovery Time，我们就可以计算我们第二个节点在Recovery Time内失败的概率，具体的计算过程如下：</p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5f9d16d70ad449898459118c1b5deceb><p class=pgc-img-caption></p></div><h2 class=pgc-h-arrow-right>在恢复时间内第三个节点出现故障的概率P3(any)</h2><p>计算方法同上，计算过程如下：</p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/61e2a8a02e2a4b8bb92524cb7ac5f782><p class=pgc-img-caption></p></div><h2 class=pgc-h-arrow-right>一年内任意副本数(R)个OSD出现故障的概率</h2><p>所以将上述概率相乘即可得到一年内任意副本数(R)个OSD出现故障的概率。</p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/884338c07e264c36aeb4bd868298bf6e><p class=pgc-img-caption></p></div><h2 class=pgc-h-arrow-right>Copy Sets(M)</h2><p>在这个计算模型中，因为任意R个OSD节点的损坏并不意外着副本的完全丢失，因为损坏的R个OSD未必保存着一个Object的全部副本信息，所以未必造成数据不可恢复，所以这里引入了Copy Sets的概念。简单来说，Copyset就是存放所有拷贝的一个集合，具体的定义和计算方法可以查看参考链接。那么这里的场景下，Copy Sets为三个机架OSD数量相乘，即M=242424。当然如果是两个副本的情况下，M应该为2424+2424+24*24。</p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/c12a7e18757a4c2da4874ba444e01a8e><p class=pgc-img-caption></p></div><h2 class=pgc-h-arrow-right>CEPH的可靠性</h2><p>所以最终归纳出CEPH可靠性的算法为：</p><p><br></p><div class=pgc-img><img alt=分布式存储Ceph可靠性的量化分析 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/24f0872e5a4046d48bbdecce5695ec76><p class=pgc-img-caption></p></div><p><br></p><p>可以看出Ceph三副本的可靠性大约为9个9，由于Recovery Time和AFR取值的问题，所以计算结果有出入。</p><h2 class=pgc-h-arrow-right>参考链接</h2><ul><li>Annualized Failure Rate</li><li>Poisson distribution</li><li>Calculating Reliability using FIT & MTTF: Arrhenius HTOL Model</li><li>Google’s Disk Failure Experience</li><li>Failure Trends in a Large Disk Drive Population</li><li>Copysets: Reducing the Frequency of Data Loss in Cloud Storage</li></ul></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'存储','Ceph','可靠性'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>