<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>TensorFlowå®ç°æ—¶é—´åºåˆ—é¢„æµ‹ | æå®¢å¿«è¨Š</title><meta property="og:title" content="TensorFlowå®ç°æ—¶é—´åºåˆ—é¢„æµ‹ - æå®¢å¿«è¨Š"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/27d989e04ac245cf9dc645bd7b60f356"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/12b78f12.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/12b78f12.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/12b78f12.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/12b78f12.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/12b78f12.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/12b78f12.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/12b78f12.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/12b78f12.html><meta property="article:published_time" content="2020-11-14T21:03:07+08:00"><meta property="article:modified_time" content="2020-11-14T21:03:07+08:00"><meta name=Keywords content><meta name=description content="TensorFlowå®ç°æ—¶é—´åºåˆ—é¢„æµ‹"><meta name=author content="æå®¢å¿«è¨Š"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/12b78f12.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>ğŸ¤“ æå®¢å¿«è®¯ Geek Bank</a></h1><p class=description>ä¸ºä½ å¸¦æ¥æœ€å…¨çš„ç§‘æŠ€çŸ¥è¯† ğŸ§¡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>çŒœä½ å–œæ­¡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=ç§‘æŠ€>ç§‘æŠ€</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=éŠæˆ²>éŠæˆ²</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=ç§‘å­¸>ç§‘å­¸</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>TensorFlowå®ç°æ—¶é—´åºåˆ—é¢„æµ‹</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>ç§‘æŠ€</a></span></div><div class=post-content><p>å¸¸å¸¸ä¼šç¢°åˆ°å„ç§å„æ ·æ—¶é—´åºåˆ—é¢„æµ‹é—®é¢˜ï¼Œå¦‚å•†åœºäººæµé‡çš„é¢„æµ‹ã€å•†å“ä»·æ ¼çš„é¢„æµ‹ã€è‚¡ä»·çš„é¢„æµ‹ï¼Œç­‰ç­‰ã€‚TensorFlowæ–°å¼•å…¥äº†ä¸€ä¸ªTensorFlow Time Seriesåº“ï¼ˆä»¥ä¸‹ç®€ç§°ä¸ºTFTSï¼‰ï¼Œå®ƒå¯ä»¥å¸®åŠ©åœ¨TensorFlowä¸­å¿«é€Ÿæ­å»ºé«˜æ€§èƒ½çš„æ—¶é—´åºåˆ—é¢„æµ‹ç³»ç»Ÿï¼Œå¹¶æä¾›åŒ…æ‹¬ARã€LSTMåœ¨å†…çš„å¤šä¸ªæ¨¡å‹ã€‚</p><h2 class=pgc-h-arrow-right>æ—¶é—´åºåˆ—é—®é¢˜</h2><p>ä¸€èˆ¬è€Œè¨€ï¼Œæ—¶é—´åºåˆ—æ•°æ®æŠ½è±¡ä¸ºä¸¤éƒ¨åˆ†ï¼šè§‚å¯Ÿçš„æ—¶é—´ç‚¹å’Œè§‚å¯Ÿçš„å€¼ ï¼ˆä»¥å•†å“ä»·æ ¼ä¸ºä¾‹ï¼ŒæŸå¹´ä¸€æœˆçš„ä»·æ ¼ä¸º120å…ƒï¼ŒäºŒæœˆçš„ä»·æ ¼ä¸º130å…ƒï¼Œä¸‰æœˆçš„ä»·æ ¼ä¸º135å…ƒï¼Œå››æœˆçš„ä»·æ ¼ä¸º132å…ƒã€‚é‚£ä¹ˆè§‚å¯Ÿçš„æ—¶é—´ç‚¹å¯ä»¥çœ‹ä½œæ˜¯1,2,3,4ï¼Œè€Œåœ¨å„æ—¶é—´ç‚¹ä¸Šè§‚å¯Ÿåˆ°çš„æ•°æ®çš„å€¼ä¸º120,130,135,132ï¼‰ ã€‚è§‚å¯Ÿçš„æ—¶é—´ç‚¹å¯ä»¥ä¸è¿ç»­ï¼Œæ¯”å¦‚äºŒæœˆçš„æ•°æ®æœ‰ç¼ºå¤±ï¼Œé‚£ä¹ˆå®é™…çš„è§‚å¯Ÿæ—¶é—´ç‚¹ä¸º1,3,4ï¼Œå¯¹åº”çš„æ•°æ®ä¸º120,135,132ã€‚ æ‰€è°“æ—¶é—´åºåˆ—é¢„æµ‹ï¼Œæ˜¯æŒ‡é¢„æµ‹æŸäº›æœªæ¥çš„æ—¶é—´ç‚¹ä¸Šï¼ˆå¦‚5,6ï¼‰æ•°æ®çš„å€¼åº”è¯¥æ˜¯å¤šå°‘ ã€‚</p><p>TFTSåº“æŒ‰ç…§æ—¶é—´ç‚¹+è§‚å¯Ÿå€¼çš„æ–¹å¼å¯¹æ—¶é—´åºåˆ—é—®é¢˜è¿›è¡ŒæŠ½è±¡åŒ…è£…ã€‚è§‚å¯Ÿçš„ <strong>æ—¶é—´ç‚¹ç”¨â€œtimesâ€è¡¨ç¤º</strong> ï¼Œå¯¹åº”çš„ <strong>å€¼ç”¨â€œvaluesâ€è¡¨ç¤º</strong> ã€‚åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œè¾“å…¥æ•°æ®éœ€è¦åŒæ—¶å…·æœ‰timeså’Œvaluesä¸¤ä¸ªå­—æ®µï¼›åœ¨é¢„æµ‹æ—¶ï¼Œéœ€è¦ç»™å®šä¸€äº›åˆå§‹çš„æ•°å€¼ï¼Œä»¥åŠéœ€è¦é¢„æµ‹çš„æ—¶é—´ç‚¹timesã€‚</p><h2 class=pgc-h-arrow-right>è¯»å…¥æ—¶é—´åºåˆ—æ•°æ®</h2><p>åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œéœ€è¦å°†æ—¶é—´åºåˆ—æ•°æ®è¯»å…¥æˆTensorçš„å½¢å¼ã€‚ <strong>TFTS</strong> åº“ä¸­æä¾›äº†ä¸¤ä¸ªæ–¹ä¾¿çš„è¯»å–å™¨</p><ul><li><strong>NumpyReader</strong> ï¼šç”¨äºä»Numpyæ•°ç»„ä¸­è¯»å…¥æ•°æ®</li><li><strong>CSVReader</strong> ï¼šç”¨äºä»CSVæ–‡ä»¶ä¸­è¯»å…¥æ•°æ®</li></ul><h2 class=pgc-h-arrow-right>ä»Numpyæ•°ç»„ä¸­è¯»å–æ—¶é—´åºåˆ—</h2><p>å¯¼å…¥éœ€è¦çš„åŒ…åŠå‡½æ•°</p><pre><code>import numpy as npimport matplotlibmatplotlib.use('agg')import matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.timeseries.python.timeseries import NumpyReader</code></pre><p>æ¥ç€ï¼Œåˆ©ç”¨np.sinç”Ÿæˆä¸€ä¸ªå®éªŒç”¨çš„æ—¶é—´åºåˆ—æ•°æ®ã€‚è¯¥æ—¶é—´åºåˆ—æ•°æ®å®é™…ä¸Šæ˜¯åœ¨æ­£å¼¦æ›²çº¿ä¸ŠåŠ å…¥äº†ä¸Šå‡çš„è¶‹åŠ¿å’Œä¸€äº›éšæœºçš„å™ªå£°ï¼š</p><pre><code>x = np.array(range(1000))noise = np.random.uniform(-0.2, 0.2, 1000)y = np.sin(np.pi * x / 100) + x / 200. + noiseplt.plot(x, y)plt.savefig('timeseries_y.jpg')</code></pre><div class=pgc-img><img alt=TensorFlowå®ç°æ—¶é—´åºåˆ—é¢„æµ‹ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/27d989e04ac245cf9dc645bd7b60f356><p class=pgc-img-caption></p></div><p>æ¨ªåº§æ ‡å¯¹åº”å˜é‡â€œxâ€ï¼Œçºµåº§æ ‡å¯¹åº”å˜é‡â€œyâ€ï¼Œå®ƒä»¬åˆ†åˆ«å¯¹åº”ä¹‹å‰æåˆ°è¿‡çš„â€œè§‚å¯Ÿçš„æ—¶é—´ç‚¹â€å’Œâ€œè§‚å¯Ÿåˆ°çš„å€¼â€ã€‚TFTSè¯»å…¥xå’Œyçš„æ–¹å¼éå¸¸ç®€å•ï¼Œè¯·çœ‹ä¸‹é¢çš„ä»£ç ï¼š</p><pre><code>data = {    tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,    tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,}reader = NumpyReader(data)</code></pre><p>é¦–å…ˆæŠŠxå’Œyå˜æˆPythonä¸­çš„å­—å…¸ï¼ˆå˜é‡dataï¼‰ã€‚ä¸Šé¢çš„å®šä¹‰ç›´æ¥å†™æˆâ€œ <strong>data={â€˜times':x, â€˜values':y}</strong> â€ä¹Ÿæ˜¯å¯ä»¥çš„ã€‚å†™æˆæ¯”è¾ƒå¤æ‚çš„å½¢å¼æ˜¯ä¸ºäº†å’Œæºç ä¸­çš„å†™æ³•ä¿æŒä¸€è‡´ã€‚</p><p>å¾—åˆ°çš„readeræœ‰ä¸€ä¸ªread_full()æ–¹æ³•ï¼Œå®ƒçš„è¿”å›å€¼æ˜¯æ—¶é—´åºåˆ—å¯¹åº”çš„Tensorï¼Œå¯ä»¥ç”¨ä¸‹é¢çš„ä»£ç è¿›è¡Œè¯•éªŒï¼š</p><pre><code>with tf.Session() as sess:    full_data = reader.read_full()    # è°ƒç”¨read_fullæ–¹æ³•ä¼šç”Ÿæˆè¯»å–é˜Ÿåˆ—    # è¦ç”¨tf.train.start_queue_runnerså¯åŠ¨é˜Ÿåˆ—æ‰èƒ½æ­£å¸¸è¿›è¡Œè¯»å–    coord = tf.train.Coordinator()    threads = tf.train.start_queue_runners(sess=sess, coord=coord)    print(sess.run(full_data))    coord.request_stop()</code></pre><p>åœ¨è®­ç»ƒæ—¶ï¼Œé€šå¸¸ä¸ä¼šä½¿ç”¨æ•´ä¸ªæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè€Œæ˜¯é‡‡ç”¨batchçš„å½¢å¼ã€‚ä»readerå‡ºå‘ï¼Œå»ºç«‹batchæ•°æ®çš„æ–¹æ³•ä¹Ÿå¾ˆç®€å•ï¼š</p><pre><code>train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(    reader, batch_size=2, window_size=10)</code></pre><p>tf.contrib.timeseries.RandomWindowInputFnä¼šåœ¨readerçš„æ‰€æœ‰æ•°æ®ä¸­ï¼Œéšæœºé€‰å–çª—å£é•¿åº¦ä¸ºwindow_sizeçš„åºåˆ—ï¼Œå¹·åŒ…è£…æˆbatch_sizeå¤§å°çš„batchæ•°æ®ã€‚æ¢å¥è¯è¯´ï¼Œä¸€ä¸ªbatchå†…å…±æœ‰batch_sizeä¸ªåºåˆ—ï¼Œæ¯ä¸ªåºåˆ—çš„é•¿åº¦ä¸ºwindow_sizeã€‚</p><p>ä»¥batch_size=2, window_size=10ä¸ºä¾‹ï¼Œå¯ä»¥æ‰“å°å‡ºä¸€ä¸ªbatchçš„æ•°æ®ï¼š</p><pre><code>with tf.Session() as sess:    batch_data = train_input_fn.create_batch()    coord = tf.train.Coordinator()    threads = tf.train.start_queue_runners(sess=sess, coord=coord)    one_batch = sess.run(batch_data[0])    coord.request_stop()print('one_batch_data:', one_batch)# one_batch_data: {'times': array([[11, 12, 13, 14, 15, 16, 17, 18, 19, 20],#        [21, 22, 23, 24, 25, 26, 27, 28, 29, 30]]), 'values': array([[[0.33901882],#         [0.29966548],#         [0.64006627],#         [0.35204604],#         [0.66049626],#         [0.57470108],#         [0.68309054],#         [0.46613038],#         [0.60309193],#         [0.84166497]],# #        [[0.77312242],#         [0.82185951],#         [0.71022706],#         [0.63987861],#         [0.7011966 ],#         [0.84051192],#         [1.05796465],#         [0.92981324],#         [1.0542786 ],#         [0.89828743]]])}</code></pre><p>åŸå…ˆçš„æ•°æ®é•¿åº¦ä¸º1000çš„æ—¶é—´åºåˆ—ï¼ˆx=np.array(range(1000))ï¼‰ï¼Œä½¿ç”¨tf.contrib.timeseries.RandomWindowInputFnï¼Œå¹¶æŒ‡å®šwindow_size=10, batch_size=2çš„åŠŸèƒ½æ˜¯åœ¨è¿™é•¿åº¦ä¸º1000çš„æ—¶é—´åºåˆ—ä¸­ï¼Œéšæœºé€‰å–é•¿åº¦ä¸º10çš„åºåˆ—ï¼Œå¹¶åœ¨æ¯ä¸ªbatché‡ŒåŒ…å«ä¸¤ä¸ªè¿™æ ·çš„åºåˆ—ã€‚è¿™ä¹Ÿå¯ä»¥ä»æ‰“å°å‡ºçš„æ•°æ®ä¸­çœ‹å‡ºæ¥ã€‚</p><p>ä½¿ç”¨tf.contrib.timeseries.RandomWindowInputFnè¿”å›çš„train_input_fnå¯ä»¥è¿›è¡Œè®­ç»ƒäº†ã€‚è¿™æ˜¯åœ¨TFTSä¸­è¯»å…¥Numpyæ•°ç»„æ—¶é—´åºåˆ—çš„åŸºæœ¬æ–¹å¼ã€‚ä¸‹é¢ä»‹ç»å¦‚ä½•è¯»å…¥CSVæ ¼å¼çš„æ•°æ®ã€‚</p><h2 class=pgc-h-arrow-right>ä»CSVæ–‡ä»¶ä¸­è¯»å–æ—¶é—´åºåˆ—</h2><p>æœ‰æ—¶ï¼Œæ—¶é—´åºåˆ—æ•°æ®æ˜¯å­˜åœ¨CSVæ–‡ä»¶ä¸­çš„ã€‚å½“ç„¶å¯ä»¥å°†å…¶å…ˆè¯»å…¥ä¸ºNumpyæ•°ç»„ï¼Œå†ä½¿ç”¨ä¹‹å‰çš„æ–¹æ³•å¤„ç†ã€‚æ›´æ–¹ä¾¿çš„åšæ³•æ˜¯ä½¿ç”¨tf.contrib.timeseries.CSVReaderè¯»å…¥ã€‚æ•°æ®æ–‡ä»¶ <strong>period_trend.csv</strong></p><p>å‡è®¾CSVæ–‡ä»¶çš„æ—¶é—´åºåˆ—æ•°æ®çš„å½¢å¼ä¸ºï¼š</p><pre><code>1,-0.66566037142,-0.11643803593,0.73986264884,0.73686330295,0.2289480898...</code></pre><p>CSVæ–‡ä»¶çš„ç¬¬ä¸€åˆ—ä¸ºæ—¶é—´ç‚¹ï¼Œç¬¬äºŒåˆ—ä¸ºè¯¥æ—¶é—´ç‚¹ä¸Šè§‚å¯Ÿåˆ°çš„å€¼ã€‚å°†å…¶è¯»å…¥çš„æ–¹æ³•ä¸ºï¼š</p><pre><code>import tensorflow as tfcsv_file_name = './period_trend.csv'reader = tf.contrib.timeseries.CSVReader(csv_file_name)</code></pre><p>å®é™…è¯»å…¥çš„ä»£ç åªæœ‰ä¸€è¡Œï¼Œç›´æ¥ä½¿ç”¨å‡½æ•°tf.contrib.timeseries.CSVReaderå¾—åˆ°äº†readerã€‚å°†readerä¸­æ‰€æœ‰æ•°æ®æ‰“å°å‡ºæ¥çš„æ–¹æ³•å’Œä¹‹å‰æ˜¯ä¸€æ ·çš„ï¼š</p><pre><code>with tf.Session() as sess:    data = reader.read_full()    coord = tf.train.Coordinator()    threads = tf.train.start_queue_runners(sess=sess, coord=coord)    print(sess.run(data))    coord.request_stop()</code></pre><p>ä»readerå‡ºå‘ï¼Œå»ºç«‹batchæ•°æ®çš„train_input_fnçš„æ–¹æ³•ä¹Ÿå®Œå…¨ç›¸åŒï¼š</p><pre><code>train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(reader, batch_size=4, window_size=16)</code></pre><p>æœ€åï¼Œå¯ä»¥æ‰“å°å‡ºä¸¤ä¸ªbatchçš„æ•°æ®è¿›è¡Œæµ‹è¯•ï¼š</p><pre><code>with tf.Session() as sess:    data = train_input_fn.create_batch()    coord = tf.train.Coordinator()    threads = tf.train.start_queue_runners(sess=sess, coord=coord)    batch1 = sess.run(data[0])    batch2 = sess.run(data[0])    coord.request_stop()print('batch1:', batch1)print('batch2:', batch2)</code></pre><p>ä»¥ä¸Šæ˜¯TFTSåº“ä¸­æ•°æ®çš„è¯»å–æ–¹å¼ã€‚æ€»çš„æ¥è¯´ï¼Œ ä»Numpyæ•°ç»„æˆ–è€…CSVæ–‡ä»¶å‡ºå‘æ„é€ ä¸€ä¸ªreaderï¼Œå†åˆ©ç”¨readerç”Ÿæˆbatchæ•°æ®ã€‚æœ€åå¾—åˆ°çš„Tensorä¸ºtrain_input_fnï¼Œè¿™ä¸ªtrain_input_fnä¼šè¢«å½“ä½œè®­ç»ƒæ—¶çš„è¾“å…¥ ã€‚</p><h2 class=pgc-h-arrow-right>ä½¿ç”¨ARæ¨¡å‹é¢„æµ‹æ—¶é—´åºåˆ—</h2><h2 class=pgc-h-arrow-right>ARæ¨¡å‹çš„è®­ç»ƒ</h2><p>è‡ªå›å½’æ¨¡å‹ï¼ˆAutoregressive modelï¼Œç®€ç§°ä¸ºARæ¨¡å‹ï¼‰æ˜¯ç»Ÿè®¡å­¦ä¸Šå¤„ç†æ—¶é—´åºåˆ—æ¨¡å‹çš„åŸºæœ¬æ–¹æ³•ä¹‹ä¸€ã€‚TFTSä¸­å·²ç»å®ç°äº†ä¸€ä¸ªè‡ªå›å½’æ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹å…¶è¿›è¡Œè°ƒç”¨å³å¯ä½¿ç”¨ã€‚</p><p>æˆ‘ä»¬å…ˆå®šä¹‰å‡ºä¸€ä¸ªtrain_input_fn</p><pre><code>x = np.array(range(1000))noise = np.random.uniform(-0.2, 0.2, 1000)y = np.sin(np.pi * x / 100) + x / 200. + noiseplt.plot(x, y)plt.savefig('timeseries_y.jpg')data = {    tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,    tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,}reader = NumpyReader(data)train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(    reader, batch_size=16, window_size=40)</code></pre><p>ä½¿ç”¨çš„æ—¶é—´åºåˆ—æ•°æ®å¦‚å›¾æ‰€ç¤ºã€‚</p><div class=pgc-img><img alt=TensorFlowå®ç°æ—¶é—´åºåˆ—é¢„æµ‹ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c61ac017d78444ed87facc63b0119a1f><p class=pgc-img-caption></p></div><p>å®šä¹‰ARæ¨¡å‹ï¼š</p><pre><code>ar = tf.contrib.timeseries.ARRegressor(    periodicities=200, input_window_size=30, output_window_size=10,    num_features=1,    loss=tf.contrib.timeseries.ARModel.NORMAL_LIKELIHOOD_LOSS)</code></pre><p>å‚æ•°ï¼š</p><ul><li><strong>periodicities</strong> ï¼šåºåˆ—çš„è§„å¾‹æ€§å‘¨æœŸã€‚åœ¨å®šä¹‰æ•°æ®æ—¶ä½¿ç”¨çš„è¯­å¥æ˜¯â€œy=np.sin(np.pi * x /100)+x /200.+noiseâ€ï¼Œå› æ­¤å‘¨æœŸä¸º200</li><li><strong>input_window_size</strong> ï¼šæ¨¡å‹æ¯æ¬¡è¾“å…¥çš„å€¼</li><li><strong>output_window_size</strong> ï¼šæ¨¡å‹æ¯æ¬¡è¾“å‡ºçš„å€¼</li><li><strong>num_features</strong> ï¼šè¡¨ç¤ºåœ¨ä¸€ä¸ªæ—¶é—´ç‚¹ä¸Šè§‚å¯Ÿåˆ°çš„æ•°çš„ç»´åº¦ã€‚è¿™é‡Œæ¯ä¸€æ­¥éƒ½æ˜¯ä¸€ä¸ªå•ç‹¬çš„å€¼ï¼Œæ‰€ä»¥num_features=1</li><li><strong>loss</strong> ï¼šæŒ‡å®šé‡‡å–å“ªä¸€ç§æŸå¤±ï¼ŒNORMAL_LIKELIHOOD_LOSS æˆ– SQUARED_LOSS</li><li><strong>model_dir</strong> ï¼šæ¨¡å‹è®­ç»ƒå¥½åä¿å­˜çš„åœ°å€ï¼Œå¦‚æœä¸æŒ‡å®šçš„è¯ï¼Œä¼šéšæœºåˆ†é…ä¸€ä¸ªä¸´æ—¶åœ°å€</li></ul><p>input_window_sizeå’Œoutput_window_sizeåŠ èµ·æ¥å¿…é¡»ç­‰äºtrain_input_fnä¸­æ€»çš„window_sizeã€‚æ€»çš„window_sizeä¸º40, input_window_sizeä¸º30,output_window_sizeä¸º10ï¼›ä¹Ÿæ˜¯è¯´ï¼Œä¸€ä¸ªbatchå†…æ¯ä¸ªåºåˆ—çš„é•¿åº¦ä¸º40ï¼Œå…¶ä¸­å‰30ä¸ªæ•°è¢«å½“ä½œæ¨¡å‹çš„è¾“å…¥å€¼ï¼Œåé¢10ä¸ªæ•°ä¸ºè¿™äº›è¾“å…¥å¯¹åº”çš„ç›®æ ‡è¾“å‡ºå€¼ã€‚</p><p>ä½¿ç”¨å˜é‡arçš„trainæ–¹æ³•å¯ä»¥ç›´æ¥è¿›è¡Œè®­ç»ƒï¼š</p><pre><code>ar.train(input_fn=train_input_fn, steps=6000)</code></pre><h2 class=pgc-h-arrow-right>ARæ¨¡å‹çš„éªŒè¯å’Œé¢„æµ‹</h2><p>TFTSä¸­éªŒè¯ï¼ˆevaluationï¼‰çš„å«ä¹‰æ˜¯ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨åŸå…ˆçš„è®­ç»ƒé›†ä¸Šè¿›è¡Œè®¡ç®—ï¼Œç”±æ­¤å¯ä»¥è§‚å¯Ÿåˆ°æ¨¡å‹çš„æ‹Ÿåˆæ•ˆæœï¼Œå¯¹åº”çš„ç¨‹åºæ®µæ˜¯ï¼š</p><pre><code>evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)# keys of evaluation: ['covariance', 'loss', 'mean', 'observed', 'start_tuple', 'times', 'global_step']evaluation = ar.evaluate(input_fn=evaluation_input_fn, steps=1)</code></pre><p>å¦‚æœæƒ³è¦æ˜ç™½è¿™é‡Œçš„é€»è¾‘ï¼Œé¦–å…ˆè¦ç†è§£ä¹‹å‰å®šä¹‰çš„ARæ¨¡å‹ï¼šå®ƒæ¯æ¬¡éƒ½æ¥æ”¶ä¸€ä¸ªé•¿åº¦ä¸º30çš„è¾“å…¥è§‚æµ‹åºåˆ—ï¼Œå¹¶è¾“å‡ºé•¿åº¦ä¸º10çš„é¢„æµ‹åºåˆ—ã€‚æ•´ä¸ªè®­ç»ƒé›†æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º1000çš„åºåˆ—ï¼Œå‰30ä¸ªæ•°é¦–å…ˆè¢«å½“ä½œâ€œåˆå§‹è§‚æµ‹åºåˆ—â€è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œç”±æ­¤å¯ä»¥è®¡ç®—å‡ºä¸‹é¢10æ­¥çš„é¢„æµ‹å€¼ã€‚æ¥ç€åˆä¼šå–30ä¸ªæ•°è¿›è¡Œé¢„æµ‹ï¼Œ è¿™30ä¸ªæ•°ä¸­æœ‰10ä¸ªæ•°æ˜¯å‰ä¸€æ­¥çš„é¢„æµ‹å€¼ ï¼Œæ–°å¾—åˆ°çš„é¢„æµ‹å€¼åˆä¼šå˜æˆä¸‹ä¸€æ­¥çš„è¾“å…¥ï¼Œä¾æ­¤ç±»æ¨ã€‚</p><p>æœ€ç»ˆå¾—åˆ°970ä¸ªé¢„æµ‹å€¼ï¼ˆ970=1000-30ï¼Œå› ä¸ºå‰30ä¸ªæ•°æ˜¯æ²¡åŠæ³•è¿›è¡Œé¢„æµ‹çš„ï¼‰ã€‚970ä¸ªé¢„æµ‹å€¼è¢«è®°å½•åœ¨ <strong>evaluation[â€˜mean']</strong> ä¸­ã€‚evaluationè¿˜æœ‰å…¶ä»–å‡ ä¸ªé”®å€¼ï¼Œå¦‚ <strong>evaluation[â€˜times']</strong> è¡¨ç¤ºevaluation[â€˜mean']å¯¹åº”çš„æ—¶é—´ç‚¹ï¼Œ <strong>evaluation[â€˜loss']</strong> è¡¨ç¤ºæ€»çš„æŸå¤±ç­‰ç­‰ã€‚</p><p>evaluation[â€˜start_tuple']ä¼šè¢«ç”¨äºä¹‹åçš„é¢„æµ‹ä¸­ï¼Œå®ƒç›¸å½“äºæœ€å30æ­¥çš„è¾“å‡ºå€¼å’Œå¯¹åº”çš„æ—¶é—´ç‚¹ã€‚ä»¥æ­¤ä¸ºèµ·ç‚¹ï¼Œå¯ä»¥å¯¹1000æ­¥ä»¥åçš„å€¼è¿›è¡Œé¢„æµ‹ï¼Œå¯¹åº”çš„ä»£ç ä¸ºï¼š</p><pre><code>(predictions,) = tuple(ar.predict(    input_fn=tf.contrib.timeseries.predict_continuation_input_fn(        evaluation, steps=250)))</code></pre><p>è¿™é‡Œçš„ä»£ç åœ¨1000æ­¥ä¹‹ååˆå‘åé¢„æµ‹äº†250ä¸ªæ—¶é—´ç‚¹ã€‚å¯¹åº”çš„å€¼ä¿å­˜åœ¨predictions[â€˜mean']ä¸­ã€‚å¯ä»¥æŠŠè§‚æµ‹åˆ°çš„å€¼ã€æ¨¡å‹æ‹Ÿåˆçš„å€¼ã€é¢„æµ‹å€¼ç”¨ä¸‹é¢çš„ä»£ç ç”»å‡ºæ¥ï¼š</p><pre><code>plt.figure(figsize=(15, 5))plt.plot(data['times'].reshape(-1), data['values'].reshape(-1), label='origin')plt.plot(evaluation['times'].reshape(-1), evaluation['mean'].reshape(-1), label='evaluation')plt.plot(predictions['times'].reshape(-1), predictions['mean'].reshape(-1), label='prediction')plt.xlabel('time_step')plt.ylabel('values')plt.legend(loc=4)plt.savefig('predict_result.jpg')</code></pre><div class=pgc-img><img alt=TensorFlowå®ç°æ—¶é—´åºåˆ—é¢„æµ‹ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f67442c9ddd748568983486f0009d1cb><p class=pgc-img-caption></p></div><p>å‰1000æ­¥æ¨¡å‹åŸå§‹è§‚æµ‹å€¼çš„æ›²çº¿å’Œæ¨¡å‹æ‹Ÿåˆå€¼éå¸¸æ¥è¿‘ï¼Œè¯´æ˜æ¨¡å‹æ‹Ÿåˆå¾—å·²ç»æ¯”è¾ƒå¥½äº†ï¼Œ1000æ­¥ä¹‹åçš„é¢„æµ‹ä¹Ÿåˆæƒ…åˆç†ã€‚</p><pre><code># coding: utf-8from __future__ import print_functionimport numpy as npimport matplotlibmatplotlib.use('agg')import matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.timeseries.python.timeseries import  NumpyReaderdef main(_):    x = np.array(range(1000))    noise = np.random.uniform(-0.2, 0.2, 1000)    y = np.sin(np.pi * x / 100) + x / 200. + noise    plt.plot(x, y)    plt.savefig('timeseries_y.jpg')    data = {        tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,        tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,    }    reader = NumpyReader(data)    train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(        reader, batch_size=16, window_size=40)    ar = tf.contrib.timeseries.ARRegressor(        periodicities=200, input_window_size=30, output_window_size=10,        num_features=1,        loss=tf.contrib.timeseries.ARModel.NORMAL_LIKELIHOOD_LOSS)    ar.train(input_fn=train_input_fn, steps=6000)    evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)    # keys of evaluation: ['covariance', 'loss', 'mean', 'observed', 'start_tuple', 'times', 'global_step']    evaluation = ar.evaluate(input_fn=evaluation_input_fn, steps=1)    (predictions,) = tuple(ar.predict(        input_fn=tf.contrib.timeseries.predict_continuation_input_fn(            evaluation, steps=250)))    plt.figure(figsize=(15, 5))    plt.plot(data['times'].reshape(-1), data['values'].reshape(-1), label='origin')    plt.plot(evaluation['times'].reshape(-1), evaluation['mean'].reshape(-1), label='evaluation')    plt.plot(predictions['times'].reshape(-1), predictions['mean'].reshape(-1), label='prediction')    plt.xlabel('time_step')    plt.ylabel('values')    plt.legend(loc=4)    plt.savefig('predict_result.jpg')if __name__ == '__main__':    tf.logging.set_verbosity(tf.logging.INFO)    tf.app.run()ç¤ºä¾‹å®Œæ•´ä»£ç </code></pre><h2 class=pgc-h-arrow-right>ä½¿ç”¨LSTMæ¨¡å‹é¢„æµ‹æ—¶é—´åºåˆ—</h2><p>ä¸ºäº†ä½¿ç”¨LSTMæ¨¡å‹ï¼Œéœ€è¦å…ˆä½¿ç”¨TFTSåº“å¯¹å…¶è¿›è¡Œå®šä¹‰ã€‚</p><h2 class=pgc-h-arrow-right>å•å˜é‡æ—¶é—´åºåˆ—é¢„æµ‹</h2><p>åŒæ ·ï¼Œç”¨å‡½æ•°åŠ å™ªå£°çš„æ–¹æ³•æ¨¡æ‹Ÿç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®ï¼š</p><pre><code>x = np.array(range(1000))noise = np.random.uniform(-0.2, 0.2, 1000)y = np.sin(np.pi * x / 50) + np.cos(np.pi * x / 50) + np.sin(np.pi * x / 25) + noisedata = {    tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,    tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,}reader = NumpyReader(data)train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(    reader, batch_size=4, window_size=100)</code></pre><p>å¾—åˆ°yå’Œxåï¼Œä½¿ç”¨NumpyReaderè¯»å…¥ä¸ºTensorå½¢å¼ï¼Œæ¥ç€ç”¨tf.contrib.timeseries.RandomWindowInputFnå°†å…¶å˜ä¸ºbatchè®­ç»ƒæ•°æ®ã€‚ä¸€ä¸ªbatchä¸­æœ‰4ä¸ªéšæœºé€‰å–çš„åºåˆ—ï¼Œæ¯ä¸ªåºåˆ—çš„é•¿åº¦ä¸º100ã€‚</p><p>æ¥ä¸‹æ¥å®šä¹‰ä¸€ä¸ªLSTMæ¨¡å‹ï¼š</p><pre><code>estimator = ts_estimators.TimeSeriesRegressor(        model=_LSTMModel(num_features=1, num_units=128),        optimizer=tf.train.AdamOptimizer(0.001))</code></pre><p>num_features=1è¡¨ç¤ºå•å˜é‡æ—¶é—´åºåˆ—ï¼Œå³æ¯ä¸ªæ—¶é—´ç‚¹ä¸Šè§‚å¯Ÿåˆ°çš„é‡åªæ˜¯ä¸€ä¸ªå•ç‹¬çš„æ•°å€¼ï¼Œnum_units=128è¡¨ç¤ºä½¿ç”¨éšå±‚ä¸º128å¤§å°çš„LSTMæ¨¡å‹ã€‚</p><p>è®­ç»ƒã€éªŒè¯å’Œé¢„æµ‹çš„æ–¹æ³•éƒ½å’Œä¹‹å‰ç±»ä¼¼ã€‚åœ¨è®­ç»ƒæ—¶ï¼Œåœ¨å·²æœ‰çš„1000æ­¥çš„è§‚å¯Ÿé‡çš„åŸºç¡€ä¸Šå‘åé¢„æµ‹200æ­¥ï¼š</p><pre><code>estimator.train(input_fn=train_input_fn, steps=2000)ã€€ã€€ã€€ã€€# è®­ç»ƒæ¨¡å‹evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)ã€€ã€€# æµ‹è¯•æ•°æ®evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)ã€€ã€€# å¾—åˆ°è¯„ä¼°åçš„æ•°æ®# è¯„ä¼°åé¢„æµ‹200æ­¥æ•°æ®(predictions,) = tuple(estimator.predict(    input_fn=tf.contrib.timeseries.predict_continuation_input_fn(        evaluation, steps=200)))</code></pre><p>å°†éªŒè¯ã€é¢„æµ‹çš„ç»“æœå–å‡ºå¹¶ç”»æˆç¤ºæ„å›¾ï¼Œç”»å‡ºçš„å›¾åƒä¼šä¿å­˜æˆâ€œpredict_result.jpgâ€æ–‡ä»¶ï¼š</p><pre><code>observed_times = evaluation["times"][0]observed = evaluation["observed"][0, :, :]evaluated_times = evaluation["times"][0]evaluated = evaluation["mean"][0]predicted_times = predictions['times']predicted = predictions["mean"]plt.figure(figsize=(15, 5))plt.axvline(999, linestyle="dotted", linewidth=4, color='r')observed_lines = plt.plot(observed_times, observed, label="observation", color="k")evaluated_lines = plt.plot(evaluated_times, evaluated, label="evaluation", color="g")predicted_lines = plt.plot(predicted_times, predicted, label="prediction", color="r")plt.legend(handles=[observed_lines[0], evaluated_lines[0], predicted_lines[0]],           loc="upper left")plt.savefig('predict_result.jpg')</code></pre><p>é¢„æµ‹æ•ˆæœå¦‚å›¾15-4æ‰€ç¤ºï¼Œæ¨ªåº§æ ‡ä¸ºæ—¶é—´è½´ï¼Œå‰1000æ­¥æ˜¯è®­ç»ƒæ•°æ®ï¼Œ1000~1200æ­¥æ˜¯æ¨¡å‹é¢„æµ‹çš„å€¼ã€‚</p><div class=pgc-img><img alt=TensorFlowå®ç°æ—¶é—´åºåˆ—é¢„æµ‹ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4e65f29cb64440e2ae522107ff38c84d><p class=pgc-img-caption></p></div><pre><code>import numpy as npimport tensorflow as tffrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimatorsfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_modelfrom tensorflow.contrib.timeseries.python.timeseries import NumpyReaderimport matplotlibmatplotlib.use("agg")import matplotlib.pyplot as pltclass _LSTMModel(ts_model.SequentialTimeSeriesModel):    """A time series model-building example using an RNNCell."""    def __init__(self, num_units, num_features, dtype=tf.float32):        """Initialize/configure the model object.        Note that we do not start graph building here. Rather, this object is a        configurable factory for TensorFlow graphs which are run by an Estimator.        Args:          num_units: The number of units in the model's LSTMCell.          num_features: The dimensionality of the time series (features per            timestep).          dtype: The floating point data type to use.        """        super(_LSTMModel, self).__init__(            # Pre-register the metrics we'll be outputting (just a mean here).            train_output_names=["mean"],            predict_output_names=["mean"],            num_features=num_features,            dtype=dtype)        self._num_units = num_units        # Filled in by initialize_graph()        self._lstm_cell = None        self._lstm_cell_run = None        self._predict_from_lstm_output = None    def initialize_graph(self, input_statistics):        """Save templates for components, which can then be used repeatedly.        This method is called every time a new graph is created. It's safe to start        adding ops to the current default graph here, but the graph should be        constructed from scratch.        Args:          input_statistics: A math_utils.InputStatistics object.        """        super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)        self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)        # Create templates so we don't have to worry about variable reuse.        self._lstm_cell_run = tf.make_template(            name_="lstm_cell",            func_=self._lstm_cell,            create_scope_now_=True)        # Transforms LSTM output into mean predictions.        self._predict_from_lstm_output = tf.make_template(            name_="predict_from_lstm_output",            func_=lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),            create_scope_now_=True)    def get_start_state(self):        """Return initial state for the time series model."""        return (            # Keeps track of the time associated with this state for error checking.            tf.zeros([], dtype=tf.int64),            # The previous observation or prediction.            tf.zeros([self.num_features], dtype=self.dtype),            # The state of the RNNCell (batch dimension removed since this parent            # class will broadcast).            [tf.squeeze(state_element, axis=0)             for state_element             in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])    def _transform(self, data):        """Normalize data based on input statistics to encourage stable training."""        mean, variance = self._input_statistics.overall_feature_moments        return (data - mean) / variance    def _de_transform(self, data):        """Transform data back to the input scale."""        mean, variance = self._input_statistics.overall_feature_moments        return data * variance + mean    def _filtering_step(self, current_times, current_values, state, predictions):        """Update model state based on observations.        Note that we don't do much here aside from computing a loss. In this case        it's easier to update the RNN state in _prediction_step, since that covers        running the RNN both on observations (from this method) and our own        predictions. This distinction can be important for probabilistic models,        where repeatedly predicting without filtering should lead to low-confidence        predictions.        Args:          current_times: A [batch size] integer Tensor.          current_values: A [batch size, self.num_features] floating point Tensor            with new observations.          state: The model's state tuple.          predictions: The output of the previous `_prediction_step`.        Returns:          A tuple of new state and a predictions dictionary updated to include a          loss (note that we could also return other measures of goodness of fit,          although only "loss" will be optimized).        """        state_from_time, prediction, lstm_state = state        with tf.control_dependencies(                [tf.assert_equal(current_times, state_from_time)]):            transformed_values = self._transform(current_values)            # Use mean squared error across features for the loss.            predictions["loss"] = tf.reduce_mean(                (prediction - transformed_values) ** 2, axis=-1)            # Keep track of the new observation in model state. It won't be run            # through the LSTM until the next _imputation_step.            new_state_tuple = (current_times, transformed_values, lstm_state)        return (new_state_tuple, predictions)    def _prediction_step(self, current_times, state):        """Advance the RNN state using a previous observation or prediction."""        _, previous_observation_or_prediction, lstm_state = state        lstm_output, new_lstm_state = self._lstm_cell_run(            inputs=previous_observation_or_prediction, state=lstm_state)        next_prediction = self._predict_from_lstm_output(lstm_output)        new_state_tuple = (current_times, next_prediction, new_lstm_state)        return new_state_tuple, {"mean": self._de_transform(next_prediction)}    def _imputation_step(self, current_times, state):        """Advance model state across a gap."""        # Does not do anything special if we're jumping across a gap. More advanced        # models, especially probabilistic ones, would want a special case that        # depends on the gap size.        return state    def _exogenous_input_step(            self, current_times, current_exogenous_regressors, state):        """Update model state based on exogenous regressors."""        raise NotImplementedError(            "Exogenous inputs are not implemented for this example.")if __name__ == '__main__':    tf.logging.set_verbosity(tf.logging.INFO)    x = np.array(range(1000))    noise = np.random.uniform(-0.2, 0.2, 1000)    y = np.sin(np.pi * x / 50) + np.cos(np.pi * x / 50) + np.sin(np.pi * x / 25) + noise    data = {        tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,        tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,    }    reader = NumpyReader(data)    train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(        reader, batch_size=4, window_size=100)    estimator = ts_estimators.TimeSeriesRegressor(        model=_LSTMModel(num_features=1, num_units=128),        optimizer=tf.train.AdamOptimizer(0.001))    estimator.train(input_fn=train_input_fn, steps=2000)    evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)    evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)    # Predict starting after the evaluation    (predictions,) = tuple(estimator.predict(        input_fn=tf.contrib.timeseries.predict_continuation_input_fn(            evaluation, steps=200)))    observed_times = evaluation["times"][0]    observed = evaluation["observed"][0, :, :]    evaluated_times = evaluation["times"][0]    evaluated = evaluation["mean"][0]    predicted_times = predictions['times']    predicted = predictions["mean"]    plt.figure(figsize=(15, 5))    plt.axvline(999, linestyle="dotted", linewidth=4, color='r')    observed_lines = plt.plot(observed_times, observed, label="observation", color="k")    evaluated_lines = plt.plot(evaluated_times, evaluated, label="evaluation", color="g")    predicted_lines = plt.plot(predicted_times, predicted, label="prediction", color="r")    plt.legend(handles=[observed_lines[0], evaluated_lines[0], predicted_lines[0]],               loc="upper left")    plt.savefig('predict_result.jpg')LSTMå•å˜é‡å®Œæ•´ä»£ç </code></pre><h2 class=pgc-h-arrow-right>å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹</h2><p>æ‰€è°“å¤šå˜é‡æ—¶é—´åºåˆ—ï¼Œæ˜¯æŒ‡åœ¨æ¯ä¸ªæ—¶é—´ç‚¹ä¸Šçš„è§‚æµ‹é‡æœ‰å¤šä¸ªå€¼ã€‚åœ¨ <strong>multivariate_periods.csv</strong> æ–‡ä»¶ä¸­ï¼Œä¿å­˜äº†ä¸€ä¸ªå¤šå˜é‡æ—¶é—´åºåˆ—çš„æ•°æ®ï¼š</p><pre><code>0    0.926906299771    1.99107237682    2.56546245685    3.07914768197    4.048390578671    0.108010001864    1.41645361423    2.1686839775    2.94963962176    4.12635033032    -0.800567600028    1.0172132907    1.96434754116    2.99885333086    4.043004858643    0.0607042871898    0.719540073421    1.9765012584    2.89265588817    4.0951014426...99    0.987764008058    1.85581989607    2.84685706149    2.94760204892    6.0212151724</code></pre><p>è¿™ä¸ªCSVæ–‡ä»¶çš„ç¬¬ä¸€åˆ—æ˜¯è§‚å¯Ÿæ—¶é—´ç‚¹ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œæ¯ä¸€è¡Œè¿˜æœ‰5ä¸ªæ•°ï¼Œè¡¨ç¤ºåœ¨è¿™ä¸ªæ—¶é—´ç‚¹ä¸Šè§‚å¯Ÿåˆ°çš„æ•°æ®ã€‚æ¢å¥è¯è¯´ï¼Œæ—¶é—´åºåˆ—ä¸Šæ¯ä¸€æ­¥éƒ½æ˜¯ä¸€ä¸ª5ç»´çš„å‘é‡ã€‚</p><p>ä½¿ç”¨TFTSè¯»å…¥è¯¥CSVæ–‡ä»¶çš„æ–¹æ³•ä¸ºï¼š</p><pre><code>csv_file_name = path.join("./data/multivariate_periods.csv")reader = tf.contrib.timeseries.CSVReader(    csv_file_name,    column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)                  + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 5))train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(    reader, batch_size=4, window_size=32)</code></pre><p>ä¸ä¹‹å‰çš„è¯»å…¥ç›¸æ¯”ï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯column_nameså‚æ•°ã€‚å®ƒå‘Šè¯‰TFTSåœ¨CSVæ–‡ä»¶ä¸­ï¼Œå“ªäº›åˆ—è¡¨ç¤ºæ—¶é—´ï¼Œå“ªäº›åˆ—è¡¨ç¤ºè§‚æµ‹é‡ã€‚</p><p>æ¥ä¸‹æ¥å®šä¹‰LSTMæ¨¡å‹ï¼š</p><pre><code>estimator = ts_estimators.TimeSeriesRegressor(    model=_LSTMModel(num_features=5, num_units=128),    optimizer=tf.train.AdamOptimizer(0.001))</code></pre><p>åŒºåˆ«åœ¨äºä½¿ç”¨num_features=5è€Œä¸æ˜¯1ï¼ŒåŸå› åœ¨äºæ¯ä¸ªæ—¶é—´ç‚¹ä¸Šçš„è§‚æµ‹é‡æ˜¯ä¸€ä¸ª5ç»´å‘é‡ã€‚</p><p>è®­ç»ƒã€éªŒè¯ã€é¢„æµ‹åŠç”»å›¾çš„ä»£ç ä¸ä¹‹å‰æ¯”è¾ƒç±»ä¼¼ï¼Œæœ€åçš„è¿è¡Œç»“æœå›¾æ‰€ç¤º</p><div class=pgc-img><img alt=TensorFlowå®ç°æ—¶é—´åºåˆ—é¢„æµ‹ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7de9c244d52f43d09e0bc5652d2ea00d><p class=pgc-img-caption></p></div><p>ä½¿ç”¨LSTMé¢„æµ‹å¤šå˜é‡æ—¶é—´åºåˆ—</p><p>å‰100æ­¥æ˜¯è®­ç»ƒæ•°æ®ï¼Œä¸€æ¡çº¿ä»£è¡¨è§‚æµ‹é‡åœ¨ä¸€ä¸ªç»´åº¦ä¸Šçš„å–å€¼ã€‚100æ­¥ä¹‹åä¸ºé¢„æµ‹å€¼ã€‚</p><pre><code>from os import pathimport tensorflow as tffrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimatorsfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_modelimport matplotlibmatplotlib.use("agg")import matplotlib.pyplot as pltclass _LSTMModel(ts_model.SequentialTimeSeriesModel):    """A time series model-building example using an RNNCell."""    def __init__(self, num_units, num_features, dtype=tf.float32):        """Initialize/configure the model object.        Note that we do not start graph building here. Rather, this object is a        configurable factory for TensorFlow graphs which are run by an Estimator.        Args:          num_units: The number of units in the model's LSTMCell.          num_features: The dimensionality of the time series (features per            timestep).          dtype: The floating point data type to use.        """        super(_LSTMModel, self).__init__(            # Pre-register the metrics we'll be outputting (just a mean here).            train_output_names=["mean"],            predict_output_names=["mean"],            num_features=num_features,            dtype=dtype)        self._num_units = num_units        # Filled in by initialize_graph()        self._lstm_cell = None        self._lstm_cell_run = None        self._predict_from_lstm_output = None    def initialize_graph(self, input_statistics):        """Save templates for components, which can then be used repeatedly.        This method is called every time a new graph is created. It's safe to start        adding ops to the current default graph here, but the graph should be        constructed from scratch.        Args:          input_statistics: A math_utils.InputStatistics object.        """        super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)        self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)        # Create templates so we don't have to worry about variable reuse.        self._lstm_cell_run = tf.make_template(            name_="lstm_cell",            func_=self._lstm_cell,            create_scope_now_=True)        # Transforms LSTM output into mean predictions.        self._predict_from_lstm_output = tf.make_template(            name_="predict_from_lstm_output",            func_=lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),            create_scope_now_=True)    def get_start_state(self):        """Return initial state for the time series model."""        return (            # Keeps track of the time associated with this state for error checking.            tf.zeros([], dtype=tf.int64),            # The previous observation or prediction.            tf.zeros([self.num_features], dtype=self.dtype),            # The state of the RNNCell (batch dimension removed since this parent            # class will broadcast).            [tf.squeeze(state_element, axis=0)             for state_element             in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])    def _transform(self, data):        """Normalize data based on input statistics to encourage stable training."""        mean, variance = self._input_statistics.overall_feature_moments        return (data - mean) / variance    def _de_transform(self, data):        """Transform data back to the input scale."""        mean, variance = self._input_statistics.overall_feature_moments        return data * variance + mean    def _filtering_step(self, current_times, current_values, state, predictions):        """Update model state based on observations.        Note that we don't do much here aside from computing a loss. In this case        it's easier to update the RNN state in _prediction_step, since that covers        running the RNN both on observations (from this method) and our own        predictions. This distinction can be important for probabilistic models,        where repeatedly predicting without filtering should lead to low-confidence        predictions.        Args:          current_times: A [batch size] integer Tensor.          current_values: A [batch size, self.num_features] floating point Tensor            with new observations.          state: The model's state tuple.          predictions: The output of the previous `_prediction_step`.        Returns:          A tuple of new state and a predictions dictionary updated to include a          loss (note that we could also return other measures of goodness of fit,          although only "loss" will be optimized).        """        state_from_time, prediction, lstm_state = state        with tf.control_dependencies(                [tf.assert_equal(current_times, state_from_time)]):            transformed_values = self._transform(current_values)            # Use mean squared error across features for the loss.            predictions["loss"] = tf.reduce_mean(                (prediction - transformed_values) ** 2, axis=-1)            # Keep track of the new observation in model state. It won't be run            # through the LSTM until the next _imputation_step.            new_state_tuple = (current_times, transformed_values, lstm_state)        return (new_state_tuple, predictions)    def _prediction_step(self, current_times, state):        """Advance the RNN state using a previous observation or prediction."""        _, previous_observation_or_prediction, lstm_state = state        lstm_output, new_lstm_state = self._lstm_cell_run(            inputs=previous_observation_or_prediction, state=lstm_state)        next_prediction = self._predict_from_lstm_output(lstm_output)        new_state_tuple = (current_times, next_prediction, new_lstm_state)        return new_state_tuple, {"mean": self._de_transform(next_prediction)}    def _imputation_step(self, current_times, state):        """Advance model state across a gap."""        # Does not do anything special if we're jumping across a gap. More advanced        # models, especially probabilistic ones, would want a special case that        # depends on the gap size.        return state    def _exogenous_input_step(            self, current_times, current_exogenous_regressors, state):        """Update model state based on exogenous regressors."""        raise NotImplementedError(            "Exogenous inputs are not implemented for this example.")if __name__ == '__main__':    tf.logging.set_verbosity(tf.logging.INFO)    csv_file_name = path.join("./data/multivariate_periods.csv")    reader = tf.contrib.timeseries.CSVReader(        csv_file_name,        column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)                      + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 5))    train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(        reader, batch_size=4, window_size=32)    estimator = ts_estimators.TimeSeriesRegressor(        model=_LSTMModel(num_features=5, num_units=128),        optimizer=tf.train.AdamOptimizer(0.001))    estimator.train(input_fn=train_input_fn, steps=200)    evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)    evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)    # Predict starting after the evaluation    (predictions,) = tuple(estimator.predict(        input_fn=tf.contrib.timeseries.predict_continuation_input_fn(            evaluation, steps=100)))    observed_times = evaluation["times"][0]    observed = evaluation["observed"][0, :, :]    evaluated_times = evaluation["times"][0]    evaluated = evaluation["mean"][0]    predicted_times = predictions['times']    predicted = predictions["mean"]    plt.figure(figsize=(15, 5))    plt.axvline(99, linestyle="dotted", linewidth=4, color='r')    observed_lines = plt.plot(observed_times, observed, label="observation", color="k")    evaluated_lines = plt.plot(evaluated_times, evaluated, label="evaluation", color="g")    predicted_lines = plt.plot(predicted_times, predicted, label="prediction", color="r")    plt.legend(handles=[observed_lines[0], evaluated_lines[0], predicted_lines[0]],               loc="upper left")    plt.savefig('predict_result.jpg')LSTMå¤šå˜é‡å®Œæ•´ä»£ç </code></pre></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'TensorFlow','å®ç°','æ—¶é—´'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=æœç´¢>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>ğŸ”</button></form></section><section class=widget><h3 class=widget-title>æœ€æ–°æ–‡ç«  âš¡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>å…¶ä»–</h3><ul class=widget-list><li><a href=TOS.html>ä½¿ç”¨æ¢æ¬¾</a></li><li><a href=CommentPolicy.html>ç•™è¨€æ”¿ç­–</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>è¯çµ¡æˆ‘å€‘</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>æå®¢å¿«è¨Š</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>