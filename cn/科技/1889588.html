<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>经典回顾！Github 上打星超过 1 万的可复现顶会论文项目 | 极客快訊</title><meta property="og:title" content="经典回顾！Github 上打星超过 1 万的可复现顶会论文项目 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RJ8xKlqAoQLKT5"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/1889588.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/1889588.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/1889588.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/1889588.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/1889588.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/1889588.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/1889588.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/1889588.html><meta property="article:published_time" content="2020-10-29T20:51:33+08:00"><meta property="article:modified_time" content="2020-10-29T20:51:33+08:00"><meta name=Keywords content><meta name=description content="经典回顾！Github 上打星超过 1 万的可复现顶会论文项目"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/1889588.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>经典回顾！Github 上打星超过 1 万的可复现顶会论文项目</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p><strong>雷锋网 AI 科技评论：</strong>Zaur Fataliyev 是在 LG 电子的一名机器学习工程师，为了方便大家对带有复现代码的经典顶会论文进行查询，他在 GitHub 上将这些论文进行了统一打包：https://github.com/zziz/pwc，该名单将每周更新一次。</p><p>雷锋网 AI 科技评论将当中用户打星数超过 1w 的论文进行简要编译，以飨读者。</p><img alt="经典回顾！Github 上打星超过 1 万的可复现顶会论文项目" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RJ8xKlqAoQLKT5><p><strong>2017 年</strong></p><p><strong>1）Bridging the Gap Between Value and Policy Based Reinforcement Learning</strong></p><p><strong>缩小价值与强化学习政策差距的方法</strong></p><p><strong>打星数：</strong>46593</p><p><strong>收录顶会：</strong>NIPS</p><p><strong>论文链接：</strong>http://papers.nips.cc/paper/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning.pdf</p><p>复现代码：https://github.com/tensorflow/models</p><p><strong>简介：</strong>论文基于熵正则化下的 softmax 时间值一致性与策略最优性之间的关系，为价值和强化学习政策建立新的联系。具体而言，作者证明了对应任意动作序列的最优熵正则化策略概率的 softmax 一致动作值——在无视出处的基础上。据此，论文提出了一种全新的强化学习算法 - 路径一致性学习（PCL），它可以最大限度减少从一致性与非策略性迹线中提取多步动作序列的一致性错误概念。作者研究了 PCL 在不同场景下的表现，且证明 PCL 可以被看作涵盖了 actor-critic 以及 Q-learning algorithms 的优点。用于表示策略和相应 softmax 状态值的模型深化了这种关系，从而消除了单独评论的需求。实验结果表明，PCL 的几个基准测试结果明显优于强大的 actor-critic 以及 Q-learning algorithms。</p><p><strong>2）REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models</strong></p><p><strong>REBAR：针对离散潜变量模型的低方差、无偏梯度估计</strong></p><p><strong>打星数：</strong>46593</p><p><strong>收录顶会：</strong>NIPS</p><p><strong>论文链接：</strong>http://papers.nips.cc/paper/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models.pdf</p><p><strong>复现代码：</strong>https://github.com/tensorflow/models</p><p><strong>简介：</strong>由于高方差梯度估计的存在，因此通过带有离散潜变量的模型进行学习是非常具有挑战性的。业内的做法一般是通过控制变量来减少 REINFORCE 估计的方差。近期的工作（Jang et al，2016; Maddi- son et al，2016）采用了不同的方法，其中包括引入持续松弛的离散变量，以产生低方差、但有偏差的梯度估计结果。论文通过一个新的控制变量将两者结合起来，以产生低方差、且无偏差的梯度估计。最后，论文介绍了对连续松弛的修正方法，证明了松弛的紧密度是可以通过在线调整的，主要将其作为超参数进行去除。模型最终在几个基准生成建模任务中获得了先进的方差减少结果，可以更快地进行收敛并达到更好的最终对数似然结果。</p><p><strong>3）Focal Loss for Dense Object Detection</strong></p><p><strong>针对密集物体检测的焦点损失方案</strong></p><p><strong>打星数：</strong>18356</p><p><strong>收录顶会：</strong>ICCV</p><p><strong>论文链接：</strong>http://openaccess.thecvf.com/content_iccv_2017/html/Lin_Focal_Loss_for_ICCV_2017_paper.html</p><p><strong>复现代码：</strong>https://github.com/facebookresearch/Detectron</p><p><strong>简介：</strong>这是迄今为止精度最高的基于 R-CNN 的两步式检测器，其分类器可应用于稀疏的候选对象位置集。与此相对的是，应用于常规、密集采样的一步式探测器具在精度上已落后于两步式探测器。论文对此作出了解释——在训练过程中遇到极端前景 - 背景类不平衡现象是其核心原因。作者建议通过重塑标准交叉熵损失来解决这种不平衡的问题，核心方法是降低分配给分类例子的损失权重。论文提及的「焦点损失」将训练重点放在一组稀疏例子上，以防止否定因素在训练期间影响探测器。为了评估损失的有效性，作者设计并训练了一个被称作 RetinaNet 的密集检测器。最后的研究结果表明，当使用焦点损失进行训练时，RetinaNet 能够达到一步式探测器的速度，且在精度上超越所有的两步式探测器。</p><p><strong>2016 年</strong></p><p><strong>1）R-FCN: Object Detection via Region-based Fully Convolutional Networks</strong></p><p><strong>R-FCN：基于区域完全卷积网络的对象检测</strong></p><p><strong>打星数：</strong>18356</p><p><strong>收录顶会：</strong>NIPS</p><p><strong>论文链接：</strong>https://papers.nips.cc/paper/6465-r-fcn-object-detection-via-region-based-fully-convolutional-networks.pdf</p><p><strong>复现代码：</strong>https://github.com/facebookresearch/Detectron</p><p><strong>简介：</strong>论文提出基于区域的完全卷积网络，以实现准确、有效的物体检测任务。与先前基于区域的检测器（如快速/更快速的 R-CNN [7,19]）相比，该基于区域的检测器是完全卷积的。为了达到这一目标，作者提出基于位置的敏感得分图，以解决图像分类中存在的平移不变性与对象检测平移方差之间的两难问题。该方法可以采用完全卷积的主要图像分类器，例如最新的残余网络（ResNets）[10]，以用于进行物体探测。该方法在基于 101 层 ResNet 的 PASCAL VOC 数据集（例如，2007 年的 83.6％mAP）上展现出有竞争力的结果。需要强调的是，模型最终实现了以每张图像 170 毫秒进行测试的时间速度，比快速 R-CNN 对要快上 2.5-20 倍。</p><p><strong>2）Image Style Transfer Using Convolutional Neural Networks</strong></p><p><strong>基于卷积神经网络的图像样式转换</strong></p><p><strong>打星数：</strong>16435</p><p><strong>收录顶会：</strong>CVPR</p><p><strong>论文链接：</strong>http://openaccess.thecvf.com/content_cvpr_2016/html/Gatys_Image_Style_Transfer_CVPR_2016_paper.html</p><p><strong>复现代码：</strong>https://github.com/jcjohnson/neural-style</p><p><strong>简介：</strong>以不同样式呈现图像的语义内容是一项很困难的图像处理任务。可以说，先前的方法最大的限制是缺乏明确表示语义信息的图像表示，以允许将图像内容与样式进行分离。作者通过使用用于物体识别的卷积神经网络导出的图像表示，使高级图像的信息显式化。论文介绍了一种艺术风格的神经算法，该方法可以分离与重新组合图像内容。该算法允许我们产生高质量的新图像，其任意照片内容与众多知名艺术品外观进行结合。论文结果为卷积神经网络学习的深度图像表示提供了一种全新见解，并展示了它们对高级阶段的图像合成潜力。</p><p><strong>2015 年</strong></p><p><strong>1）Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</strong></p><p><strong>更快的 R-CNN：通过区域提议网络实现实时目标检测</strong></p><p><strong>打星数：</strong>18356</p><p><strong>收录顶会：</strong>NIPS</p><p><strong>论文链接：</strong>https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf</p><p><strong>复现代码：</strong>https://github.com/facebookresearch/Detectron</p><p><strong>简介：</strong>目前最先进的物体检测网络有赖区域提议算法来假设物体位置。类似 SPPnet [7] 和 Fast R-CNN [5] 这样进步网络的出现，有效减少了网络检测的运行时间，然而也使区域提议计算成为瓶颈。作者在本文中介绍了能与检测网络共享全图像卷积特征的区域提议网络（RPN），从而实现几乎无成本的区域提议检测。RPN 是一个完全卷积的网络，可同时预测每个位置的对象边界与对象分数。RPN 通过端到端的训练以生成高质量的区域提议，被快速 R-CNN 用于检测任务。通过简单的交替优化，还可以训练 RPN 和快速 R-CNN 以共享卷积特征。对于非常深的 VGG-16 模型 [19]，该检测系统在 GPU 上的帧速率为 5fps（包含所有步骤），同时在 PASCAL VOC 上实现了最先进的物体检测精度（2007 年：73.2％mAP、2012 年：70.4％mAP），平均每张图片使用了 300 个提案。</p><p><strong>2）Fast R-CNN</strong></p><p><strong>打星数：</strong>18356</p><p><strong>收录顶会：</strong>ICCV</p><p><strong>论文链接：</strong>http://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html</p><p><strong>复现代码：</strong>https://github.com/facebookresearch/Detectron</p><p><strong>简介：</strong>本文提出一种基于快速区域的卷积网络方法（Fast R-CNN）用于物体检测。快速 R-CNN 建立在先前工作基础上，通过使用深度卷积网络有效地对对象提议进行分类。与之前的工作相比，Fast R-CNN 采用了多项创新来提高训练和测试速度，同时有效提高了检测精度。快速 R-CNN 可以训练出非常深的 VGG16 网络，不止比 R-CNN 快 9 倍，同时在测试时间上快了近 213 倍，且能在 PASCAL VOC 2012 上实现更高的 mAP。与 SPPnet 相比，Fast R-CNN 训练出的 VGG16 要快上 3 倍，测试速度上要快上 10 倍，同时结果也更准确。</p><p>雷锋网 AI 科技评论</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'经典','回顾','Github'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>