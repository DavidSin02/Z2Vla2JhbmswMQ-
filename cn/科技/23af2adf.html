<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹æ¥äº†ï¼šå°æ¨¡å‹ç™»é¡¶GLUEï¼ŒBaseç‰ˆæ¨¡å‹å°10å€ã€é€Ÿåº¦å¿«1å€ | æå®¢å¿«è¨Š</title><meta property="og:title" content="ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹æ¥äº†ï¼šå°æ¨¡å‹ç™»é¡¶GLUEï¼ŒBaseç‰ˆæ¨¡å‹å°10å€ã€é€Ÿåº¦å¿«1å€ - æå®¢å¿«è¨Š"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/RcGwzyuCkrlJ0I"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><meta property="article:published_time" content="2020-11-14T21:02:26+08:00"><meta property="article:modified_time" content="2020-11-14T21:02:26+08:00"><meta name=Keywords content><meta name=description content="ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹æ¥äº†ï¼šå°æ¨¡å‹ç™»é¡¶GLUEï¼ŒBaseç‰ˆæ¨¡å‹å°10å€ã€é€Ÿåº¦å¿«1å€"><meta name=author content="æå®¢å¿«è¨Š"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/23af2adf.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>ğŸ¤“ æå®¢å¿«è®¯ Geek Bank</a></h1><p class=description>ä¸ºä½ å¸¦æ¥æœ€å…¨çš„ç§‘æŠ€çŸ¥è¯† ğŸ§¡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>çŒœä½ å–œæ­¡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=ç§‘æŠ€>ç§‘æŠ€</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=éŠæˆ²>éŠæˆ²</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=ç§‘å­¸>ç§‘å­¸</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹æ¥äº†ï¼šå°æ¨¡å‹ç™»é¡¶GLUEï¼ŒBaseç‰ˆæ¨¡å‹å°10å€ã€é€Ÿåº¦å¿«1å€</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>ç§‘æŠ€</a></span></div><div class=post-content><img alt=ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹æ¥äº†ï¼šå°æ¨¡å‹ç™»é¡¶GLUEï¼ŒBaseç‰ˆæ¨¡å‹å°10å€ã€é€Ÿåº¦å¿«1å€ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RcGwzyuCkrlJ0I><p class=pgc-img-caption>ï¼ˆå›¾ç‰‡ç”±AIç§‘æŠ€å¤§æœ¬è¥ä»˜è´¹ä¸‹è½½è‡ªè§†è§‰ä¸­å›½ï¼‰</p><p>ä½œè€… | å¾äº®ï¼ˆå®åœ¨æ™ºèƒ½ç®—æ³•ä¸“å®¶)</p><p>æ¥æº | AINLPï¼ˆIDï¼šnlpjobï¼‰</p><p>è°·æ­ŒALBERTè®ºæ–‡åˆšåˆšå‡ºç‚‰ä¸€å‘¨ï¼Œä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹æ¥äº†ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥ç›´æ¥å°é²œè¯•ç”¨ã€‚</p><p>é¡¹ç›®é“¾æ¥ï¼š</p><p>https://github<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">.com</i>/brightmart/albert_zh</p><p>An Implementation of A Lite Bert For Self-Supervised Learning Language Representations with TensorFlow.</p><p>ALBert is based on Bert, but with some improvements. It achieves state of the art performance on main benchmarks with 30% parameters less.</p><p>For albert_base_zh it only has ten percentage parameters compare of original bert model, and main accuracy is retained.</p><p>Chinese version of ALBERT pre-trained model, including checkpoints both for TensorFlow and PyTorch, will be available.</p><p>æµ·é‡ä¸­æ–‡è¯­æ–™ä¸Šé¢„è®­ç»ƒALBERTæ¨¡å‹ï¼šå‚æ•°æ›´å°‘ï¼Œæ•ˆæœæ›´å¥½ã€‚é¢„è®­ç»ƒå°æ¨¡å‹ä¹Ÿèƒ½æ‹¿ä¸‹13é¡¹NLPä»»åŠ¡ï¼ŒALBERTä¸‰å¤§æ”¹é€ ç™»é¡¶GLUEåŸºå‡†ã€‚</p><p><strong>***** 2019-10-02: albert_large_zh *****</strong></p><p>Relased albert_large_zh with only 16% parameters of bert_base(<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">64</i>M)</p><p><strong>***** 2019-10-01: albert_base_zh *****</strong></p><p>Relesed albert_base_zh with only 10% parameters of bert_base, a small model(40M) & training can be very fast.</p><p><strong>***** 2019-09-28: codes and test functions *****</strong></p><p>A<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">dd</i> codes and test functions for three main changes of albert from bert</p><p><strong>æ¨¡å‹ä¸‹è½½ Download Pre-trained Models of Chinese</strong></p><p>1ã€albert_large_zh,å‚æ•°é‡ï¼Œå±‚æ•°24ï¼Œå¤§å°ä¸º<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">64</i>M</p><pre><p>å‚æ•°é‡å’Œæ¨¡å‹å¤§å°ä¸ºbert_baseçš„å…­åˆ†ä¹‹ä¸€ï¼›åœ¨å£è¯­åŒ–æè¿°ç›¸ä¼¼æ€§æ•°æ®é›†LCQMCçš„æµ‹è¯•é›†ä¸Šç›¸æ¯”bert_baseä¸Šå‡0.2ä¸ªç‚¹</p></pre><pre>2ã€albert_base_zh(å°æ¨¡å‹ä½“éªŒç‰ˆ)ï¼Œ å‚æ•°é‡12Mï¼Œå±‚æ•°12ï¼Œå¤§å°ä¸º40M<br></pre><pre><pre><p>å‚æ•°é‡ä¸ºbert_baseçš„ååˆ†ä¹‹ä¸€ï¼Œæ¨¡å‹å¤§å°ä¹Ÿååˆ†ä¹‹ä¸€ï¼›åœ¨å£è¯­åŒ–æè¿°ç›¸ä¼¼æ€§æ•°æ®é›†LCQMCçš„æµ‹è¯•é›†ä¸Šç›¸æ¯”bert_baseä¸‹é™çº¦1ä¸ªç‚¹ï¼›</p><p>ç›¸æ¯”æœªé¢„è®­ç»ƒï¼Œalbert_baseæå‡14ä¸ªç‚¹</p></pre><br></pre><p>3ã€albert_xlargeã€ albert_xxlarge will coming recently.</p><pre><p>if you want use a albert model with best performance among all pre-trained models, just wait a few days.</p></pre><pre><div><p>ALBERTæ¨¡å‹ä»‹ç» Introduction of ALBERT</p></div></pre><p>ALBERTæ¨¡å‹æ˜¯BERTçš„æ”¹è¿›ç‰ˆï¼Œä¸æœ€è¿‘å…¶ä»–State of the artçš„æ¨¡å‹ä¸åŒçš„æ˜¯ï¼Œè¿™æ¬¡æ˜¯é¢„è®­ç»ƒå°æ¨¡å‹ï¼Œæ•ˆæœæ›´å¥½ã€å‚æ•°æ›´å°‘ã€‚</p><p><strong>å®ƒå¯¹BERTè¿›è¡Œäº†ä¸‰ä¸ªæ”¹é€  Three main changes of ALBert from Bertï¼š</strong></p><p>1ï¼‰è¯åµŒå…¥å‘é‡å‚æ•°çš„å› å¼åˆ†è§£ Factorized embe<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">dd</i>ing parameterization</p><pre><br><pre><p>O(V * H) to O(V * E + E * H)</p><p>å¦‚ä»¥ALBert_xxlargeä¸ºä¾‹ï¼ŒV=30000, H=4096, E=128</p><p>é‚£ä¹ˆåŸå…ˆå‚æ•°ä¸ºV * H= 30000 * 4096 = 1.23äº¿ä¸ªå‚æ•°ï¼Œç°åœ¨åˆ™ä¸ºV * E + E * H = 30000*128+128*4096 = 384ä¸‡ + 52ä¸‡ = 436ä¸‡ï¼Œ</p><p>è¯åµŒå…¥ç›¸å…³çš„å‚æ•°å˜åŒ–å‰æ˜¯å˜æ¢åçš„28å€ã€‚</p></pre><br></pre><p>2ï¼‰è·¨å±‚å‚æ•°å…±äº« Cross-Layer Parameter Sharing</p><pre><p>å‚æ•°å…±äº«èƒ½æ˜¾è‘—å‡å°‘å‚æ•°ã€‚å…±äº«å¯ä»¥åˆ†ä¸ºå…¨è¿æ¥å±‚ã€æ³¨æ„åŠ›å±‚çš„å‚æ•°å…±äº«ï¼›æ³¨æ„åŠ›å±‚çš„å‚æ•°å¯¹æ•ˆæœçš„å‡å¼±å½±å“å°ä¸€ç‚¹ã€‚</p></pre><pre>3ï¼‰æ®µè½è¿ç»­æ€§ä»»åŠ¡ Inter-sentence coherence loss.<br></pre><pre><pre><p>ä½¿ç”¨æ®µè½è¿ç»­æ€§ä»»åŠ¡ã€‚æ­£ä¾‹ï¼Œä½¿ç”¨ä»ä¸€ä¸ªæ–‡æ¡£ä¸­è¿ç»­çš„ä¸¤ä¸ªæ–‡æœ¬æ®µè½ï¼›è´Ÿä¾‹ï¼Œä½¿ç”¨ä»ä¸€ä¸ªæ–‡æ¡£ä¸­è¿ç»­çš„ä¸¤ä¸ªæ–‡æœ¬æ®µè½ï¼Œä½†ä½ç½®è°ƒæ¢äº†ã€‚</p><p>é¿å…ä½¿ç”¨åŸæœ‰çš„NSPä»»åŠ¡ï¼ŒåŸæœ‰çš„ä»»åŠ¡åŒ…å«éšå«äº†é¢„æµ‹ä¸»é¢˜è¿™ç±»è¿‡äºç®€å•çš„ä»»åŠ¡ã€‚</p><p>We maintain that inter-sentence modeling is an important aspect of language understanding, but we propose a loss </p><p>based primarily on coherence. That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic </p><p>prediction and instead focuses on modeling inter-sentence coherence. The SOP loss uses as positive examples the </p><p>same technique as BERT (two consecutive segments from the same document), and as negative examples the same two </p><p>consecutive segments but with their order swapped. This forces the model to learn finer-grained distinctions about</p><p>discourse-level coherence properties. </p></pre><p>å…¶ä»–å˜åŒ–ï¼Œè¿˜æœ‰ Other changesï¼š</p><pre><p>1ï¼‰å»æ‰äº†dropout Remvoe dropout to enlarge capacity of model.</p><p>æœ€å¤§çš„æ¨¡å‹ï¼Œè®­ç»ƒäº†1ç™¾ä¸‡æ­¥åï¼Œè¿˜æ˜¯æ²¡æœ‰è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚è¯´æ˜æ¨¡å‹çš„å®¹é‡è¿˜å¯ä»¥æ›´å¤§ï¼Œå°±ç§»é™¤äº†dropout</p><p>ï¼ˆdropoutå¯ä»¥è®¤ä¸ºæ˜¯éšæœºçš„å»æ‰ç½‘ç»œä¸­çš„ä¸€éƒ¨åˆ†ï¼ŒåŒæ—¶ä½¿ç½‘ç»œå˜å°ä¸€äº›ï¼‰</p><p>We also note that, even after training for 1M steps, our largest models still do not overfit to their training data. </p><p>As a result, we decide to remove dropout to further increase our model capacity.</p><p>å…¶ä»–å‹å·çš„æ¨¡å‹ï¼Œåœ¨æˆ‘ä»¬çš„å®ç°ä¸­æˆ‘ä»¬è¿˜æ˜¯ä¼šä¿ç•™åŸå§‹çš„dropoutçš„æ¯”ä¾‹ï¼Œé˜²æ­¢æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„è¿‡æ‹Ÿåˆã€‚</p><p>2ï¼‰ä¸ºåŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œä½¿ç”¨LAMBåšä¸ºä¼˜åŒ–å™¨ Use lAMB as optimizer, to train with big batch size</p><p>ä½¿ç”¨äº†å¤§çš„batch_sizeæ¥è®­ç»ƒ(4096)ã€‚LAMBä¼˜åŒ–å™¨ä½¿å¾—æˆ‘ä»¬å¯ä»¥è®­ç»ƒï¼Œç‰¹åˆ«å¤§çš„æ‰¹æ¬¡batch_sizeï¼Œå¦‚é«˜è¾¾6ä¸‡ã€‚</p><p>3ï¼‰ä½¿ç”¨n-gram(uni-gram,bi-gram, tri-gramï¼‰æ¥åšé®è”½è¯­è¨€æ¨¡å‹ Use n-gram as make language model</p><p>å³ä»¥ä¸åŒçš„æ¦‚ç‡ä½¿ç”¨n-gram,uni-gramçš„æ¦‚ç‡æœ€å¤§ï¼Œbi-gramå…¶æ¬¡ï¼Œtri-gramæ¦‚ç‡æœ€å°ã€‚</p><p>æœ¬é¡¹ç›®ä¸­ç›®å‰ä½¿ç”¨çš„æ˜¯åœ¨ä¸­æ–‡ä¸Šåšwhole word maskï¼Œç¨åä¼šæ›´æ–°ä¸€ä¸‹ä¸n-gram maskçš„æ•ˆæœå¯¹æ¯”ã€‚n-gramä»spanBERTä¸­æ¥ã€‚</p></pre></pre><p></p><h1 toutiao-origin=h2>å‘å¸ƒè®¡åˆ’ Release <i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">Plan</i></h1><p>1ã€albert_baseï¼Œå‚æ•°é‡12M, å±‚æ•°12ï¼Œ10æœˆ7å·</p><p>2ã€albert_largeï¼Œå‚æ•°é‡18M, å±‚æ•°24ï¼Œ10æœˆ13å·</p><p>3ã€albert_xlargeï¼Œå‚æ•°é‡59M, å±‚æ•°24ï¼Œ10æœˆ6å·</p><p>4ã€albert_xxlargeï¼Œå‚æ•°é‡233M, å±‚æ•°12ï¼Œ10æœˆ7å·ï¼ˆæ•ˆæœæœ€ä½³çš„æ¨¡å‹ï¼‰</p><p><strong>è®­ç»ƒè¯­æ–™/è®­ç»ƒé…ç½® Training Data & Configuration</strong></p><p>30gä¸­æ–‡è¯­æ–™ï¼Œè¶…è¿‡100äº¿æ±‰å­—ï¼ŒåŒ…æ‹¬å¤šä¸ªç™¾ç§‘ã€æ–°é—»ã€äº’åŠ¨ç¤¾åŒºã€‚</p><p>é¢„è®­ç»ƒåºåˆ—é•¿åº¦sequence_lengthè®¾ç½®ä¸º512ï¼Œæ‰¹æ¬¡batch_sizeä¸º4096ï¼Œè®­ç»ƒäº§ç”Ÿäº†3.5äº¿ä¸ªè®­ç»ƒæ•°æ®(instance)ï¼›æ¯ä¸€ä¸ªæ¨¡å‹é»˜è®¤ä¼šè®­ç»ƒ125kæ­¥ï¼Œalbert_xxlargeå°†è®­ç»ƒæ›´ä¹…ã€‚</p><p>ä½œä¸ºæ¯”è¾ƒï¼Œroberta_zhé¢„è®­ç»ƒäº§ç”Ÿäº†2.5äº¿ä¸ªè®­ç»ƒæ•°æ®ã€åºåˆ—é•¿åº¦ä¸º256ã€‚ç”±äºalbert_zhé¢„è®­ç»ƒç”Ÿæˆçš„è®­ç»ƒæ•°æ®æ›´å¤šã€ä½¿ç”¨çš„åºåˆ—é•¿åº¦æ›´é•¿ï¼Œ</p><pre><br><pre><p>æˆ‘ä»¬é¢„è®¡albert_zhä¼šæœ‰æ¯”roberta_zhæ›´å¥½çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä¸”èƒ½æ›´å¥½å¤„ç†è¾ƒé•¿çš„æ–‡æœ¬ã€‚</p></pre><br></pre><p>è®­ç»ƒä½¿ç”¨TPU v3 Podï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯v3-256ï¼Œå®ƒåŒ…å«32ä¸ªv3-8ã€‚æ¯ä¸ªv3-8æœºå™¨ï¼Œå«æœ‰128Gçš„æ˜¾å­˜ã€‚</p><p><strong>æ¨¡å‹æ€§èƒ½ä¸å¯¹æ¯”(è‹±æ–‡) Performance and Comparision</strong></p><img alt=ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹æ¥äº†ï¼šå°æ¨¡å‹ç™»é¡¶GLUEï¼ŒBaseç‰ˆæ¨¡å‹å°10å€ã€é€Ÿåº¦å¿«1å€ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Re42wuX547Yk5y><img alt=ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹æ¥äº†ï¼šå°æ¨¡å‹ç™»é¡¶GLUEï¼ŒBaseç‰ˆæ¨¡å‹å°10å€ã€é€Ÿåº¦å¿«1å€ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Re42wv55YS5Qv4><img alt=ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹æ¥äº†ï¼šå°æ¨¡å‹ç™»é¡¶GLUEï¼ŒBaseç‰ˆæ¨¡å‹å°10å€ã€é€Ÿåº¦å¿«1å€ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Re42wvUA3u7Dj3><p><strong>ä¸­æ–‡ä»»åŠ¡é›†ä¸Šæ•ˆæœå¯¹æ¯”æµ‹è¯• Performance on Chinese datasets</strong></p><ul><li><h2 toutiao-origin=h3><strong>è‡ªç„¶è¯­è¨€æ¨æ–­ï¼šXNLI of Chinese Version</strong></h2></li></ul><img alt=ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹æ¥äº†ï¼šå°æ¨¡å‹ç™»é¡¶GLUEï¼ŒBaseç‰ˆæ¨¡å‹å°10å€ã€é€Ÿåº¦å¿«1å€ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Re42wvpDiRUbWB><p>æ³¨ï¼šBERT-wwm-extæ¥è‡ªäºè¿™é‡Œï¼›XLNetæ¥è‡ªäºè¿™é‡Œ; RoBERTa-zh-baseï¼ŒæŒ‡12å±‚RoBERTaä¸­æ–‡æ¨¡å‹</p><ul><li><p><strong>é—®é¢˜åŒ¹é…è¯­ä»»åŠ¡ï¼šLCQMC(Sentence Pair Matching)</strong></p></li></ul><img alt=ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹æ¥äº†ï¼šå°æ¨¡å‹ç™»é¡¶GLUEï¼ŒBaseç‰ˆæ¨¡å‹å°10å€ã€é€Ÿåº¦å¿«1å€ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Re42x0V17WQ0MB><ul><li><p><strong>è¯­è¨€æ¨¡å‹ã€æ–‡æœ¬æ®µé¢„æµ‹å‡†ç¡®æ€§ã€è®­ç»ƒæ—¶é—´ Mask Language Model Accuarcy & Training Time</strong></p></li></ul><img alt=ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹æ¥äº†ï¼šå°æ¨¡å‹ç™»é¡¶GLUEï¼ŒBaseç‰ˆæ¨¡å‹å°10å€ã€é€Ÿåº¦å¿«1å€ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Re42x0r380iJUY><p>æ³¨ï¼š? å°†å¾ˆå¿«æ›¿æ¢</p><p></p><h1 toutiao-origin=h2>æ¨¡å‹å‚æ•°å’Œé…ç½® Configuration of Models</h1><img alt=ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹æ¥äº†ï¼šå°æ¨¡å‹ç™»é¡¶GLUEï¼ŒBaseç‰ˆæ¨¡å‹å°10å€ã€é€Ÿåº¦å¿«1å€ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Re42x16DoN9bGp><p><strong>ä»£ç å®ç°å’Œæµ‹è¯• Implementation and Code Testing</strong></p><p>é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤æµ‹è¯•ä¸»è¦çš„æ”¹è¿›ç‚¹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºè¯åµŒå…¥å‘é‡å‚æ•°çš„å› å¼åˆ†è§£ã€è·¨å±‚å‚æ•°å…±äº«ã€æ®µè½è¿ç»­æ€§ä»»åŠ¡ç­‰ã€‚</p><pre><p>python <strong class=highlight-text toutiao-origin=span>test_changes</strong><strong class=highlight-text toutiao-origin=span>.py</strong></p></pre><p></p><h1 toutiao-origin=h2>é¢„è®­ç»ƒ Pre-training</h1><ul><li><p><strong>ç”Ÿæˆç‰¹å®šæ ¼å¼çš„æ–‡ä»¶(tfrecords) Generate tfrecords Files</strong></p></li></ul><p>è¿è¡Œä»¥ä¸‹å‘½ä»¤å³å¯ã€‚é¡¹ç›®è‡ªåŠ¨äº†ä¸€ä¸ªç¤ºä¾‹çš„æ–‡æœ¬æ–‡ä»¶(data/news_zh_1.txt)</p><pre><br><pre><p>bash <strong class=highlight-text toutiao-origin=span>create_pretrain_data</strong><strong class=highlight-text toutiao-origin=span>.sh</strong></p></pre><br><p>å¦‚æœä½ æœ‰å¾ˆå¤šæ–‡æœ¬æ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡ä¼ å…¥å‚æ•°çš„æ–¹å¼ï¼Œç”Ÿæˆå¤šä¸ªç‰¹å®šæ ¼å¼çš„æ–‡ä»¶(tfrecordsï¼‰</p></pre><p></p><h2 toutiao-origin=h4>æ‰§è¡Œé¢„è®­ç»ƒ pre-training on GPU/TPU</h2><pre><br><pre><p>GPU:</p><p>export BERT_BASE_DIR=albert_config</p><p>nohup python3 run_pretraining.py --input_file=./data/tf*.tfrecord \</p><p>--output_dir=my_new_model_path --do_train=True --do_eval=True --bert_config_file=$BERT_BASE_DIR/albert_config_xxlarge.json \</p><p>--train_batch_size=4096 --max_seq_length=512 --max_predictions_per_seq=76 \</p><p>--num_train_steps=125000 --num_warmup_steps=12500 --learning_rate=0.00<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-4">176</i> \</p><p>--save_checkpoints_steps=2000 --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt &amp;</p><p>TPU, a<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">dd</i> following information:</p><p>--use_tpu=True --tpu_name=grpc://10.240.1.66:8470 --tpu_zone=us-central1-a</p><p>æ³¨ï¼šå¦‚æœä½ ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå¯ä»¥ä¸æŒ‡å®šinit_checkpointï¼›</p><p>å¦‚æœä½ ä»ç°æœ‰çš„æ¨¡å‹åŸºç¡€ä¸Šè®­ç»ƒï¼ŒæŒ‡å®šä¸€ä¸‹BERT_BASE_DIRçš„è·¯å¾„ï¼Œå¹¶ç¡®ä¿bert_config_fileå’Œinit_checkpointä¸¤ä¸ªå‚æ•°çš„å€¼èƒ½å¯¹åº”åˆ°ç›¸åº”çš„æ–‡ä»¶ä¸Šï¼›</p><p>é¢†åŸŸä¸Šçš„é¢„è®­ç»ƒï¼Œæ ¹æ®æ•°æ®çš„å¤§å°ï¼Œå¯ä»¥ä¸ç”¨è®­ç»ƒç‰¹åˆ«ä¹…ã€‚</p></pre><br></pre><p></p><h1 toutiao-origin=h2>ä¸‹æ¸¸ä»»åŠ¡ Fine-tuning</h1><p>ä»¥ä½¿ç”¨albert_baseåšLCQMCä»»åŠ¡ä¸ºä¾‹ã€‚LCQMCä»»åŠ¡æ˜¯åœ¨å£è¯­åŒ–æè¿°çš„æ•°æ®é›†ä¸Šåšæ–‡æœ¬çš„ç›¸ä¼¼æ€§é¢„æµ‹ã€‚</p><p>ä¸‹è½½LCQMCæ•°æ®é›†ï¼ŒåŒ…å«è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†ï¼Œè®­ç»ƒé›†åŒ…å«24ä¸‡å£è¯­åŒ–æè¿°çš„ä¸­æ–‡å¥å­å¯¹ï¼Œæ ‡ç­¾ä¸º1æˆ–0ã€‚1ä¸ºå¥å­è¯­ä¹‰ç›¸ä¼¼ï¼Œ0ä¸ºè¯­ä¹‰ä¸ç›¸ä¼¼ã€‚</p><p>é€šè¿‡è¿è¡Œä¸‹åˆ—å‘½ä»¤åšLCQMCæ•°æ®é›†ä¸Šçš„fine-tuning:</p><pre><br><pre><div><p>1. Clone this project:</p><p>git clone https://github<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">.com</i>/brightmart/albert_zh.git</p><p>2. Fine-tuning by running the following commandï¼š</p><p>export BERT_BASE_DIR=./albert_large_zh</p><p>export TEXT_DIR=./lcqmc</p><p>nohup python3 run_classifier.py --task_name=lcqmc_pair --do_train=False --do_eval=true --data_dir=$TEXT_DIR --vocab_file=./albert_config/vocab.txt \</p><p>--bert_config_file=./albert_config/albert_config_large.json --max_seq_length=128 --train_batch_size=<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">64</i> --learning_rate=2e-5 --num_train_epochs=3 \</p><p>--output_dir=albert_large_lcqmc_checkpoints --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt &amp;</p><p>Notice/æ³¨ï¼š</p><p>you need to download pre-trained chinese albert model, and also download LCQMC dataset </p><p>ä½ éœ€è¦ä¸‹è½½é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶æ”¾å…¥åˆ°é¡¹ç›®å½“å‰é¡¹ç›®ï¼Œå‡è®¾ç›®å½•åç§°ä¸ºalbert_large_zh; éœ€è¦ä¸‹è½½LCQMCæ•°æ®é›†ï¼Œå¹¶æ”¾å…¥åˆ°å½“å‰é¡¹ç›®ï¼Œ</p><p>å‡è®¾æ•°æ®é›†ç›®å½•åç§°ä¸ºlcqmc</p></div></pre><br></pre><p></p><h2 toutiao-origin=h4>Reference</h2><p>1ã€ALBERT: A Lite BERT For Self-Supervised Learning Of Language Representations</p><p>2ã€BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p><p>3ã€SpanBERT: Improving Pre-training by Representing and Predicting Spans</p><p>4ã€RoBERTa: A Robustly Optimized BERT Pretraining Approach</p><p>5ã€Large Batch Optimization for Deep Learning: Training BERT in 76 minutes(LAMB)</p><p>6ã€LAMB Optimizer,TensorFlow version</p><p><em>ï¼ˆ*æœ¬æ–‡ä¸º AIç§‘æŠ€å¤§æœ¬è¥è½¬è½½æ–‡ç« ï¼Œè½¬</em><em>è½½è¯·è”ç³»åŸä½œè€…ï¼‰</em></p><p>â—†</p><p>â—†</p><p>2019 ä¸­å›½å¤§æ•°æ®æŠ€æœ¯å¤§ä¼šï¼ˆBDTCï¼‰å†ç»åä¸€è½½ï¼Œå†åº¦ç«çƒ­æ¥è¢­ï¼è±ªåä¸»å¸­é˜µå®¹åŠç™¾ä½æŠ€æœ¯ä¸“å®¶é½èšï¼Œ15 åœºç²¾é€‰ä¸“é¢˜æŠ€æœ¯å’Œè¡Œä¸šè®ºå›ï¼Œè¶…å¼ºå¹²è´§+æŠ€æœ¯å‰–æ+è¡Œä¸šå®è·µç«‹ä½“è§£è¯»ï¼Œæ·±å…¥è§£æçƒ­é—¨æŠ€æœ¯åœ¨è¡Œä¸šä¸­çš„å®è·µè½åœ°ã€‚<strong class=highlight-text toutiao-origin=span>ã€æ—©é¸Ÿç¥¨ã€‘</strong>ä¸<strong class=highlight-text toutiao-origin=span>ã€ç‰¹æƒ å­¦ç”Ÿç¥¨ã€‘</strong>é™æ—¶æŠ¢è´­ï¼Œæ‰«ç äº†è§£è¯¦æƒ…ï¼</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'æ¨¡å‹','é¢„è®­ç»ƒ','ALBERT'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=æœç´¢>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>ğŸ”</button></form></section><section class=widget><h3 class=widget-title>æœ€æ–°æ–‡ç«  âš¡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>å…¶ä»–</h3><ul class=widget-list><li><a href=TOS.html>ä½¿ç”¨æ¢æ¬¾</a></li><li><a href=CommentPolicy.html>ç•™è¨€æ”¿ç­–</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>è¯çµ¡æˆ‘å€‘</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>æå®¢å¿«è¨Š</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>