<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>中文预训练ALBERT模型来了：小模型登顶GLUE，Base版模型小10倍、速度快1倍 | 极客快訊</title><meta property="og:title" content="中文预训练ALBERT模型来了：小模型登顶GLUE，Base版模型小10倍、速度快1倍 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/RcGwzyuCkrlJ0I"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><meta property="article:published_time" content="2020-11-14T21:02:26+08:00"><meta property="article:modified_time" content="2020-11-14T21:02:26+08:00"><meta name=Keywords content><meta name=description content="中文预训练ALBERT模型来了：小模型登顶GLUE，Base版模型小10倍、速度快1倍"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/23af2adf.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>中文预训练ALBERT模型来了：小模型登顶GLUE，Base版模型小10倍、速度快1倍</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><img alt=中文预训练ALBERT模型来了：小模型登顶GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RcGwzyuCkrlJ0I><p class=pgc-img-caption>（图片由AI科技大本营付费下载自视觉中国）</p><p>作者 | 徐亮（实在智能算法专家)</p><p>来源 | AINLP（ID：nlpjob）</p><p>谷歌ALBERT论文刚刚出炉一周，中文预训练ALBERT模型来了，感兴趣的同学可以直接尝鲜试用。</p><p>项目链接：</p><p>https://github<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">.com</i>/brightmart/albert_zh</p><p>An Implementation of A Lite Bert For Self-Supervised Learning Language Representations with TensorFlow.</p><p>ALBert is based on Bert, but with some improvements. It achieves state of the art performance on main benchmarks with 30% parameters less.</p><p>For albert_base_zh it only has ten percentage parameters compare of original bert model, and main accuracy is retained.</p><p>Chinese version of ALBERT pre-trained model, including checkpoints both for TensorFlow and PyTorch, will be available.</p><p>海量中文语料上预训练ALBERT模型：参数更少，效果更好。预训练小模型也能拿下13项NLP任务，ALBERT三大改造登顶GLUE基准。</p><p><strong>***** 2019-10-02: albert_large_zh *****</strong></p><p>Relased albert_large_zh with only 16% parameters of bert_base(<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">64</i>M)</p><p><strong>***** 2019-10-01: albert_base_zh *****</strong></p><p>Relesed albert_base_zh with only 10% parameters of bert_base, a small model(40M) & training can be very fast.</p><p><strong>***** 2019-09-28: codes and test functions *****</strong></p><p>A<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">dd</i> codes and test functions for three main changes of albert from bert</p><p><strong>模型下载 Download Pre-trained Models of Chinese</strong></p><p>1、albert_large_zh,参数量，层数24，大小为<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">64</i>M</p><pre><p>参数量和模型大小为bert_base的六分之一；在口语化描述相似性数据集LCQMC的测试集上相比bert_base上升0.2个点</p></pre><pre>2、albert_base_zh(小模型体验版)， 参数量12M，层数12，大小为40M<br></pre><pre><pre><p>参数量为bert_base的十分之一，模型大小也十分之一；在口语化描述相似性数据集LCQMC的测试集上相比bert_base下降约1个点；</p><p>相比未预训练，albert_base提升14个点</p></pre><br></pre><p>3、albert_xlarge、 albert_xxlarge will coming recently.</p><pre><p>if you want use a albert model with best performance among all pre-trained models, just wait a few days.</p></pre><pre><div><p>ALBERT模型介绍 Introduction of ALBERT</p></div></pre><p>ALBERT模型是BERT的改进版，与最近其他State of the art的模型不同的是，这次是预训练小模型，效果更好、参数更少。</p><p><strong>它对BERT进行了三个改造 Three main changes of ALBert from Bert：</strong></p><p>1）词嵌入向量参数的因式分解 Factorized embe<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">dd</i>ing parameterization</p><pre><br><pre><p>O(V * H) to O(V * E + E * H)</p><p>如以ALBert_xxlarge为例，V=30000, H=4096, E=128</p><p>那么原先参数为V * H= 30000 * 4096 = 1.23亿个参数，现在则为V * E + E * H = 30000*128+128*4096 = 384万 + 52万 = 436万，</p><p>词嵌入相关的参数变化前是变换后的28倍。</p></pre><br></pre><p>2）跨层参数共享 Cross-Layer Parameter Sharing</p><pre><p>参数共享能显著减少参数。共享可以分为全连接层、注意力层的参数共享；注意力层的参数对效果的减弱影响小一点。</p></pre><pre>3）段落连续性任务 Inter-sentence coherence loss.<br></pre><pre><pre><p>使用段落连续性任务。正例，使用从一个文档中连续的两个文本段落；负例，使用从一个文档中连续的两个文本段落，但位置调换了。</p><p>避免使用原有的NSP任务，原有的任务包含隐含了预测主题这类过于简单的任务。</p><p>We maintain that inter-sentence modeling is an important aspect of language understanding, but we propose a loss </p><p>based primarily on coherence. That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic </p><p>prediction and instead focuses on modeling inter-sentence coherence. The SOP loss uses as positive examples the </p><p>same technique as BERT (two consecutive segments from the same document), and as negative examples the same two </p><p>consecutive segments but with their order swapped. This forces the model to learn finer-grained distinctions about</p><p>discourse-level coherence properties. </p></pre><p>其他变化，还有 Other changes：</p><pre><p>1）去掉了dropout Remvoe dropout to enlarge capacity of model.</p><p>最大的模型，训练了1百万步后，还是没有过拟合训练数据。说明模型的容量还可以更大，就移除了dropout</p><p>（dropout可以认为是随机的去掉网络中的一部分，同时使网络变小一些）</p><p>We also note that, even after training for 1M steps, our largest models still do not overfit to their training data. </p><p>As a result, we decide to remove dropout to further increase our model capacity.</p><p>其他型号的模型，在我们的实现中我们还是会保留原始的dropout的比例，防止模型对训练数据的过拟合。</p><p>2）为加快训练速度，使用LAMB做为优化器 Use lAMB as optimizer, to train with big batch size</p><p>使用了大的batch_size来训练(4096)。LAMB优化器使得我们可以训练，特别大的批次batch_size，如高达6万。</p><p>3）使用n-gram(uni-gram,bi-gram, tri-gram）来做遮蔽语言模型 Use n-gram as make language model</p><p>即以不同的概率使用n-gram,uni-gram的概率最大，bi-gram其次，tri-gram概率最小。</p><p>本项目中目前使用的是在中文上做whole word mask，稍后会更新一下与n-gram mask的效果对比。n-gram从spanBERT中来。</p></pre></pre><p></p><h1 toutiao-origin=h2>发布计划 Release <i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">Plan</i></h1><p>1、albert_base，参数量12M, 层数12，10月7号</p><p>2、albert_large，参数量18M, 层数24，10月13号</p><p>3、albert_xlarge，参数量59M, 层数24，10月6号</p><p>4、albert_xxlarge，参数量233M, 层数12，10月7号（效果最佳的模型）</p><p><strong>训练语料/训练配置 Training Data & Configuration</strong></p><p>30g中文语料，超过100亿汉字，包括多个百科、新闻、互动社区。</p><p>预训练序列长度sequence_length设置为512，批次batch_size为4096，训练产生了3.5亿个训练数据(instance)；每一个模型默认会训练125k步，albert_xxlarge将训练更久。</p><p>作为比较，roberta_zh预训练产生了2.5亿个训练数据、序列长度为256。由于albert_zh预训练生成的训练数据更多、使用的序列长度更长，</p><pre><br><pre><p>我们预计albert_zh会有比roberta_zh更好的性能表现，并且能更好处理较长的文本。</p></pre><br></pre><p>训练使用TPU v3 Pod，我们使用的是v3-256，它包含32个v3-8。每个v3-8机器，含有128G的显存。</p><p><strong>模型性能与对比(英文) Performance and Comparision</strong></p><img alt=中文预训练ALBERT模型来了：小模型登顶GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Re42wuX547Yk5y><img alt=中文预训练ALBERT模型来了：小模型登顶GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Re42wv55YS5Qv4><img alt=中文预训练ALBERT模型来了：小模型登顶GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Re42wvUA3u7Dj3><p><strong>中文任务集上效果对比测试 Performance on Chinese datasets</strong></p><ul><li><h2 toutiao-origin=h3><strong>自然语言推断：XNLI of Chinese Version</strong></h2></li></ul><img alt=中文预训练ALBERT模型来了：小模型登顶GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Re42wvpDiRUbWB><p>注：BERT-wwm-ext来自于这里；XLNet来自于这里; RoBERTa-zh-base，指12层RoBERTa中文模型</p><ul><li><p><strong>问题匹配语任务：LCQMC(Sentence Pair Matching)</strong></p></li></ul><img alt=中文预训练ALBERT模型来了：小模型登顶GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Re42x0V17WQ0MB><ul><li><p><strong>语言模型、文本段预测准确性、训练时间 Mask Language Model Accuarcy & Training Time</strong></p></li></ul><img alt=中文预训练ALBERT模型来了：小模型登顶GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Re42x0r380iJUY><p>注：? 将很快替换</p><p></p><h1 toutiao-origin=h2>模型参数和配置 Configuration of Models</h1><img alt=中文预训练ALBERT模型来了：小模型登顶GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Re42x16DoN9bGp><p><strong>代码实现和测试 Implementation and Code Testing</strong></p><p>通过运行以下命令测试主要的改进点，包括但不限于词嵌入向量参数的因式分解、跨层参数共享、段落连续性任务等。</p><pre><p>python <strong class=highlight-text toutiao-origin=span>test_changes</strong><strong class=highlight-text toutiao-origin=span>.py</strong></p></pre><p></p><h1 toutiao-origin=h2>预训练 Pre-training</h1><ul><li><p><strong>生成特定格式的文件(tfrecords) Generate tfrecords Files</strong></p></li></ul><p>运行以下命令即可。项目自动了一个示例的文本文件(data/news_zh_1.txt)</p><pre><br><pre><p>bash <strong class=highlight-text toutiao-origin=span>create_pretrain_data</strong><strong class=highlight-text toutiao-origin=span>.sh</strong></p></pre><br><p>如果你有很多文本文件，可以通过传入参数的方式，生成多个特定格式的文件(tfrecords）</p></pre><p></p><h2 toutiao-origin=h4>执行预训练 pre-training on GPU/TPU</h2><pre><br><pre><p>GPU:</p><p>export BERT_BASE_DIR=albert_config</p><p>nohup python3 run_pretraining.py --input_file=./data/tf*.tfrecord \</p><p>--output_dir=my_new_model_path --do_train=True --do_eval=True --bert_config_file=$BERT_BASE_DIR/albert_config_xxlarge.json \</p><p>--train_batch_size=4096 --max_seq_length=512 --max_predictions_per_seq=76 \</p><p>--num_train_steps=125000 --num_warmup_steps=12500 --learning_rate=0.00<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-4">176</i> \</p><p>--save_checkpoints_steps=2000 --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt &amp;</p><p>TPU, a<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">dd</i> following information:</p><p>--use_tpu=True --tpu_name=grpc://10.240.1.66:8470 --tpu_zone=us-central1-a</p><p>注：如果你从头开始训练，可以不指定init_checkpoint；</p><p>如果你从现有的模型基础上训练，指定一下BERT_BASE_DIR的路径，并确保bert_config_file和init_checkpoint两个参数的值能对应到相应的文件上；</p><p>领域上的预训练，根据数据的大小，可以不用训练特别久。</p></pre><br></pre><p></p><h1 toutiao-origin=h2>下游任务 Fine-tuning</h1><p>以使用albert_base做LCQMC任务为例。LCQMC任务是在口语化描述的数据集上做文本的相似性预测。</p><p>下载LCQMC数据集，包含训练、验证和测试集，训练集包含24万口语化描述的中文句子对，标签为1或0。1为句子语义相似，0为语义不相似。</p><p>通过运行下列命令做LCQMC数据集上的fine-tuning:</p><pre><br><pre><div><p>1. Clone this project:</p><p>git clone https://github<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">.com</i>/brightmart/albert_zh.git</p><p>2. Fine-tuning by running the following command：</p><p>export BERT_BASE_DIR=./albert_large_zh</p><p>export TEXT_DIR=./lcqmc</p><p>nohup python3 run_classifier.py --task_name=lcqmc_pair --do_train=False --do_eval=true --data_dir=$TEXT_DIR --vocab_file=./albert_config/vocab.txt \</p><p>--bert_config_file=./albert_config/albert_config_large.json --max_seq_length=128 --train_batch_size=<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">64</i> --learning_rate=2e-5 --num_train_epochs=3 \</p><p>--output_dir=albert_large_lcqmc_checkpoints --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt &amp;</p><p>Notice/注：</p><p>you need to download pre-trained chinese albert model, and also download LCQMC dataset </p><p>你需要下载预训练的模型，并放入到项目当前项目，假设目录名称为albert_large_zh; 需要下载LCQMC数据集，并放入到当前项目，</p><p>假设数据集目录名称为lcqmc</p></div></pre><br></pre><p></p><h2 toutiao-origin=h4>Reference</h2><p>1、ALBERT: A Lite BERT For Self-Supervised Learning Of Language Representations</p><p>2、BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p><p>3、SpanBERT: Improving Pre-training by Representing and Predicting Spans</p><p>4、RoBERTa: A Robustly Optimized BERT Pretraining Approach</p><p>5、Large Batch Optimization for Deep Learning: Training BERT in 76 minutes(LAMB)</p><p>6、LAMB Optimizer,TensorFlow version</p><p><em>（*本文为 AI科技大本营转载文章，转</em><em>载请联系原作者）</em></p><p>◆</p><p>◆</p><p>2019 中国大数据技术大会（BDTC）历经十一载，再度火热来袭！豪华主席阵容及百位技术专家齐聚，15 场精选专题技术和行业论坛，超强干货+技术剖析+行业实践立体解读，深入解析热门技术在行业中的实践落地。<strong class=highlight-text toutiao-origin=span>【早鸟票】</strong>与<strong class=highlight-text toutiao-origin=span>【特惠学生票】</strong>限时抢购，扫码了解详情！</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'模型','预训练','ALBERT'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>