<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>谷歌Transformer再升级—新模型实现性能、速度双提升 | 极客快訊</title><meta property="og:title" content="谷歌Transformer再升级—新模型实现性能、速度双提升 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/aa63807597454a2e875c855491eb7d82"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a1f2cc9.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a1f2cc9.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2a1f2cc9.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a1f2cc9.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a1f2cc9.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2a1f2cc9.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2a1f2cc9.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a1f2cc9.html><meta property="article:published_time" content="2020-11-14T21:05:18+08:00"><meta property="article:modified_time" content="2020-11-14T21:05:18+08:00"><meta name=Keywords content><meta name=description content="谷歌Transformer再升级—新模型实现性能、速度双提升"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/2a1f2cc9.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>谷歌Transformer再升级—新模型实现性能、速度双提升</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>当我们在翻译软件上输入 “Transformer is a novel neural network architecture based on a self-attention mechanism” 后，计算机就可以迅速将它翻译为 “Transformer 是一种基于自注意力机制的新型神经网络架构”，神奇的机器翻译使得多语种互译成为可能。</p><p><br>近年来，得益于机器学习的快速发展，自然语言处理（NLP）技术不断突破，在人机交互、在线翻译工具等领域的应用层出不穷，不同语种的人与人、人与机器之间的无障碍自由交流得以实现。</p><p><br>当前的主流机器翻译主要是基于神经网络机器翻译，这类方法是一个 “编码器-解码器”（encoder-decoder）架构的系统，编码器对源语言序列进行编码，并提取信息，然后通过解码器把信息转换为目标语言，完成语言翻译过程。</p><p><br>自 2017 年问世以来，基于“编码器-解码器”架构设计的 Transformer 模型凭借其优越的性能，已然成为机器翻译领域的主流模型，在深度学习领域产生了巨大影响。</p><div class=pgc-img><img alt=谷歌Transformer再升级—新模型实现性能、速度双提升 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/aa63807597454a2e875c855491eb7d82><p class=pgc-img-caption></p></div><p>然而，Transformer 模型并非完美，模型引入self-attention机制虽实现了快速并行的训练，但在长序列文本的处理问题上，却需要占据大量计算资源，导致模型训练成本提高。</p><p><br>近日，由 Google、剑桥大学、DeepMind 和<span style="color:#4d5156;--tt-darkmode-color: #8E939A">艾伦·图灵研究院（Alan Turing Institute）</span>的研究人员组成的团队基于正交随机特征的快速注意力（Fast Attention Via Positive Orthogonal Random Features，FAVOR+）机制，提出了一种新的 Transformer 模型——Performer。相比于 Transformer 模型，新模型无需做出过度调整就可以变得更加高效和节能。</p><p><br></p><h1 class=pgc-h-arrow-right>Performer 模型的技术突破<br></h1><p>2017 年，谷歌大脑（Google Brain）的 Ashish Vaswani 等人发表了一篇题为 “Attention Is All You Need” 的论文，<span style="color:#ab1942;--tt-darkmode-color: #DF285B">首次提出一种基于自注意力机制的 Transformer 模型</span>。</p><p><br></p><p><br>Transformer 模型颠覆了传统神经网络的架构，弥补了卷积神经网络（CNN）和递归神经网络（RNN）存在的不足，在语义特征提取、长距离特征捕获、任务综合特征抽取等自然语言处理方面表现出了更优的性能，在自然语言处理、人机对话、图像处理等许多领域都达到了当时最好的水平（<span style="color:#4c4c4c;--tt-darkmode-color: #9A9A9A">SOTA</span>）。</p><p><br>Transformer 架构的核心模块是自注意力模块，模型在处理每个单词（输入序列中的每个位置）时，自注意力模块通过计算输入序列中所有位置对的相似度分数，来寻找能够帮助更好地编码该单词的线索。</p><p><br>然而，随着输入序列长度的增加，模型需要二次方的计算时间来产生所有相似度分数，所需计算内存也随之增加，注意力机制面临的效率问题也越来越突出。</p><p><br>针对那些需要长距离关注的应用，在 Transformer 基础上已经有一些研究者提出了几种快速的、空间利用率高的改进方法，但是大部分常见方法都依赖于稀疏注意力机制。</p><p><br>然而，稀疏注意力机制仍存在一定的局限性。<br>（1）它们需要高效的稀疏矩阵乘法运算，而这些运算并不是在所有加速器上都能实现的；</p><p>（2）它们通常不能为其表示能力提供严格的理论保证；</p><p>（3）它们主要针对 Transformer 模型和生成式预训练进行优化；</p><p>（4）它们通常会叠加更多的注意力层来补偿稀疏表示，这使得它们很难与其他预训练模型一起使用，因此需要重新训练并消耗大量能量。</p><p><br>此外，稀疏注意机制通常仍然不足以解决常规注意方法应用的全部问题，如指针网络。还有一些运算不能被稀疏化，如在工业级推荐系统中被大量应用的 softmax 运算。</p><p><br>Performer 使用了一个高效的（线性）广义注意力框架，能够对常规（softmax）全阶注意力进行可证明的、准确的、实用的估计，不依赖于任何稀疏性或低阶等先验条件，从而实现更快的训练速度，同时允许模型处理更长的序列，这一特性恰恰满足了 ImageNet64 图像数据集和PG-19文本数据集的要求。</p><p><br>Performer 模型通过正交随机特征（FAVOR+）算法实现快速注意力机制，并改用 Positive Orthogonal Random Features 估计 softmax 和高斯核函数，以实现在 FAVOR+ 机制中对常规 softmax 注意力进行鲁棒且无偏的估计。</p><p><br>研究人员表示：<strong><span style="color:#ab1942;--tt-darkmode-color: #DF285B">“Performer 是第一个通过微调可以与常规 Transformers 进行完全兼容的线性架构”</span></strong>。</p><div class=pgc-img><img alt=谷歌Transformer再升级—新模型实现性能、速度双提升 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/21726768b5c24ce99c02199b364b7589><p class=pgc-img-caption></p></div><blockquote class=pgc-blockquote-abstract><p><span style="color:#888;--tt-darkmode-color: #888888">图 | 原点对称的通用函数 r（定义为建立在：三角随机特征和正随机特征上的估计器的均方误差（MSEs）的比值）是输入特征向量与其长度l之间的角度 φ（以弧度为单位）的函数, 函数的数值越大表示正随机特征性能越好的（φ，l）空间区域（左）；当 l 为定值 1 时，与变化的角度 φ 构成的函数 r 为正切函数，以及比较低 softmax 内核值区域中两个估算器的 MSE（右）。</span></p><p><span style="color:#888;--tt-darkmode-color: #888888"><br></span></p></blockquote><p>作者通过比较发现，对于 φ 足够大的临界区域，该方法所使用的正交随机特征比任意的三角随机特征更精确。</p><p><br></p><div class=pgc-img><img alt=谷歌Transformer再升级—新模型实现性能、速度双提升 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d584aba4a9764a9c95f02d5ad7d8be23><p class=pgc-img-caption></p></div><blockquote class=pgc-blockquote-abstract><p><span style="color:#888;--tt-darkmode-color: #888888">图 | 将原始的经过预训练的 Transformer 的权重转移到 Performer 中，Performer 产的精度达到 0.07 （橙色虚线），但在原来的梯度步数的一小部分中，很快就恢复了精度。</span></p><p><span style="color:#888;--tt-darkmode-color: #888888">然而在 PG-19 上，三角法(TRIG) softmax 逼近变得非常不稳定，而正特征(POS)(不重绘)和 Linformer (也是逼近 softmax)即使在重绘投影的情况下，也会在同样的复杂度中趋于平稳。具有特征重绘的正 softmax 是匹配 Transformer 的必要条件，SMREG 可实现更快的收敛。</span><br></p></blockquote><p>这篇论文利用详细的数学定理，证明了与其单纯依靠计算资源来提升性能，还不如开发出改进的、高效的 Transformer 架构，来显著降低能耗。同时，由于 Performers 使用了与 Transformer 相同的训练超参数，也可以有效训练基于 softmax 的线性 Transformer。因此 FAVOR+ 机制可以作为一个简单的插件，而无需进行过多的调整。</p><p><br></p><h1 class=pgc-h-arrow-right>Performer 模型应用前景广泛<br></h1><p>研究人员表示，Performer 模型的提出，显著降低了常规 Transformer 的空间和时间复杂度，并在 Transformer 的研究以及非稀疏注意机制的作用方面开辟了新的途径。</p><p><br></p><p><br>该论文利用详细的数学定理，证明了与其单纯依靠计算资源来提升性能，还不如开发出改进的、高效的 Transformer 架构，来显著降低能耗。同时，由于 Performers 使用了与 Transformer 相同的训练超参数，因此 FAVOR+ 机制可以作为一个简单的插件，而无需进行过多的调整。</p><p><br>该团队在一系列丰富的场景下测试了 Performers 的性能，执行的任务包括像素预测、蛋白质序列建模。在实验设置中，一个 Performer 只用 FAVOR+ 机制取代了常规 Transformer 的注意力组件。</p><p><br>在使用蛋白质序列训练一个 36 层模型的挑战性任务上，基于 Performer 的模型（Performer-RELU）的性能优于基线 Transformer 模型：Reformer 和 Linformer，后者的准确率显著下降。</p><p><br>在标准的 ImageNet64 基准上，具有 6 层的 Performer 与具有 12 层的 Reformer 的准确性相当。优化后，Performer 的速度达到了 Reformer 的两倍。</p><p><br>研究人员表示，由于基于 Performer 的可扩展 Transformer 架构可以处理更长的序列，而不受注意力机制结构的限制，同时保持准确和鲁棒性，相信它们可以在生物信息学领域带来新的突破，如蛋白质的语言建模等技术已经显示出强大的潜力。</p><p><br></p><p><strong><span style="color:#888;--tt-darkmode-color: #888888">代码地址：</span></strong><span style="color:#888;--tt-darkmode-color: #888888">https://github.com/google-research/google-research/tree/master/performer<br></span><strong><span style="color:#888;--tt-darkmode-color: #888888">论文地址：</span></strong><span style="color:#888;--tt-darkmode-color: #888888">https://arxiv.org/abs/2009.14794</span></p><p><br></p><p><strong><span style="color:#888;--tt-darkmode-color: #888888">参考资料：</span></strong><span style="color:#888;--tt-darkmode-color: #888888"><br>https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html<br>https://syncedreview.com/2020/10/02/google-cambridge-deepmind-alan-turing-institutes-performer-transformer-slashes-compute-costs/<br>https://www.youtube.com/watch?v=xJrKIPwVwGM</span></p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'Transformer','再升级','实现'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>