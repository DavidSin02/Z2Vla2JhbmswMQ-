<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>ç¿»è¯‘ï¼šç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œå›¾åƒåºåˆ—è¯†åˆ«åŠå…¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„åº”ç”¨ | æå®¢å¿«è¨Š</title><meta property="og:title" content="ç¿»è¯‘ï¼šç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œå›¾åƒåºåˆ—è¯†åˆ«åŠå…¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„åº”ç”¨ - æå®¢å¿«è¨Š"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/b43c051705ae41f18cf033aafc0a9ab8"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3139c00d.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3139c00d.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/3139c00d.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3139c00d.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3139c00d.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/3139c00d.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/3139c00d.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3139c00d.html><meta property="article:published_time" content="2020-11-14T21:03:07+08:00"><meta property="article:modified_time" content="2020-11-14T21:03:07+08:00"><meta name=Keywords content><meta name=description content="ç¿»è¯‘ï¼šç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œå›¾åƒåºåˆ—è¯†åˆ«åŠå…¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„åº”ç”¨"><meta name=author content="æå®¢å¿«è¨Š"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/3139c00d.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>ğŸ¤“ æå®¢å¿«è®¯ Geek Bank</a></h1><p class=description>ä¸ºä½ å¸¦æ¥æœ€å…¨çš„ç§‘æŠ€çŸ¥è¯† ğŸ§¡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>çŒœä½ å–œæ­¡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=ç§‘æŠ€>ç§‘æŠ€</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=éŠæˆ²>éŠæˆ²</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=ç§‘å­¸>ç§‘å­¸</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>ç¿»è¯‘ï¼šç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œå›¾åƒåºåˆ—è¯†åˆ«åŠå…¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„åº”ç”¨</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>ç§‘æŠ€</a></span></div><div class=post-content><p>An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition</p><p>åŸºäºç«¯åˆ°ç«¯çš„å¯è®­ç»ƒç¥ç»ç½‘ç»œåŸºäºå›¾åƒçš„åºåˆ—è¯†åˆ«åŠå…¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„åº”ç”¨</p><p>Abstract</p><p><br></p><p>Image-based sequence recognition has been a longstanding research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.</p><p>åŸºäºå›¾åƒçš„åºåˆ—è¯†åˆ«å·²æˆä¸ºè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é•¿æœŸç ”ç©¶è¯¾é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœºæ™¯æ–‡æœ¬è¯†åˆ«é—®é¢˜ï¼Œè¿™æ˜¯åŸºäºå›¾åƒçš„åºåˆ—è¯†åˆ«ä¸­æœ€é‡è¦å’Œæœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¹‹ä¸€ã€‚æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå®ƒå°†ç‰¹å¾æå–ï¼Œåºåˆ—å»ºæ¨¡å’Œè½¬å½•é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ã€‚ä¸ä»¥å‰çš„ç”¨äºåœºæ™¯æ–‡æœ¬è¯†åˆ«çš„ç³»ç»Ÿç›¸æ¯”ï¼Œæ‰€æå‡ºçš„ä½“ç³»ç»“æ„å…·æœ‰å››ä¸ªç‹¬ç‰¹çš„ç‰¹æ€§ï¼šï¼ˆ1ï¼‰ä¸å¤§å¤šæ•°ç°æœ‰çš„ç®—æ³•ï¼ˆå…¶ç»„ä»¶åˆ†åˆ«ç»è¿‡è®­ç»ƒå’Œè°ƒæ•´ï¼‰ç›¸æ¯”ï¼Œå®ƒæ˜¯ç«¯å¯¹ç«¯å¯è®­ç»ƒçš„ã€‚ ï¼ˆ2ï¼‰å®ƒè‡ªç„¶åœ°å¤„ç†ä»»æ„é•¿åº¦çš„åºåˆ—ï¼Œä¸æ¶‰åŠå­—ç¬¦åˆ†å‰²æˆ–æ°´å¹³å°ºåº¦å½’ä¸€åŒ–ã€‚ ï¼ˆ3ï¼‰å®ƒä¸é™äºä»»ä½•é¢„å®šä¹‰çš„è¯å…¸ï¼Œå¹¶ä¸”åœ¨æ— è¯å…¸å’ŒåŸºäºè¯å…¸çš„åœºæ™¯æ–‡æœ¬è¯†åˆ«ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ã€‚ ï¼ˆ4ï¼‰ç”Ÿæˆæœ‰æ•ˆä½†å°å¾—å¤šçš„æ¨¡å‹ï¼Œè¿™å¯¹äºå®é™…åº”ç”¨åœºæ™¯æ›´å®ç”¨ã€‚åœ¨åŒ…æ‹¬IIIT-5Kï¼Œè¡—æ™¯æ–‡å­—å’ŒICDARæ•°æ®é›†åœ¨å†…çš„æ ‡å‡†åŸºå‡†ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†è¯¥ç®—æ³•ä¼˜äºç°æœ‰æŠ€æœ¯çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•åœ¨åŸºäºå›¾åƒçš„ä¹è°±è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œæ˜¾ç„¶è¯æ˜äº†å…¶é€šç”¨æ€§ã€‚</p><p><br></p><p>1. Introduction</p><p>Recently, the community has seen a strong revival of neural networks, which is mainly stimulated by the great success of deep neural network models, specifically Deep Convolutional Neural Networks (DCNN), in various vision tasks. However, majority of the recent works related to deep neural networks have devoted to detection or classification of object categories [12, 25]. In this paper, we are concerned with a classic problem in computer vision: imagebased sequence recognition. In real world, a stable of visual objects, such as scene text, handwriting and musical score, tend to occur in the form of sequence, not in isolation. Unlike general object recognition, recognizing such sequence-like objects often requires the system to predict a series of object labels, instead of a single label. Therefore, recognition of such objects can be naturally cast as a sequence recognition problem. Another unique property of sequence-like objects is that their lengths may vary drastically. For instance, English words can either consist of 2 characters such as "OK" or 15 characters such as "congratulations". Consequently, the most popular deep models like DCNN [25, 26] cannot be directly applied to sequence prediction, since DCNN models often operate on inputs and outputs with fixed dimensions, and thus are incapable of producing a variable-length label sequence.</p><p><br></p><p>æœ€è¿‘ï¼Œç¤¾åŒºçœ‹åˆ°äº†ç¥ç»ç½‘ç»œçš„å¼ºå¤§å¤å…´ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆå°¤å…¶æ˜¯æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆDCNNï¼‰ï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­çš„å·¨å¤§æˆåŠŸæ‰€æ¿€å‘ã€‚ä½†æ˜¯ï¼Œä¸æ·±åº¦ç¥ç»ç½‘ç»œæœ‰å…³çš„æœ€æ–°è‘—ä½œå¤§å¤šæ•°éƒ½è‡´åŠ›äºå¯¹è±¡ç±»åˆ«çš„æ£€æµ‹æˆ–åˆ†ç±»[12ï¼Œ25]ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªç»å…¸é—®é¢˜ï¼šåŸºäºå›¾åƒçš„åºåˆ—è¯†åˆ«ã€‚åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œç¨³å®šçš„è§†è§‰å¯¹è±¡ï¼ˆä¾‹å¦‚åœºæ™¯æ–‡æœ¬ï¼Œæ‰‹å†™å’Œä¹è°±ï¼‰å€¾å‘äºä»¥é¡ºåºè€Œä¸æ˜¯å­¤ç«‹çš„å½¢å¼å‡ºç°ã€‚ä¸ä¸€èˆ¬å¯¹è±¡è¯†åˆ«ä¸åŒï¼Œè¯†åˆ«æ­¤ç±»ç±»ä¼¼åºåˆ—çš„å¯¹è±¡é€šå¸¸éœ€è¦ç³»ç»Ÿé¢„æµ‹ä¸€ç³»åˆ—å¯¹è±¡æ ‡ç­¾ï¼Œè€Œä¸æ˜¯å•ä¸ªæ ‡ç­¾ã€‚å› æ­¤ï¼Œè¿™ç§å¯¹è±¡çš„è¯†åˆ«è‡ªç„¶å¯ä»¥è¢«çœ‹ä½œæ˜¯åºåˆ—è¯†åˆ«é—®é¢˜ã€‚ç±»åºåˆ—å¯¹è±¡çš„å¦ä¸€ä¸ªç‹¬ç‰¹å±æ€§æ˜¯å®ƒä»¬çš„é•¿åº¦å¯èƒ½ä¼šæ€¥å‰§å˜åŒ–ã€‚ä¾‹å¦‚ï¼Œè‹±è¯­å•è¯å¯ä»¥ç”±2ä¸ªå­—ç¬¦ç»„æˆï¼Œä¾‹å¦‚"ç¡®å®š"ï¼Œä¹Ÿå¯ä»¥ç”±15ä¸ªå­—ç¬¦ç»„æˆï¼Œä¾‹å¦‚"ç¥è´º"ã€‚å› æ­¤ï¼ŒåƒDCNN [25ï¼Œ26]è¿™æ ·æœ€æµè¡Œçš„æ·±åº¦æ¨¡å‹ä¸èƒ½ç›´æ¥åº”ç”¨äºåºåˆ—é¢„æµ‹ï¼Œå› ä¸ºDCNNæ¨¡å‹é€šå¸¸å¯¹å…·æœ‰å›ºå®šå°ºå¯¸çš„è¾“å…¥å’Œè¾“å‡ºè¿›è¡Œæ“ä½œï¼Œå› æ­¤æ— æ³•ç”Ÿæˆå¯å˜é•¿åº¦çš„æ ‡ç­¾åºåˆ—ã€‚</p><p><br></p><p>Some attempts have been made to address this problem for a specific sequence-like object (e.g. scene text). For example, the algorithms in [35, 8] firstly detect individual characters and then recognize these detected characters with DCNN models, which are trained using labeled character images. Such methods often require training a strong character detector for accurately detecting and cropping each character out from the original word image. Some other approaches (such as [22]) treat scene text recognition as an image classification problem, and assign a class label to each English word (90K words in total). It turns out a large trained model with a huge number of classes, which is difficult to be generalized to other types of sequencelike objects, such as Chinese texts, musical scores, etc., because the numbers of basic combinations of such kind of sequences can be greater than 1 million. In summary, current systems based on DCNN can not be directly used for image-based sequence recognition.</p><p><br></p><p>å¯¹äºç‰¹å®šçš„ç±»ä¼¼åºåˆ—çš„å¯¹è±¡ï¼ˆä¾‹å¦‚åœºæ™¯æ–‡æœ¬ï¼‰ï¼Œå·²ç»å°è¯•è§£å†³è¯¥é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œ[35ï¼Œ8]ä¸­çš„ç®—æ³•é¦–å…ˆæ£€æµ‹å•ä¸ªå­—ç¬¦ï¼Œç„¶åä½¿ç”¨DCNNæ¨¡å‹è¯†åˆ«è¿™äº›æ£€æµ‹åˆ°çš„å­—ç¬¦ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨æ ‡è®°çš„å­—ç¬¦å›¾åƒè¿›è¡Œè®­ç»ƒã€‚æ­¤ç±»æ–¹æ³•é€šå¸¸éœ€è¦è®­ç»ƒå¼ºå¤§çš„å­—ç¬¦æ£€æµ‹å™¨ï¼Œä»¥å‡†ç¡®åœ°ä»åŸå§‹æ–‡å­—å›¾åƒä¸­æ£€æµ‹å¹¶è£å‰ªå‡ºæ¯ä¸ªå­—ç¬¦ã€‚å…¶ä»–ä¸€äº›æ–¹æ³•ï¼ˆä¾‹å¦‚[22]ï¼‰å°†åœºæ™¯æ–‡æœ¬è¯†åˆ«è§†ä¸ºå›¾åƒåˆ†ç±»é—®é¢˜ï¼Œå¹¶ä¸ºæ¯ä¸ªè‹±è¯­å•è¯ï¼ˆæ€»å…±90Kä¸ªå•è¯ï¼‰åˆ†é…ä¸€ä¸ªç±»åˆ«æ ‡ç­¾ã€‚äº‹å®è¯æ˜ï¼Œè¿™ç§è®­ç»ƒæœ‰ç´ çš„æ¨¡å‹å…·æœ‰å¤§é‡çš„ç±»ï¼Œå¾ˆéš¾å°†å…¶æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„ç±»ä¼¼åºåˆ—çš„å¯¹è±¡ï¼Œä¾‹å¦‚ä¸­æ–‡æ–‡æœ¬ï¼Œä¹è°±ç­‰ï¼Œå› ä¸ºæ­¤ç±»åºåˆ—çš„åŸºæœ¬ç»„åˆæ•°é‡å¯ä»¥å¤§äºä¸€ç™¾ä¸‡ã€‚æ€»ä¹‹ï¼Œå½“å‰åŸºäºDCNNçš„ç³»ç»Ÿä¸èƒ½ç›´æ¥ç”¨äºåŸºäºå›¾åƒçš„åºåˆ—è¯†åˆ«ã€‚</p><p><br></p><p>Recurrent neural networks (RNN) models, another important branch of the deep neural networks family, were mainly designed for handling sequences. One of the advantages of RNN is that it does not need the position of each element in a sequence object image in both training and testing. However, a preprocessing step that converts an input object image into a sequence of image features, is usually essential. For example, Graves et al. [16] extract a set of geometrical or image features from handwritten texts, while Su and Lu [33] convert word images into sequential HOG features. The preprocessing step is independent of the subsequent components in the pipeline, thus the existing systems based on RNN can not be trained and optimized in an end-to-end fashion.</p><p><br></p><p>é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ¨¡å‹æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œå®¶æ—çš„å¦ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä¸»è¦è®¾è®¡ç”¨äºå¤„ç†åºåˆ—ã€‚ RNNçš„ä¼˜ç‚¹ä¹‹ä¸€æ˜¯ï¼Œåœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­ï¼ŒRNNéƒ½ä¸éœ€è¦åºåˆ—å¯¹è±¡å›¾åƒä¸­æ¯ä¸ªå…ƒç´ çš„ä½ç½®ã€‚ ä½†æ˜¯ï¼Œé€šå¸¸å¿…é¡»æ‰§è¡Œå°†è¾“å…¥å¯¹è±¡å›¾åƒè½¬æ¢ä¸ºå›¾åƒç‰¹å¾åºåˆ—çš„é¢„å¤„ç†æ­¥éª¤ã€‚ ä¾‹å¦‚ï¼ŒGravesç­‰ã€‚ [16]ä»æ‰‹å†™æ–‡æœ¬ä¸­æå–å‡ºä¸€ç»„å‡ ä½•æˆ–å›¾åƒç‰¹å¾ï¼Œè€ŒSuå’ŒLu [33]å°†å•è¯å›¾åƒè½¬æ¢ä¸ºè¿ç»­çš„HOGç‰¹å¾ã€‚ é¢„å¤„ç†æ­¥éª¤ç‹¬ç«‹äºæµæ°´çº¿ä¸­çš„åç»­ç»„ä»¶ï¼Œå› æ­¤æ— æ³•ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è®­ç»ƒå’Œä¼˜åŒ–åŸºäºRNNçš„ç°æœ‰ç³»ç»Ÿã€‚</p><p><br></p><p>Several conventional scene text recognition methods that are not based on neural networks also brought insightful ideas and novel representations into this field. For example, Almazan` et al. [5] and Rodriguez-Serrano et al. [30] proposed to embed word images and text strings in a common vectorial subspace, and word recognition is converted into a retrieval problem. Yao et al. [36] and Gordo et al. [14] used mid-level features for scene text recognition. Though achieved promising performance on standard benchmarks, these methods are generally outperformed by previous algorithms based on neural networks [8, 22], as well as the approach proposed in this paper.</p><p><br></p><p>å‡ ç§ä¸åŸºäºç¥ç»ç½‘ç»œçš„å¸¸è§„åœºæ™¯æ–‡æœ¬è¯†åˆ«æ–¹æ³•ä¹Ÿä¸ºè¯¥é¢†åŸŸå¸¦æ¥äº†æœ‰è§åœ°çš„æƒ³æ³•å’Œæ–°é¢–çš„è¡¨ç¤ºå½¢å¼ã€‚ ä¾‹å¦‚ï¼ŒAlmazan`ç­‰ã€‚ [5]å’ŒRodriguez-Serranoç­‰ã€‚ [30]æå‡ºå°†å•è¯å›¾åƒå’Œæ–‡æœ¬å­—ç¬¦ä¸²åµŒå…¥åˆ°ä¸€ä¸ªå…¬å…±çš„å‘é‡å­ç©ºé—´ä¸­ï¼Œå¹¶å°†å•è¯è¯†åˆ«è½¬æ¢ä¸ºæ£€ç´¢é—®é¢˜ã€‚ å§šç­‰ã€‚ [36]å’Œæˆˆå¤šç­‰ã€‚ [14]ä½¿ç”¨ä¸­çº§ç‰¹å¾è¿›è¡Œåœºæ™¯æ–‡æœ¬è¯†åˆ«ã€‚ å°½ç®¡åœ¨æ ‡å‡†åŸºå‡†ä¸Šå–å¾—äº†ä»¤äººæ»¡æ„çš„æ€§èƒ½ï¼Œä½†æ˜¯è¿™äº›æ–¹æ³•é€šå¸¸æ¯”ä»¥å‰åŸºäºç¥ç»ç½‘ç»œçš„ç®—æ³•[8ï¼Œ22]ä»¥åŠæœ¬æ–‡æå‡ºçš„æ–¹æ³•è¦å¥½ã€‚</p><p><br></p><p>The main contribution of this paper is a novel neural network model, whose network architecture is specifically designed for recognizing sequence-like objects in images. The proposed neural network model is named as Convolutional Recurrent Neural Network (CRNN), since it is a combination of DCNN and RNN. For sequence-like objects, CRNN possesses several distinctive advantages over conventional neural network models: 1) It can be directly learned from sequence labels (for instance, words), requiring no detailed annotations (for instance, characters); 2) It has the same property of DCNN on learning informative representations directly from image data, requiring neither hand-craft features nor preprocessing steps, including binarization/segmentation, component localization, etc.; 3) It has the same property of RNN, being able to produce a sequence of labels; 4) It is unconstrained to the lengths of sequence-like objects, requiring only height normalization in both training and testing phases; 5) It achieves better or highly competitive performance on scene texts (word recognition) than the prior arts [23, 8]; 6) It contains much less parameters than a standard DCNN model, consuming less storage space.</p><p><br></p><p>æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯ä¸€ç§æ–°é¢–çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œè¯¥ç½‘ç»œæ¨¡å‹æ˜¯ä¸“é—¨ä¸ºè¯†åˆ«å›¾åƒä¸­ç±»ä¼¼åºåˆ—çš„å¯¹è±¡è€Œè®¾è®¡çš„ã€‚æ‰€æå‡ºçš„ç¥ç»ç½‘ç»œæ¨¡å‹æ˜¯DCNNå’ŒRNNçš„ç»„åˆï¼Œå› æ­¤è¢«ç§°ä¸ºå·ç§¯é€’å½’ç¥ç»ç½‘ç»œï¼ˆCRNNï¼‰ã€‚å¯¹äºç±»ä¼¼åºåˆ—çš„å¯¹è±¡ï¼ŒCRNNä¸ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œæ¨¡å‹ç›¸æ¯”å…·æœ‰å‡ ä¸ªæ˜æ˜¾çš„ä¼˜åŠ¿ï¼š1ï¼‰å¯ä»¥ç›´æ¥ä»åºåˆ—æ ‡ç­¾ï¼ˆä¾‹å¦‚å•è¯ï¼‰ä¸­å­¦ä¹ ï¼Œä¸éœ€è¦è¯¦ç»†çš„æ³¨é‡Šï¼ˆä¾‹å¦‚å­—ç¬¦ï¼‰ï¼› 2ï¼‰å®ƒå…·æœ‰ç›´æ¥ä»å›¾åƒæ•°æ®ä¸­å­¦ä¹ ä¿¡æ¯è¡¨ç¤ºçš„DCNNçš„ç‰¹æ€§ï¼Œæ—¢ä¸éœ€è¦æ‰‹å·¥åŠŸèƒ½ä¹Ÿä¸éœ€è¦é¢„å¤„ç†æ­¥éª¤ï¼ŒåŒ…æ‹¬äºŒå€¼åŒ–/åˆ†å‰²ï¼Œç»„ä»¶å®šä½ç­‰ï¼› 3ï¼‰å…·æœ‰RNNçš„ç›¸åŒå±æ€§ï¼Œèƒ½å¤Ÿäº§ç”Ÿä¸€ç³»åˆ—æ ‡ç­¾ï¼› 4ï¼‰å®ƒä¸å—åºåˆ—çŠ¶ç‰©ä½“é•¿åº¦çš„é™åˆ¶ï¼Œåœ¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µéƒ½åªéœ€è¦é«˜åº¦æ ‡å‡†åŒ–å³å¯ï¼› 5ï¼‰ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œå®ƒåœ¨åœºæ™¯æ–‡æœ¬ï¼ˆå•è¯è¯†åˆ«ï¼‰ä¸Šè¡¨ç°å‡ºæ›´å¥½æˆ–æå…·ç«äº‰åŠ›çš„è¡¨ç°[23ï¼Œ8]ï¼› 6ï¼‰å®ƒåŒ…å«çš„å‚æ•°æ¯”æ ‡å‡†DCNNæ¨¡å‹å°‘å¾—å¤šï¼Œå ç”¨çš„å­˜å‚¨ç©ºé—´ä¹Ÿæ›´å°‘ã€‚</p><p><br></p><p>2. The Proposed Network Architecture</p><p>The network architecture of CRNN, as shown in Fig. 1, consists of three components, including the convolutional layers, the recurrent layers, and a transcription layer, from bottom to topã€‚</p><p>å¦‚å›¾1æ‰€ç¤ºï¼ŒCRNNçš„ç½‘ç»œæ¶æ„ä»ä¸‹åˆ°ä¸Šç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼ŒåŒ…æ‹¬å·ç§¯å±‚ï¼Œå¾ªç¯å±‚å’Œè½¬å½•å±‚ã€‚</p><p><br></p><p>At the bottom of CRNN, the convolutional layers automatically extract a feature sequence from each input image. On top of the convolutional network, a recurrent network is built for making prediction for each frame of the feature sequence, outputted by the convolutional layers. The transcription layer at the top of CRNN is adopted to translate the per-frame predictions by the recurrent layers into a label sequence. Though CRNN is composed of different kinds of network architectures (eg. CNN and RNN), it can be jointly trained with one loss function.</p><p><br></p><p>åœ¨CRNNçš„åº•éƒ¨ï¼Œå·ç§¯å±‚ä¼šè‡ªåŠ¨ä»æ¯ä¸ªè¾“å…¥å›¾åƒä¸­æå–ç‰¹å¾åºåˆ—ã€‚ åœ¨å·ç§¯ç½‘ç»œä¹‹ä¸Šï¼Œæ„å»ºäº†ä¸€ä¸ªé€’å½’ç½‘ç»œï¼Œç”¨äºå¯¹ç”±å·ç§¯å±‚è¾“å‡ºçš„ç‰¹å¾åºåˆ—çš„æ¯ä¸€å¸§è¿›è¡Œé¢„æµ‹ã€‚ é‡‡ç”¨CRNNé¡¶éƒ¨çš„è½¬å½•å±‚ï¼Œå°†å¾ªç¯å±‚çš„æ¯å¸§é¢„æµ‹è½¬æ¢ä¸ºæ ‡è®°åºåˆ—ã€‚ å°½ç®¡CRNNç”±ä¸åŒç±»å‹çš„ç½‘ç»œä½“ç³»ç»“æ„ï¼ˆä¾‹å¦‚CNNå’ŒRNNï¼‰ç»„æˆï¼Œä½†å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæŸå¤±å‡½æ•°è¿›è¡Œè”åˆè®­ç»ƒã€‚</p><div class=pgc-img><img alt=ç¿»è¯‘ï¼šç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œå›¾åƒåºåˆ—è¯†åˆ«åŠå…¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„åº”ç”¨ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b43c051705ae41f18cf033aafc0a9ab8><p class=pgc-img-caption></p></div><p>Figure 1. The network architecture. The architecture consists of three parts: 1) convolutional layers, which extract a feature sequence from the input image; 2) recurrent layers, which predict a label distribution for each frame; 3) transcription layer, which translates the per-frame predictions into the final label sequence.</p><p>å›¾1.ç½‘ç»œæ¶æ„ã€‚ è¯¥ä½“ç³»ç»“æ„åŒ…æ‹¬ä¸‰ä¸ªéƒ¨åˆ†ï¼š1ï¼‰å·ç§¯å±‚ï¼Œä»è¾“å…¥å›¾åƒä¸­æå–ç‰¹å¾åºåˆ—ï¼› 2ï¼‰å¾ªç¯å±‚ï¼Œé¢„æµ‹æ¯ä¸ªå¸§çš„æ ‡ç­¾åˆ†å¸ƒï¼› 3ï¼‰è½¬å½•å±‚ï¼Œå®ƒå°†æ¯å¸§çš„é¢„æµ‹ç¿»è¯‘æˆæœ€ç»ˆçš„æ ‡è®°åºåˆ—ã€‚</p><p><br></p><p>2.1. Feature Sequence Extraction</p><p>In CRNN model, the component of convolutional layers is constructed by taking the convolutional and max-pooling layers from a standard CNN model (fully-connected layers are removed). Such component is used to extract a sequential feature representation from an input image. Before being fed into the network, all the images need to be scaled to the same height. Then a sequence of feature vectors is extracted from the feature maps produced by the component of convolutional layers, which is the input for the recurrent layers. Specifically, each feature vector of a feature sequence is generated from left to right on the feature maps by column. This means the i-th feature vector is the concatenation of the i-th columns of all the maps. The width of each column in our settings is fixed to single pixel.</p><p>åœ¨CRNNæ¨¡å‹ä¸­ï¼Œå·ç§¯å±‚çš„ç»„ä»¶æ˜¯é€šè¿‡ä»æ ‡å‡†CNNæ¨¡å‹ä¸­è·å–å·ç§¯å±‚å’Œæœ€å¤§æ± åŒ–å±‚ï¼ˆé™¤å»å®Œå…¨è¿æ¥çš„å±‚ï¼‰è€Œæ„é€ çš„ã€‚ è¿™æ ·çš„ç»„ä»¶ç”¨äºä»è¾“å…¥å›¾åƒä¸­æå–é¡ºåºç‰¹å¾è¡¨ç¤ºã€‚ åœ¨é€å…¥ç½‘ç»œä¹‹å‰ï¼Œæ‰€æœ‰å›¾åƒéƒ½éœ€è¦ç¼©æ”¾åˆ°ç›¸åŒçš„é«˜åº¦ã€‚ ç„¶åï¼Œä»å·ç§¯å±‚åˆ†é‡äº§ç”Ÿçš„ç‰¹å¾å›¾ä¸­æå–ç‰¹å¾å‘é‡åºåˆ—ï¼Œè¯¥å·ç§¯å±‚æ˜¯å¾ªç¯å±‚çš„è¾“å…¥ã€‚ å…·ä½“åœ°ï¼Œç‰¹å¾åºåˆ—çš„æ¯ä¸ªç‰¹å¾å‘é‡åœ¨ç‰¹å¾å›¾ä¸ŠæŒ‰åˆ—ä»å·¦åˆ°å³ç”Ÿæˆã€‚ è¿™æ„å‘³ç€ç¬¬iä¸ªç‰¹å¾å‘é‡æ˜¯æ‰€æœ‰åœ°å›¾çš„ç¬¬iåˆ—çš„ä¸²è”ã€‚ æˆ‘ä»¬è®¾ç½®ä¸­æ¯åˆ—çš„å®½åº¦å›ºå®šä¸ºå•ä¸ªåƒç´ ã€‚</p><p><br></p><p>As the layers of convolution, max-pooling, and elementwise activation function operate on local regions, they are translation invariant. Therefore, each column of the feature maps corresponds to a rectangle region of the original im- age (termed the receptive field), and such rectangle regions are in the same order to their corresponding columns on the feature maps from left to right. As illustrated in Fig. 2, each vector in the feature sequence is associated with a receptive field, and can be considered as the image descriptor for that region.</p><p>å½“å·ç§¯å±‚ï¼Œæœ€å¤§æ± åŒ–å±‚å’Œå…ƒç´ æ¿€æ´»å‡½æ•°åœ¨å±€éƒ¨åŒºåŸŸä¸Šè¿è¡Œæ—¶ï¼Œå®ƒä»¬æ˜¯å¹³ç§»ä¸å˜çš„ã€‚ å› æ­¤ï¼Œç‰¹å¾å›¾çš„æ¯ä¸€åˆ—å¯¹åº”äºåŸå§‹å›¾åƒçš„ä¸€ä¸ªçŸ©å½¢åŒºåŸŸï¼ˆç§°ä¸ºæ¥æ”¶åœºï¼‰ï¼Œå¹¶ä¸”è¿™äº›çŸ©å½¢åŒºåŸŸä»å·¦åˆ°å³ä¸å®ƒä»¬åœ¨ç‰¹å¾å›¾ä¸Šç›¸åº”åˆ—çš„é¡ºåºç›¸åŒã€‚ å¦‚å›¾2æ‰€ç¤ºï¼Œç‰¹å¾åºåˆ—ä¸­çš„æ¯ä¸ªå‘é‡éƒ½ä¸ä¸€ä¸ªæ¥æ”¶åœºç›¸å…³è”ï¼Œå¹¶ä¸”å¯ä»¥è¢«è§†ä¸ºè¯¥åŒºåŸŸçš„å›¾åƒæè¿°ç¬¦ã€‚</p><p><br></p><div class=pgc-img><img alt=ç¿»è¯‘ï¼šç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œå›¾åƒåºåˆ—è¯†åˆ«åŠå…¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„åº”ç”¨ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8165f25de9e04abab306865548432730><p class=pgc-img-caption></p></div><p>Figure 2. The receptive field. Each vector in the extracted feature sequence is associated with a receptive field on the input image, and can be considered as the feature vector of that field.</p><p>å›¾2.æ¥æ”¶åœºã€‚ æå–çš„ç‰¹å¾åºåˆ—ä¸­çš„æ¯ä¸ªå‘é‡éƒ½ä¸è¾“å…¥å›¾åƒä¸Šçš„ä¸€ä¸ªæ¥æ”¶åœºç›¸å…³è”ï¼Œå¹¶ä¸”å¯ä»¥è§†ä¸ºè¯¥åœºçš„ç‰¹å¾å‘é‡ã€‚</p><p><br></p><p>Being robust, rich and trainable, deep convolutional features have been widely adopted for different kinds of visual recognition tasks [25, 12]. Some previous approaches have employed CNN to learn a robust representation for sequence-like objects such as scene text [22]. However, these approaches usually extract holistic representation of the whole image by CNN, then the local deep features are collected for recognizing each component of a sequencelike object. Since CNN requires the input images to be scaled to a fixed size in order to satisfy with its fixed input dimension, it is not appropriate for sequence-like objects due to their large length variation. In CRNN, we convey deep features into sequential representations in order to be invariant to the length variation of sequence-like objects.</p><p>ä½œä¸ºå¼ºå¤§ï¼Œä¸°å¯Œå’Œå¯è®­ç»ƒçš„æ·±åº¦å·ç§¯ç‰¹å¾å·²è¢«å¹¿æ³›ç”¨äºå„ç§è§†è§‰è¯†åˆ«ä»»åŠ¡[25ï¼Œ12]ã€‚ æŸäº›å…ˆå‰çš„æ–¹æ³•å·²ç»ä½¿ç”¨CNNæ¥å­¦ä¹ å¯¹è¯¸å¦‚åœºæ™¯æ–‡æœ¬ä¹‹ç±»çš„åºåˆ—å¯¹è±¡çš„é²æ£’è¡¨ç¤º[22]ã€‚ ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸é€šè¿‡CNNæå–æ•´ä¸ªå›¾åƒçš„æ•´ä½“è¡¨ç¤ºï¼Œç„¶åæ”¶é›†å±€éƒ¨æ·±å±‚ç‰¹å¾ä»¥è¯†åˆ«åºåˆ—çŠ¶å¯¹è±¡çš„æ¯ä¸ªç»„æˆéƒ¨åˆ†ã€‚ ç”±äºCNNè¦æ±‚å°†è¾“å…¥å›¾åƒç¼©æ”¾åˆ°å›ºå®šå¤§å°ï¼Œä»¥æ»¡è¶³å…¶å›ºå®šçš„è¾“å…¥å°ºå¯¸ï¼Œå› æ­¤ï¼Œç”±äºåºåˆ—é•¿åº¦è¾ƒå¤§ï¼Œå› æ­¤ä¸é€‚åˆç”¨äºç±»ä¼¼åºåˆ—çš„å¯¹è±¡ã€‚ åœ¨CRNNä¸­ï¼Œæˆ‘ä»¬å°†æ·±å±‚ç‰¹å¾ä¼ è¾¾åˆ°é¡ºåºè¡¨ç¤ºä¸­ï¼Œä»¥ä¾¿ä¸å˜äºåºåˆ—çŠ¶å¯¹è±¡çš„é•¿åº¦å˜åŒ–ã€‚</p><p><br></p><p>2.2. Sequence Labeling</p><p>A deep bidirectional Recurrent Neural Network is built on the top of the convolutional layers, as the recurrent layers. The recurrent layers predict a label distribution y_t for each frame x_t in the feature sequence x=x_1,â€¦,x_T . The advantages of the recurrent layers are three-fold. Firstly, RNN has a strong capability of capturing contextual information within a sequence. Using contextual cues for image-based sequence recognition is more stable and helpful than treating each symbol independently. Taking scene text recognition as an example, wide characters may require several successive frames to fully describe (refer to Fig. 2). Besides, some ambiguous characters are easier to distinguish when observing their contexts, e.g. it is easier to recognize â€œilâ€ by contrasting the character heights than by recognizing each of them separately. Secondly, RNN can back-propagates error differentials to its input, i.e. the convolutional layer, allowing us to jointly train the recurrent layers and the convolutional layers in a unified network. Thirdly, RNN is able to operate on sequences of arbitrary lengths, traversing from starts to ends.</p><p>ä¸€ä¸ªæ·±å±‚çš„åŒå‘é€’å½’ç¥ç»ç½‘ç»œè¢«æ„å»ºåœ¨å·ç§¯å±‚çš„é¡¶éƒ¨ï¼Œä½œä¸ºé€’å½’å±‚ã€‚å¾ªç¯å±‚é’ˆå¯¹ç‰¹å¾åºåˆ—x=x_1,â€¦,x_Tä¸­çš„æ¯ä¸ªå¸§x_té¢„æµ‹æ ‡ç­¾åˆ†å¸ƒy_tã€‚å¾ªç¯å±‚çš„ä¼˜ç‚¹æ˜¯ä¸‰æ–¹é¢çš„ã€‚é¦–å…ˆï¼ŒRNNå…·æœ‰åœ¨åºåˆ—ä¸­æ•è·ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å¼ºå¤§åŠŸèƒ½ã€‚ä¸å•ç‹¬å¤„ç†æ¯ä¸ªç¬¦å·ç›¸æ¯”ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡æç¤ºè¿›è¡ŒåŸºäºå›¾åƒçš„åºåˆ—è¯†åˆ«æ›´åŠ ç¨³å®šå’Œæœ‰ç”¨ã€‚ä»¥åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸ºä¾‹ï¼Œå®½å­—ç¬¦å¯èƒ½éœ€è¦å‡ ä¸ªè¿ç»­çš„å¸§æ‰èƒ½å®Œæ•´æè¿°ï¼ˆè¯·å‚é˜…å›¾2ï¼‰ã€‚æ­¤å¤–ï¼ŒæŸäº›æ¨¡æ£±ä¸¤å¯çš„å­—ç¬¦åœ¨è§‚å¯Ÿå…¶ä¸Šä¸‹æ–‡æ—¶æ›´å®¹æ˜“åŒºåˆ†ï¼Œä¾‹å¦‚é€šè¿‡å¯¹æ¯”å­—ç¬¦é«˜åº¦æ¥è¯†åˆ«â€œ ilâ€è¦æ¯”åˆ†åˆ«è¯†åˆ«æ¯ä¸ªå­—ç¬¦è¦å®¹æ˜“ã€‚å…¶æ¬¡ï¼ŒRNNå¯ä»¥å°†è¯¯å·®å·®åˆ†åå‘ä¼ æ’­åˆ°å…¶è¾“å…¥å³å·ç§¯å±‚ï¼Œä»è€Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ç»Ÿä¸€ç½‘ç»œä¸­å…±åŒè®­ç»ƒé€’å½’å±‚å’Œå·ç§¯å±‚. ç¬¬ä¸‰ï¼ŒRNNå¯ä»¥å¯¹ä»»æ„é•¿åº¦çš„åºåˆ—è¿›è¡Œæ“ä½œï¼Œä»å¼€å§‹åˆ°ç»“æŸã€‚</p><p><br></p><div class=pgc-img><img alt=ç¿»è¯‘ï¼šç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œå›¾åƒåºåˆ—è¯†åˆ«åŠå…¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„åº”ç”¨ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c2ddd1237ce5456ca3b794cf4563264c><p class=pgc-img-caption></p></div><p>Figure 3. (a) The structure of a basic LSTM unit. An LSTM consists of a cell module and three gates, namely the input gate, the output gate and the forget gate. (b) The structure of deep bidirectional LSTM we use in our paper. Combining a forward (left to right) and a backward (right to left) LSTMs results in a bidirectional LSTM. Stacking multiple bidirectional LSTM results in a deep bidirectional LSTM.</p><p>å›¾3.ï¼ˆaï¼‰LSTMåŸºæœ¬å•å…ƒçš„ç»“æ„ã€‚ LSTMç”±å•å…ƒæ¨¡å—å’Œä¸‰ä¸ªé—¨ç»„æˆï¼Œå³è¾“å…¥é—¨ï¼Œè¾“å‡ºé—¨å’Œå¿˜è®°é—¨ã€‚ ï¼ˆbï¼‰æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­ä½¿ç”¨çš„æ·±åº¦åŒå‘LSTMçš„ç»“æ„ã€‚ å°†å‘å‰ï¼ˆä»å·¦åˆ°å³ï¼‰å’Œå‘åï¼ˆä»å³åˆ°å·¦ï¼‰LSTMç»„åˆåœ¨ä¸€èµ·å°†äº§ç”ŸåŒå‘LSTMã€‚ å †å å¤šä¸ªåŒå‘LSTMä¼šå¯¼è‡´æ·±åº¦åŒå‘LSTMã€‚</p><p><br></p><p>A traditional RNN unit has a self-connected hidden layer between its input and output layers. Each time it receives a frame x_t in the sequence, it updates its internal state ht with a non-linear function that takes both current input xt and past state h_t-1 as its inputs: h_t = g(x_t,h_t-1). Then the prediction y_t is made based on ht. In this way, past contexts ã€–{x_(t^' )}ã€—_(t^'&lt;t) are captured and utilized for prediction. Traditional RNN unit, however, suffers from the vanishing gradient problem [7], which limits the range of context it can store, and adds burden to the training process. Long-Short Term Memory [18, 11] (LSTM) is a type of RNN unit that is specially designed to address this problem. An LSTM (illustrated in Fig. 3) consists of a memory cell and three multiplicative gates, namely the input, output and forget gates. Conceptually, the memory cell stores the past contexts, and the input and output gates allow the cell to store contexts for a long period of time. Meanwhile, the memory in the cell can be cleared by the forget gate. The special design of LSTM allows it to capture long-range dependencies, which often occur in image-based sequences.</p><p>ä¼ ç»Ÿçš„RNNå•å…ƒåœ¨å…¶è¾“å…¥å’Œè¾“å‡ºå±‚ä¹‹é—´å…·æœ‰è‡ªè¿æ¥çš„éšè—å±‚ã€‚æ¯æ¬¡æ”¶åˆ°åºåˆ—ä¸­çš„å¸§x_tæ—¶ï¼Œå®ƒéƒ½ä¼šä½¿ç”¨éçº¿æ€§å‡½æ•°æ›´æ–°å…¶å†…éƒ¨çŠ¶æ€h_tï¼Œè¯¥å‡½æ•°å°†å½“å‰è¾“å…¥x_tå’Œè¿‡å»çŠ¶æ€ht-1éƒ½ä½œä¸ºå…¶è¾“å…¥ï¼šh_t = g(x_t,h_t-1)ã€‚ç„¶åï¼ŒåŸºäºh_tåšå‡ºé¢„æµ‹y_tã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ•è·è¿‡å»çš„ä¸Šä¸‹æ–‡ã€–{x_(t^' )}ã€—_(t^'&lt;t)å¹¶å°†å…¶ç”¨äºé¢„æµ‹ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„RNNå•å…ƒé­å—æ¢¯åº¦æ¶ˆå¤±çš„å›°æ‰°[7]ï¼Œè¿™é™åˆ¶äº†å®ƒå¯ä»¥å­˜å‚¨çš„ä¸Šä¸‹æ–‡èŒƒå›´ï¼Œå¹¶å¢åŠ äº†è®­ç»ƒè¿‡ç¨‹çš„è´Ÿæ‹…ã€‚é•¿æœŸå†…å­˜[18ï¼Œ11]ï¼ˆLSTMï¼‰æ˜¯ä¸€ç§RNNå•å…ƒï¼Œä¸“é—¨è®¾è®¡ç”¨äºè§£å†³æ­¤é—®é¢˜ã€‚ LSTMï¼ˆå›¾3æ‰€ç¤ºï¼‰ç”±ä¸€ä¸ªå­˜å‚¨å•å…ƒå’Œä¸‰ä¸ªä¹˜æ³•é—¨ç»„æˆï¼Œå³è¾“å…¥ï¼Œè¾“å‡ºå’Œå¿˜è®°é—¨ã€‚ä»æ¦‚å¿µä¸Šè®²ï¼Œå­˜å‚¨å•å…ƒå­˜å‚¨äº†è¿‡å»çš„ä¸Šä¸‹æ–‡ï¼Œè€Œè¾“å…¥å’Œè¾“å‡ºé—¨åˆ™å…è®¸è¯¥å•å…ƒé•¿æ—¶é—´å­˜å‚¨ä¸Šä¸‹æ–‡ã€‚åŒæ—¶ï¼Œå¯ä»¥é€šè¿‡å¿˜è®°é—¨æ¸…é™¤å•å…ƒä¸­çš„å­˜å‚¨å™¨ã€‚ LSTMçš„ç‰¹æ®Šè®¾è®¡ä½¿å…¶å¯ä»¥æ•è·é•¿æœŸä¾èµ–å…³ç³»ï¼Œè¿™ç§ä¾èµ–å…³ç³»ç»å¸¸å‘ç”Ÿåœ¨åŸºäºå›¾åƒçš„åºåˆ—ä¸­ã€‚</p><p>LSTM is directional, it only uses past contexts. However, in image-based sequences, contexts from both directions are useful and complementary to each other. Therefore, we follow [17] and combine two LSTMs, one forward and one backward, into a bidirectional LSTM. Furthermore, multiple bidirectional LSTMs can be stacked, resulting in a deep bidirectional LSTM as illustrated in Fig. 3.b. The deep structure allows higher level of abstractions than a shallow one, and has achieved significant performance improvements in the task of speech recognition [17].</p><p>LSTMæ˜¯å®šå‘çš„ï¼Œå®ƒä»…ä½¿ç”¨è¿‡å»çš„ä¸Šä¸‹æ–‡ã€‚ ä½†æ˜¯ï¼Œåœ¨åŸºäºå›¾åƒçš„åºåˆ—ä¸­ï¼Œæ¥è‡ªä¸¤ä¸ªæ–¹å‘çš„ä¸Šä¸‹æ–‡éƒ½æ˜¯æœ‰ç”¨çš„å¹¶ä¸”å½¼æ­¤äº’è¡¥ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬éµå¾ª[17]ï¼Œå°†ä¸¤ä¸ªLSTMï¼ˆä¸€ä¸ªå‘å‰å’Œä¸€ä¸ªå‘åï¼‰ç»„åˆæˆåŒå‘LSTMã€‚ æ­¤å¤–ï¼Œå¯ä»¥å †å å¤šä¸ªåŒå‘LSTMï¼Œä»è€Œäº§ç”Ÿå¦‚å›¾3.bæ‰€ç¤ºçš„æ·±å±‚åŒå‘LSTMã€‚ è¾ƒä¹‹è¾ƒæµ…çš„ç»“æ„ï¼Œè¾ƒæ·±çš„ç»“æ„å¯ä»¥å®ç°æ›´é«˜çº§åˆ«çš„æŠ½è±¡ï¼Œå¹¶ä¸”åœ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­å·²ç»å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡[17]ã€‚</p><p><br></p><p>In recurrent layers, error differentials are propagated in the opposite directions of the arrows shown in Fig. 3.b, i.e. Back-Propagation Through Time (BPTT). At the bottom of the recurrent layers, the sequence of propagated differentials are concatenated into maps, inverting the operation of converting feature maps into feature sequences, and fed back to the convolutional layers. In practice, we create a custom network layer, called "Map-to-Sequence", as the bridge between convolutional layers and recurrent layers.</p><p>åœ¨å¾ªç¯å±‚ä¸­ï¼Œè¯¯å·®å·®æ²¿å›¾3.bæ‰€ç¤ºç®­å¤´çš„ç›¸åæ–¹å‘ä¼ æ’­ï¼Œå³åå‘ä¼ æ’­æ—¶é—´ï¼ˆBPTTï¼‰ã€‚ åœ¨å¾ªç¯å±‚çš„åº•éƒ¨ï¼Œå°†ä¼ æ’­çš„å·®å¼‚åºåˆ—è¿æ¥æˆå›¾ï¼Œå°†å°†ç‰¹å¾å›¾è½¬æ¢ä¸ºç‰¹å¾åºåˆ—çš„æ“ä½œåè½¬ï¼Œç„¶ååé¦ˆåˆ°å·ç§¯å±‚ã€‚ å®é™…ä¸Šï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªè‡ªå®šä¹‰ç½‘ç»œå±‚ï¼Œç§°ä¸º"æ˜ å°„åˆ°åºåˆ—"ï¼Œä½œä¸ºå·ç§¯å±‚å’Œå¾ªç¯å±‚ä¹‹é—´çš„æ¡¥æ¢ã€‚</p><p><br></p><p>2.3. Transcription</p><p>Transcription is the process of converting the per-frame predictions made by RNN into a label sequence. Mathematically, transcription is to find the label sequence with the highest probability conditioned on the per-frame predictions. In practice, there exists two modes of transcription, namely the lexicon-free and lexicon-based transcriptions. A lexicon is a set of label sequences that prediction is constraint to, e.g. a spell checking dictionary. In lexiconfree mode, predictions are made without any lexicon. In lexicon-based mode, predictions are made by choosing the label sequence that has the highest probability.</p><p>è½¬å½•æ˜¯å°†RNNè¿›è¡Œçš„æ¯å¸§é¢„æµ‹è½¬æ¢ä¸ºæ ‡ç­¾åºåˆ—çš„è¿‡ç¨‹ã€‚ åœ¨æ•°å­¦ä¸Šï¼Œè½¬å½•æ˜¯è¦æ ¹æ®æ¯å¸§é¢„æµ‹æ‰¾åˆ°å…·æœ‰æœ€é«˜æ¦‚ç‡çš„æ ‡è®°åºåˆ—ã€‚ å®é™…ä¸Šï¼Œå­˜åœ¨ä¸¤ç§è½¬å½•æ–¹å¼ï¼Œå³æ— è¯å…¸å’ŒåŸºäºè¯å…¸çš„è½¬å½•ã€‚ è¯å…¸æ˜¯é¢„æµ‹å—å…¶çº¦æŸçš„ä¸€ç»„æ ‡ç­¾åºåˆ—ï¼Œä¾‹å¦‚ æ‹¼å†™æ£€æŸ¥å­—å…¸ã€‚ åœ¨æ— è¯å…¸æ¨¡å¼ä¸‹ï¼Œæ— éœ€ä»»ä½•è¯å…¸å³å¯è¿›è¡Œé¢„æµ‹ã€‚ åœ¨åŸºäºè¯å…¸çš„æ¨¡å¼ä¸‹ï¼Œé€šè¿‡é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„æ ‡ç­¾åºåˆ—æ¥è¿›è¡Œé¢„æµ‹ã€‚</p><p><br></p><p>2.3.1 Probability of label sequence æ ‡ç­¾åºåˆ—çš„æ¦‚ç‡</p><p><br></p><p>We adopt the conditional probability defined in the Connectionist Temporal Classification (CTC) layer proposed by Graves et al. [15]. The probability is defined for label sequence l conditioned on the per-frame predictions y =y_1,...,y_T , and it ignores the position where each label in l is located. Consequently, when we use the negative log-likelihood of this probability as the objective to train the network, we only need images and their corresponding label sequences, avoiding the labor of labeling positions of individual characters.</p><p>æˆ‘ä»¬é‡‡ç”¨Gravesç­‰äººæå‡ºçš„åœ¨è¿æ¥ä¸»ä¹‰æ—¶é—´åˆ†ç±»ï¼ˆCTCï¼‰å±‚ä¸­å®šä¹‰çš„æ¡ä»¶æ¦‚ç‡ã€‚ [15]ã€‚ è¯¥æ¦‚ç‡æ˜¯é’ˆå¯¹ä»¥æ¯å¸§é¢„æµ‹ y =y_1,...,y_Tä¸ºæ¡ä»¶çš„æ ‡ç­¾åºåˆ—lå®šä¹‰çš„ï¼Œå®ƒå¿½ç•¥äº†lä¸­æ¯ä¸ªæ ‡ç­¾æ‰€å¤„çš„ä½ç½®ã€‚ å› æ­¤ï¼Œå½“æˆ‘ä»¬ä»¥è¿™ç§å¯èƒ½æ€§çš„è´Ÿå¯¹æ•°ä¼¼ç„¶åº¦ä¸ºç›®æ ‡æ¥è®­ç»ƒç½‘ç»œæ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦å›¾åƒåŠå…¶ç›¸åº”çš„æ ‡ç­¾åºåˆ—ï¼Œä»è€Œé¿å…äº†ä¸ºå„ä¸ªå­—ç¬¦æ ‡æ³¨ä½ç½®çš„éº»çƒ¦ã€‚</p><p>The formulation of the conditional probability is briefly described as follows: The input is a sequence y =ã€– yã€—_1,...,ã€– yã€—_T where T is the sequence length. Here, each ã€– yã€—_t ÏµR^(|L^' |) is a probability distribution over the set L^'=Lâˆª , where L^' contains all labels in the task (e.g. all English characters), as well as a â€™blankâ€™ label denoted by . A sequence-to-sequence mapping function B is defined on sequence DD, where T is the length. B maps Ï€ onto l by firstly removing the repeated labels, then removing the â€™blankâ€™s. For example, B maps â€œ--hh-e-l-ll-oo--â€ (â€™-â€™ represents â€™blankâ€™) onto â€œhelloâ€. Then, the conditional probability is defined as the sum of probabilities of all Ï€ that are mapped by B onto l:</p><p>æ¡ä»¶æ¦‚ç‡çš„å…¬å¼ç®€è¦æè¿°å¦‚ä¸‹ï¼šè¾“å…¥æ˜¯åºåˆ—y =ã€– yã€—_1,...,ã€– yã€—_Tå…¶ä¸­ï¼ŒTæ˜¯åºåˆ—é•¿åº¦ã€‚ åœ¨è¿™é‡Œï¼Œæ¯ä¸ªã€– yã€—_t ÏµR^(|L^' |)éƒ½æ˜¯é›†åˆL^'=Lâˆªä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼Œå…¶ä¸­L^'åŒ…å«ä»»åŠ¡ä¸­çš„æ‰€æœ‰æ ‡ç­¾ï¼ˆä¾‹å¦‚ï¼Œæ‰€æœ‰è‹±æ–‡å­—ç¬¦ï¼‰ä»¥åŠä»¥è¡¨ç¤ºçš„â€œç©ºç™½â€æ ‡ç­¾ã€‚ åœ¨åºåˆ—DDä¸Šå®šä¹‰äº†åºåˆ—åˆ°åºåˆ—çš„æ˜ å°„å‡½æ•°Bï¼Œå…¶ä¸­Tæ˜¯é•¿åº¦ã€‚ Bé¦–å…ˆåˆ é™¤é‡å¤çš„æ ‡ç­¾ï¼Œç„¶ååˆ é™¤â€œç©ºç™½â€ï¼Œä»è€Œå°†Ï€æ˜ å°„åˆ°lä¸Šã€‚ ä¾‹å¦‚ï¼ŒBå°†â€œ --hh-e-l-ll-oo-â€ï¼ˆâ€œ-â€ä»£è¡¨â€œç©ºç™½â€ï¼‰æ˜ å°„åˆ°â€œ helloâ€ã€‚ ç„¶åï¼Œå°†æ¡ä»¶æ¦‚ç‡å®šä¹‰ä¸ºBæ˜ å°„åˆ°lä¸Šçš„æ‰€æœ‰Ï€çš„æ¦‚ç‡ä¹‹å’Œï¼š</p><p>p(lâ”‚y)=âˆ‘_(Ï€:B(Ï€)=1)â–’ã€–p(Ï€â”‚y) ã€— (1)</p><p>where the probability of Ï€ is defined as p(Ï€â”‚y)=âˆ_(t=1)^Tâ–’y_(Ï€_t)^t , y_(Ï€_t)^t is the probability of having label Ï€_t at time stamp t. Directly computing Eq. 1 would be computationally infeasible due to the exponentially large number of summation items. However, Eq. 1 can be efficiently computed using the forward-backward algorithm described in [15].</p><p>å…¶ä¸­Ï€çš„æ¦‚ç‡å®šä¹‰ä¸ºp(Ï€â”‚y)=âˆ_(t=1)^Tâ–’y_(Ï€_t)^t ï¼Œy_(Ï€_t)^tæ˜¯åœ¨æ—¶é—´æˆ³tå¤„å…·æœ‰æ ‡ç­¾Ï€_tçš„æ¦‚ç‡ã€‚ ç›´æ¥è®¡ç®—å¼ ç”±äºæ±‚å’Œé¡¹çš„æ•°é‡æˆæŒ‡æ•°å¢åŠ ï¼Œå› æ­¤1åœ¨è®¡ç®—ä¸Šæ˜¯ä¸å¯è¡Œçš„ã€‚ ä½†æ˜¯ï¼Œç­‰å¼ã€‚ ä½¿ç”¨[15]ä¸­æè¿°çš„å‰å‘-åå‘ç®—æ³•å¯ä»¥æœ‰æ•ˆåœ°è®¡ç®—å›¾1ã€‚</p><p>2.3.2 Lexicon-free transcription æ— è¯å…¸çš„è½¬å½•</p><p>In this mode, the sequence l^* that has the highest probability as defined in Eq. 1 is taken as the prediction. Since there exists no tractable algorithm to precisely find the solution, we use the strategy adopted in [15]. The sequencel^* is approximately found by l^* â‰ˆ B(ã€–arg maxã€—_Ï€ p(Ï€|y)), i.e. taking the most probable label Ï€_t at each time stamp t, and map the resulted sequence onto l^* .</p><p>åœ¨è¿™ç§æ¨¡å¼ä¸‹ï¼Œå°†å…·æœ‰ç­‰å¼1ä¸­å®šä¹‰çš„æœ€é«˜æ¦‚ç‡çš„åºåˆ—l^*ä½œä¸ºé¢„æµ‹ã€‚ ç”±äºæ²¡æœ‰å¯ç²¾ç¡®è®¡ç®—çš„ç²¾ç¡®ç®—æ³•ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨[15]ä¸­é‡‡ç”¨çš„ç­–ç•¥ã€‚ åºåˆ—l^*ç”±l^* â‰ˆ B(ã€–arg maxã€—_Ï€ p(Ï€|y))è¿‘ä¼¼æ‰¾åˆ°ï¼Œå³åœ¨æ¯ä¸ªæ—¶é—´æˆ³tå¤„å–æœ€å¯èƒ½çš„æ ‡è®°Ï€tï¼Œå¹¶å°†å¾—åˆ°çš„åºåˆ—æ˜ å°„åˆ°l^*ä¸Šã€‚</p><p><br></p><p>2.3.3 Lexicon-based transcription 2.3.3åŸºäºè¯å…¸çš„è½¬å½•</p><p>In lexicon-based mode, each test sample is associated with a lexicon D. Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in Eq. 1, i.e. l^*=ã€–arg maxã€—_(IâˆˆD) p(l|y). However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e. to compute Equation 1 for all sequences in the lexicon and choose the one with the highest probability. To solve this problem, we observe that the label sequences predicted via lexicon-free transcription, described in 2.3.2, are often close to the ground-truth under the edit distance metric. This indicates that we can limit our search to the nearest-neighbor candidates N_Î´ (l^'), where Î´ is the maximal edit distance and l^' is the sequence transcribed from y in lexicon-free mode:</p><p>åœ¨åŸºäºè¯å…¸çš„æ¨¡å¼ä¸‹ï¼Œæ¯ä¸ªæµ‹è¯•æ ·æœ¬éƒ½ä¸ä¸€ä¸ªè¯å…¸Dç›¸å…³è”ã€‚åŸºæœ¬ä¸Šï¼Œé€šè¿‡é€‰æ‹©è¯å…¸ä¸­æ–¹ç¨‹å¼1ä¸­å®šä¹‰çš„æ¡ä»¶æ¦‚ç‡æœ€é«˜çš„åºåˆ—æ¥è¯†åˆ«æ ‡ç­¾åºåˆ—ï¼Œå³l^*=ã€–arg maxã€—_(IâˆˆD) p(l|y)ã€‚ ä½†æ˜¯ï¼Œå¯¹äºå¤§å‹è¯å…¸ï¼Œä¾‹å¦‚ åœ¨ä½¿ç”¨5ä¸‡ä¸ªå•è¯çš„Hunspellæ‹¼å†™æ£€æŸ¥å­—å…¸[1]æ—¶ï¼Œè¦åœ¨è¯å…¸ä¸Šè¿›è¡Œè¯¦å°½æœç´¢ï¼Œå³ä¸ºè¯å…¸ä¸­çš„æ‰€æœ‰åºåˆ—è®¡ç®—ç­‰å¼1å¹¶é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åºåˆ—ï¼Œå°†éå¸¸è€—æ—¶ã€‚ ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°åœ¨2.3.2ä¸­æè¿°çš„é€šè¿‡æ— è¯å…¸è½¬å½•é¢„æµ‹çš„æ ‡ç­¾åºåˆ—åœ¨ç¼–è¾‘è·ç¦»åº¦é‡æ ‡å‡†ä¸‹é€šå¸¸æ¥è¿‘äºçœŸå®æƒ…å†µã€‚ è¿™è¡¨æ˜æˆ‘ä»¬å¯ä»¥å°†æœç´¢èŒƒå›´é™åˆ¶ä¸ºæœ€é‚»è¿‘çš„å€™é€‰å¯¹è±¡N_Î´ (l^')ï¼Œå…¶ä¸­Î´æ˜¯æœ€å¤§ç¼–è¾‘è·ç¦»ï¼Œè€Œl^'æ˜¯åœ¨æ— è¯å…¸æ¨¡å¼ä¸‹ä»yè½¬å½•çš„åºåˆ—ï¼š</p><p>l^* â‰ˆ B(ã€–arg maxã€—_( lâˆˆN_Î´ (l^' ) ) p(lâ”‚y)). (2)</p><p>The candidates N_Î´ (l^')can be found efficiently with the BK-tree data structure [9], which is a metric tree specifically adapted to discrete metric spaces. The search time complexity of BK-tree is O(log |D|), where |D| is the lexicon size. Therefore this scheme readily extends to very large lexicons. In our approach, a BK-tree is constructed offline for a lexicon. Then we perform fast online search with the tree, by finding sequences that have less or equal to Î´ edit distance to the query sequence.</p><p>å¯ä»¥ä½¿ç”¨BKæ ‘æ•°æ®ç»“æ„[9]æœ‰æ•ˆåœ°æ‰¾åˆ°å€™é€‰N_Î´ (l^')ï¼ŒBKæ ‘æ•°æ®ç»“æ„æ˜¯ä¸“é—¨é€‚åˆäºç¦»æ•£åº¦é‡ç©ºé—´çš„åº¦é‡æ ‘ã€‚ BKæ ‘çš„æœç´¢æ—¶é—´å¤æ‚åº¦ä¸ºO(log |D|)ï¼Œå…¶ä¸­|D|æ˜¯è¯å…¸å¤§å°ã€‚ å› æ­¤ï¼Œè¯¥æ–¹æ¡ˆå¾ˆå®¹æ˜“æ‰©å±•åˆ°éå¸¸å¤§çš„è¯å…¸ã€‚ åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œä¸ºè¯å…¸ç¦»çº¿æ„å»ºBKæ ‘ã€‚ ç„¶åï¼Œé€šè¿‡æŸ¥æ‰¾ä¸æŸ¥è¯¢åºåˆ—å…·æœ‰å°äºæˆ–ç­‰äºÎ´ç¼–è¾‘è·ç¦»çš„åºåˆ—ï¼Œæˆ‘ä»¬å¯¹æ ‘è¿›è¡Œå¿«é€Ÿåœ¨çº¿æœç´¢ã€‚</p><p><br></p><p>2.4. Network Training</p><p>Denote the training dataset by X = ã€–{I_i ,I_i}ã€—_i , whereI_i is the training image and I_i is the ground truth label sequence. The objective is to minimize the negative log-likelihood of conditional probability of ground truth:</p><p>O=-âˆ‘_(I_i ,I_iâˆˆX)â–’ã€–log p(I_iâ”‚y_i ),(3)ã€—</p><p>where y_i is the sequence produced by the recurrent and convolutional layers from I_i . This objective function calculates a cost value directly from an image and its ground truth label sequence. Therefore, the network can be end-to-end trained on pairs of images and sequences, eliminating the procedure of manually labeling all individual components in training images.</p><p>å…¶ä¸­y_iæ˜¯ç”±I_içš„å¾ªç¯å±‚å’Œå·ç§¯å±‚äº§ç”Ÿçš„åºåˆ—ã€‚ è¯¥ç›®æ ‡å‡½æ•°ç›´æ¥ä»å›¾åƒåŠå…¶åœ°é¢çœŸç›¸æ ‡ç­¾åºåˆ—è®¡ç®—æˆæœ¬å€¼ã€‚ å› æ­¤ï¼Œå¯ä»¥åœ¨æˆå¯¹çš„å›¾åƒå’Œåºåˆ—ä¸Šå¯¹ç½‘ç»œè¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œä»è€Œçœå»äº†æ‰‹åŠ¨æ ‡è®°è®­ç»ƒå›¾åƒä¸­æ‰€æœ‰å•ä¸ªç»„ä»¶çš„è¿‡ç¨‹ã€‚</p><p><br></p><p>The network is trained with stochastic gradient descent (SGD). Gradients are calculated by the back-propagation algorithm. In particular, in the transcription layer, error differentials are back-propagated with the forward-backward algorithm, as described in [15]. In the recurrent layers, the Back-Propagation Through Time (BPTT) is applied to calculate the error differentials.</p><p>è¯¥ç½‘ç»œä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰è¿›è¡Œè®­ç»ƒã€‚ æ¢¯åº¦æ˜¯é€šè¿‡åå‘ä¼ æ’­ç®—æ³•è®¡ç®—çš„ã€‚ ç‰¹åˆ«æ˜¯ï¼Œåœ¨è½¬å½•å±‚ä¸­ï¼Œè¯¯å·®å·®å¼‚é€šè¿‡å‰å‘åç®—æ³•å‘åä¼ æ’­ï¼Œå¦‚[15]æ‰€è¿°ã€‚ åœ¨å¾ªç¯å±‚ä¸­ï¼Œåº”ç”¨åå‘ä¼ æ’­æ—¶é—´ï¼ˆBPTTï¼‰æ¥è®¡ç®—è¯¯å·®å·®å¼‚ã€‚</p><p><br></p><p>For optimization, we use the ADADELTA [37] to automatically calculate per-dimension learning rates. Compared with the conventional momentum [31] method, ADADELTA requires no manual setting of a learning rate. More importantly, we find that optimization using ADADELTA converges faster than the momentum method.</p><p>ä¸ºäº†ä¼˜åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨ADADELTA [37]è‡ªåŠ¨è®¡ç®—æ¯ç»´åº¦çš„å­¦ä¹ ç‡ã€‚ ä¸ä¼ ç»Ÿçš„åŠ¨é‡[31]æ–¹æ³•ç›¸æ¯”ï¼ŒADADELTAä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®å­¦ä¹ é€Ÿç‡ã€‚ æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨ADADELTAè¿›è¡Œä¼˜åŒ–çš„æ”¶æ•›é€Ÿåº¦å¿«äºåŠ¨é‡æ³•ã€‚</p><p><br></p><p>3. Experiments</p><p>To evaluate the effectiveness of the proposed CRNN model, we conducted experiments on standard benchmarks for scene text recognition and musical score recognition, which are both challenging vision tasks. The datasets and setting for training and testing are given in Sec.3.1, the detailed settings of CRNN for scene text images is provided in Sec.3.2, and the results with the comprehensive comparisons are reported in Sec.3.3. To further demonstrate the generality of CRNN, we verify the proposed algorithm on a music score recognition task in Sec.3.4.</p><p>ä¸ºäº†è¯„ä¼°æ‰€æå‡ºçš„CRNNæ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬é’ˆå¯¹åœºæ™¯æ–‡æœ¬è¯†åˆ«å’Œä¹è°±è¯†åˆ«çš„æ ‡å‡†åŸºå‡†è¿›è¡Œäº†å®éªŒï¼Œè¿™ä¸¤è€…éƒ½æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰ä»»åŠ¡ã€‚ è®­ç»ƒå’Œæµ‹è¯•çš„æ•°æ®é›†å’Œè®¾ç½®åœ¨ç¬¬3.1èŠ‚ä¸­ç»™å‡ºï¼Œåœºæ™¯æ–‡æœ¬å›¾åƒçš„CRNNçš„è¯¦ç»†è®¾ç½®åœ¨ç¬¬3.2èŠ‚ä¸­æä¾›ï¼Œç»è¿‡å…¨é¢æ¯”è¾ƒçš„ç»“æœåœ¨ç¬¬3.3èŠ‚ä¸­è¿›è¡Œäº†æŠ¥å‘Šã€‚ ä¸ºäº†è¿›ä¸€æ­¥è¯æ˜CRNNçš„é€šç”¨æ€§ï¼Œæˆ‘ä»¬åœ¨ç¬¬3.4èŠ‚ä¸­å¯¹éŸ³ä¹åˆ†æ•°è¯†åˆ«ä»»åŠ¡éªŒè¯äº†æ‰€æå‡ºçš„ç®—æ³•ã€‚</p><p><br></p><p>3.1. Datasets</p><p>For all the experiments for scene text recognition, we use the synthetic dataset (Synth) released by Jaderberg et al. [20] as the training data. The dataset contains 8 millions training images and their corresponding ground truth words. Such images are generated by a synthetic text engine and are highly realistic. Our network is trained on the synthetic data once, and tested on all other real-world test datasets without any fine-tuning on their training data. Even though the CRNN model is purely trained with synthetic text data, it works well on real images from standard text recognition benchmarks.</p><p>å¯¹äºæ‰€æœ‰ç”¨äºåœºæ™¯æ–‡æœ¬è¯†åˆ«çš„å®éªŒï¼Œæˆ‘ä»¬ä½¿ç”¨Jaderbergç­‰äººå‘å¸ƒçš„åˆæˆæ•°æ®é›†ï¼ˆSynthï¼‰ã€‚ [20]ä½œä¸ºè®­ç»ƒæ•°æ®ã€‚ æ•°æ®é›†åŒ…å«800ä¸‡ä¸ªè®­ç»ƒå›¾åƒåŠå…¶ç›¸åº”çš„åœ°é¢çœŸå®å•è¯ã€‚ è¿™æ ·çš„å›¾åƒæ˜¯ç”±åˆæˆæ–‡æœ¬å¼•æ“ç”Ÿæˆçš„ï¼Œå…·æœ‰å¾ˆé«˜çš„é€¼çœŸåº¦ã€‚ æˆ‘ä»¬çš„ç½‘ç»œæ¥å—è¿‡ä¸€æ¬¡ç»¼åˆæ•°æ®è®­ç»ƒï¼Œå¹¶åœ¨æ‰€æœ‰å…¶ä»–çœŸå®ä¸–ç•Œçš„æµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œè€Œæ— éœ€å¯¹å…¶è®­ç»ƒæ•°æ®è¿›è¡Œä»»ä½•å¾®è°ƒã€‚ å³ä½¿CRNNæ¨¡å‹æ˜¯å®Œå…¨ç”±åˆæˆæ–‡æœ¬æ•°æ®è®­ç»ƒè€Œæˆçš„ï¼Œå®ƒä¹Ÿå¯ä»¥åœ¨æ ‡å‡†æ–‡æœ¬è¯†åˆ«åŸºå‡†çš„çœŸå®å›¾åƒä¸Šå¾ˆå¥½åœ°å·¥ä½œã€‚</p><p><br></p><p>Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT).</p><p>å››ä¸ªæµè¡Œçš„åœºæ™¯æ–‡æœ¬è¯†åˆ«åŸºå‡†ç”¨äºæ€§èƒ½è¯„ä¼°ï¼Œå³ICDAR 2003ï¼ˆIC03ï¼‰ï¼ŒICDAR 2013ï¼ˆIC13ï¼‰ï¼ŒIIIT 5kå­—ï¼ˆIIIT5kï¼‰å’Œè¡—æ™¯æ–‡æœ¬ï¼ˆSVTï¼‰ã€‚</p><p><br></p><p>IC03 [27] test dataset contains 251 scene images with labeled text bounding boxes. Following Wang et al. [34], we ignore images that either contain non-alphanumeric characters or have less than three characters, and get a test set with 860 cropped text images. Each test image is associated with a 50-words lexicon which is defined by Wang et al. [34]. A full lexicon is built by combining all the per-image lexicons. In addition, we use a 50k words lexicon consisting of the words in the Hunspell spell-checking dictionary [1].</p><p>IC03 [27]æµ‹è¯•æ•°æ®é›†åŒ…å«251ä¸ªå¸¦æœ‰æ ‡è®°æ–‡æœ¬è¾¹ç•Œæ¡†çš„åœºæ™¯å›¾åƒã€‚ ç»§ç‹ç­‰ã€‚ [34]ï¼Œæˆ‘ä»¬å°†å¿½ç•¥åŒ…å«éå­—æ¯æ•°å­—å­—ç¬¦æˆ–å°‘äºä¸‰ä¸ªå­—ç¬¦çš„å›¾åƒï¼Œå¹¶ä½¿ç”¨860ä¸ªè£å‰ªçš„æ–‡æœ¬å›¾åƒè·å–æµ‹è¯•é›†ã€‚ æ¯ä¸ªæµ‹è¯•å›¾åƒéƒ½ä¸Wangç­‰äººå®šä¹‰çš„50ä¸ªå•è¯çš„è¯å…¸ç›¸å…³ã€‚ [34]ã€‚ é€šè¿‡åˆå¹¶æ‰€æœ‰æŒ‰å›¾åƒçš„è¯å…¸æ¥æ„å»ºå®Œæ•´çš„è¯å…¸ã€‚ å¦å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ç”±Hunspellæ‹¼å†™æ£€æŸ¥å­—å…¸[1]ä¸­çš„å•è¯ç»„æˆçš„5ä¸‡ä¸ªå•è¯è¯å…¸ã€‚</p><div class=pgc-img><img alt=ç¿»è¯‘ï¼šç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œå›¾åƒåºåˆ—è¯†åˆ«åŠå…¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„åº”ç”¨ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/04b9f98b2e6d43ed90760d6049f24031><p class=pgc-img-caption></p></div><p>Table 1. Network configuration summary. The first row is the top layer. 'k', 's' and 'p' stand for kernel size, stride and padding size respectively</p><p>è¡¨1.ç½‘ç»œé…ç½®æ‘˜è¦ã€‚ ç¬¬ä¸€è¡Œæ˜¯é¡¶å±‚ã€‚ " k"ï¼Œ" s"å’Œ" p"åˆ†åˆ«ä»£è¡¨å†…æ ¸å¤§å°ï¼Œæ­¥å¹…å’Œå¡«å……å¤§å°</p><p><br></p><p>IC13 [24] test dataset inherits most of its data from IC03. It contains 1,015 ground truths cropped word images.</p><p>IIIT5k [28] contains 3,000 cropped word test images collected from the Internet. Each image has been associated to a 50-words lexicon and a 1k-words lexicon.</p><p>SVT [34] test dataset consists of 249 street view images collected from Google Street View. From them 647 word images are cropped. Each word image has a 50 words lexicon defined by Wang et al. [34].</p><p>IC13 [24]æµ‹è¯•æ•°æ®é›†ç»§æ‰¿äº†IC03çš„å¤§éƒ¨åˆ†æ•°æ®ã€‚ å®ƒåŒ…å«1,015ä¸ªåœ°é¢çœŸç›¸è£å‰ªçš„å•è¯å›¾åƒã€‚</p><p>IIIT5k [28]åŒ…å«ä»äº’è”ç½‘æ”¶é›†çš„3,000ä¸ªè£å‰ªçš„å•è¯æµ‹è¯•å›¾åƒã€‚ æ¯ä¸ªå›¾åƒå·²ä¸50ä¸ªå•è¯çš„è¯å…¸å’Œ1000ä¸ªå•è¯çš„è¯å…¸ç›¸å…³è”ã€‚</p><p>SVT [34]æµ‹è¯•æ•°æ®é›†åŒ…å«ä»Googleè¡—æ™¯æ”¶é›†çš„249å¹…è¡—æ™¯å›¾åƒã€‚ ä»ä¸­è£å‰ªå‡º647ä¸ªå•è¯å›¾åƒã€‚ æ¯ä¸ªå•è¯å›¾åƒéƒ½æœ‰ä¸€ä¸ªç”±Wangç­‰äººå®šä¹‰çš„50ä¸ªå•è¯çš„è¯å…¸ã€‚[34]ã€‚</p><p><br></p><p>3.2. Implementation Details</p><p>The network configuration we use in our experiments is summarized in Table 1. The architecture of the convolutional layers is based on the VGG-VeryDeep architectures [32]. A tweak is made in order to make it suitable for recognizing English texts. In the 3rd and the 4th maxpooling layers, we adopt 1 Ã— 2 sized rectangular pooling windows instead of the conventional squared ones. This tweak yields feature maps with larger width, hence longer feature sequence. For example, an image containing 10 characters is typically of size 100Ã—32, from which a feature sequence 25 frames can be generated. This length exceeds the lengths of most English words. On top of that, the rectangular pooling windows yield rectangular receptive fields (illustrated in Fig. 2), which are beneficial for recognizing some characters that have narrow shapes, such as 'i' and 'l'.</p><p>è¡¨1æ€»ç»“äº†æˆ‘ä»¬åœ¨å®éªŒä¸­ä½¿ç”¨çš„ç½‘ç»œé…ç½®ã€‚å·ç§¯å±‚çš„ä½“ç³»ç»“æ„åŸºäºVGG-VeryDeepä½“ç³»ç»“æ„[32]ã€‚ ä¸ºäº†ä½¿å®ƒé€‚åˆäºè¯†åˆ«è‹±æ–‡æ–‡æœ¬ï¼Œè¿›è¡Œäº†ä¸€äº›è°ƒæ•´ã€‚ åœ¨ç¬¬3å’Œç¬¬4ä¸ªmaxpoolingå±‚ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨1Ã—2å¤§å°çš„çŸ©å½¢æ± çª—å£ï¼Œè€Œä¸æ˜¯å¸¸è§„çš„æ­£æ–¹å½¢æ± çª—å£ã€‚ è¿™ç§è°ƒæ•´ä¼šäº§ç”Ÿå…·æœ‰è¾ƒå¤§å®½åº¦çš„ç‰¹å¾å›¾ï¼Œå› æ­¤ç‰¹å¾åºåˆ—æ›´é•¿ã€‚ ä¾‹å¦‚ï¼ŒåŒ…å«10ä¸ªå­—ç¬¦çš„å›¾åƒé€šå¸¸å¤§å°ä¸º100Ã—32ï¼Œå¯ä»¥ä»ä¸­ç”Ÿæˆ25å¸§çš„ç‰¹å¾åºåˆ—ã€‚ è¯¥é•¿åº¦è¶…è¿‡å¤§å¤šæ•°è‹±è¯­å•è¯çš„é•¿åº¦ã€‚ æœ€é‡è¦çš„æ˜¯ï¼ŒçŸ©å½¢åˆå¹¶çª—å£ä¼šäº§ç”ŸçŸ©å½¢çš„æ¥æ”¶åœºï¼ˆå¦‚å›¾2æ‰€ç¤ºï¼‰ï¼Œè¿™å¯¹äºè¯†åˆ«æŸäº›å½¢çŠ¶è¾ƒçª„çš„å­—ç¬¦ï¼ˆä¾‹å¦‚" i"å’Œ" l"ï¼‰å¾ˆæœ‰å¸®åŠ©ã€‚</p><p><br></p><p>The network not only has deep convolutional layers, but also has recurrent layers. Both are known to be hard to train. We find that the batch normalization [19] technique is extremely useful for training network of such depth. Two batch normalization layers are inserted after the 5th and 6th convolutional layers respectively. With the batch normalization layers, the training process is greatly accelerated.</p><p>ç½‘ç»œä¸ä»…å…·æœ‰æ·±å±‚çš„å·ç§¯å±‚ï¼Œè€Œä¸”å…·æœ‰å¾ªç¯å±‚ã€‚ ä¼—æ‰€å‘¨çŸ¥ï¼Œä¸¤è€…éƒ½å¾ˆéš¾è®­ç»ƒã€‚ æˆ‘ä»¬å‘ç°æ‰¹é‡å½’ä¸€åŒ–[19]æŠ€æœ¯å¯¹äºè®­ç»ƒè¿™ç§æ·±åº¦çš„ç½‘ç»œéå¸¸æœ‰ç”¨ã€‚ åœ¨ç¬¬äº”å’Œç¬¬å…­å·ç§¯å±‚ä¹‹ååˆ†åˆ«æ’å…¥ä¸¤ä¸ªæ‰¹å¤„ç†å½’ä¸€åŒ–å±‚ã€‚ ä½¿ç”¨æ‰¹å¤„ç†å½’ä¸€åŒ–å±‚ï¼Œå¯ä»¥å¤§å¤§åŠ å¿«åŸ¹è®­è¿‡ç¨‹ã€‚</p><p><br></p><p>We implement the network within the Torch7 [10] framework, with custom implementations for the LSTM units (in Torch7/CUDA), the transcription layer (in C++) and the BK-tree data structure (in C++). Experiments are carried out on a workstation with a 2.50 GHz Intel(R) Xeon(R) E5- 2609 CPU, 64GB RAM and an NVIDIA(R) Tesla(TM) K40 GPU. Networks are trained with ADADELTA, setting the parameter Ï to 0.9. During training, all images are scaled to 100 Ã— 32 in order to accelerate the training process. The training process takes about 50 hours to reach convergence. Testing images are scaled to have height 32. Widths are proportionally scaled with heights, but at least 100 pixels. The average testing time is 0.16s/sample, as measured on IC03 without a lexicon. The approximate lexicon search is applied to the 50k lexicon of IC03, with the parameter Î´ set to 3. Testing each sample takes 0.53s on average.</p><p>æˆ‘ä»¬åœ¨Torch7 [10]æ¡†æ¶å†…å®ç°ç½‘ç»œï¼Œå¹¶ä¸ºLSTMå•å…ƒï¼ˆåœ¨Torch7 / CUDAä¸­ï¼‰ï¼Œè½¬å½•å±‚ï¼ˆåœ¨C ++ä¸­ï¼‰å’ŒBKæ ‘æ•°æ®ç»“æ„ï¼ˆåœ¨C ++ä¸­ï¼‰è‡ªå®šä¹‰å®ç°ã€‚ å®éªŒæ˜¯åœ¨è£…æœ‰2.50 GHzIntelÂ®XeonÂ®E5- 2609 CPUï¼Œ64GB RAMå’ŒNVIDIAÂ®TeslaÂ®K40 GPUçš„å·¥ä½œç«™ä¸Šè¿›è¡Œçš„ã€‚ ä½¿ç”¨ADADELTAè®­ç»ƒç½‘ç»œï¼Œå°†å‚æ•°Ïè®¾ç½®ä¸º0.9ã€‚ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰å›¾åƒå‡æŒ‰æ¯”ä¾‹ç¼©æ”¾ä¸º100Ã—32ï¼Œä»¥åŠ å¿«è®­ç»ƒè¿‡ç¨‹ã€‚ åŸ¹è®­è¿‡ç¨‹å¤§çº¦éœ€è¦50ä¸ªå°æ—¶æ‰èƒ½è¾¾åˆ°æ”¶æ•›ã€‚ å°†æµ‹è¯•å›¾åƒç¼©æ”¾ä¸ºé«˜åº¦32ã€‚å®½åº¦ä¸é«˜åº¦æˆæ¯”ä¾‹åœ°ç¼©æ”¾ï¼Œä½†è‡³å°‘100åƒç´ ã€‚ åœ¨æ²¡æœ‰è¯å…¸çš„IC03ä¸Šæµ‹å¾—çš„å¹³å‡æµ‹è¯•æ—¶é—´ä¸º0.16s /æ ·å“ã€‚ å°†è¿‘ä¼¼è¯å…¸æœç´¢åº”ç”¨äºIC03çš„50kè¯å…¸ï¼Œå¹¶å°†å‚æ•°Î´è®¾ç½®ä¸º3ã€‚æµ‹è¯•æ¯ä¸ªæ ·æœ¬å¹³å‡éœ€è¦0.53sã€‚</p><p><br></p><p>3.3. Comparative Evaluation</p><p>All the recognition accuracies on the above four public datasets, obtained by the proposed CRNN model and the recent state-of-the-arts techniques including the approaches based on deep models [23, 22, 21], are shown in Table 2.</p><p>è¡¨2åˆ—å‡ºäº†é€šè¿‡å»ºè®®çš„CRNNæ¨¡å‹å’Œæœ€æ–°æŠ€æœ¯ï¼ˆåŒ…æ‹¬åŸºäºæ·±åº¦æ¨¡å‹çš„æ–¹æ³•ï¼‰è·å¾—çš„ä¸Šè¿°å››ä¸ªå…¬å…±æ•°æ®é›†çš„æ‰€æœ‰è¯†åˆ«å‡†ç¡®æ€§ã€‚</p><p><br></p><p>In the constrained lexicon cases, our method consistently outperforms most state-of-the-arts approaches, and in average beats the best text reader proposed in [22]. Specifically, we obtain superior performance on IIIT5k, and SVT compared to [22], only achieved lower performance on IC03 with the "Full" lexicon. Note that the model in[22] is trained on a specific dictionary, namely that each word is associated to a class label. Unlike [22], CRNN is not limited to recognize a word in a known dictionary, and able to handle random strings (e.g. telephone numbers), sentences or other scripts like Chinese words. Therefore, the results of CRNN are competitive on all the testing datasets.</p><p>åœ¨å—é™çš„è¯å…¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºå¤§å¤šæ•°æœ€æ–°æŠ€æœ¯ï¼Œå¹¶ä¸”å¹³å‡è€Œè¨€èƒœè¿‡[22]ä¸­æå‡ºçš„æœ€ä½³æ–‡æœ¬é˜…è¯»å™¨ã€‚ å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨IIIT5kä¸Šè·å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè€Œä¸[22]ç›¸æ¯”ï¼ŒSVTä»…åœ¨ä½¿ç”¨"å®Œæ•´"è¯å…¸çš„IC03ä¸Šè·å¾—äº†è¾ƒä½çš„æ€§èƒ½ã€‚ æ³¨æ„ï¼Œin [22]ä¸­çš„æ¨¡å‹æ˜¯åœ¨ç‰¹å®šè¯å…¸ä¸Šè®­ç»ƒçš„ï¼Œå³æ¯ä¸ªå•è¯éƒ½ä¸ä¸€ä¸ªç±»åˆ«æ ‡ç­¾ç›¸å…³è”ã€‚ ä¸[22]ä¸åŒï¼ŒCRNNä¸ä»…é™äºè¯†åˆ«å·²çŸ¥è¯å…¸ä¸­çš„å•è¯ï¼Œè¿˜å¯ä»¥å¤„ç†éšæœºå­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ç”µè¯å·ç ï¼‰ï¼Œå¥å­æˆ–å…¶ä»–è„šæœ¬ï¼ˆå¦‚ä¸­æ–‡å•è¯ï¼‰ã€‚ å› æ­¤ï¼ŒCRNNçš„ç»“æœåœ¨æ‰€æœ‰æµ‹è¯•æ•°æ®é›†ä¸Šéƒ½å…·æœ‰ç«äº‰åŠ›ã€‚</p><p><br></p><p>In the unconstrained lexicon cases, our method achieves the best performance on SVT, yet, is still behind some approaches [8, 22] on IC03 and IC13. Note that the blanks in the "none" columns of Table 2 denote that such approaches are unable to be applied to recognition without lexicon or did not report the recognition accuracies in the unconstrained cases. Our method uses only synthetic text with word level labels as the training data, very different to PhotoOCR [8] which used 7.9 millions of real word images with character-level annotations for training. The best performance is reported by [22] in the unconstrained lexicon cases, benefiting from its large dictionary, however, it is not a model strictly unconstrained to a lexicon as mentioned before. In this sense, our results in the unconstrained lexicon case are still promising.</p><p>åœ¨ä¸å—çº¦æŸçš„è¯å…¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨SVTä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œä½†ä»è½åäºIC03å’ŒIC13çš„æŸäº›æ–¹æ³•[8ï¼Œ22]ã€‚ è¯·æ³¨æ„ï¼Œè¡¨2ä¸­"æ— "åˆ—ä¸­çš„ç©ºç™½è¡¨ç¤ºåœ¨æ²¡æœ‰è¯æ±‡çš„æƒ…å†µä¸‹ï¼Œæ­¤ç±»æ–¹æ³•æ— æ³•åº”ç”¨äºè¯†åˆ«ï¼Œæˆ–è€…åœ¨æ— é™åˆ¶çš„æƒ…å†µä¸‹æœªæŠ¥å‘Šè¯†åˆ«å‡†ç¡®æ€§ã€‚ æˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨å¸¦æœ‰å•è¯çº§åˆ«æ ‡ç­¾çš„åˆæˆæ–‡æœ¬ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œè¿™ä¸PhotoOCR [8]å®Œå…¨ä¸åŒï¼Œåè€…ä½¿ç”¨790ä¸‡ä¸ªå¸¦æœ‰å­—ç¬¦çº§åˆ«æ³¨é‡Šçš„çœŸå®å•è¯å›¾åƒè¿›è¡Œè®­ç»ƒã€‚ å—ç›Šäºå…¶åºå¤§çš„å­—å…¸ï¼Œ[22]åœ¨ä¸å—çº¦æŸçš„è¯å…¸æƒ…å†µä¸‹æŠ¥å‘Šäº†æœ€ä½³æ€§èƒ½ï¼Œä½†æ˜¯ï¼Œå®ƒå¹¶ä¸æ˜¯å¦‚ä¸Šæ‰€è¿°ä¸¥æ ¼ä¸å—è¯å…¸çº¦æŸçš„æ¨¡å‹ã€‚ ä»è¿™ä¸ªæ„ä¹‰ä¸Šè®²ï¼Œæˆ‘ä»¬åœ¨æ— çº¦æŸè¯å…¸æƒ…å†µä¸‹çš„ç»“æœä»ç„¶å¾ˆæœ‰å¸Œæœ›ã€‚</p><p><br></p><p>For further understanding the advantages of the proposed algorithm over other text recognition approaches, we provide a comprehensive comparison on several properties named E2E Train, Conv Ftrs, CharGT-Free, Unconstrained, and Model Size, as summarized in Table 3.</p><p>ä¸ºäº†è¿›ä¸€æ­¥äº†è§£è¯¥ç®—æ³•ç›¸å¯¹äºå…¶ä»–æ–‡æœ¬è¯†åˆ«æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬å¯¹åä¸ºE2E Trainï¼ŒConv Ftrsï¼ŒCharGT-Freeï¼ŒUnconstrainedå’ŒModel Sizeçš„å‡ ä¸ªå±æ€§è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒï¼Œå¦‚è¡¨3æ‰€ç¤ºã€‚</p><p><br></p><div class=pgc-img><img alt=ç¿»è¯‘ï¼šç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œå›¾åƒåºåˆ—è¯†åˆ«åŠå…¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„åº”ç”¨ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f58ff480b79542c1b87ed136b8cc29e8><p class=pgc-img-caption></p></div><p>Table 3. Comparison among various methods. Attributes for comparison include: 1) being end-to-end trainable (E2E Train); 2) using convolutional features that are directly learned from images rather than using hand-crafted ones (Conv Ftrs); 3) requiring no ground truth bounding boxes for characters during training (CharGT-Free); 4) not confined to a pre-defined dictionary (Unconstrained); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).</p><p>è¡¨3.å„ç§æ–¹æ³•ä¹‹é—´çš„æ¯”è¾ƒã€‚ æ¯”è¾ƒçš„å±æ€§åŒ…æ‹¬ï¼š1ï¼‰ç«¯åˆ°ç«¯å¯åŸ¹è®­ï¼ˆE2EåŸ¹è®­ï¼‰ï¼› 2ï¼‰ä½¿ç”¨ç›´æ¥ä»å›¾åƒä¸­å­¦ä¹ çš„å·ç§¯ç‰¹å¾ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ‰‹å·¥çš„å·ç§¯ç‰¹å¾ï¼ˆConv Ftrsï¼‰ï¼› 3ï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸éœ€è¦è§’è‰²çš„åœ°é¢çœŸç›¸è¾¹ç•Œæ¡†ï¼ˆæ— CharGTï¼‰ï¼› 4ï¼‰ä¸é™äºé¢„å®šä¹‰çš„å­—å…¸ï¼ˆæ— çº¦æŸï¼‰ï¼› 5ï¼‰æ¨¡å‹å¤§å°ï¼ˆå¦‚æœä½¿ç”¨äº†ç«¯åˆ°ç«¯å¯è®­ç»ƒæ¨¡å‹ï¼‰ï¼Œç”±æ¨¡å‹å‚æ•°çš„æ•°é‡ï¼ˆæ¨¡å‹å¤§å°ï¼ŒMä»£è¡¨ç™¾ä¸‡ï¼‰è¡¡é‡ã€‚</p><p><br></p><p>E2E Train: This column is to show whether a certain text reading model is end-to-end trainable, without any preprocess or through several separated steps, which indicates such approaches are elegant and clean for training. As can be observed from Table 3, only the models based on deep neural networks including [22, 21] as well as CRNN have this property.</p><p>ç«¯åˆ°ç«¯åŸ¹è®­ï¼šæ­¤åˆ—ç”¨äºæ˜¾ç¤ºæŸç§æ–‡æœ¬é˜…è¯»æ¨¡å‹æ˜¯å¦å¯ä»¥è¿›è¡Œç«¯åˆ°ç«¯çš„åŸ¹è®­ï¼Œè€Œæ— éœ€ä»»ä½•é¢„å¤„ç†æˆ–é€šè¿‡å‡ ä¸ªå•ç‹¬çš„æ­¥éª¤ï¼Œè¿™è¡¨æ˜æ­¤ç±»æ–¹æ³•å¯¹äºåŸ¹è®­è€Œè¨€æ˜¯ä¼˜é›…è€Œå¹²å‡€çš„ã€‚ ä»è¡¨3ä¸­å¯ä»¥çœ‹å‡ºï¼Œåªæœ‰åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ¨¡å‹ï¼ˆåŒ…æ‹¬[22ã€21]å’ŒCRNNï¼‰æ‰å…·æœ‰æ­¤å±æ€§ã€‚</p><p><br></p><p>Conv Ftrs: This column is to indicate whether an approach uses the convolutional features learned from training images directly or handcraft features as the basic representations.</p><p>Conv Ftrsï¼šæ­¤åˆ—æŒ‡ç¤ºæ–¹æ³•æ˜¯ç›´æ¥ä½¿ç”¨ä»è®­ç»ƒå›¾åƒä¸­å­¦åˆ°çš„å·ç§¯ç‰¹å¾è¿˜æ˜¯æ‰‹å·¥ç‰¹å¾ä½œä¸ºåŸºæœ¬è¡¨ç¤ºã€‚</p><p><br></p><p>CharGT-Free: This column is to indicate whether the character-level annotations are essential for training the model. As the input and output labels of CRNN can be a sequence, character-level annotations are not necessary.</p><p>CharGT-Freeï¼šæ­¤åˆ—ç”¨äºæŒ‡ç¤ºå­—ç¬¦çº§æ³¨é‡Šå¯¹äºè®­ç»ƒæ¨¡å‹æ˜¯å¦å¿…ä¸å¯å°‘ã€‚ ç”±äºCRNNçš„è¾“å…¥å’Œè¾“å‡ºæ ‡ç­¾å¯ä»¥æ˜¯ä¸€ä¸ªåºåˆ—ï¼Œå› æ­¤ä¸éœ€è¦å­—ç¬¦çº§æ³¨é‡Šã€‚</p><p><br></p><p>Unconstrained: This column is to indicate whether the trained model is constrained to a specific dictionary, unable to handling out-of-dictionary words or random sequences.Notice that though the recent models learned by label embedding [5, 14] and incremental learning [22] achieved highly competitive performance, they are constrained to a specific dictionary.</p><p>Unconstrainedï¼šæ­¤åˆ—ç”¨äºæŒ‡ç¤ºè®­ç»ƒåçš„æ¨¡å‹æ˜¯å¦ä»…é™äºç‰¹å®šè¯å…¸ï¼Œæ— æ³•å¤„ç†å­—å…¸å¤–å•è¯æˆ–éšæœºåºåˆ—ã€‚è¯·æ³¨æ„ï¼Œå°½ç®¡æœ€è¿‘çš„æ¨¡å‹æ˜¯é€šè¿‡æ ‡ç­¾åµŒå…¥[5ï¼Œ14]å’Œå¢é‡å­¦ä¹ [ 22]å–å¾—äº†æå¥½çš„ç«äº‰è¡¨ç°ï¼Œå®ƒä»¬è¢«é™åˆ¶åœ¨ç‰¹å®šçš„è¯å…¸ä¸­ã€‚</p><p><br></p><div class=pgc-img><img alt=ç¿»è¯‘ï¼šç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œå›¾åƒåºåˆ—è¯†åˆ«åŠå…¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„åº”ç”¨ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/20bd7fabd4fb47b5a90104b8c138e65e><p class=pgc-img-caption></p></div><p>Table 2. Recognition accuracies (%) on four datasets. In the second row, "50", "1k", "50k" and "Full" denote the lexicon used, and "None" denotes recognition without a lexicon. (*[22] is not lexicon-free in the strict sense, as its outputs are constrained to a 90k dictionary.</p><p>è¡¨2.å››ä¸ªæ•°æ®é›†çš„è¯†åˆ«å‡†ç¡®ç‡ï¼ˆï¼…ï¼‰ã€‚ åœ¨ç¬¬äºŒè¡Œä¸­ï¼Œ" 50"ï¼Œ" 1k"ï¼Œ" 50k"å’Œ"å®Œæ•´"è¡¨ç¤ºä½¿ç”¨çš„è¯å…¸ï¼Œ"æ— "è¡¨ç¤ºä¸ä½¿ç”¨è¯å…¸çš„è¯†åˆ«ã€‚ ï¼ˆ* [22]åœ¨ä¸¥æ ¼æ„ä¹‰ä¸Šä¸æ˜¯æ²¡æœ‰è¯å…¸çš„ï¼Œå› ä¸ºå®ƒçš„è¾“å‡ºè¢«é™åˆ¶åœ¨ä¸€ä¸ª90kçš„å­—å…¸ä¸­ã€‚</p><p><br></p><p>Model Size: This column is to report the storage space of the learned model. In CRNN, all layers have weightsharing connections, and the fully-connected layers are not needed. Consequently, the number of parameters of CRNN is much less than the models learned on the variants of CNN [22, 21], resulting in a much smaller model compared with [22, 21]. Our model has 8.3 million parameters, taking only 33MB RAM (using 4-bytes single-precision float for each parameter), thus it can be easily ported to mobile devices.</p><p>æ¨¡å‹å¤§å°ï¼šæ­¤åˆ—ç”¨äºæŠ¥å‘Šå­¦ä¹ çš„æ¨¡å‹çš„å­˜å‚¨ç©ºé—´ã€‚ åœ¨CRNNä¸­ï¼Œæ‰€æœ‰å±‚éƒ½å…·æœ‰æƒé‡å…±äº«è¿æ¥ï¼Œå¹¶ä¸”ä¸éœ€è¦å®Œå…¨è¿æ¥çš„å±‚ã€‚ å› æ­¤ï¼ŒCRNNçš„å‚æ•°æ•°é‡è¿œå°‘äºä»CNNçš„å˜ä½“ä¸­å­¦ä¹ çš„æ¨¡å‹[22ï¼Œ21]ï¼Œå› æ­¤ä¸[22ï¼Œ21]ç›¸æ¯”ï¼Œæ¨¡å‹è¦å°å¾—å¤šã€‚ æˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰830ä¸‡ä¸ªå‚æ•°ï¼Œä»…å ç”¨33MB RAMï¼ˆæ¯ä¸ªå‚æ•°ä½¿ç”¨4å­—èŠ‚å•ç²¾åº¦æµ®ç‚¹æ•°ï¼‰ï¼Œå› æ­¤å¯ä»¥è½»æ¾åœ°å°†å…¶ç§»æ¤åˆ°ç§»åŠ¨è®¾å¤‡ä¸Šã€‚</p><p>Table 3 clearly shows the differences among different approaches in details, and fully demonstrates the advantages of CRNN over other competing methods. In addition, to test the impact of parameter Î´, we experiment different values of Î´ in Eq. 2. In Fig. 4 we plot the recognition accuracy as a function of Î´. Larger Î´ results in more candidates, thus more accurate lexicon-based transcription. On the other hand, the computational cost grows with larger Î´, due to longer BK-tree search time, as well as larger number of candidate sequences for testing. In practice, we choose Î´ = 3 as a tradeoff between accuracy and speed.</p><p>è¡¨3æ¸…æ¥šåœ°è¯¦ç»†æ˜¾ç¤ºäº†ä¸åŒæ–¹æ³•ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶å……åˆ†è¯æ˜äº†CRNNç›¸å¯¹äºå…¶ä»–ç«äº‰æ–¹æ³•çš„ä¼˜åŠ¿ã€‚ å¦å¤–ï¼Œä¸ºäº†æµ‹è¯•å‚æ•°Î´çš„å½±å“ï¼Œæˆ‘ä»¬åœ¨å¼ä¸­è¯•éªŒäº†ä¸åŒçš„Î´å€¼ã€‚ 2.åœ¨å›¾4ä¸­ï¼Œæˆ‘ä»¬å°†è¯†åˆ«ç²¾åº¦ç»˜åˆ¶ä¸ºÎ´çš„å‡½æ•°ã€‚ Î´è¶Šå¤§ï¼Œå€™é€‰è€…è¶Šå¤šï¼Œå› æ­¤åŸºäºè¯å…¸çš„è½¬å½•æ›´åŠ å‡†ç¡®ã€‚ å¦ä¸€æ–¹é¢ï¼Œç”±äºè¾ƒé•¿çš„BKæ ‘æœç´¢æ—¶é—´ä»¥åŠç”¨äºæµ‹è¯•çš„å€™é€‰åºåˆ—æ•°é‡å¢åŠ ï¼Œè®¡ç®—æˆæœ¬éšç€Î´çš„å¢åŠ è€Œå¢é•¿ã€‚ å®é™…ä¸Šï¼Œæˆ‘ä»¬é€‰æ‹©Î´= 3ä½œä¸ºç²¾åº¦å’Œé€Ÿåº¦ä¹‹é—´çš„æŠ˜è¡·ã€‚</p><div class=pgc-img><img alt=ç¿»è¯‘ï¼šç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œå›¾åƒåºåˆ—è¯†åˆ«åŠå…¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„åº”ç”¨ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ca0ffe7e9f654690bf5b57352d01a378><p class=pgc-img-caption></p></div><p>Figure 4. Blue line graph: recognition accuracy as a function parameter Î´. Red bars: lexicon search time per sample. Tested on the IC03 dataset with the 50k lexicon.</p><p>å›¾4.è“çº¿å›¾ï¼šè¯†åˆ«ç²¾åº¦ä½œä¸ºå‡½æ•°å‚æ•°Î´ã€‚ çº¢æ¡ï¼šæ¯ä¸ªæ ·æœ¬çš„è¯å…¸æœç´¢æ—¶é—´ã€‚ ä½¿ç”¨50kè¯å…¸åœ¨IC03æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚</p><p><br></p><p>3.4. Musical Score Recognition</p><p>A musical score typically consists of sequences of musical notes arranged on staff lines. Recognizing musical scores in images is known as the Optical Music Recognition (OMR) problem. Previous methods often requires image preprocessing (mostly binirization), staff lines detection and individual notes recognition [29]. We cast the OMR as a sequence recognition problem, and predict a sequence of musical notes directly from the image with CRNN. For simplicity, we recognize pitches only, ignore all chords and assume the same major scales (C major) for all scores.</p><p>ä¹è°±é€šå¸¸ç”±æ’åˆ—åœ¨è°±çº¿ä¸Šçš„éŸ³ç¬¦åºåˆ—ç»„æˆã€‚ è¯†åˆ«å›¾åƒä¸­çš„ä¹è°±è¢«ç§°ä¸ºå…‰å­¦éŸ³ä¹è¯†åˆ«ï¼ˆOMRï¼‰é—®é¢˜ã€‚ ä»¥å‰çš„æ–¹æ³•é€šå¸¸éœ€è¦å›¾åƒé¢„å¤„ç†ï¼ˆä¸»è¦æ˜¯äºŒå€¼åŒ–ï¼‰ï¼Œäººå‘˜çº¿æ£€æµ‹å’Œä¸ªäººç¬”è®°è¯†åˆ«[29]ã€‚ æˆ‘ä»¬å°†OMRè§†ä¸ºåºåˆ—è¯†åˆ«é—®é¢˜ï¼Œå¹¶ä½¿ç”¨CRNNç›´æ¥ä»å›¾åƒä¸­é¢„æµ‹éŸ³ç¬¦åºåˆ—ã€‚ ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä»…è¯†åˆ«éŸ³é«˜ï¼Œå¿½ç•¥æ‰€æœ‰å’Œå¼¦ï¼Œå¹¶ä¸ºæ‰€æœ‰ä¹è°±é‡‡ç”¨ç›¸åŒçš„å¤§éŸ³é˜¶ï¼ˆCå¤§è°ƒï¼‰ã€‚</p><p><br></p><p>To the best of our knowledge, there exists no public datasets for evaluating algorithms on pitch recognition. To prepare the training data needed by CRNN, we collect 2650 images from [2]. Each image contains a fragment of score containing 3 to 20 notes. We manually label the ground truth label sequences (sequences of not ezpitches) for all the images. The collected images are augmented to 265k training samples by being rotated, scaled and corrupted with noise, and by replacing their backgrounds with natural images. For testing, we create three datasets: 1) "Clean", which contains 260 images collected from [2]. Examples are shown in Fig. 5.a; 2) "Synthesized", which is created from "Clean", using the augmentation strategy mentioned above. It contains 200 samples, some of which are shown in Fig. 5.b; 3) "Real-World", which contains 200 images of score fragments taken from music books with a phone camera. Examples are shown in Fig. 5.c.1</p><p>æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œç›®å‰å°šæ— ç”¨äºè¯„ä¼°éŸ³é«˜è¯†åˆ«ç®—æ³•çš„å…¬å…±æ•°æ®é›†ã€‚ ä¸ºäº†å‡†å¤‡CRNNæ‰€éœ€çš„è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬ä»[2]ä¸­æ”¶é›†äº†2650å¼ å›¾åƒã€‚ æ¯ä¸ªå›¾åƒåŒ…å«ä¸€ä¸ªåˆ†æ•°ç‰‡æ®µï¼Œå…¶ä¸­åŒ…å«3è‡³20ä¸ªéŸ³ç¬¦ã€‚ æˆ‘ä»¬ä¸ºæ‰€æœ‰å›¾åƒæ‰‹åŠ¨æ ‡è®°åœ°é¢çœŸç›¸æ ‡è®°åºåˆ—ï¼ˆéezpitchesåºåˆ—ï¼‰ã€‚ é€šè¿‡æ—‹è½¬ï¼Œç¼©æ”¾å’Œå—å™ªå£°ç ´åï¼Œä»¥åŠé€šè¿‡å°†å…¶èƒŒæ™¯æ›¿æ¢ä¸ºè‡ªç„¶å›¾åƒï¼Œå¯ä»¥å°†æ”¶é›†çš„å›¾åƒå¢å¼ºåˆ°265kè®­ç»ƒæ ·æœ¬ã€‚ ä¸ºäº†è¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸‰ä¸ªæ•°æ®é›†ï¼š1ï¼‰" Clean"ï¼Œå…¶ä¸­åŒ…å«ä»[2]ä¸­æ”¶é›†çš„260å¼ å›¾åƒã€‚ ç¤ºä¾‹å¦‚å›¾5.aæ‰€ç¤ºã€‚ 2ï¼‰ä½¿ç”¨ä¸Šé¢æåˆ°çš„æ‰©å……ç­–ç•¥ï¼Œä»"æ¸…æ´"åˆ›å»ºçš„"åˆæˆ"ã€‚ å®ƒåŒ…å«200ä¸ªæ ·æœ¬ï¼Œå…¶ä¸­ä¸€äº›å¦‚å›¾5.bæ‰€ç¤ºã€‚ 3ï¼‰"çœŸå®ä¸–ç•Œ"ï¼Œå…¶ä¸­åŒ…å«200å¼ ä½¿ç”¨æ‰‹æœºæ‘„åƒå¤´ä»ä¹è°±ä¸­æ‹æ‘„çš„ä¹è°±ç‰‡æ®µå›¾åƒã€‚ ç¤ºä¾‹å¦‚å›¾5.c.1æ‰€ç¤ºã€‚</p><p><br></p><div class=pgc-img><img alt=ç¿»è¯‘ï¼šç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œå›¾åƒåºåˆ—è¯†åˆ«åŠå…¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„åº”ç”¨ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/86492826ec984004ad678bb44920fab4><p class=pgc-img-caption></p></div><p>Figure 5. (a) Clean musical scores images collected from [2] (b) Synthesized musical score images. (c) Real-world score images taken with a mobile phone camera.</p><p>å›¾5.ï¼ˆaï¼‰ä»[2]æ”¶é›†çš„å¹²å‡€çš„ä¹è°±å›¾åƒã€‚ï¼ˆbï¼‰åˆæˆçš„ä¹è°±å›¾åƒã€‚ ï¼ˆcï¼‰ç”¨æ‰‹æœºç›¸æœºæ‹æ‘„çš„çœŸå®åˆ†æ•°å›¾åƒã€‚</p><p><br></p><p>Since we have limited training data, we use a simplified CRNN configuration in order to reduce model capacity. Different from the configuration specified in Tab. 1, the 4th and 6th convolution layers are removed, and the 2-layer bidirectional LSTM is replaced by a 2-layer single directional LSTM. The network is trained on the pairs of images and corresponding label sequences. Two measures are used for evaluating the recognition performance: 1) fragment accuracy, i.e. the percentage of score fragments correctly recognized; 2) average edit distance, i.e. the average edit distance between predicted pitch sequences and the ground truths. For comparison, we evaluate two commercial OMR engines, namely the Capella Scan [3] and the PhotoScore [4].</p><p>ç”±äºè®­ç»ƒæ•°æ®æœ‰é™ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨ç®€åŒ–çš„CRNNé…ç½®ä»¥å‡å°‘æ¨¡å‹å®¹é‡ã€‚ ä¸é€‰é¡¹å¡ä¸­æŒ‡å®šçš„é…ç½®ä¸åŒã€‚ å¦‚å›¾1æ‰€ç¤ºï¼Œåˆ é™¤äº†ç¬¬4å’Œç¬¬6å·ç§¯å±‚ï¼Œå¹¶å°†2å±‚åŒå‘LSTMæ›¿æ¢ä¸º2å±‚å•å‘LSTMã€‚ åœ¨å›¾åƒå¯¹å’Œç›¸åº”çš„æ ‡ç­¾åºåˆ—å¯¹ä¸Šè®­ç»ƒç½‘ç»œã€‚ ä¸¤ç§æ–¹æ³•å¯ç”¨äºè¯„ä¼°è¯†åˆ«æ€§èƒ½ï¼š1ï¼‰ç‰‡æ®µå‡†ç¡®æ€§ï¼Œå³æ­£ç¡®è¯†åˆ«çš„å¾—åˆ†ç‰‡æ®µçš„ç™¾åˆ†æ¯”ï¼› 2ï¼‰å¹³å‡ç¼–è¾‘è·ç¦»ï¼Œå³é¢„æµ‹éŸ³é«˜åºåˆ—ä¸åŸºæœ¬äº‹å®ä¹‹é—´çš„å¹³å‡ç¼–è¾‘è·ç¦»ã€‚ ä¸ºäº†è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸¤ç§å•†ç”¨OMRå¼•æ“ï¼Œå³Capella Scan [3]å’ŒPhotoScore [4]ã€‚</p><p><br></p><div class=pgc-img><img alt=ç¿»è¯‘ï¼šç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œå›¾åƒåºåˆ—è¯†åˆ«åŠå…¶åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„åº”ç”¨ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/dd70375a8fe64987bcc78863f98a76c7><p class=pgc-img-caption></p></div><p>Table 4. Comparison of pitch recognition accuracies, among CRNN and two commercial OMR systems, on the three datasets we have collected. Performances are evaluated by fragment accuracies and average edit distance ("fragment accuracy/average edit distance").</p><p>è¡¨4.åœ¨æˆ‘ä»¬æ”¶é›†çš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šï¼ŒCRNNå’Œä¸¤ä¸ªå•†ä¸šOMRç³»ç»Ÿä¹‹é—´çš„éŸ³é«˜è¯†åˆ«ç²¾åº¦æ¯”è¾ƒã€‚ é€šè¿‡ç‰‡æ®µç²¾åº¦å’Œå¹³å‡ç¼–è¾‘è·ç¦»ï¼ˆ"ç‰‡æ®µå‡†ç¡®æ€§/å¹³å‡ç¼–è¾‘è·ç¦»"ï¼‰è¯„ä¼°æ¼”å¥ã€‚</p><p><br></p><p>Tab.4 summarizes the results. The CRNN outperforms the two commercial systems by a large margin. The Capella Scan and PhotoScore systems perform reasonably well on the Clean dataset, but their performances drop significantly on synthesized and real-world data. The main reason is that they rely on robust binarization to detect staff lines and notes, but the binarization step often fails on synthesized and real-world data due to bad lighting condition, noise corruption and cluttered background. The CRNN, on the other hand, uses convolutional features that are highly robust to noises and distortions. Besides, recurrent layers in CRNN can utilize contextual information in the score. Each note is recognized not only itself, but also by the nearby notes. Consequently, some notes can be recognized by comparing them with the nearby notes, e.g. contrasting their vertical positions.</p><p>è¡¨4æ€»ç»“äº†ç»“æœã€‚ CRNNå¤§å¤§ä¼˜äºä¸¤ä¸ªå•†ä¸šç³»ç»Ÿã€‚ Capella Scanå’ŒPhotoScoreç³»ç»Ÿåœ¨Cleanæ•°æ®é›†ä¸Šçš„è¡¨ç°ç›¸å½“ä¸é”™ï¼Œä½†åœ¨åˆæˆå’ŒçœŸå®æ•°æ®ä¸Šçš„æ€§èƒ½å´å¤§å¤§ä¸‹é™ã€‚ ä¸»è¦åŸå› æ˜¯ä»–ä»¬ä¾é å¯é çš„äºŒå€¼åŒ–æ¥æ£€æµ‹äººå‘˜çº¿å’Œä¾¿æ¡ï¼Œä½†æ˜¯ç”±äºä¸è‰¯çš„å…‰ç…§æ¡ä»¶ï¼Œå™ªå£°ç ´åå’ŒèƒŒæ™¯æ··ä¹±ï¼ŒäºŒå€¼åŒ–æ­¥éª¤é€šå¸¸æ— æ³•åœ¨åˆæˆçš„å’ŒçœŸå®çš„æ•°æ®ä¸Šè¿›è¡Œã€‚ å¦ä¸€æ–¹é¢ï¼ŒCRNNä½¿ç”¨å¯¹å™ªå£°å’Œå¤±çœŸå…·æœ‰é«˜åº¦é²æ£’æ€§çš„å·ç§¯ç‰¹å¾ã€‚ æ­¤å¤–ï¼ŒCRNNä¸­çš„å¾ªç¯å±‚å¯ä»¥åˆ©ç”¨åˆ†æ•°ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ æ¯ä¸ªéŸ³ç¬¦ä¸ä»…å¯ä»¥è‡ªå·±è¯†åˆ«ï¼Œè¿˜å¯ä»¥è¢«é™„è¿‘çš„éŸ³ç¬¦è¯†åˆ«ã€‚ å› æ­¤ï¼Œå¯ä»¥é€šè¿‡å°†å®ƒä»¬ä¸é™„è¿‘çš„éŸ³ç¬¦è¿›è¡Œæ¯”è¾ƒæ¥è¯†åˆ«æŸäº›éŸ³ç¬¦ï¼Œä¾‹å¦‚ å¯¹æ¯”ä»–ä»¬çš„å‚ç›´ä½ç½®ã€‚</p><p><br></p><p>The results have shown the generality of CRNN, in that it can be readily applied to other image-based sequence recognition problems, requiring minimal domain knowledge. Compared with Capella Scan and PhotoScore, our CRNN-based system is still preliminary and misses many functionalities. But it provides a new scheme for OMR, and has shown promising capabilities in pitch recognition.</p><p>ç»“æœæ˜¾ç¤ºäº†CRNNçš„æ™®éæ€§ï¼Œå› ä¸ºå®ƒå¯ä»¥è½»æ¾åº”ç”¨äºå…¶ä»–åŸºäºå›¾åƒçš„åºåˆ—è¯†åˆ«é—®é¢˜ï¼Œè€Œæ‰€éœ€çš„é¢†åŸŸçŸ¥è¯†æœ€å°‘ã€‚ ä¸Capella Scanå’ŒPhotoScoreç›¸æ¯”ï¼Œæˆ‘ä»¬åŸºäºCRNNçš„ç³»ç»Ÿä»æ˜¯åˆæ­¥çš„ï¼Œç¼ºå°‘è®¸å¤šåŠŸèƒ½ã€‚ ä½†æ˜¯ï¼Œå®ƒä¸ºOMRæä¾›äº†ä¸€ç§æ–°æ–¹æ¡ˆï¼Œå¹¶ä¸”åœ¨éŸ³é«˜è¯†åˆ«æ–¹é¢æ˜¾ç¤ºå‡ºäº†ä»¤äººé¼“èˆçš„åŠŸèƒ½ã€‚</p><p><br></p><p>4. Conclusion</p><p>In this paper, we have presented a novel neural network architecture, called Convolutional Recurrent Neural Network (CRNN), which integrates the advantages of both Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). CRNN is able to take input images of varying dimensions and produces predictions with different lengths. It directly runs on coarse level labels (e.g. words), requiring no detailed annotations for each individual element (e.g. characters) in the training phase. Moreover, as CRNN abandons fully connected layers used in conventional neural networks, it results in a much more compact and efficient model. All these properties make CRNN an excellent approach for image-based sequence recognition.</p><p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç§°ä¸ºå·ç§¯é€’å½’ç¥ç»ç½‘ç»œï¼ˆCRNNï¼‰ï¼Œå®ƒèåˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œé€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„ä¼˜ç‚¹ã€‚ CRNNèƒ½å¤Ÿæ‹æ‘„ä¸åŒå°ºå¯¸çš„è¾“å…¥å›¾åƒï¼Œå¹¶äº§ç”Ÿä¸åŒé•¿åº¦çš„é¢„æµ‹ã€‚ å®ƒç›´æ¥åœ¨ç²—ç³™çº§åˆ«çš„æ ‡ç­¾ï¼ˆä¾‹å¦‚å•è¯ï¼‰ä¸Šè¿è¡Œï¼Œåœ¨è®­ç»ƒé˜¶æ®µæ— éœ€ä¸ºæ¯ä¸ªå•ç‹¬çš„å…ƒç´ ï¼ˆä¾‹å¦‚å­—ç¬¦ï¼‰æä¾›è¯¦ç»†çš„æ³¨é‡Šã€‚ æ­¤å¤–ï¼Œç”±äºCRNNæ”¾å¼ƒäº†å¸¸è§„ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨çš„å®Œå…¨è¿æ¥çš„å±‚ï¼Œå› æ­¤å®ƒå¯¼è‡´äº†æ›´åŠ ç´§å‡‘å’Œæœ‰æ•ˆçš„æ¨¡å‹ã€‚ æ‰€æœ‰è¿™äº›ç‰¹æ€§ä½¿CRNNæˆä¸ºåŸºäºå›¾åƒçš„åºåˆ—è¯†åˆ«çš„ç»ä½³æ–¹æ³•ã€‚</p><p><br></p><p>The experiments on the scene text recognition benchmarks demonstrate that CRNN achieves superior or highly competitive performance, compared with conventional methods as well as other CNN and RNN based algorithms. This confirms the advantages of the proposed algorithm. In addition, CRNN significantly outperforms other competitors on a benchmark for Optical Music Recognition (OMR), which verifies the generality of CRNN.</p><p>ä¸ä¼ ç»Ÿæ–¹æ³•ä»¥åŠå…¶ä»–åŸºäºCNNå’ŒRNNçš„ç®—æ³•ç›¸æ¯”ï¼Œç°åœºæ–‡æœ¬è¯†åˆ«åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜CRNNå…·æœ‰ä¼˜å¼‚æˆ–æå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ è¿™è¯å®äº†æ‰€æå‡ºç®—æ³•çš„ä¼˜ç‚¹ã€‚ æ­¤å¤–ï¼ŒCRNNåœ¨å…‰å­¦éŸ³ä¹è¯†åˆ«ï¼ˆOMRï¼‰çš„åŸºå‡†ä¸Šæ˜æ˜¾ä¼˜äºå…¶ä»–ç«äº‰å¯¹æ‰‹ï¼Œè¿™è¯æ˜äº†CRNNçš„æ™®éæ€§ã€‚</p><p><br></p><p>Actually, CRNN is a general framework, thus it can be applied to other domains and problems (such as Chinese character recognition), which involve sequence prediction in images. To further speed up CRNN and make it more practical in real-world applications is another direction that is worthy of exploration in the future.</p><p>å®é™…ä¸Šï¼ŒCRNNæ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå› æ­¤å¯ä»¥åº”ç”¨äºæ¶‰åŠå›¾åƒåºåˆ—é¢„æµ‹çš„å…¶ä»–é¢†åŸŸå’Œé—®é¢˜ï¼ˆä¾‹å¦‚æ±‰å­—è¯†åˆ«ï¼‰ã€‚ è¿›ä¸€æ­¥åŠ å¿«CRNNçš„é€Ÿåº¦ï¼Œä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ å®ç”¨æ˜¯å¦ä¸€ä¸ªå€¼å¾—æœªæ¥æ¢ç´¢çš„æ–¹å‘ã€‚</p><p>åŸæ–‡ï¼š An End-to-End Trainable Neural Network for Image-based Sequence Its Application to Scene Text Recognition ï¼ˆarXiV 1507.05717ï¼‰</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'è¯†åˆ«','ç¿»è¯‘','ç«¯åˆ°'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=æœç´¢>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>ğŸ”</button></form></section><section class=widget><h3 class=widget-title>æœ€æ–°æ–‡ç«  âš¡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>å…¶ä»–</h3><ul class=widget-list><li><a href=TOS.html>ä½¿ç”¨æ¢æ¬¾</a></li><li><a href=CommentPolicy.html>ç•™è¨€æ”¿ç­–</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>è¯çµ¡æˆ‘å€‘</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>æå®¢å¿«è¨Š</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>