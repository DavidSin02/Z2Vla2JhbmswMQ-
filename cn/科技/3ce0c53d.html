<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>TensorFlow系列专题（十二）：CNN最全原理剖析（序） | 极客快訊</title><meta property="og:title" content="TensorFlow系列专题（十二）：CNN最全原理剖析（序） - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/27859481f604430bb47e6886f1904e69"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3ce0c53d.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3ce0c53d.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/3ce0c53d.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3ce0c53d.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3ce0c53d.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/3ce0c53d.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/3ce0c53d.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3ce0c53d.html><meta property="article:published_time" content="2020-10-29T21:12:41+08:00"><meta property="article:modified_time" content="2020-10-29T21:12:41+08:00"><meta name=Keywords content><meta name=description content="TensorFlow系列专题（十二）：CNN最全原理剖析（序）"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/3ce0c53d.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>TensorFlow系列专题（十二）：CNN最全原理剖析（序）</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><h1><strong>目录</strong></h1><ul><li><strong>前言</strong></li><li><strong>卷积层（余下部分）</strong></li><li class=ql-indent-1><strong>卷积的基本结构</strong></li><li><strong>卷积层</strong></li><li class=ql-indent-1><strong>什么是卷积</strong></li><li class=ql-indent-1><strong>滑动步长和零填充</strong></li><li><strong>池化层</strong></li><li><strong>卷积神经网络的基本结构</strong></li><li><strong>总结</strong></li><li><strong>参考文献</strong></li></ul><h1><strong>一、前言</strong></h1><p>上一篇我们一直说到了CNN [1]卷积层的特性，今天会继续讲解卷积层的基本结构</p><h1><strong>二、卷积层（余下部分）</strong></h1><p><strong>1. 卷积的基本结构</strong></p><p>如图1所示，假设输入到神经网络中的是一张大小为256 * 256的图像，第一层隐藏层的神经元个数为241 * 241。在只考虑单通道的情况下，全连接神经网络输入层到第一层隐藏层的连接数为(256 * 256) * (241 * 241)，也就是说输入层到第一层隐藏层有(256 * 256) * (241 * 241)+1个参数（1为偏置项参数个数）。而在卷积神经网络中，假设我们使用了一个大小为16 * 16的卷积核，则输入层到第一层隐藏层的连接数为(256 * 256) * (16 * 16)，由于我们的卷积核是共享的，因此参数个数仅为 (16 * 16) + 1个。有时候为了提取图像中不同的特征，我们可能会使用多个卷积核，假设这里我们使用了100个大小为16 * 16的卷积核，则输入层到第一层隐藏层的参数个数也仅为100 * (16 * 16) + 1，这依然远远少于全连接神经网络的参数个数。</p><p>根据图4-11所示的例子我们可以看到卷积神经网络的两个重要特性：</p><p><strong>局部连接</strong>：全连接神经网络中，第层的每一个神经元和第层的每一个神经元之间都有连接。而在卷积神经网络中，第层的每一个神经元都只和第层的部分神经元之间有连接，而这个"部分"有多大，则具体取决于卷积核的大小。</p><p><strong>权值共享</strong>：在卷积神经网络中，同一隐藏层的每一个神经元所使用的卷积核都是相同的，卷积核对同一隐藏层的神经元来说是共享的。</p><div class=pgc-img><img alt=TensorFlow系列专题（十二）：CNN最全原理剖析（序） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/27859481f604430bb47e6886f1904e69><p class=pgc-img-caption>图1全连接神经网络（左）和卷积神经网络（右）连接数示例</p></div><p>在卷积层中，特征图（feature map，又称为特征映射）是输入层的图像（或其它的特征图）经过卷积之后得到的特征输出。一个卷积核只负责提取某一类特定的特征，为了充分的提取出图像中的信息，我们通常会使用多个卷积核。卷积层的一般性结构可以表示如下：</p><p><strong>1. 输入特征映射组</strong>：输入特征映射组 X(X∈R^(M×N×D)) 是一个三维的张量（tensor），其中每个切片（slice）矩阵 X^d (X^d∈R^(M×N),1≤d≤D)是一个输入特征映射。每个特征映射的大小为M×N，D是输入特征映射的个数。</p><p><strong>2. 输出特征映射组</strong>：输出特征映射组Y(Y∈R^(M^'×N^'×P))也是一个三维张量，其中每个切片矩阵 Y^p (Y^p∈R^(M^'×N^' ),1≤p≤P)是一个输出特征映射。每个特征映射的大小为M^'×N^'，P是输出特征映射的个数。</p><p><strong>3. 卷积核</strong>：卷积核W(W∈R^(m×n×D×P))是一个四维张量，其中每个切片矩阵 W^(p,d) (W^(p,d)∈R^(m×n),1≤p≤P,1≤d≤D)是一个二维的卷积核。</p><p>为了更直观的理解，我们看如图2所示的例子。示例中的输入特征映射组有两个特征映射，每个特征映射的大小为5*5，对应有M=5,N=5,D=2。输出特征映射组有三个特征映射，每个特征映射的大小为3*3，对应有M=3,N=3,P=3。卷积核的维度是3*3*2*3，每个二维卷积核的大小为3*3，对应有m=3,n=3,D=2,P=3。</p><div class=pgc-img><img alt=TensorFlow系列专题（十二）：CNN最全原理剖析（序） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/683f6011f5754376830e80e16228cfdb><p class=pgc-img-caption>图2一个卷积层的结构示例</p></div><p>图3所示是卷积层中从输入特征映射组到输出特征映射组的计算过程示例。卷积核W^(p,1),W^(p,2),⋯,W^(p,D)分别对输入的特征映射X^1,X^2,⋯,X^D进行卷积，然后将卷积得到的结果相加，再加上一个偏置b^p后得到卷积层的净输入Z^p，如式1。最后经过一个非线性激活函数后得到输出特征映射Y^p，如式2，其中函数f(∙)为非线性激活函数。</p><div class=pgc-img><img alt=TensorFlow系列专题（十二）：CNN最全原理剖析（序） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a3890d3045e64d64864bad2a2e6ae616><p class=pgc-img-caption></p></div><p>在图3所示的例子中，每一个输入特征映射都需要个卷积核和一个偏置。假设每个二维卷积核的大小为m×n，那么该层卷积层共需要的参数个数为：(m×n)×P×D+P。</p><div class=pgc-img><img alt=TensorFlow系列专题（十二）：CNN最全原理剖析（序） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/deb686b66b284d9a8293cbcb27f9714d><p class=pgc-img-caption>图3卷积层中计算过程示例</p></div><h1><strong>三、池化层</strong></h1><p>池化层（pooling layer）也称为子采样层（subsampling layer），池化层一般都是紧跟在卷积层之后，它的作用是进行特征选择，减少特征的数量，进而减少网络中参数的数量。</p><p>对于一个特征映射，我们可以将其划分为多个区域（这些区域可以有重合部分），池化就是对这些划分后的区域进行下采样（down sampling），然后得到一个值，并将这个值作为该区域的概括。池化层的方式有多种，一般常用的有最大池化（maximum pooling）和平均池化（mean pooling）。</p><p><strong>最大池化（maximum pooling）</strong>：选取区域内的最大值的神经元作为该区域的概括。</p><p><strong>平均池化（mean pooling）</strong>：取区域内所有神经元的均值作为该区域的概括。</p><p>如图4是一个最大池化和均值池化的示例，这里我们将一个特征映射划分为了4个区域，即池化窗口的大小为2×2，步长为2。</p><div class=pgc-img><img alt=TensorFlow系列专题（十二）：CNN最全原理剖析（序） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3b3bfcb085914980ab2e1a9ac73a5efe><p class=pgc-img-caption>图4 最大池化和平均池化示例</p></div><p>目前大多数卷积神经网络中，池化层仅包含下采样操作，池化层没有需要训练的参数。但在一些早期的卷积网络中，会在池化层中使用一个非线性激活函数，例如我们会面会介绍的LeNet-5。现在，池化层的作用已经越来越小，通过增加卷积的步长也可以达到池化层同样的效果。因此在目前一些比较新的卷积神经网络中，池化层出现的频率已经越来越低。</p><h1><strong>四、卷积神经网络的基本结构</strong></h1><p>一个基本的卷积神经网络通常是由卷积层、池化层和全连接层交叉堆叠而成。如图5所示，由连续M个卷积层和h个池化层构成一个卷积块（M的取值一般为1~5，h的取值一般为0或1），一个卷积神经网络中可以堆叠N个连续的卷积块（N的取值可以很大，较深的网络可以达到100多层）。在个连续的卷积块之后是K个连续的全连接层（K一般取1~2）。</p><div class=pgc-img><img alt=TensorFlow系列专题（十二）：CNN最全原理剖析（序） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/13d2aae01b3b487db6fe6f7254a44ab0><p class=pgc-img-caption>图5基本的卷积神经网络结构示意图</p></div><h1><strong>五、总结</strong></h1><p>本节介绍了卷积层的结构及池化层和卷积神经网络的特性，下一节将结合代码介绍基于CNN的一个实际比赛的冰山雷达波图像识别项目。</p><h1><strong>六、参考文献</strong></h1><p>[1] Alex Krizhevsky: ImageNet Classification with Deep Convolutional Neural Networks .NIPS 2012</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'TensorFlow','专题','CNN'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../cn/%E7%A7%91%E6%8A%80/96b22e09.html alt=TensorFlow系列专题（十二）：CNN最全原理剖析 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/0d594a544ebf49b0ae6f6a7915b953af style=border-radius:25px></a>
<a href=../../cn/%E7%A7%91%E6%8A%80/96b22e09.html title=TensorFlow系列专题（十二）：CNN最全原理剖析>TensorFlow系列专题（十二）：CNN最全原理剖析</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>