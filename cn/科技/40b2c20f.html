<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结 | 极客快訊</title><meta property="og:title" content="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/8b4b00004d6898c5eeee"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/40b2c20f.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/40b2c20f.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/40b2c20f.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/40b2c20f.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/40b2c20f.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/40b2c20f.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/40b2c20f.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/40b2c20f.html><meta property="article:published_time" content="2020-10-29T21:10:44+08:00"><meta property="article:modified_time" content="2020-10-29T21:10:44+08:00"><meta name=Keywords content><meta name=description content="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/40b2c20f.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p><strong>雷锋网 AI 科技评论按：</strong>在大数据时代，标注足够多的训练样本往往耗费巨大。弱监督学习方法往往能够减轻对正确标签的过度依赖，达到与监督学习相近的性能。然而，在设计弱监督学习方法时，我们需要理解无标签样本的分布情况（比如 semi-supervised learning），或者带噪声标签样本的噪声大小（比如 learning with label noise），这些问题的本质就是混合比例估计。因此，混合比例估计在弱监督学习中占有至关重要的作用。</p><p>在雷锋网 (公众号：雷锋网) 旗下学术频道 AI 科技评论的数据库项目「AI 影响因子」中，优必选悉尼 AI 研究院凭借4 篇 CVPR 录用论文、8.2亿美元的C轮融资，AI首席科学家陶大程当选澳大利亚科学院院士的不俗表现，排在「AI 影响因子」前列。</p><p>近期，在 GAIR 大讲堂上，优必选悉尼 AI 研究院博士生余席宇分享了他在混合比例估计中新的研究成果，以及其在弱监督学习，迁移学习中的延伸应用。视频回放地址：http://www.mooc.ai/open/course/493</p><p>余席宇，悉尼大学 FEIT 四年级博士生，优必选悉尼 AI 研究院学生。北京航空航天大学控制科学与工程学士，硕士。主要研究方向为矩阵分解，深度网络模型压缩以及弱监督学习。</p><p><strong>分享主题：</strong>混合比例估计（Mixture Proportion Estimation）及其应用</p><p><strong>分享提纲</strong></p><blockquote><ul><li><p>混合比例估计的背景，问题描述以及基本假设。</p></li><li><p>利用最大平均差异的方法快速求解混合比例估计问题，并提供理论保证。</p></li><li><p>混合比例估计应用：辅助领域（source domain）中的样本含有标签噪声时的迁移学习。</p></li></ul></blockquote><p>以下为雷锋网 AI 科技评论整理的分享内容：</p><p>优必选成立于 2012 年，是一家全球领先的人工智能和人形机器人公司，目前已经推出了消费级人形机器人 Alpha 系列，STEM 教育智能编程机器人 Jimu，智能云平台商用服务机器人 Cruzr 等多款产品，并成功入驻全球 Apple Store 零售店。</p><p>此外，优必选还与清华大学成立了智能服务机器人联合实验室，与悉尼大学成立了人工智能研究院，与华中科技大学成立了机器人联合实验室，在人形机器人驱动伺服、步态运动控制算法、机器视觉、语音/语义理解、情感识别、U-SLAM(即时定位与地图构建) 等领域深度布局。2018 年，优必选完成了 C 轮融资，估值 50 亿美元。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/8b4b00004d6898c5eeee><p>今天的展示中，我想感谢对我的工作提供过很多帮助的合作者们。他们分别是刘同亮老师（悉尼大学助理教授），宫明明博士，张坤老师（悉尼大学助理教授），Kayhan Batmanghelich（悉尼大学助理教授）以及陶大程教授。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/865b000b82c42d42492f><p>今天的分享内容主要分为以下四个部分来讲解：</p><ol><li><p>第一部分为混合比例估计（MPE）的定义和此前的研究工作。</p></li><li><p>第二部分介绍我们 CVPR2018 年的工作 。</p></li><li><p>第三部分讲解混合比例估计在 Target Shift 这类问题中的延伸和应用。</p></li><li><p>最后一部分讲解混合比例估计在一般的迁移学习中的延伸和应用。</p></li></ol><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b4a000650c2228083dc><p></p><h3><strong>混合比例估计（MPE）的定义和此前研究</strong></h3><p>开始第一部分。假设我们有一系列用于检测病人是否患有肺炎的 X 光片，在该系列 X 光片中，一部分病人患有肺炎，另一部分病人健康。我们通常对有多大比例的病人患有肺炎比较感兴趣。为了估计该比例，我们需要什么信息，又如何对这个问题建模呢？</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/865b000b82c36f97d2d0><p>一般地，可以假设这一系列的 X 光片从一个混合分布 P0 中采样得出，而拥有肺炎病人的数据和没有肺炎病人的数据分别从两个组成分布 P1 和 P2 中采样得到。此时，P0 就是这两个组成分布的一个线性组合，同时要求这个线性组合的系数（也就是通常所说的混合分布的比例）要满足非负且加和为 1 的条件。我们定于 Si 这个集合对应从分布 Pi 中采样得到的训练样本集合。我们关心的是需要给定什么样的数据，有怎样的假设，才能成功地求取这些比例λi？</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/865b000b82c2da6499cd><p>以前的工作主要研究以下几个 settings：</p><p>第一个 setting，是指假设只从混合分布 P0 中采样了一部分数据 S0 时，如何估计λi。其实没有任何组成分布的信息是无法完成这项工作的，所以往往要对这些分布加非常强的假设。在这种情况下，我们一般假设组成分布满足高斯分布的假设（当然也可以是其他分布假设），这就得到了我们通常所说的混合高斯模型。高斯模型可以通过 EM 算法来求解各个组成分布的均值和方差矩阵，同时也能求出λi。该模型的问题在于：利用 EM 求解混合高斯模型时，不能保证得到唯一解。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/865b000b82cbec4d01d7><p>另外一些传统的混合比例估计的方法，通常研究下面问题：假设混合分布由 M 个组成分布线性组合而成。如果给定从 P0 这个混合分布中采样得到的样本，以及 M-1 个组成分布中采得的一部分样本（第 M 个组成分布中，没有任何样本），如何来估计这个混合分布的比例？</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b4b00004d6b1af1b82e><p>首先假设对这些分布没有任何假设的情况下，其实可以看出这个分布 P0，可以存在任意多的分解，比如把 P1 的一部分组合到 P2 之中，就可以得到一个新的分解。这样，如果只知道 P0 和 P1 的信息，我们是无法求得 P1 和 P2 的比例，因为这个比例可以是任意多的。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b480007478f19e1f5e0><p>因此，需要对 P1 和 P2 这些组合分布进行假设，传统的方法通常有两类假设：</p><p>第一种假设称之为不可约的假设，如果说一个分布 P2 对于 P1 这个分布是不可约的，那么认为 P2 是无法表示成 P1 和另外任意一个分布的线性组合。在这种情况下，如果 P0 是由一个 P1，P2 混合而成，此时可以知道，P1 分布的比例就是 P1 在 P0 之中的最大的那个比例，因为 P2 中没有任何 P1 的信息。这个比例一般可以通过接收者操作特征曲线，也就是通常所说的 ROC 曲线来进行估计。这个方法一般要估计一个概率密度函数。而且通过 No Free Lunch 的方式证明，该方法虽然能够保证收敛到最优比例，但这个收敛可以是任意慢的。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/8b4900064ff4100da8d8><p>既然不可约的假设不能保证收敛速率，后来的研究者又提出了一种新的假设：Anchor set condition。比如拿两个分布来说，两个分布中一个分布的密度函数不为 0，而另外一个分布的密度函数为 0 的自变量的集合，就叫 Anchor set。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/8b4b00004d6df4528360><p>我们可以简单通过上图示例的右边部分来了解，两条黑线所标注的集合就是通常的 Anchor set，在这种情况下，如果拥有 Anchor set 的集合数据，比如有 P0 的数据和 P1 在 Anchor set 中的数据，其实就能通过 Anchor set 的数据来估计出 P1 的比例。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/865c00094b62719e0b7f><p>在 Anchor set codition 的条件下，前面的方法已经证明了对比例的估计能够以一定的速率，收敛到最优的那个比例，但是这些方法都具有一定的局限性。首先它们需要很强的对于组成分布的假设，而这些组成分布的假设往往可能被 challenge，后面将举几个简单的例子来看这个问题。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/8b4b00004d6fd66787f5><p>第二个，比如前面提到的基于 ROC 方法，它们往往要估计概率密度函数，而概率密度函数估计往往需要比较好的模型，而且对高维数据的概率密度的估计往往不是很可靠，而且这种估计也不是很高效。</p><p>第三个，前面那些方法大多都集中只有两个组成分布的条件下，它对于如果组成分布是多个的情况下的延伸不是那么直接，所以需要寻求另外一种新的方式。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/865b000b82cf20cfdc7d><p>首先来介绍 CVPR 研究工作的 setting：假设从所有的分布（包括混合分布和各个组成分布）中都采集了一定的样本的条件下，估计它们的比例。这个问题又被称为 class proportion estimation（CPE），即类比例估计的问题。</p><p>研究这个问题的贡献主要是以下几点：</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b4d00002942f95ba8ec><p>第一，寻求一种对于组成分布限制假设和需要标签数据的一种 Trade-off。虽然我们的 setting 中多用了一个组成分布（第 M 个组成分布）的数据，但是用了更弱的假设来证明了很多有意义的结论。</p><p>第二，在一个比较弱的假设下，证明了这些比例分布的唯一性以及可识别性。</p><p>第三，我们设计了一个非常快速的算法，也证明了该估计的一致收敛性。所谓一致收敛性就是指估计的收敛与采得的训练数据是没有关系的。</p><p>第四个，该方法能够延伸到多类问题，有非常广泛的应用，可以应用到具有噪声标签的学习以及弱监督学习之中。</p><p></p><h3><strong>CVPR 2018 工作</strong></h3><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b4900064ff756ad9ac7><p>首先介绍对于混合比例估计问题的假设，也就是线性独立假设。该线性独立假设是延引线性代数里的线性独立概念，如果 M 个组成分布是线性独立的话，那么不存在一组非 0 的系数使得这些组成分布的线性组合等于 0。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/8b4900064ffa60125332><p>通过这个定义可以得到一个很直接的推论：如果两个分布线性独立，当且仅当这两个分布不等同。从这个推论可以看出，比如前面的 anchor set 需要有两个不一样的非 0 自定义域，但是对于线性独立假设，仅仅要求两个分布不一样就可以了。</p><p>可以证明线性独立的假设弱于不可约假设，它可以推导出不可约假设，但是不可约假设不可推导出线性独立假设。这里不进行详细的证明，给一个例子：假设 P1 是正态分布，P2 也是另外一个正态分布，而 P2 是 P1 和 Q 的线性组合，可以看出假设 p1 和 P2 是不一样的两个分布，所以它们俩是线性独立的（根据前面一页的推论）。根据不可约的定律，可以知道 P1 和 P2 是可约的，因为 P2 表示成 P1 和另外一个分布 Q 的组合。这个例子中的两个分布是线性独立的，但是可约。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/865c00094b66aa9c1e83><p>同时也可以证明线性独立假设弱于 anchor set condition 假设。同样给一个例子来简单说明，比如说右图这两个分布，我们可以看出这两个分布的 suppose set 都是 0 到 10，但是这两个分布显然是不一样的，所以它们俩不符合 anchor set 的假设，但它们俩线性独立。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/8b4b00004d713ffcd6a0><p>在线性独立的假设条件下，能够证明出这个混合比例的唯一性，假设 P0 是由 P1 到 PM 的线性组合而成，而且组成分布满足线性独立假设。如果给定这个 P0 和所有组合分布的情况下，该比例是唯一的。证明非常简单：利用反证法（和线性代数里面证明的方法一致）证明。假设存在另外一组系数使得这个混合同样成立，通过两个不同组合的系数相减等于 0，借此可以进行推导。具体推导讲解，大家可以回放视频至第 19 分钟查看。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b480007479454b52139><p></p><h5>注：核均值匹配中最核心的方法是核均值嵌入，关于核均值嵌入的具体讲解大家可回放视频至第 21 分钟查看</h5><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b4a000650ca949129bd><p>mp（1）代表均值。如果该核均值嵌入是一个一对一的映射，通常称该方程是特征化的。在这种情况下，核均值嵌入就拥有了分布 P 所有的信息。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b49000650011b7400ab><p></p><h5>注：在这种情况下，有一个非常重要的定理。假设这个核均值嵌入是个一对一的映射，且组成分布满足线性独立的假设。在这种情况下，可以推导出核均值嵌入同理也满足线性独立的假设，详细的证明过程大家可以回放视频至第 23 分钟查看</h5><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b4900064ffef7ca6798><p>同理，我们可以证明出λi 的唯一性，与之前证明一模一样，此处不再详述。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/8b4a000650cc29f2195d><p>我们可以利用最大平均差异的方法来求出λi 的解，利用的定理为：如果两个分布它的最大平均差异是 0 的话，当且仅当这两个分布是同一个分布。所以最小化平方后的最大平均差异的值，就可以求出λi。但问题是我们没有 Pi 的表达式，不知道核均值嵌入到底是多少。</p><p>那么我们通常是怎么解决？</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/8b4a000650cd8778a0f0><p></p><h5>注：利用一个经验的近似估计，利用所有数据对 feature map 的均值来近似核均值嵌入，当拥有这个近似的核均值嵌入以后，同样可以代入这个最大均值差异的方程中，最后变成这样一个问题：详细讲解可回放视频至 27 分钟查看</h5><p>我们还关心另外一个问题，这样估计出来的混合比例能不能收敛到最优的解，它以多快的速率收敛到最优的解？</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b480007479c5b326c54><p></p><h5>注：此处详细讲解可将视频回放至第 30 分钟查看</h5><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b490006500a16a08f9d><p>可以看出，该收敛证明是一致性的，收敛没有任何与训练数据相关的项。这个结论是与之前证明 class proportion estimation（CPE）的收敛性的工作是不一样的，它们往往都有训练数据的项。这也是本文主要贡献之一。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/8b4b00004d745b9c48cb><p></p><h5>注：此处详细讲解可将视频回放至第 33 分钟查看</h5><p>混合比例估计的应用场景：第一个应用场景是在具有噪声标签的学习上，我们把椭圆内的所有样本都标记成」汽车「，其实我们可以发现只有绿色的图像才是」汽车「，其他的样本都是从其他的类别中标记错误得来的，也就是带有噪声的标签，通常可以假设带有噪声的每个类别的样本是真实数据的每个类别的线性组合。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b480007479fd3f4a7b9><p>我们基于 UCI 数据做了一系列的实验，对比了 ROC 以前其他两个估计噪声率的方法，其中只有 ROC 的方法用了 M-1 个混合分布的数据，而其他两个方法都用了所有组成分布中采样的数据。可以看出，当噪声越来越大的情况下，我们的方法通常有比较一致的表现，同时当样本大小逐渐增加的时候，我们的方法也逐渐收敛。而且从这两个图像中也可以看出，我们的图像往往能更好地估算出混合比例。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/865b000b82da10f51d27><p>第二个应用：半监督学习。所谓半监督学习是指，拥有少量的标记样本，还有一大部分的样本是没有标签的，希望从这些没有标签的样本中也能学习到一定的信息。通常假设没有标签的样本是所有类别样本的一个组合。也就是数据 X 的分布是每一个类别中数据的分布一个组合，而混合比例也就是各类别的比例。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/8b4d0000294fa31664ca><p></p><h5>注：一个对 UCI 数据的实验，该方法也获得了比较高的正确率。详细讲解大家可以回放视频至第 36 分钟查看</h5><h3><strong>混合比例估计在 Target Shift 这类问题中的延伸和应用</strong></h3><p>第三部分我们研究一种比较特殊的迁移学习，一般称之为 Target Shift 或 Label Shift。我们看看 MPE 在该问题中是否有延伸应用。首先来看一下这个问题是什么？</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/865c00094b744fdb007c><p>在传统的数据训练中，我们假设训练数据和测试数据都采自同样的分布，而在 Target Shift 这个问题中，我们假设各个类别的分布都一样（如图中圈内的紫色和黄色分布），但是每个类的比例发生了变化。Target Shift 就是来检测 P(Y) 的变化。有一个更大的挑战：我们研究在训练数据中有一部分数据标记错了，把第一类标记进了第二类，第二类标记进了第一类，在这种情况下，我们是否还能够检测出 P(Y) 的变化呢？</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/865b000b82dd5ae9453d><p>对此，可以考虑一个实际的问题，比如说我们有一系列 9 月份的胸腔 X 光片，在上面我们已经收集过了一部分 8 月份的 X 光片，而且这些 X 光片都已经标记了谁有肺炎，谁是没有肺炎。但是由于这些数据被一些非专家标记，或者机器标记，甚至可能是有些医学样本就是很难分辨，所以很多标签发生了错误。另外，由于某些原因 9 月份得肺炎的病人比例增加了。在这个问题中，我们有 8 月份各类别的噪声样本，然后又有 9 月份的混合数据，而且 9 月份和 8 月份的数据中，肺炎病人比例还变化，我们如何能检测出 9 月份肺炎病人的比例？</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/865c00094b739688c00e><p></p><h5>注：详细讲解大家可以回放视频至第 42 分钟查看</h5><p>我们首先来定义这个问题，假设我们有一些带有噪声标签的训练数据和一系列没有标签的测试数据，我们通常假设在每个类别的分布中都是相等的，在这种情况下，我们如何正确估计测试数据中的 P(Y)？</p><p>我们首先引入一个假设，也就是前面提到的带有噪声标签的假设，我们假设这个噪声数据是真实数据的混合，它们的分布也是满足下图中混合的假设。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/8b4a000650d134122098><p>可以看出测试样本的数据是由测试数据中每个类别的数据混合而成，之前，在半监督学习中就有这个混合形式出现，而测试数据中每个类别的分布又与训练数据的每个类别的分布一样，也就是说测试数据分布可以表示成训练数据每个类别分布的混合。同时我们又假设带噪声数据是所有训练数据每个类别分布的混合。那能不能把所有测试数据的混合数据的分布表示成训练数据中带噪声噪声的每个类别的分布的混合？</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/865c00094b7592390207><p>假设有这种混合形式，通过简单的推导并得到结论。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/865b000b82e722d201cb><p></p><h5>注：关于该混合形式，简单推导以及 Estimate Q 等内容，余席宇做了十分详细的讲解，大家可回放视频从第 44 分钟到 55 分钟进行查看</h5><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/865b000b82ea69fa658f><p></p><h5>注：实验内容的详细讲解，大家可查看回放视频的第 55 分钟到第 57 分钟的内容</h5><p>最后，我们通过一个实验来验证。训练数据和测试数据的分布均由两个高斯分布混合而成，但比例不一样。这里 beta 是测试数据 P^te(Y) 比上训练数据 P^tr(Y) 的比例，可以看出在不同的 beta 时（图中最左），我们的方法能够得到比较一致的结果。在训练数据中有不同的噪声大小的时候，我们的方法表现也是比较一致的（图中）。可以看出当训练样本逐渐增大，我们的算法也逐渐收敛到最优的解。而对比的方法都出现了比较大的误差，甚至有时候是错误的（图中最左）。通过这个实验可以验证出在该问题中我们的方法的有效性，也可以看出 MPE 在这个问题中的延伸应用。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b48000747a367ff1f5d><p></p><h5>注：详细讲解可回放视频至第 57 分钟查看</h5><h3><strong>混合比例估计在一般的迁移学习中的延伸和应用</strong></h3><p>最后，我们来介绍一般的迁移学习，我们首先来看一下迁移学习的定义，在前面提到的 Target Shift 中，我们假设每个类别的条件分布都是一样的，但是 Y 的分布是在变化的，而在一般的迁移学习中，我们假设每个类别的分布和 Y 的分布都要发生变化，在图中的下方的两个示例中，我们假设有一部分数据标记错误，在这种情况下，我们通常定义这个训练数据为辅助领域，从辅助领域中学习到比较有用的信息，来帮助这个目标领域中数据的学习到一个比较好的分类器。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b4d00002959130bf3dc><p>我们同样来这个例子，假设 8 月份的数据（已经标记好），由于某些原因标记出现了错误，而在 9 月份，得到了一些核磁共振图像，我们能不能在 8 月份的 X 光片成像中提取一些有用的信息来辅助核磁共振成像的最后分类？这个问题比较有意义，因为在现实医学生活中我们往往有很多可以辅助你的医学数据，这些数据往往很难标记，很多数据可能标记错误，我们希望用这些辅助数据来学习到一个新的病例中，帮助一个新的病例来提取一些不便的信息，帮助它们的学习，这个问题我们该如何来解决？</p><p>同样首先来看一下该问题的定义：</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b4b00004d7ed3bad7d6><p></p><h5>注：关于该问题的定义、基本假设、具体方法，大家可回放视频至 59 分钟处开始查看</h5><p>ppt 内容如下：</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b4a0006518538c5fe2f><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b49000650c6eb047b82><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b480007485103def918><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/865c00094c3ad56c14f9><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/8b49000650c7200a0112><p>我们从这些噪声数据中学习到不变的信息以及 P(Y) 的变化之后，我们利用这些不变的信息来训练分类器。</p><p>可以看出我们的方法较其他的方法有非常大的提升，也就证明了我们的方法能够克服噪声标签对于学习不变表征的影响。这里主要从 MPE 的角度介绍了这个方法，具体的方法和细节可以参看我的论文（文末附有参考文献）。</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b480007485515dad811><p>该带有噪声标签的迁移学习方法的贡献：</p><ol><li><p>该方法研究了对有噪声标签的迁移学习的影响。</p></li><li><p>提出了统一的框架，能够克服噪声辅助数据对于学习不变表征的有害影响，然后学到一些有用的信息。</p></li><li><p>提供了一致性的收敛的证明，可以从前面的讲解中看出，这个问题收敛的结论其实是和前面 MPE 的收敛结论是相似的。</p></li><li><p>性能上得到了显著的提升，我们提取了一些不变的表征来克服影响，使得性能有极大的提升。</p></li></ol><p>参考文献：</p><img alt="优必选悉尼 AI 研究院博士生：混合比例估计在弱监督学习和迁移学习中的延伸与应用｜分享总结" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/8b4a0006518890e23855><p></p><h5>注：本次公开课主要讲解内容来自余席宇的前两篇论文</h5><p>以上就是本期嘉宾的全部分享内容。更多公开课视频请到雷锋网 AI 慕课学院观看。关注微信公众号：AI 科技评论，可获取最新公开课直播时间预告。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'优必选','AI','估计'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>