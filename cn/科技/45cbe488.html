<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>NLP基础-通用句子向量漫谈 | 极客快訊</title><meta property="og:title" content="NLP基础-通用句子向量漫谈 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/db54188d47524055b7a45d90aed407ac"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/45cbe488.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/45cbe488.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/45cbe488.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/45cbe488.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/45cbe488.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/45cbe488.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/45cbe488.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/45cbe488.html><meta property="article:published_time" content="2020-11-14T21:01:58+08:00"><meta property="article:modified_time" content="2020-11-14T21:01:58+08:00"><meta name=Keywords content><meta name=description content="NLP基础-通用句子向量漫谈"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/45cbe488.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>NLP基础-通用句子向量漫谈</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><h1><strong>背景</strong></h1><p>​ 近期业务需要使用文本上下文语义特征，而将文本进行编码和表征是NLP最核心的技术之一，于是调研了表征文本的相关技术，总结如下, 以飨后人。</p><p>混沌未开</p><p>​ 在word2vec诞生之前，NLP中并没有一个统一的方法去表示一段文本。从one-hot表示一个词到用bag-of-words来表示一段文本，从k-shingles把一段文本切分成一些文字片段到汉语中用各种序列标注方法将文本按语义进行分割，从tf-idf中用频率的手段来表征词语的重要性到text-rank中借鉴了page-rank的方法来表征词语的权重，从基于SVD纯数学分解词文档矩阵的LSA，到pLSA中用概率手段来表征文档形成过程并将词文档矩阵的求解结果赋予概率含义，再到LDA中引入两个共轭分布从而完美引入先验，句子表征走过了漫长的黑暗时期。</p><p>一丝曙光</p><p>​ 2003年Bengio的经典论文《A Neural Probabilistic Language Model》打开了一丝曙光， 经网络语言模型（NNLM）的第一层参数当做词的分布式表征时，能够很好的获取词语之间的相似度，从而构造句子表征。自NNLM于2003年被提出后，后面又出现了很多类似和改进的工作，诸如LBL, C&W和RNNLM模型等等，这些方法主要从两个方面去优化NNLM的思想，其一是NNLM只用了左边的n-1个词，如何利用更多的上下文信息便成为了很重要的一个优化思路（如Mikolov等人提出的RNNLM）；其二是NNLM的一个非常大的缺点是输出层计算量太大，如何减小计算量使得大规模语料上的训练变得可行，这也是工程应用上至关重要的优化方向（如Mnih和Hinton提出的LBL以及后续的一系列模型）。</p><p>​ 2013年， Tomas Mikolov 于论文《Efficient estimation of word representations in vector space》提出CBOW和Skip-gram模型，模型使用了Hierarchical Softmax或者Negative Sampling等优化方法， 并开源成为word2vec工具。其后又出现了考虑词共现的GloVe， 基于监督训练信号的fastText。至此为止，大多数NLP任务均为先预训练词向量，然后对词向量求均值获得句子表征 （通过监督信号使用TextCNN/RNN等也可以获得任务相关句子表征，本文只探讨通用句子表征）。</p><p>​ 除了简单的求平均之外， 在word2vec开源随后的第一年，即2014年，Mikolov在他和另一位作者合作的一篇论文《Distributed Representations of Sentences and Documents》中，提出了可以借鉴word2vec思想的两种结构：PV-DM和PV-DBOW，分别对应word2vec中的CBOW和Skip-gram。 2015年，多伦多大学的Kiros等人提出了一个很有意思的方法叫Skip-thoughts，同样也是借鉴了Skip-gram的思想。Skip-thoughts直接在句子间进行预测，也就是将Skip-gram中以词为基本单位，替换成了以句子为基本单位，具体做法就是选定一个窗口，遍历其中的句子，然后分别利用当前句子去预测和输出它的上一句和下一句。2018年，Google Brain在Skip-thoughts的基础上将这一思想做了进一步改进，把Skip-thoughts的生成任务改进成为了一个分类任务，具体说来就是把同一个上下文窗口中的句子对标记为正例，把不是出现在同一个上下文窗口中的句子对标记为负例。 此外，在ICLR 2017 上论文《A Simple but Tough-to-Beat Baseline for Sentence Embeddings》是一个很好句子表征算法，算法的大致描述如下：选择一个流行的词嵌入方法，通过词向量的线性的加权组合对一个句子进行编码，并且删除共有的部分（删除它们的第一个主成分上的投影）。</p><p>​ 除了Skip-thoughts和Quick-thoughts这两种不需要人工标记数据的模型之外，2017年Facebook提出的InferSent框架，先设计一个模型在斯坦福的SNLI数据集上训练，之后将训练好的模型当做特征提取器获得句子向量， 此外2018年ICLR论文《Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning》提出了利用四种不同的监督任务来联合学习句子的表征, Google 在 《Universal Sentence Encoder》利用Transformer和DAN提取通用句子表征。</p><p>​ 对中文而言, 腾讯于NAACL2018发表论文《Directional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word Embeddings》对Skipgram增加词共现和方向信息(窗口中词在左边还是右边)， 并发布号称最大的中文词向量。作为类比推理论文《Analogical Reasoning on Chinese Morphological and Semantic Relations》的副产品，学者们也发布了100+各种中文语料各种模型预训练的词向量。此外，也有蚂蚁的同学发表在AAAI2018的高分论文《cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information》，提出基于笔画的词向量训练方法见一笔一画之间的奥秘，针对此方法也有不同声音见是否需要一笔一画。 类似的, 香侬科技发表论文《Glyce: Glyph-vectors for Chinese Character Representations》提出在深度学习的框架下使用中文字形信息。 然而 针对此方法也有很多不同的声音。</p><p>​ 词向量，句子表征五花八门，是不是最新的就是最好的呢，如何选择呢，Facebook 在EMNLP2018 发表论文《Dynamic Meta-Embeddings for Improved Sentence Representations》允许NLP模型动态选择在给定环境中表现最佳的字嵌入算法 (其实这个算法并不能)。</p><p>巨人肩膀</p><p>ELMo</p><p>​ AllenNLP发表论文《Deep contextualized word representations》（NAACL2018 best paper）首次提出了ELMo，ELMo的基本框架便是2-stacked biLSTM + Residual的结构，不过和普通RNN结构的不同之处在于，ELMo借鉴了2016年Google Brain的Rafal Jozefowicz等人发表的一篇论文《Exploring the Limits of Language Modeling》，其主要改进在于输入层和输出层不再是word，而是变为了一个char-based CNN结构。ELMo并没有本质上的创新，连模型也基本是引用和拼接别人的工作。</p><p>ULMFit</p><p>​ ELMo同期，有另一个来自FastAI的非常惊艳的工作。在论文《Universal Language Model Fine-tuning for Text Classification》中，他们提出了ULMFit结构，基本的思路也是预训练完成后去具体任务上进行finetune。ULMFit主要可以分为三个阶段，分别是在大规模语料集上先预训练，然后再将预训练好的模型在具体任务的数据上重新利用语言模型来finetune一下（这是第一次finetune，叫做LM finetune），尔后再根据具体任务设计的一个模型上，将预训练好的模型当做这个任务模型的多层，再一次finetune。所使用的模型也非原始的LSTM而是AWD-LSTM。此方法来自Salseforce 论文《Regularizing and Optimizing LSTM Language Models》。</p><p>GPT1.0</p><p>​ OpenAI于2018年6月发布论文《Improving Language Understanding by Generative Pre-Training》,核心思想为: 1) 预训练阶段采用“单向语言模型”作为训练任务，把语言知识编码到Transformer里。2) 在第一阶段训练好的模型基础上，通过Finetuning来做具体的NLP任务。GPT 1.0本身效果就很好，因为不会PR，默默无闻。</p><div class=pgc-img><img alt=NLP基础-通用句子向量漫谈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/db54188d47524055b7a45d90aed407ac><p class=pgc-img-caption></p></div><p>BERT</p><p>​ 获得NAACL2019 best paper的BERT模型不亚于NLP的明星， BERT站在无数巨人之上， 具有以下特点1）利用了真双向的Transformer2）改进了普通语言模型成为完形填空式的Mask-LM3）利用Next Sentence Prediction任务学习句子级别信息4）进一步完善和扩展了GPT中设计的通用任务框架。由于BERT模型过于庞大，针对BERT训练出现了若干优化方案 1） 谷歌的LAMB 优化器将 BERT 预训练的批量大小扩展到 64K，且不会造成准确率损失。2 ）阿里云的Perseus-BERT, 采用统一分布式通信框架、混合精度、XLA编译器优化等技术。</p><div class=pgc-img><img alt=NLP基础-通用句子向量漫谈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8b7083b20dcf41db9d1c217dab77f447><p class=pgc-img-caption></p></div><p>GPT2.0</p><p>​ 号称能续写红楼梦后40回， OpenAI （CloseAI）发布的GPT2.0 模型。 本质上，GPT2.0做了两件事。首先把Transformer模型参数扩容，其次找更大数量的无监督训练数据。同BERT相比，最大的差异在于它是单向的。</p><div class=pgc-img><img alt=NLP基础-通用句子向量漫谈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5545d442c5994dd5b1bcc6a0202f1b39><p class=pgc-img-caption></p></div><p>MT_DNN</p><p>​ 微软的MT-DNN 通过整合BERT扩展了微软在 2015 年提出的多任务深度神经网络模型。MT-DNN 模型较低层在所有任务中共享，而顶层是针对任务特定的。首先，输入 X（一个句子或一对句子）在 l_1 层中被表示为嵌入向量序列，每个单词都对应一个嵌入向量序列。然后，基于变换器的编码器捕获每个单词的上下文信息，并在 l_2 层中生成共享的上下文嵌入向量。最后，额外的特定任务层针对每个任务生成特定任务表示，随后是分类、相似性评分或相关性排序所必需的操作。MT-DNN 使用 BERT 将其共享层初始化，然后通过 MTL 对其进行优化。一句话来说，MT_DNN就是多任务学习加BERT。</p><div class=pgc-img><img alt=NLP基础-通用句子向量漫谈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cddd401bd08641a893856f9426bc8202><p class=pgc-img-caption></p></div><p>ERINE</p><p>​ 百度改进了BERT，提出ERNIE模型通过建模海量数据中的实体概念等先验语义知识，学习真实世界的语义关系。具体来说，ERNIE模型通过对词、实体等语义单元的掩码，使得模型学习完整概念的语义表示。相较于 BERT 学习原始语言信号，ERNIE直接对先验语义知识单元进行建模，增强了模型语义表示能力。ERINE实际效果（中文问答）也比BERT稍好。</p><div class=pgc-img><img alt=NLP基础-通用句子向量漫谈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/23e08ebce2a2485298d25298f93d46cd><p class=pgc-img-caption></p></div><p>其他</p><p>2018年至今NLP领域产出了非常多成果。 如谷歌针对超长上下文句子的Tranformer XL， 斯坦福和谷歌的新型自训练算法 Cross-View Training (CVT)，结合了预训练词向量和自训练算法。Facebook提出跨语言预训练模型XLM, 其实就是跨语言的BERT。 此外，引入图像等跨模态数据也是增强句子表征的一个方向。论文《Image-Enhanced Multi-Level Sentence Representation Net for Natural Language Inference》通过将图像信息引入到自然语言推理中，利用额外补充的信息对句子语义进行增强，从而更加准确地理解句子语义。然而，此文章对图像的利用、图像的有效性分析等都值得改进。</p><p>小结</p><p>2018年以来NLP取得的诸多突破是站在巨人的肩膀依托海量的数据资源上取得的, MT_DNN 和ERINE是在BERT基础上的进一步改进。以上预训练模型标志NLP进入ImageNet时代，带来的优势如下：</p><ol><li>近乎无限量的优质数据</li><li>无需人工标注，非监督/半监督</li><li>一次学习多次复用，多任务学习</li><li>学习到的表征可在多个任务中进行快速迁移，迁移学习</li></ol><p>展望未来</p><p>机会与挑战并存</p><ol><li>OOV (字级别不存在OOV)及新词Embedding获取。现实NLP场景中很可能会由于词表不够大出现OOV或新词不在词表中，如何获取此类词的Embedding进而获取句子表征仍然是一个值得研究的问题，现有的方案多采用随机初始化或语义相近的字向量拼接加噪声等方式，也许可以通过挖掘词之间的关联结合GCN等方式。</li><li>很多词在不同的场景或不同的时间会有不同的含义，甚至相同的上下文下，相同的词在不同的时间都具有不同的含义，也会影响句子都表征，词表征的演化也是一个挑战，也许可以对句子加入时间编码。</li><li>Finetune真的是万能的吗，从零开始训练句子表征也许也不错。 现实NLP问题，有监督的模型大多都比非监督模型效果好，那是否真的需要预训练呢？ 从零开始训练是否就一定比Finetune差呢，在CV领域何恺明发表论文《Rethinking ImageNet Pre-training》质疑ImageNet 预训练， NLP又是怎么样呢？</li><li>句子表征缺乏常识信息，ERNIE在某些程度加入了额外的信息，如何更加显式的加入语义/句法等信息（BERT等模型隐式得到了语义语法信息）降低模型训练成本 （预训练和Finetune成本非常高）仍是严峻挑战。</li><li>新模型复杂模型并不定是最优的，某些简单的模型如GLoVe在一些特殊的任务中可以获得媲美于BERT的效果，no free lunch， 对不同的问题需要使用相应的句子表征，Meta-Embedding也许是解决之道。</li></ol><p>未来方向</p><ol><li>引入领域知识、语法句法知识、业务知识、常识，训练更好的句子表征。领域知识包含知识图谱等结构化数据，给定一个知识图谱和一个自然语言问题，如何将该问题转化为机器能够理解和执行的语义表示，受到了来自全世界研究者的广泛关注和深入探索。</li><li>Meta-learning。对于低资源的语言或长尾问题，样本稀缺，Meta-learning作为解决Few-shot问题的方法之一已在CV领域取得了诸多进展，而在NLP领域仍存在广阔的空间。</li><li>图神经网络。图作为一个可推理，研究人员开始研究如何将卷积神经网络迁移到图数据上，涌现出ChevNet、MoNet、GCN、GAT等一系列方法，在基于图的半监督分类和图表示学习等任务中表现出很好的性能。句子本身是具有语法结构信息的，基于图神经网络的句子表征已经在一些特定的任务如事件抽取、关系抽取取得了不错的效果，是否有进一步的扩展空间值得研究。</li><li>多模态。婴儿在掌握语言功能前，首先通过视觉、听觉和触觉等感官去认识并了解外部世界。语言并不是人类在幼年时期与外界进行沟通的首要手段。因此，构建通用人工智能也应该充分地考虑自然语言和其他模态之间的互动，并从中进行学习，这就是多模态学习。</li><li>可解释性。词、句子表征一直都是大黑盒，虽然存在一些学者对向量维度进行可解释信性研究，然而如何平衡可解释和性能，如何将NLP模型预测结果解释给客户是一个方向。</li></ol><p>P.S.</p><p>本文未覆盖任务相关句子表征训练，NLP方法太多，有所遗漏见谅</p><p>有用的github链接: https://github.com/Separius/awesome-sentence-embedding</p><p>中文词向量:https://github.com/Embedding/Chinese-Word-Vectors</p><p>BERT:https://github.com/google-research/bert/</p><p>ERINE: https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE</p><p>Tencent word embedding: https://ai.tencent.com/ailab/nlp/embedding.html</p><p>MT_DNN: https://github.com/namisan/mt-dnn</p><p>感谢师兄: 王俞霖</p><p>感谢团队成员: 贾强槐、 杨起腾、张卫星、董良、高枫、</p><p>感谢各位老板: 华能威、王祥志</p><p>参考文献:</p><ol><li>https://zhuanlan.zhihu.com/p/50443871</li><li>Rethinking ImageNet Pre-training</li><li>Dynamic Meta-Embeddings for Improved Sentence Representations</li><li>Improving Language Understanding by Generative Pre-Training</li><li>Directional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word Embeddings</li><li>Analogical Reasoning on Chinese Morphological and Semantic Relations</li><li>Image-Enhanced Multi-Level Sentence Representation Net for Natural Language Inference</li><li>Efficient estimation of word representations in vector space</li><li>A Neural Probabilistic Language Model</li><li>Universal Sentence Encoder</li><li>Universal Language Model Fine-tuning for Text Classification</li></ol><p>转自：https://yq.aliyun.com/articles/700006?spm=a2c4e.11155435.0.0.dad0283bzo6SFr</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'NLP','基础','漫谈'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>