<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>爱可可AI论文推介(10月9日) | 极客快訊</title><meta property="og:title" content="爱可可AI论文推介(10月9日) - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/6bf012fba2904c8cbafc6dad1079a51f"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/491e299e.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><meta property="article:published_time" content="2020-10-29T21:12:41+08:00"><meta property="article:modified_time" content="2020-10-29T21:12:41+08:00"><meta name=Keywords content><meta name=description content="爱可可AI论文推介(10月9日)"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/491e299e.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>爱可可AI论文推介(10月9日)</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>AI - 人工智能 LG - 机器学习 CV - 计算机视觉 CL - 计算与语言</p><p><br></p><p><strong>1、[CV]*Contrastive Learning of Medical Visual Representations from Paired Images and Text</strong></p><p>Y Zhang, H Jiang, Y Miura, C D. Manning, C P. Langlotz</p><p>[Stanford University]</p><p><strong>用无监督对比学习方法(ConVIRT)从图像-文本对学习医学视觉表示，用图像表示与文本数据两模态间的双向对比目标，进行医学图像编码器的预训练，ConVIRT是领域不可知(domain-agnostic)的，无需额外的专家输入。在4个医学图像分类任务和2个图像检索任务中，ConVIRT的表现优于其他同样使用文本数据的强域内初始化方法，表示质量显著提高。与ImageNet预训练相比，ConVIRT能以更少的标记数据实现同水平的分类精度。</strong></p><blockquote><p>Learning visual representations of medical images is core to medical image understanding but its progress has been held back by the small size of hand-labeled datasets. Existing work commonly relies on transferring weights from ImageNet pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. We propose an alternative unsupervised strategy to learn medical visual representations directly from the naturally occurring pairing of images and textual data. Our method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test our method by transferring our pretrained weights to 4 medical image classification tasks and 2 zero-shot retrieval tasks, and show that our method leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classification tasks, our method requires only 10% as much labeled training data as an ImageNet initialized counterpart to achieve better or comparable performance, demonstrating superior data efficiency.</p></blockquote><p>https://weibo.com/1402400261/JokrT9o4s</p><div class=pgc-img><img alt=爱可可AI论文推介(10月9日) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6bf012fba2904c8cbafc6dad1079a51f><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=爱可可AI论文推介(10月9日) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/abba76e01694402ab0f7c62199bf09ff><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=爱可可AI论文推介(10月9日) onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/20c8135a263d459a82df8efc234d43d8><p class=pgc-img-caption></p></div><p><br></p><p><strong>2、[CL]Autoregressive Entity Retrieval</strong></p><p>N D Cao, G Izacard, S Riedel, F Petroni</p><p>[University of Amsterdam & Facebook AI Research]</p><p><strong>自回归实体检索(GENRE)，用自回归方式生成实体名，以上下文为条件，通过从左到右逐词条自回归方式生成实体唯一名称来检索实体。自回归允许直接捕捉上下文和实体名称之间的关系，有效地对两者进行交叉编码；编解码器架构的参数随词汇表大小而非实体数量而变化，大大减少了内存占用；可有效计算精确的软最大损失，而不需要对负数据进行子采样。</strong></p><blockquote><p>Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity information such as descriptions. This approach leads to several shortcomings: i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; ii) a large memory footprint is needed to store dense representations when considering large entity sets; iii) an appropriately hard set of negative data has to be subsampled at training time. We propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion, and conditioned on the context. This enables to mitigate the aforementioned technical issues: i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new SOTA, or very competitive results while using a tiny fraction of the memory of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name.</p></blockquote><div class=pgc-img><img alt=爱可可AI论文推介(10月9日) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/70cc8bd78b324e17a7547137297644af><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=爱可可AI论文推介(10月9日) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c283709ae67e45d680ab6897b041d0fd><p class=pgc-img-caption></p></div><p><br></p><p><strong>3、[AI]Human-Level Performance in No-Press Diplomacy via Equilibrium Search</strong></p><p>J Gray, A Lerer, A Bakhtin, N Brown</p><p>[Facebook AI Research]</p><p><strong>用均衡搜索玩经典棋类桌游《外交风云》达到人类水平，《外交风云》是个复杂游戏，涉及合作与竞争，对AI技术提出了重大的理论和实践挑战。新的AI智能体通过对人类数据的监督学习并使用外部遗憾最小化的单步前向搜索，在该游戏中实现了人类水平性能。用该智能体在流行的《外交风云》网站上匿名玩游戏，在1128名人类玩家中排名第23。</strong></p><blockquote><p>Prior AI breakthroughs in complex games have focused on either the purely adversarial or purely cooperative settings. In contrast, Diplomacy is a game of shifting alliances that involves both cooperation and competition. For this reason, Diplomacy has proven to be a formidable research challenge. In this paper we describe an agent for the no-press variant of Diplomacy that combines supervised learning on human data with one-step lookahead search via external regret minimization. External regret minimization techniques have been behind previous AI successes in adversarial games, most notably poker, but have not previously been shown to be successful in large-scale games involving cooperation. We show that our agent greatly exceeds the performance of past no-press Diplomacy bots, is unexploitable by expert humans, and achieves a rank of 23 out of 1,128 human players when playing anonymous games on a popular Diplomacy website.</p></blockquote><div class=pgc-img><img alt=爱可可AI论文推介(10月9日) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/270c5c909cdb4c44a934e9c0df8f0a78><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=爱可可AI论文推介(10月9日) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4f7fd456f3364850afbdc71a85c5aca6><p class=pgc-img-caption></p></div><p><br></p><p><strong>4、[CL]CATBERT: Context-Aware Tiny BERT for Detecting Social Engineering Emails</strong></p><p>Y Lee, J Saxe, R Harang</p><p>[Sophos AI]</p><p><strong>用上下文感知的超小BERT检测钓鱼邮件，为了识别那些不包含恶意代码、不与已知攻击共享单词选择的手工社会工程邮件，通过微调一个预训练的、大规模修剪的BERT模型，加上来自邮件头的附加上下文特征，从中学习邮件内容和上下文特性间的上下文表示，即使存在故意拼写错误和恶意逃避的状况，也能有效检测出有针对性的钓鱼邮件。该方法优于用适配器和上下文层的强基线模型，钓鱼邮件检出率达到87%。</strong></p><blockquote><p>Targeted phishing emails are on the rise and facilitate the theft of billions of dollars from organizations a year. While malicious signals from attached files or malicious URLs in emails can be detected by conventional malware signatures or machine learning technologies, it is challenging to identify hand-crafted social engineering emails which don't contain any malicious code and don't share word choices with known attacks. To tackle this problem, we fine-tune a pre-trained BERT model by replacing the half of Transformer blocks with simple adapters to efficiently learn sophisticated representations of the syntax and semantics of the natural language. Our Context-Aware network also learns the context representations between email's content and context features from email headers. Our CatBERT(Context-Aware Tiny Bert) achieves a 87% detection rate as compared to DistilBERT, LSTM, and logistic regression baselines which achieve 83%, 79%, and 54% detection rates at false positive rates of 1%, respectively. Our model is also faster than competing transformer approaches and is resilient to adversarial attacks which deliberately replace keywords with typos or synonyms.</p></blockquote><div class=pgc-img><img alt=爱可可AI论文推介(10月9日) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b7caec640ef945f9a9811c7f39653da5><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=爱可可AI论文推介(10月9日) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/704ec79ab0bc4a3ab1d91318d20331a7><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=爱可可AI论文推介(10月9日) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6dd5d2d9bcea48f89cb21f7510af24f7><p class=pgc-img-caption></p></div><p><br></p><p><strong>5、[CL]WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization</strong></p><p>F Ladhak, E Durmus, C Cardie, K McKeown</p><p>[Columbia University & Cornell University]</p><p><strong>跨语种抽象摘要新基准WikiLingua，一个跨语言和多语言抽象摘要的基准数据集，从WikiHow中提取了18种语言的文章和摘要对，WikiHow是个高质量的协作资源，提供了人工撰写的一系列不同主题的操作指南。通过对齐文章中用于描述每个how-to步骤的图像，创建了跨语言的金标准文章-摘要对齐。</strong></p><blockquote><p>We introduce WikiLingua, a large-scale, multilingual dataset for the evaluation of crosslingual abstractive summarization systems. We extract article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. We create gold-standard article-summary alignments across languages by aligning the images that are used to describe each how-to step in an article. As a set of baselines for further studies, we evaluate the performance of existing cross-lingual abstractive summarization methods on our dataset. We further propose a method for direct crosslingual summarization (i.e., without requiring translation at inference time) by leveraging synthetic data and Neural Machine Translation as a pre-training step. Our method significantly outperforms the baseline approaches, while being more cost efficient during inference.</p></blockquote><p>https://weibo.com/1402400261/JokOvyCDJ</p><div class=pgc-img><img alt=爱可可AI论文推介(10月9日) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6a010acb47e847f3bd332d7818936a00><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=爱可可AI论文推介(10月9日) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d5daad8a4f514f5b9e01f10f02c33fdd><p class=pgc-img-caption></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'AI','论文','10'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>