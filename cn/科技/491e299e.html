<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥) | æå®¢å¿«è¨Š</title><meta property="og:title" content="çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥) - æå®¢å¿«è¨Š"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/6bf012fba2904c8cbafc6dad1079a51f"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/491e299e.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><meta property="article:published_time" content="2020-10-29T21:12:41+08:00"><meta property="article:modified_time" content="2020-10-29T21:12:41+08:00"><meta name=Keywords content><meta name=description content="çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥)"><meta name=author content="æå®¢å¿«è¨Š"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/491e299e.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>ğŸ¤“ æå®¢å¿«è®¯ Geek Bank</a></h1><p class=description>ä¸ºä½ å¸¦æ¥æœ€å…¨çš„ç§‘æŠ€çŸ¥è¯† ğŸ§¡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>çŒœä½ å–œæ­¡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=ç§‘æŠ€>ç§‘æŠ€</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=éŠæˆ²>éŠæˆ²</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=ç§‘å­¸>ç§‘å­¸</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥)</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>ç§‘æŠ€</a></span></div><div class=post-content><p>AI - äººå·¥æ™ºèƒ½ LG - æœºå™¨å­¦ä¹  CV - è®¡ç®—æœºè§†è§‰ CL - è®¡ç®—ä¸è¯­è¨€</p><p><br></p><p><strong>1ã€[CV]*Contrastive Learning of Medical Visual Representations from Paired Images and Text</strong></p><p>Y Zhang, H Jiang, Y Miura, C D. Manning, C P. Langlotz</p><p>[Stanford University]</p><p><strong>ç”¨æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•(ConVIRT)ä»å›¾åƒ-æ–‡æœ¬å¯¹å­¦ä¹ åŒ»å­¦è§†è§‰è¡¨ç¤ºï¼Œç”¨å›¾åƒè¡¨ç¤ºä¸æ–‡æœ¬æ•°æ®ä¸¤æ¨¡æ€é—´çš„åŒå‘å¯¹æ¯”ç›®æ ‡ï¼Œè¿›è¡ŒåŒ»å­¦å›¾åƒç¼–ç å™¨çš„é¢„è®­ç»ƒï¼ŒConVIRTæ˜¯é¢†åŸŸä¸å¯çŸ¥(domain-agnostic)çš„ï¼Œæ— éœ€é¢å¤–çš„ä¸“å®¶è¾“å…¥ã€‚åœ¨4ä¸ªåŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡å’Œ2ä¸ªå›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­ï¼ŒConVIRTçš„è¡¨ç°ä¼˜äºå…¶ä»–åŒæ ·ä½¿ç”¨æ–‡æœ¬æ•°æ®çš„å¼ºåŸŸå†…åˆå§‹åŒ–æ–¹æ³•ï¼Œè¡¨ç¤ºè´¨é‡æ˜¾è‘—æé«˜ã€‚ä¸ImageNeté¢„è®­ç»ƒç›¸æ¯”ï¼ŒConVIRTèƒ½ä»¥æ›´å°‘çš„æ ‡è®°æ•°æ®å®ç°åŒæ°´å¹³çš„åˆ†ç±»ç²¾åº¦ã€‚</strong></p><blockquote><p>Learning visual representations of medical images is core to medical image understanding but its progress has been held back by the small size of hand-labeled datasets. Existing work commonly relies on transferring weights from ImageNet pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. We propose an alternative unsupervised strategy to learn medical visual representations directly from the naturally occurring pairing of images and textual data. Our method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test our method by transferring our pretrained weights to 4 medical image classification tasks and 2 zero-shot retrieval tasks, and show that our method leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classification tasks, our method requires only 10% as much labeled training data as an ImageNet initialized counterpart to achieve better or comparable performance, demonstrating superior data efficiency.</p></blockquote><p>https://weibo.com/1402400261/JokrT9o4s</p><div class=pgc-img><img alt=çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6bf012fba2904c8cbafc6dad1079a51f><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/abba76e01694402ab0f7c62199bf09ff><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥) onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/20c8135a263d459a82df8efc234d43d8><p class=pgc-img-caption></p></div><p><br></p><p><strong>2ã€[CL]Autoregressive Entity Retrieval</strong></p><p>N D Cao, G Izacard, S Riedel, F Petroni</p><p>[University of Amsterdam & Facebook AI Research]</p><p><strong>è‡ªå›å½’å®ä½“æ£€ç´¢(GENRE)ï¼Œç”¨è‡ªå›å½’æ–¹å¼ç”Ÿæˆå®ä½“åï¼Œä»¥ä¸Šä¸‹æ–‡ä¸ºæ¡ä»¶ï¼Œé€šè¿‡ä»å·¦åˆ°å³é€è¯æ¡è‡ªå›å½’æ–¹å¼ç”Ÿæˆå®ä½“å”¯ä¸€åç§°æ¥æ£€ç´¢å®ä½“ã€‚è‡ªå›å½’å…è®¸ç›´æ¥æ•æ‰ä¸Šä¸‹æ–‡å’Œå®ä½“åç§°ä¹‹é—´çš„å…³ç³»ï¼Œæœ‰æ•ˆåœ°å¯¹ä¸¤è€…è¿›è¡Œäº¤å‰ç¼–ç ï¼›ç¼–è§£ç å™¨æ¶æ„çš„å‚æ•°éšè¯æ±‡è¡¨å¤§å°è€Œéå®ä½“æ•°é‡è€Œå˜åŒ–ï¼Œå¤§å¤§å‡å°‘äº†å†…å­˜å ç”¨ï¼›å¯æœ‰æ•ˆè®¡ç®—ç²¾ç¡®çš„è½¯æœ€å¤§æŸå¤±ï¼Œè€Œä¸éœ€è¦å¯¹è´Ÿæ•°æ®è¿›è¡Œå­é‡‡æ ·ã€‚</strong></p><blockquote><p>Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity information such as descriptions. This approach leads to several shortcomings: i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; ii) a large memory footprint is needed to store dense representations when considering large entity sets; iii) an appropriately hard set of negative data has to be subsampled at training time. We propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion, and conditioned on the context. This enables to mitigate the aforementioned technical issues: i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new SOTA, or very competitive results while using a tiny fraction of the memory of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name.</p></blockquote><div class=pgc-img><img alt=çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/70cc8bd78b324e17a7547137297644af><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c283709ae67e45d680ab6897b041d0fd><p class=pgc-img-caption></p></div><p><br></p><p><strong>3ã€[AI]Human-Level Performance in No-Press Diplomacy via Equilibrium Search</strong></p><p>J Gray, A Lerer, A Bakhtin, N Brown</p><p>[Facebook AI Research]</p><p><strong>ç”¨å‡è¡¡æœç´¢ç©ç»å…¸æ£‹ç±»æ¡Œæ¸¸ã€Šå¤–äº¤é£äº‘ã€‹è¾¾åˆ°äººç±»æ°´å¹³ï¼Œã€Šå¤–äº¤é£äº‘ã€‹æ˜¯ä¸ªå¤æ‚æ¸¸æˆï¼Œæ¶‰åŠåˆä½œä¸ç«äº‰ï¼Œå¯¹AIæŠ€æœ¯æå‡ºäº†é‡å¤§çš„ç†è®ºå’Œå®è·µæŒ‘æˆ˜ã€‚æ–°çš„AIæ™ºèƒ½ä½“é€šè¿‡å¯¹äººç±»æ•°æ®çš„ç›‘ç£å­¦ä¹ å¹¶ä½¿ç”¨å¤–éƒ¨é—æ†¾æœ€å°åŒ–çš„å•æ­¥å‰å‘æœç´¢ï¼Œåœ¨è¯¥æ¸¸æˆä¸­å®ç°äº†äººç±»æ°´å¹³æ€§èƒ½ã€‚ç”¨è¯¥æ™ºèƒ½ä½“åœ¨æµè¡Œçš„ã€Šå¤–äº¤é£äº‘ã€‹ç½‘ç«™ä¸ŠåŒ¿åç©æ¸¸æˆï¼Œåœ¨1128åäººç±»ç©å®¶ä¸­æ’åç¬¬23ã€‚</strong></p><blockquote><p>Prior AI breakthroughs in complex games have focused on either the purely adversarial or purely cooperative settings. In contrast, Diplomacy is a game of shifting alliances that involves both cooperation and competition. For this reason, Diplomacy has proven to be a formidable research challenge. In this paper we describe an agent for the no-press variant of Diplomacy that combines supervised learning on human data with one-step lookahead search via external regret minimization. External regret minimization techniques have been behind previous AI successes in adversarial games, most notably poker, but have not previously been shown to be successful in large-scale games involving cooperation. We show that our agent greatly exceeds the performance of past no-press Diplomacy bots, is unexploitable by expert humans, and achieves a rank of 23 out of 1,128 human players when playing anonymous games on a popular Diplomacy website.</p></blockquote><div class=pgc-img><img alt=çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/270c5c909cdb4c44a934e9c0df8f0a78><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4f7fd456f3364850afbdc71a85c5aca6><p class=pgc-img-caption></p></div><p><br></p><p><strong>4ã€[CL]CATBERT: Context-Aware Tiny BERT for Detecting Social Engineering Emails</strong></p><p>Y Lee, J Saxe, R Harang</p><p>[Sophos AI]</p><p><strong>ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¶…å°BERTæ£€æµ‹é’“é±¼é‚®ä»¶ï¼Œä¸ºäº†è¯†åˆ«é‚£äº›ä¸åŒ…å«æ¶æ„ä»£ç ã€ä¸ä¸å·²çŸ¥æ”»å‡»å…±äº«å•è¯é€‰æ‹©çš„æ‰‹å·¥ç¤¾ä¼šå·¥ç¨‹é‚®ä»¶ï¼Œé€šè¿‡å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„ã€å¤§è§„æ¨¡ä¿®å‰ªçš„BERTæ¨¡å‹ï¼ŒåŠ ä¸Šæ¥è‡ªé‚®ä»¶å¤´çš„é™„åŠ ä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œä»ä¸­å­¦ä¹ é‚®ä»¶å†…å®¹å’Œä¸Šä¸‹æ–‡ç‰¹æ€§é—´çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œå³ä½¿å­˜åœ¨æ•…æ„æ‹¼å†™é”™è¯¯å’Œæ¶æ„é€ƒé¿çš„çŠ¶å†µï¼Œä¹Ÿèƒ½æœ‰æ•ˆæ£€æµ‹å‡ºæœ‰é’ˆå¯¹æ€§çš„é’“é±¼é‚®ä»¶ã€‚è¯¥æ–¹æ³•ä¼˜äºç”¨é€‚é…å™¨å’Œä¸Šä¸‹æ–‡å±‚çš„å¼ºåŸºçº¿æ¨¡å‹ï¼Œé’“é±¼é‚®ä»¶æ£€å‡ºç‡è¾¾åˆ°87%ã€‚</strong></p><blockquote><p>Targeted phishing emails are on the rise and facilitate the theft of billions of dollars from organizations a year. While malicious signals from attached files or malicious URLs in emails can be detected by conventional malware signatures or machine learning technologies, it is challenging to identify hand-crafted social engineering emails which don't contain any malicious code and don't share word choices with known attacks. To tackle this problem, we fine-tune a pre-trained BERT model by replacing the half of Transformer blocks with simple adapters to efficiently learn sophisticated representations of the syntax and semantics of the natural language. Our Context-Aware network also learns the context representations between email's content and context features from email headers. Our CatBERT(Context-Aware Tiny Bert) achieves a 87% detection rate as compared to DistilBERT, LSTM, and logistic regression baselines which achieve 83%, 79%, and 54% detection rates at false positive rates of 1%, respectively. Our model is also faster than competing transformer approaches and is resilient to adversarial attacks which deliberately replace keywords with typos or synonyms.</p></blockquote><div class=pgc-img><img alt=çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b7caec640ef945f9a9811c7f39653da5><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/704ec79ab0bc4a3ab1d91318d20331a7><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6dd5d2d9bcea48f89cb21f7510af24f7><p class=pgc-img-caption></p></div><p><br></p><p><strong>5ã€[CL]WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization</strong></p><p>F Ladhak, E Durmus, C Cardie, K McKeown</p><p>[Columbia University & Cornell University]</p><p><strong>è·¨è¯­ç§æŠ½è±¡æ‘˜è¦æ–°åŸºå‡†WikiLinguaï¼Œä¸€ä¸ªè·¨è¯­è¨€å’Œå¤šè¯­è¨€æŠ½è±¡æ‘˜è¦çš„åŸºå‡†æ•°æ®é›†ï¼Œä»WikiHowä¸­æå–äº†18ç§è¯­è¨€çš„æ–‡ç« å’Œæ‘˜è¦å¯¹ï¼ŒWikiHowæ˜¯ä¸ªé«˜è´¨é‡çš„åä½œèµ„æºï¼Œæä¾›äº†äººå·¥æ’°å†™çš„ä¸€ç³»åˆ—ä¸åŒä¸»é¢˜çš„æ“ä½œæŒ‡å—ã€‚é€šè¿‡å¯¹é½æ–‡ç« ä¸­ç”¨äºæè¿°æ¯ä¸ªhow-toæ­¥éª¤çš„å›¾åƒï¼Œåˆ›å»ºäº†è·¨è¯­è¨€çš„é‡‘æ ‡å‡†æ–‡ç« -æ‘˜è¦å¯¹é½ã€‚</strong></p><blockquote><p>We introduce WikiLingua, a large-scale, multilingual dataset for the evaluation of crosslingual abstractive summarization systems. We extract article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. We create gold-standard article-summary alignments across languages by aligning the images that are used to describe each how-to step in an article. As a set of baselines for further studies, we evaluate the performance of existing cross-lingual abstractive summarization methods on our dataset. We further propose a method for direct crosslingual summarization (i.e., without requiring translation at inference time) by leveraging synthetic data and Neural Machine Translation as a pre-training step. Our method significantly outperforms the baseline approaches, while being more cost efficient during inference.</p></blockquote><p>https://weibo.com/1402400261/JokOvyCDJ</p><div class=pgc-img><img alt=çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6a010acb47e847f3bd332d7818936a00><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=çˆ±å¯å¯AIè®ºæ–‡æ¨ä»‹(10æœˆ9æ—¥) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d5daad8a4f514f5b9e01f10f02c33fdd><p class=pgc-img-caption></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'AI','è®ºæ–‡','10'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=æœç´¢>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>ğŸ”</button></form></section><section class=widget><h3 class=widget-title>æœ€æ–°æ–‡ç«  âš¡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>å…¶ä»–</h3><ul class=widget-list><li><a href=TOS.html>ä½¿ç”¨æ¢æ¬¾</a></li><li><a href=CommentPolicy.html>ç•™è¨€æ”¿ç­–</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>è¯çµ¡æˆ‘å€‘</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>æå®¢å¿«è¨Š</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>