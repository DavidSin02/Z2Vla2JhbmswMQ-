<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>设计数据密集型应用 第一部分第一节 可靠性，可扩展性和可维护性 | 极客快訊</title><meta property="og:title" content="设计数据密集型应用 第一部分第一节 可靠性，可扩展性和可维护性 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/1536400464147264a5443a5"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/547fec10.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/547fec10.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/547fec10.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/547fec10.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/547fec10.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/547fec10.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/547fec10.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/547fec10.html><meta property="article:published_time" content="2020-11-14T21:01:42+08:00"><meta property="article:modified_time" content="2020-11-14T21:01:42+08:00"><meta name=Keywords content><meta name=description content="设计数据密集型应用 第一部分第一节 可靠性，可扩展性和可维护性"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/547fec10.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>设计数据密集型应用 第一部分第一节 可靠性，可扩展性和可维护性</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p>互联网发展的如此之好，以至于很多人把它当作像太平洋一般的自然资源，而非人造事物。上一次出现造成这种错觉的规模技术是什么时候？ —Alan Kay, in interview with Dr Dobb’s Journal (2012)</p><p>现在很多应用是数据密集型的，与计算密集型应用不同，这些应用的限制因素很少是因为CPU能力，而通常是数据体量、数据复杂度以及数据变化速度。</p><p>数据密集型应用通常由提供通用功能的标准组件（standard building blocks）构建。例如，很多应用需要：</p><ul><li>存储数据以方便本应用及其他应用后续查找数据（数据库）</li><li>记录复杂操作的结果以加速读（缓存）</li><li>允许用户按照关键词查询或者以多种方式进行筛选（查询索引）</li><li>定期将大量累积数据碎片化（批处理）</li><li>如果这些听起来令人痛苦，那是因为数据系统是如此成功的一种抽象：我们一直在使用它却很少思考它。当构建应用的时候，大部分工程师不会试图去自己写一个全新的数据存储引擎，因为数据库可以很好的完成工作。</li><li>但是实际上事情并没有这么简单，因为不同的应用满足不同的需求，各种数据库系统有不同的特征。缓存的方式是多种多样的，创建查询索引的方法是多种多样的，诸如此类。在构建应用的时候，我们任然需要去决策哪些工具，哪些策略对于我们现在所处理的工作是最适合的。而且当单一工具无法完成我们的工作的时候，组合运用多种工具也并非易事。</li><li>在本节，我们以想要达到的目标开始进行探索：可靠，可扩展，可维护数据系统。我们将会澄清这些概念的意义，整理理解这些概念的方法，为后面的章节打下基础。在后面的章节中将会层层展开，看一下在构建数据密集型应用的时候，哪些因素是需要考虑的。</li></ul><h2>关于数据系统</h2><p>我们通常认为数据库、消息队列、缓存等是非常不同的几种工具。尽管一个数据库和一个消息队列一列非常重要的相似点，两者都存储一段时间的数据，但是他们拥有截然不同的访问模式，这意味着不同的性能特征和截然不同的应用场景。</p><p>那么为什么我们还把他们放在同一种概念中进行考察呢，比如说数据库系统？</p><p>近些年出现了很多新的数据存储和运算工具。它们针对一些特殊的一定用场景进行了优化，我们也很难将它们整齐地归纳到传统的类别中。比如有些数据存储工具经常被当作消息队列使用（Redis），也有些消息队列具有数据库持久化能力（Kafka）。不同种类之间的界限正在逐渐变得模糊化。</p><p>其次，越来越多应用需要满足各种维度的需求，单一的工具已经无法满足它们的数据计算和存储需要，所以，应用中的任务会被分解成为各个task分发到对应的单一工具中执行解决，这些工具通过应用编号彼此链接。</p><p>比如，如果一个应用拥有一个它管理的缓存层（用Memcached或者其他工具实现），或者有一个全文本检索服务（比如Elasticsearch或者Solr）部署在应用的主存储中，那么通常来讲，应用编码（application code）的责任就是去保证缓存和索引与主存储是同步的。图1-1展示了这种关系的示意图（在后续的章节中会详细讲述）。</p><div class=pgc-img><img alt="设计数据密集型应用 第一部分第一节 可靠性，可扩展性和可维护性" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1536400464147264a5443a5><p class=pgc-img-caption></p></div><p>图1-1. 一种包含多种总监的可用数据系统架构</p><p>当你组合各种工具去构建一种服务时，服务接口或者应用程序接口（API）通常隐藏在客户端的实现细节中。现在你本质上是基于通用的更小组件构建了一个全新的，具有特定用途的数据系统。你的复合数据系统可能能够提供特性的保证，比如说能够在数据写入过程中能够有效清理或者升级缓存，以保证外界客户端能够读取到一致性的结果。那么你现在就不再仅仅是一个应用开发者，也是一个数据系统设计者。</p><p>如果你正在设计一个数据系统或者一种数据服务，那么你将面临很多小问题的来临。你如何在正常或者内在错误发生情况下保证数据的正确性和完整性，如何为客户端持续提供良好的性能，面对负载的增加你该采取怎样的扩展策略，在服务端一个好的API是如何定义的？</p><p>有很多因素会影响数据系统觉得设计，包括相关人员的技术能力和相关经验，遗留系统觉得依赖性，交付的时间周期，你所在组织对于各种各样风险的承受能力，监督约束条件等等。这些因素变动不居，非常依赖于具体情势。</p><p>在本书中，我们主要关注在大多数软件系中都非常重要的三个因子。</p><p>可靠性</p><p>系统应该持续正确工作（以期望觉得性能水平正确执行功能），即使在灾难中（硬件或者软件错误，甚至是认为错误）也能保持如此。</p><pre>可扩展性 应该能够合理地处理系统的增长（数据量级，交互量级和复杂度）。</pre><p>可维护性</p><pre> 随着应用运行时间的增长，不同的人群会在系统上工作（工程师和操作员，两者均能够保持当前的操作并将系统应用于新的应用场景），系统要能够保证他们工作的有效性。</pre><p>这些概念在并没有明确其含义的情况下已经广为传播。我们将会在接下来的章节中，以工程师的视角来探索可靠性，可扩展性和可维护性的意义。然后，我们会能够实现这些目标的不同技术，架构和算法进行说明。</p><h2>可靠性</h2><p>每个人对于可靠和不可靠都有自己主观的认知。在软件领域，可靠通常包括：</p><ul><li>应用以用户期望的方式运行</li><li>应用能够容忍用户犯错误或者以错误的方式使用软件</li><li>在期望的数据量级和负载下，应用能够在需要的场景中拥有足够好的性能表现</li><li>系统能够阻止未获认证的访问和攻击</li><li>如果这些加在一起意味着正确工作的话，那么我们可以大致将可靠性理解为“持续正确工作，即使情势不佳”。</li><li>“情势不佳”就是我们通常所说的错误，能够预知错误并且能够处理错误的系统我们称之为容错的（fault-tolerant）或者有弹性的（resilient）。以上的描述会引起一些误解：它意味着我们可以创造系统能够容忍的任何错误，这在实际上是不可能的。如果整个地球被黑洞吞噬了，对这种错误的容忍性需要将主机部署在太空，那么首先你要足够幸运能够获得项目的预算。所以将容忍性局限在特定类型错误上进行讨论才是有意义的。</li><li>需要说明的是，错误（fault）并不等同于失败（failure）。错误通常意味着系统的某些组件偏离了它正常的运行轨道，而失败则意味着系统停止对用户提供他们期望的服务。将错误降低到0是不可能的，所以最好设计容错机制组织错误导致失败。本书讲了几种基于不可靠组件构建可靠系统的技术。</li><li>与常识相反，在这样的容错系统中，通过故意触发提高错误率是有意义的，比如在没有警告的情况下随机杀掉系统的进程。许多致命的bug都是因为错误的处理机制导致的。通过故意触发错误，你可以确保容错机制被不断的训练和测试，这有助于提高你对错误自然发生时应用正确处理能力的自信心。Netflix Chaos Monkey正是采取这种方法的事迹案例。</li><li>尽管相比阻止错误发生，我们更经常采用提高容错能力的方法，但是在有些情况下，阻止比治愈更好（因为有些情况下根本没有治愈这一说）。在关于安全的领域内，阻止明显是更好的方法，比如说，如果一个attacker 侵入了一个系统并且获得了敏感数据，这个事情便是不可挽回的。但是如下面章节所讲，本书提及的大多数是可以治愈的错误。</li></ul><h4>硬件错误</h4><p>当提到系统失败原因的时候，首先浮出脑海的便是硬件错误。硬盘奔溃，内存错误，电源损坏，网线错拔等。凡是接触过大型数据中心的工作人员会告诉你，只要是你拥有大量的设备，这些问题几乎每时每刻都在发生。</p><p>据报道，硬盘的平均出故障时间（MTTF）大约是10到50年。所以如果有一存储集群拥有10000块硬盘，那么我们可以推测这个集群平均每天会有一块硬盘挂掉。</p><p>对于硬件错误，首先想到的策略是对硬件组件做冗余备份，从而降低硬件错误造成系统失败的可能性。硬盘可以做RAID配置，服务器可以做双电源和热插拔CPU，数据中心可能有用于做备用电源的电池和柴油发动机。当一个组件挂掉，备用组件可以替代它继续工作。这种方法不能完全阻止硬件问题导致系统失败，但是容易理解而且通常可以保持一台设备运行数年而不宕机。</p><p>直到现在，硬件组件备份在大多数应用中都是有效且意义重大的，因为它使得单机奔溃的可能性极小。只要您能够相当快地将备份恢复到一台新机器上，在大多数应用程序中，故障发生的停机时间都不是灾难性的。因此，只有那些高可用性要求较高的应用才需要做多机器冗余备份。</p><p>但是，随着数据量级和计算能力要求的提高，越来越多的应用已经开始用大量的机器，这显而易见也就增加了硬件错误出现的机率。此外，在一些像Amazon Web Services (AWS) 一样的云平台，没有任何的警示的情况下一台云主机就不可用了的情况是非常常见的，因为平台首先考虑的是灵活性和伸缩性，而非单台机器的可靠性。</p><p>因此有一种向可以容忍机器丢失的系统的转变，这种系统通过使用软件容错技术或者在硬件备份的基础上使用软件容错技术来实现。这种系统也具有明显的有时：单机系统在重启（如打安全补丁）的时候需要计划停机，但是在可以容忍机器错误的系统可以每次为一台设备打补丁，从而不用将整个系统停机（见第四节）。</p><h4>软件错误</h4><p>我们通常认为硬件错误是随机的，设备与设备之间也是相互独立的：一台设备硬盘问题并不意味着其他设备的硬盘也出现问题。他们之间也有可能存在微弱的联系（比如因为机架温度等常见原因），但是大规模硬件组件同时挂掉也是不大可能的。</p><p>另一种错误是系统内在的系统性错误。这种错误更难预测，因为它们是跨节点的，它们也比硬件错误导致更多的系统失败。实例包括：</p><ul><li>当给定一个特定的异常输入时，一个软件bug会导致每个应用服务实例奔溃。比如在2012年6月30日，linux内核将时钟额外增加一闰秒导致很多应用同时挂起。</li><li>一个跑飞的程序用尽了共享资源——CPU时间，内存，硬盘空间或者带宽。</li><li>系统依赖的服务变慢，无响应或者返回异常结果。</li><li>级联错误，某一个组件的小错误引发另一个组件的错误，进而触发更多的错误。</li><li>能够导致这些软件错误的bug通常会存活很长时间直到被异常场景触发。在这些场景下，软件试图对环境做出某种猜测——正常情况下猜测应该时正确的，但是因为某些原因，它猜错了。</li><li>软件的系统性错误并没有快速的解决办法。很多小技巧可能能够提供帮助：深思熟虑系统做出的各种假设和组件间的相互作用；严格进行软件测试；进程隔离；允许进程奔溃和重启；测量、检测和分析生产过程中的系统行为。如果一个系统要提供一些保证（比如，在消息队列中，入队和出队的数据量应该是一致的），那么它应该能够在运行过程中做到自我监测，当发现异常时能够发出警告。</li></ul><h4>人为错误</h4><p>人类设计和构建了软件系统，同时人类也是保持系统持续运行的操作者。但是人类从来都不是十分靠谱的，例如据统计，人为配置错误是导致断电事故的主要原因，而硬件错误（服务器或者网络）只占了断电事故原因的10-25%。</p><p>尽管人类是不可靠的，我们可以采取以下手段来保证系统的可靠性。</p><ul><li>以最小化错误出现概率的方法设计系统。例如，良好的抽象，API和管理接口降低做正确事情的难度而提高做错误事情的难度。但是如果接口限制过多，人们会刻意避免使用它而对它的优点视而不见，所以如何在易用性和保证正确性之间取得平衡也并非易事。</li><li>将人们容易犯错的环境和容易导致失败的环境解耦。可以为人们提供拥有系统完整特性的非生产沙箱环境，人们可以在沙箱环境中安全地进行开发和测试，使用真实的数据却不影响真实的用户。</li><li>从单元测试到全系统集成测试和手工测试，在所有的层级全方位测试。自动化测试被广泛使用，很好理解，对于在正常操作中很少出现的边界案例尤其具有价值。</li><li>允许从人为错误中快速而容易地恢复，以尽量减少失败的影响。例如，快速地回滚配置更改，逐步推出新代码（这样，任何意外的bug都会只影响到一小部分用户），并提供重新计算数据的工具（以防旧的计算结果是不正确的）。</li><li>建立详细和清晰的监视，例如性能指标和错误率。在其他工程学科中，这被称为遥测技术。（一旦火箭离开地面，遥测技术对于追踪正在发生的事情和理解失败是至关重要的。）监控可以向我们显示早期预警信号，并允许我们检查是否有任何假设或约束被违反了。当出现问题时，度量标准对于诊断问题是非常有价值的。</li><li>进行良好的管理实践和培训——这是一个复杂而重要的方面，超出了本书的范围。</li></ul><h4>可靠性的重要性</h4><p>可靠性不仅适用于核电站和空中交通控制软件——普通的应用程序也应该可靠地工作。业务应用程序中的bug会导致生产力的损失（如果信息被错误地报告，则会带来法律风险），而电子商务网站的中断可能会造成收入损失和声誉受损，成本巨大。</p><p>即使在“非关键”应用程序中，我们也有责任保证应用是可靠的。想象以下，一个家长在你的照片应用程序中存储了他们孩子的所有照片和视频。如果那个数据库突然损坏了，他们会有什么感觉？他们知道如何从备份中恢复它吗？</p><p>有些情况下,我们可能会选择牺牲可靠性以减少开发成本(例如,为未知市场开发一个原型产品)或运营成本(例如,一个利润率非常小的服务)，但是我们应该对于这种偷工减料的做法保持谨慎和警醒。</p><h2>可扩展性</h2><p>即使系统今天可靠地运行，也并不代表未来它会一直稳定运行。系统退化的一个比较常见的原因便是负载变重：也许并发用户量从10000增长到100000，或者从一百万增长到一千万，也许程序处理的数据量猛增。</p><p>我们用扩展性来描述一个系统处理递增负载的能力。但是，不得不提的是扩展性并不是一种单维的指标，X可扩展或者Y可扩展的说法是没有意义的。扩展性意味着我们对以下问题进行思考，如果系统以一种特定的方式增长，我们应该如何应对这种增长，或者，我们应该如何增加计算资源去应对额外的负载。</p><h4>关于负载</h4><p>首先，我们应该量化系统负载的现状，只有这样讨论负载增长才有意义（如果我们的负载翻倍会发生什么？）。负载可以使用集中负载参数(load parameters)来进行量化。最优负载参数取决于系统的架构：它可能WEB服务器每秒的请求数、数据库中的读写比率、聊天室中同时活跃用户量、缓存命中率或其他东西。也许平均值对你来说是最关注的参数，业务少量的边界值对你来说又至关重要。</p><p>为了更加具体的描述这些概念，我们拿Twitter为例，使用2012年11月发布的数据。Twitter包含两个主要业务：</p><ul><li>发布tweet消息</li><li>用户可以向其追随者发布一条新消息（平均4.6 k请求/秒，峰值超过12 k请求/秒）。</li><li>访问时间线主页</li><li>用户可以查看他们关注的人所发布的推文（300 k请求/秒）。</li></ul><p>每秒12000个简易写操作（发布tweet的峰值速率）是相当容易的。然而，Twitter的规模挑战并不主要是由于推文的数量，而是由于粉丝的数量——每个用户都有很多粉丝，这些用户同时又是很多人的粉丝。事先这两种操作大致有两种方式：</p><p>1、发布tweet的时候将消息错处到一个全局的tweet集合中。当一个用户请求访问时间线主页时，遍历所有他关注的人并且找到他们每个人发布的所有tweets，对这些tweet进行合并（按照时间进行排序）。在如图1-2所示的关系数据库中，你可以编写如下的一个查询：</p><pre>SELECT tweets.*, users.* FROM tweets JOIN users ON tweets.sender_id = users.id JOIN follows ON follows.followee_id = users.id WHERE follows.follower_id = current_user</pre><p>2、为每个用户的时间线主页保存缓存，就好像是每个tweeter用户都拥有一个邮箱（如图一1-3）。当一个用户发布一个tweet的时候，遍历所有关注这个用户的人，并且将新发布的tweet放入关注者的缓存中。这样的话，浏览时间线主页的成本就会降低，因为结果在浏览之前就已经计算好了。</p><div class=pgc-img><img alt="设计数据密集型应用 第一部分第一节 可靠性，可扩展性和可维护性" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/153640046434467b5891b00><p class=pgc-img-caption></p></div><p>图1-2：实现主页存储的简易表结构</p><div class=pgc-img><img alt="设计数据密集型应用 第一部分第一节 可靠性，可扩展性和可维护性" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1536400464086de55fbc4bc><p class=pgc-img-caption></p></div><p>图1-3：通过消息队列将tweet推送给粉丝</p><p>Twitter的第一个版本采用方法1，但是系统难以跟上时间线主页查询的负载，所以公司转向了方法2。这样做的效果更好，因为发布tweet的平均速率比主页读取速度要低两个数量级，在这种情况下，应该将更多的操作放到写操作中而简化读操作。</p><p>但是第二种方法的弊端是，当发布一条tweet的时候需要太多的额外工作量。平均情况下，一条tweet会被送往75个粉丝那里，所以4.6k/s的tweet缓存写入量被扩大为345k/s的写入量。而且，这种平均值也隐藏了一个事实，即每个用户的追随者数量差别很大，有些用户甚至拥有300万粉丝。这就意味着一条tweet发布有可能会导致300万次写操作。在规定的时间内（tweeter尝试在5s内将tweet传送给粉丝）完成这项工作挑战巨大。</p><p>在Twitter的例子中，一个用户的粉丝分布情况是可扩展性关键的负载参数（可能根据用户的推文频率来衡量），因为它决定了粉丝量负载。你的应用程序可能拥有截然不同的特性，但是同样可以应用类似的原理堆负载进行推理。</p><p>Twitter实例的最后一个发展是：既然方法2得到了有效实施，Twitter正在促进两种方法的融合。在发布推文时，大多数用户的推文仍然会传递到粉丝的时间线主页，但是如果一个用户拥有庞大的粉丝群（例如名人），则不会对他的消息采取这种策略。用户关注的任何名人的推文都是单独获取的，在该用户获取名人推文的时候，推文会如方法1一样和该用户的时间轴主页进行合并。这种混合方法能够提供持续良好的性能。在我们讨论了更多基础技术之后，我们将在第12章重新讨论这个例子。</p><h4>关于性能</h4><p>一旦你能够量化负载，就可以研究负载增大对系统的影响。你可以从两方面看待这个问题。</p><ul><li>保持系统资源不变（CPU,内存，网络带宽等）的前提下增加一个负载参数，看系统性能会如何发生变化。</li><li>当一个负载参数增加后，应该如何系统资源才能保证性能不受影响。</li><li>这两个方面都需要考察性能数据，那么我们首先来简单描述一下系统性能。</li><li>在像Hadoop这样的批处理系统中，我们通常关注吞吐量（throughput），也就是每秒可以处理记录数或者固定数据集所需的处理时间。而在线系统更加看重的相应时间，也就是客户端发送请求和得到回应之间的时间差。</li></ul><pre>延迟和响应时间通常是同义的，但是它们并不相同。响应时间是从客户端来看的：除了处理请求的实际时间（服务时间）之外，它还包括网络延迟和排队延迟。延迟是请求等待处理的持续时间，在等待的时候并没有进行请求处理。</pre><p>即使一次次发送同样的请求，每次请求的响应时间也会有微弱的差异。实际上，一个处理大量请求的系统中，响应时间有很大的差别。所以我们不能把响应时间当做是单一的数值，而要看做可度量的值的分布。</p><p>在图1-4中，每一个灰色的柱子代表一个服务的响应，柱子的高度代表相应时间的长短。图中大部分的请求响应很快，但是也存在一些响应较慢的离群值。也许请求缓慢是因为他们本质上所需的时间就长，比如说因为他们需要处理更多的数据。但是即使在一个你认为响应时间应该一致的环境下，响应时间依然会有差异：当后台进程切换的时候回引入随机附加延迟，网络数据包和TCP传输成本，垃圾回收引起的暂停，一个页面错误引起的磁盘读操作，服务器机架的机械振动等，或许还有更多其他的原因。</p><div class=pgc-img><img alt="设计数据密集型应用 第一部分第一节 可靠性，可扩展性和可维护性" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153640046417933a231997b><p class=pgc-img-caption></p></div><p>图1-4：100个服务请求的响应时间实例</p><p>服务的平均响应时间经常被提及（严格地说，“平均值”这个词并不指代任何特定的公式，但在实践中，它通常被理解为算术平均数：n个值之和除以n）。但是，如果你想知道服务的“典型”响应时间，那么平均值并不是一个很好的度量标准，因为它并不能告诉你有多少用户经历了这种延迟。</p><p>用分位数来描述会更好一些。如果你讲响应时间按照从快到慢进行排序，那么中间的数值就是中位数：例如，如果中位响应时间是200ms，那么就意味着一半的响应时间少于200ms，一半的响应时间高于200ms。</p><p>如果你想知道用户“典型”的等待时间，中位数是一个很好的参考标准：一半的用户需要等待比中位数更短的时间，其他的一半则需要等待比中位数更长的时间。中位数也称作50分位，简称p50。需要注意的是，中位数指的是单个请求，如果用户提出了多个请求（在会话过程中，或者一个页面包含了多个资源），那么这些请求中至少有一个有很大概率会慢于中位数。</p><p>为了描述哪些离散点到底有多糟糕，你可以查看更高的分位数：通常用95分位，99分位或者99.9分位（简称p95,p99和p999）。它们分别表示95%，99%和99.9%的响应时间快于的数值。比如说，95分位数对应的响应时间是1.5s，那么就意味着100个请求中有95个请求响应时间快于1.5s，5个请求耗费了1.5s或者更多。这些概念在图1-4中也有体现。</p><p>尾部延迟，也就是高分位数对应的相应时间是很重要的，因为它们直接影响用户对服务的体验。例如，Amazon以99.9分位数来描述内部服务的响应时间需求，即使它只影响1,000个请求中的1个。这是因为那些拥有最慢请求的客户通常是账户中数据量最大的用户，即购买了很多次东西的用户，也就是说，他们是最有价值的客户。通过提高他们浏览网页的速度来获得他们的满意是非常重要的，亚马逊还发现，响应时间每增加100 ms，销售额就会减少1%，另有报道称，1秒的放缓会导致客户满意度指标降低16%。</p><p>另一方面，优化99.99分位数（10000个请求中最慢的那一个）对于亚马逊来说确实成本太高了，而且也并不能带来太高的经济收益。降低高分位数对应的响应时间非常困难，因为它很容易被超出你控制范围之外的随机事件所影响，而且收益也不高。</p><p>比如说，百分位数通常用于服务水平目标（SLOs）和服务水平协议（SLAs），这些约定定义了服务的预期性能和可用性。一个SLA可能意味着，如果一个服务平均响应时间低于200ms，99分位数低于1s就认为这个服务是好的（如果响应时间较长，则可能意味着这个服务是差的），那么一个服务至少在99.9%的时间内是好的。这些参数为客户端用户设定了期望，当SLA并没有满足他们的期望时，他们便可以提出退款。</p><p>在高百分位的长响应时间中，排队延迟通常占了很大的一部分。如果一个服务只可以并发处理少量的事情（比如说因为CPU核数的限制），它只需要少量的缓慢请求来支撑起后续请求的处理，这种效果有事被称为“线头阻塞”（head-of-line blocking）。即使后续请求在服务器上快速得以处理，由于要等待线头响应完成，客户端看到的也只是一个缓慢的总体响应时间。由于这种现象的存在，在客户端测量响应时间是非常重要的。</p><p>当为了测试系统的可伸缩性而人为生成负载时，生成负载的客户端不需考虑响应时间而只管不停的发送请求即可。如果客户端等待一个请求相应完成后再发送另外一个请求的话，就会得到比实际情况更短的请求响应时间，这会扭曲测量的结果。</p><pre> 分位数实战  高分位数在后端服务中尤为重要，作为服务单终端用户请求的一部分，这些服务会被多次调用。即使并发调用，客户仍然需要等待并发请求中响应最慢的那一个完成。只要存在一个缓慢调用就会让终端用户的请求变慢。如图1-5所示，如果终端用户发出多个后台调用，即使只有一小部分调用较慢，也有很大的机率造成慢调用，因此更大比例的终端用户获得慢调用响应（通常称为尾部延迟放大）。 如果你想要将响应时间分位数添加到服务的监测仪表板中，你需要持续有效地计算这些分位数。例如，你可能想要做一个滚动窗口滚动显示最近10分钟的请求响应时间。每一分钟你都需要计算窗口中响应时间的中位数和其他的分位数并绘制在仪表盘上。 比较低级的实现方法是在时间窗口中保存所有请求的响应时间列表，并每分钟对该列表进行排序。如果觉得这种方法效率太低，有一些算法可以在最小的CPU和内存成本下计算出分位数的近似值，比如 forward decay [25]，t-digest[26]，或者HdrHistogram[27]。需要注意的是，百分位平均化，如减小时间分辨率或将不同机器的数据组合在一起，在数学上是没有意义的——聚合响应时间数据的正确方法是添加直方图[28]。 </pre><div class=pgc-img><img alt="设计数据密集型应用 第一部分第一节 可靠性，可扩展性和可维护性" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15364004641357d609c69ec><p class=pgc-img-caption></p></div><p>图1-5：当有多个后台服务响应一个请求时，只要一个后台服务响应较慢就会让终端用户的请求变慢</p><h4>处理负载的方法</h4><p>既然我们已经讨论了描述负载和度量性能的参数，现在我们可以认真地讨论可扩展性了：当负载参数增加时如何保持良好的性能。</p><p>一种能够良好应对某一等级负载的架构很难应对10倍的负载。如果你正在开发一个快速增长的服务，那么在每一次负载增加一个量级的时候你可能要重新考虑你的架构，或者更频繁地考虑它。</p><p>人们经常谈论在纵向扩展（垂直扩展，移动到更强大的机器）和横向扩展（水平扩展，在多个较小的机器上分配负载）进行取舍。在多台机器上分配负载也称为无共享（shared-nothing）体系结构。单机系统往往容易管理，但是高端的机器往往又比较昂贵，所以一个负载较重的系统往往不可避免地要进行横向扩展。实际上，一个好的结构通常会将两者有效地结合起来：例如，使用几个高性能的机器仍然比大量的小型虚拟机更简单、成本更低。</p><p>有一些系统是弹性的，也就是说当检测到负载增加时系统能够自动增加计算资源，而另外一些系统需要手动实现可扩展性（人为地分析系统负载能力并为集群增加更多的机器）。如果负载变化很难预测，弹性系统会更加实用，但是手动进行系统扩展更加简单，操作彩蛋也会更少（见“分区再平衡”）。</p><p>虽然跨机器分发无状态服务相当简单，但是有状态数据系统从单机部署迁入到分布式部署可能会带来许多额外的复杂性。因此，至今常见的做法是将数据库保存在单个节点上（纵向扩展），直到扩展成本或者高可用性要求迫使不得不进行分布式部署。</p><p>随着分布式系统工具和抽象化的进步，这种常见的做法可能会有所改变，至少对有些类别的应用来说如此。可以预见，未来即使在不需要处理大规模数据或流量的场景下，分布式数据系统也会成为默认的部署配置。在本书的余下内容中，我们会对几种分布式系统进行研究，并在它们的可扩展性、易用性和可维护性方面进行讨论。</p><p>并没有一种通用的，万能的可扩展性框架（俗称万能胶水），那些大规模运行的系统，框架往往是针对应用而深度定制的。架构面临的问题可能包括数据读的量级，数据写的量级，数据存储量级，数据复杂度，响应性能要求，访问模式等，或者将这些问题加起来再加上其他的更多问题。</p><p>例如，一个被设计用来每秒处理100000条请求，每个请求1kb的系统与每分钟处理3个请求，每个请求2GB的系统是截然不同的，即使两个系统拥有相同的数据吞吐量。</p><p>对于特定的应用程序来说，一个强扩展性系统是围绕着常见操作和罕见操作的假设创建的（也就是负载参数）。如果这些假设是错误的，所有为扩展性所做的努力全部都会付之东流，甚至会适得其反。在早期的创业公司或未经验证的产品中，通常更重要的是能够快速地迭代产品特性，而不是基于对未来负载的假设进行扩展（过度设计）。</p><p>虽然是特定于目标应用的，但可扩展架构通常由通用模块构建，采用常见的排列模式。在这本书中我们队这些模块和模式进行了讨论。</p><h2>可维护性</h2><p>众所周知，软件成本不仅包括最初的开发成本，还包括运维成本——解决bug，保证系统的可操作性，定位失败原因，平台移植，适配新的应用场景，偿还技术负债和添加新特性。</p><p>但不幸的是，许多软件从业人员不喜欢维护所谓的遗留系统——修复别人的bug，在过时的平台上工作或者被逼迫做自己不愿意做的事情。每个遗留系统都有自己让人不爽的地方，所以很难给出通用的处理建议。</p><p>但是我们应该以最小化运维痛苦的方法设计软件，并且避免自己创建遗留系统。最后，我们需要格外留意软件系统的三个设计原则：</p><p>可操作性</p><pre>让操作团队更容易保持系统的稳定运行。</pre><p>简易性</p><pre>让新的开发者易于理解系统，尽可能的降低系统的复杂度（需要注意的是，这并不同于用户界面的简易性）。</pre><p>可演化性</p><pre>让开发者未来容易改进系统，当需求变动时系统能够适应未知场景。又称为可扩展性，可修改性或者可塑性。</pre><p>就像以前的可靠性和可伸缩性一样，并没有简单的方案可以达到这些目标。但是，我们将尝试思考具有可操作性、简单性和可演化性的系统。</p><h4>可操作性：让操作变得容易</h4><p>有人曾说“好的操作通常可以绕过坏（或不完整）软件的限制，但是好的软件不能在糟糕的操作中可靠地运行”。虽然操作的某些方面可以而且应该是自动化的，但是对于人来说，构建自动化并保证它正确的运行是非常重要的。</p><p>操作团队对于保持系统稳定运行至关重要。一个好的操作团队需要对以下事情（或者更多）负责：</p><ul><li>监测系统的健康状况，当系统状态变差时快速地重构服务。</li><li>跟踪问题原因，比如系统失败或者性能下降。</li><li>保持软件平台升级，包括安全补丁。</li><li>密切关注不同系统之间的交互影响，这样就可以在造成危害之前扼杀一个有问题的修改。</li><li>对未来问题进行预测并在发生之前就消灭它们（比如容量规划）。</li><li>构建用于部署、配置管理等功能的优秀实践和工具。</li><li>执行复杂的运维任务，例如将应用程序从一个平台移植到另一个平台。</li><li>当配置修改时保证系统的安全性。</li><li>设定操作规范化流程，并帮助保持生产环境的稳定。</li><li>维持组织对系统的了解，即使有人员变更。</li><li>好的操作性意味着让日常事务变得简单，让操作团队专注于更有价值的活动。数据系统可以做各种各样的事情简化日常任务，包括：</li><li>通过良好的监测，将系统运行是行为和内部机制可视化。</li><li>为自动化和标准工具集成提供良好的支持。</li><li>避免对单机的依赖（允许系统在作为一个整体运行时关机）。</li><li>提供良好的文档和易于理解的操作指南（如果我做了X，Y就会发生）。</li><li>提供良好的默认行为，但是同时给与管理员在必要时修改默认行为的权限。</li><li>最好有自愈机制，但同时给与管理员在必要时控制系统状态的权限。</li><li>引导操作流程，减少彩蛋。</li></ul><h4>简易性：管理复杂度</h4><p>小软件的代码可以清爽简洁，但是随着工程变大，代码通常会变得复杂而难以理解。这种复杂度降低了每个开发者的效率，从而进一步增加维护成本。一个陷入复杂度泥潭的软件工程有时被称作“大泥团”（big ball of mud）。</p><p>复杂性有各种各样的可能的征兆：状态空间爆炸，模块之间紧耦合，复杂的依赖关系，不一致的命名和术语，旨在解决性能问题的黑客攻击，在其他地方解决问题的外挂等。这个话题已经被很多人广泛讨论了[31,32,33]。</p><p>当复杂性使维护变得困难时，预算和时间表就变得不再明朗。复杂度高的软件，在进行修改时引入bug的风险也更大：当系统变得更加难以理解和推理，我们往往就会忽略隐藏的假设、意外的结果和意外的交互。相反，减少复杂性则极大地提高了软件的可维护性，因此简易性应当是我们构建的系统的一个关键目标。</p><p>让系统变得简单并不一定意味着要削弱系统的功能，它也可能意味着消除系统的偶然性复杂度。Moseley和Marks将那些不是来自于要解决问题本身的复杂性，而仅来自于软件实现的复杂性的复杂度称为偶然性复杂度。</p><p>抽象化是消除偶然性复杂度最好用的工具之一。一个好的抽象能够隐藏清晰易懂表象下大量的实现细节。好的抽象同样可以大规模运用在其他的应用程序上。这种抽象概念的复用不仅比重复实现简易功能更加有效，而且能够引导开发出更高质量的软件，抽象组件的质量改进也会让所有使用它的应用程序受益。</p><p>例如，高等级的编程语言是抽象的，它会隐藏机器核心，CPU注册和系统调用信息。SQL是一种抽象，它隐藏了复杂的磁盘和内存中的数据结构、来自其他客户机的并发请求，以及崩溃后的不一致性。当然，当使用高等级语言编程的时候，我们使用的依旧是机器编码，但是我们不是直接使用，因为编程语言的抽象允许我们不用去考虑这些。</p><p>但是，找到好的抽象是非常困难的，在分布式系统中，尽管有很多好的算法，但是我们并不知道如何将他们封装到一种抽象中用来将系统的复杂度保持在可控的范围内。</p><p>在这本书中，我们持续关注良好的抽象，以帮助我们能够将大系统的组成部分提取到定义良好、可复用的组件中。</p><h4>可演化性：让改变变得容易</h4><p>你的系统需求永远不变几乎是绝不可能的。它更可能处于不断变化的状态：你找到新的事实，出现未曾预料到的用例，业务优先级改变，用户请求新的功能，新平台取代旧平台，法律法规要求的改变，系统的增长迫使架构调整等等。</p><p>在组织流程上，敏捷工作法提供了适应变化的框架。敏捷社区同时开发了技术工具和模式去帮助开发者在多边的环境中开发软件，比如测试驱动开发（TDD）和重构。</p><p>大多数关于这些敏捷技术的讨论都局限在一个相当小、局部的规模上（在同一个应用程序内有几个源代码文件）。在本书中，我们寻求在更大的数据系统上提高敏捷性的方法，也许这些系统由几种不同的应用程序或者不同功能的服务构成。例如，你将如何重构Twitter的架构，将实现主页时间线的方法从第一种方法转换为成第二种方法。</p><p>你可以轻松地修改一个数据系统，并使其适应不断变化的需求，这与系统的简易性和抽象密切相关：简单易懂的系统通常比复杂系统更容易去调整。但是由于这是一个非常重要的想法，我们将使用一个其他的词汇来指代数据系统级别的敏捷性：可演化性[34]。</p><h2>小结</h2><p>本节，我们探索了一些研究数据密集型应用的基本方法。这些原则将会贯穿本书剩余部分，我们将在剩余章节中深入研究这些方法的技术细节。</p><p>一个实用的应用需要迎合各种各样的需求。包括一些功能性需求（它需要做什么，例如允许以各种方式存储、检索、搜索和处理数据），以及一些非功能需求（注入安全性，可靠性，遵从性，可扩展性，兼容性和可维护性等一般属性）。在本节中，我们详细讨论了可靠性，可扩展性和可维护性。</p><p>可靠性意味着让系统正确地运行，即使当错误发生的时候也能如此。错误可能是硬件上的（典型随机和不相关的），软件上的（系统性，难以处理的bug），人为性的（人类总是不可避免的反复犯错）。容错技术可以做到向终端用户隐藏错误类型。</p><p>可扩展性意味着，即使负载增加的情况下，也能够保持系统性能良好。为了方便对可扩展性进行讨论，我们首先需要能够描述负载和量化性能。我们以Twitter的主页时间线为例简要描述了负载，并且用响应时间分位数作为量化性能的参数。在可扩展性系统中，你可以增加处理能力以在高负载的情况下保证可靠性。</p><p>可维护性有很多方面，但是本质上来说，它着力于让那些与系统相依为命的开发者和操作团队过得更好。好的抽象有助于降低福再度，让系统更容易去修改和适应新的用例。良好的可操作性意味着良好地可视化系统健康状况，并有效地管理它。</p><p>不幸的是，并没有简单的方法可以保证系统的可靠性，可扩展性或者可维护性。但是有些模式和技术可以适用在不同种类的应用上。在接下来的章节中我们会看一下一些数据系统的实例，并且分析如何让它们达到这些目标。</p><p>稍后，在第三部分，我们将研究向图1-1系统那样由几个组件一起工作的系统模式。</p><p>参考文献：</p><p>[1] Michael Stonebraker and Uğur Çetintemel: “‘One Size Fits All’: An Idea Whose Time Has Come and Gone,” at 21st International Conference on Data Engineering (ICDE), April 2005.</p><p>[2] Walter L. Heimerdinger and Charles B. Weinstock: “A Conceptual Framework for System Fault Tolerance,” Technical Report CMU/SEI-92-TR-033, Software Engineering Institute, Carnegie Mellon University, October 1992.</p><p>[3] Ding Yuan, Yu Luo, Xin Zhuang, et al.: “Simple Testing Can Prevent Most Critical Failures: An Analysis of Production Failures in Distributed Data-Intensive Systems,” at 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI), October 2014.</p><p>[4] Yury Izrailevsky and Ariel Tseitlin: “The Netflix Simian Army,” techblog.netflix.com, July 19, 2011.</p><p>[5] Daniel Ford, François Labelle, Florentina I. Popovici, et al.: “Availability in Globally Distributed Storage Systems,” at 9th USENIX Symposium on Operating Systems Design and Implementation (OSDI), October 2010.</p><p>[6] Brian Beach: “Hard Drive Reliability Update – Sep 2014,” backblaze.com, September 23, 2014.</p><p>[7] Laurie Voss: “AWS: The Good, the Bad and the Ugly,” blog.awe.sm, December 18, 2012.</p><p>[8] Haryadi S. Gunawi, Mingzhe Hao, Tanakorn Leesatapornwongsa, et al.: “What Bugs Live in the Cloud?,” at 5th ACM Symposium on Cloud Computing (SoCC), November 2014. doi:10.1145/2670979.2670986</p><p>[9] Nelson Minar: “Leap Second Crashes Half the Internet,” somebits.com, July 3, 2012.</p><p>[10] Amazon Web Services: “Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region,” aws.amazon.com, April 29, 2011.</p><p>[11] Richard I. Cook: “How Complex Systems Fail,” Cognitive Technologies Laboratory, April 2000.</p><p>[12] Jay Kreps: “Getting Real About Distributed System Reliability,” blog.empathybox.com, March 19, 2012.</p><p>[13] David Oppenheimer, Archana Ganapathi, and David A. Patterson: “Why Do Internet Services Fail, and What Can Be Done About It?,” at 4th USENIX Symposium on Internet Technologies and Systems (USITS), March 2003.</p><p>[14] Nathan Marz: “Principles of Software Engineering, Part 1,” nathanmarz.com, April 2, 2013.</p><p>[15] Michael Jurewitz: “The Human Impact of Bugs,” jury.me, March 15, 2013.</p><p>[16] Raffi Krikorian: “Timelines at Scale,” at QCon San Francisco, November 2012. [17] Martin Fowler: Patterns of Enterprise Application Architecture. Addison Wesley, 2002. ISBN: 978-0-321-12742-6</p><p>[18] Kelly Sommers: “After all that run around, what caused 500ms disk latency even when we replaced physical server?” twitter.com, November 13, 2014.</p><p>[19] Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, et al.: “Dynamo: Amazon’s Highly Available Key-Value Store,” at 21st ACM Symposium on Operating Systems Principles (SOSP), October 2007.</p><p>[20] Greg Linden: “Make Data Useful,” slides from presentation at Stanford University Data Mining class (CS345), December 2006.</p><p>[21] Tammy Everts: “The Real Cost of Slow Time vs Downtime,” webperformancetoday.com, November 12, 2014.</p><p>[22] Jake Brutlag: “Speed Matters for Google Web Search,” googleresearch.blogspot.co.uk, June 22, 2009.</p><p>[23] Tyler Treat: “Everything You Know About Latency Is Wrong,” bravenewgeek.com, December 12, 2015.</p><p>24 | Chapter 1: Reliable, Scalable, and Maintainable Applications</p><p>[24] Jeffrey Dean and Luiz André Barroso: “The Tail at Scale,” Communications of the ACM, volume 56, number 2, pages 74–80, February 2013. doi: 10.1145/2408776.2408794</p><p>[25] Graham Cormode, Vladislav Shkapenyuk, Divesh Srivastava, and Bojian Xu: “Forward Decay: A Practical Time Decay Model for Streaming Systems,” at 25th IEEE International Conference on Data Engineering (ICDE), March 2009.</p><p>[26] Ted Dunning and Otmar Ertl: “Computing Extremely Accurate Quantiles Using t-Digests,” github.com, March 2014.</p><p>[27] Gil Tene: “HdrHistogram,” hdrhistogram.org.</p><p>[28] Baron Schwartz: “Why Percentiles Don’t Work the Way You Think,” vividcortex.com, December 7, 2015.</p><p>[29] James Hamilton: “On Designing and Deploying Internet-Scale Services,” at 21st Large Installation System Administration Conference (LISA), November 2007. [30] Brian Foote and Joseph Yoder: “Big Ball of Mud,” at 4th Conference on Pattern Languages of Programs (PLoP), September 1997.</p><p>[31] Frederick P Brooks: “No Silver Bullet – Essence and Accident in Software Engineering,” in The Mythical Man-Month, Anniversary edition, Addison-Wesley, 1995. ISBN: 978-0-201-83595-3</p><p>[32] Ben Moseley and Peter Marks: “Out of the Tar Pit,” at BCS Software Practice Advancement (SPA), 2006.</p><p>[33] Rich Hickey: “Simple Made Easy,” at Strange Loop, September 2011.</p><p>[34] Hongyu Pei Breivold, Ivica Crnkovic, and Peter J. Eriksson: “Analyzing Software Evolvability,” at 32nd Annual IEEE International Computer Software and Applications Conference (COMPSAC), July 2008. doi:10.1109/COMPSAC.2008.50</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'展性','设计','数据'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>