<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 | 极客快訊</title><meta property="og:title" content="机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/84c47890a2c44654997e63bd5cdf0c72"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/59b3843e.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/59b3843e.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/59b3843e.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/59b3843e.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/59b3843e.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/59b3843e.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/59b3843e.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/59b3843e.html><meta property="article:published_time" content="2020-10-29T21:11:12+08:00"><meta property="article:modified_time" content="2020-10-29T21:11:12+08:00"><meta name=Keywords content><meta name=description content="机器学习总结（基础）：指数分布、矩匹配、矩阵分解等"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/59b3843e.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>机器学习总结（基础）：指数分布、矩匹配、矩阵分解等</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/84c47890a2c44654997e63bd5cdf0c72><p class=pgc-img-caption></p></div><h1>指数分布</h1><p>高斯分布、二项分布、多项分布、泊松分布、伽玛分布和贝塔分布都属于指数分布。它的一般形式是</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2dab3b676e1449cba4a56e4995152744><p class=pgc-img-caption></p></div><p><em>A</em>（<em>η</em>）是累积量函数。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/c86e504f468d424fba2c24478113ec81><p class=pgc-img-caption></p></div><p>其指数eᴬ是归一化因子，A（η）也称为对数配分函数。η是自然参数。T（x）被称为充分统计量。在许多特定的分布中，如伯努利分布，它等于x。</p><p>考虑以下伯努利分布，其取值为1的概率为α，值为0的概率为1- α。我们可以用指数形式重写伯努利分布。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a24a467ac5364b64b3809318e3f0b2e7><p class=pgc-img-caption></p></div><p>然后</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cd78256ed665485a84e10e21832afd62><p class=pgc-img-caption></p></div><p>h，T和A的选固定择将定义一个特定的指数分布，如伯努利分布。如果我们转换η，它将成为恢复伯努利分布的模型参数α的逻辑函数。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3bc8d6aa66f44605970a468f106a51ac><p class=pgc-img-caption></p></div><p>因此，它可以用自然参数η表示为指数，而不是用参数α来建模伯努利分布。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a3e3791a62b742018380844576809c8b><p class=pgc-img-caption></p></div><p>对于二项式和泊松分布</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9771756f45ea485da9c37714f33b5680><p class=pgc-img-caption></p></div><p>到目前为止，我们的分布只需要一个参数来建模。对于由多个参数建模的分布，<em>η</em>将包含值向量。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0be2fc9407ca4485872d867ec0854abd><p class=pgc-img-caption></p></div><p>许多概率模型中的概率密度，如在图模型中由马尔可夫随机场MRF建模的概率密度，可以表示为指数。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/24f3cfa694f5415bb3e5ef4b080a7f3d><p class=pgc-img-caption></p></div><p>因此，指数族分布成为建模概率模型的自然选择。</p><p>让我们来看看<em>A</em>（<em>η</em>）的导数</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/f6d85ecc969f41b9aa162ff5c4e6ce3a><p class=pgc-img-caption></p></div><p>它的一阶导数是充分统计量T(x)的期望。对于T(x)=x，这个导数等于分布的均值。</p><p>在泊松分布中，用传统的积分定义计算E[x](均值)并不容易。将T（x）定义为泊松分布中的x，A '（η）等于E [ x ]。一般来说，微分比积分简单，我们利用它来解期望。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2fb631f3975146d8a7ff114804118886><p class=pgc-img-caption></p></div><p>二阶导数<em>A</em> '（<em>η</em>）等于方差。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/5d21567c01fc4cda8ee0b499e62d050b><p class=pgc-img-caption></p></div><p>A的导数实际上帮助我们定义了分布。</p><h1>矩匹配</h1><p>矩定量地描述了函数的形状。定义为</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ae971f8aee0346de834e58853fbcdd67><p class=pgc-img-caption></p></div><p>这一矩被称为关于零的矩。但是如果我们先用平均值减去x，它将被称为中心矩。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8783aca5718c42fd9ac28127bef06044><p class=pgc-img-caption></p></div><p>k阶矩等于a（η）的k阶导数。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/879d7a867d9345c5957c7bc10cf0faac><p class=pgc-img-caption></p></div><p>A（η）是凸函数（其二阶导数大于0）。由于A'（η）= μ，η具有与μ（力矩参数）的一对一映射。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b0746a42c2f34158a4f9f7cf7c372cb9><p class=pgc-img-caption></p></div><p>根据充分统计量t（x）的定义，导数<em>A'</em>（<em>η</em>），<em>A''</em>（<em>η</em>），...... <em>Aᵏ</em>（<em>η</em>）具有特殊的意义，可以通过采样数据进行估计。因此，我们在样本数据、分布矩和分布参数之间创建一个链接。在机器学习中，我们要用q*来模拟种群密度p。在矩匹配中，我们从样本数据中计算矩，以使它们的充分统计量的期望值相匹配。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/339b0bc70b2d4729a3e54424385b7b43><p class=pgc-img-caption></p></div><p>假设绘制的所有数据都是iid，最大似然估计将是：</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/41b6fdb422574d8189f339a34ff49861><p class=pgc-img-caption></p></div><p>可以通过从样本数据中找出充分统计量的平均值来计算μ。这称为矩匹配。估计后，我们可以找到分布的参数。</p><p>考虑一个简单的zero-centered分布f</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3cbfb50bd7454f2691dae8de1e27a6fb><p class=pgc-img-caption></p></div><p>让我们看看如何通过采样计算分布参数σ。矩计算如下：</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2589583d76f84b368fbaea99c51688d6><p class=pgc-img-caption></p></div><p>这些矩是钟形分布的均值和方差。我们可以通过采样来估计二阶矩。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/5bfec0107c184e0bae7bb9c0de7f7fd2><p class=pgc-img-caption></p></div><p>通过将理论矩和样本矩联系起来，得到了对σ（sampled σ）的估计。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d7a983fc48284c01ad225c37b792b5af><p class=pgc-img-caption></p></div><p>在上面的例子中，通过积分求E (x)和E (x²)很容易。一般来说。对于许多其他指数分布来说，这并不容易，比如gamma分布。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cb25bdaa6f8c4cb880ca50e299e2eacc><p class=pgc-img-caption></p></div><p>自然参数及其逆定义为：</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0aa00228f0254e61b223cbf9f7ae065f><p class=pgc-img-caption></p></div><p>充分统计为（log x，x），a（η）为</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/af60aa1881da4a59976c2b40b2df2db3><p class=pgc-img-caption></p></div><p>使用<em>A</em>（<em>η</em>）的导数，我们找到了充分统计的期望</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5b6e400d43f6472895abe4c5f7e72e8f><p class=pgc-img-caption></p></div><p>然后利用样本数据计算充分统计量的平均值，对上述参数α和β进行反求。</p><h1>贝叶斯推断</h1><p>频率推断从事件的频率得出结论。如果我们两次掷硬币两次正面（head），p（head）等于100％吗？然而，由于样本量太小，频率推断不太可能发布这样的结果。</p><p>贝叶斯推断利用贝叶斯定理从似然和先验信念中导出后验分布。当有新的观测结果时，我们将后验转换为先验，并根据新的证据计算新的后验。由于后验是一个确定性分布而不是一个点估计，我们可以继续将其与新的证据相结合，形成一个新的belief。简言之，我们从某个p（h）开始，并在新的证据下继续更新后验。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1dbc93582bd4436a8b081abda908ef29><p class=pgc-img-caption></p></div><p>例如，可以通过结合汽车如何移动的动态模型和GPS之前的测量数据来开始对汽车位置的预先判断。或者我们甚至可以完全从直觉或经验开始一个先验。给定当前传感器读数，我们形成了给定不同位置假设的当前传感器读数的可能性。利用贝叶斯推理，我们可以得到给定传感器读数的当前汽车位置的概率分布P(H|E)。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/146202a959104455ba37cc3dc0ba9b26><p class=pgc-img-caption></p></div><p>我们将后验转换为前验，以便下一次迭代时进行新的观察。样本量越小，似然曲线越宽，峰值越低。我们还没有画出足够的数据来排除许多可能性。因此，如果后验是强的(窄的和尖的)，后验将与前验相似。当收集到的数据越多，似然值越尖，后验分布越接近似然曲线。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fe82cfdd7c0148438822f299f773221b><p class=pgc-img-caption></p></div><p><strong>Frequentist vs Bayesian</strong></p><p>Frequentist应用最大似然估计来找到解释观察结果的最佳模型参数。贝叶斯聚焦在模型参数θ上，并使用贝叶斯定理计算模型参数的后验。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/01d78c49fb41478aae0d6814146e308b><p class=pgc-img-caption></p></div><p>贝叶斯推断在给定观察的情况下计算不同模型的概率。当然，对于高维或大的连续空间，这可能非常复杂。进一步简化似然模型和先验模型是可行的。或者我们可以通过采样或近似来解决这个问题。</p><p>根据样本收集的方式，回答P(x|y)可能比回答P(y|x)更容易。有时，概率很容易在相反的方向上建模。例如，P（y | x， θ）和P（θ）通常用高斯分布或β分布建模。下面是贝叶斯线性回归的一个例子。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/379d000acee74cb39f308b39a9e9bf0d><p class=pgc-img-caption></p></div><p>我们忽略贝叶斯定理中的分母P（y | X），因为它不是θ的函数。对于P（y | x， θ）和P（θ），我们在贝叶斯线性回归中用单独的高斯模型对它们进行建模。实际上，P(y |X)或P(X)通常很难计算，所以这是优化后验的一个很好的简化。</p><p>在贝叶斯定理,我们有相对较大的自由选择模型P(θ)。但并不是每个选择都是相等的，这个选择影响后验分析计算的难易程度。如果相应的后验函数属于前验函数的同一类分布，则前验函数是共轭前验函数。由于后验在下一次迭代中经常被用作先验，我们可以简单地重复同样的数学计算后验。例如，如果似然和先验都可以用高斯函数建模，那么后验函数也是高斯函数，易于计算。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/05ef33ecf06e449db566a2cbde163fab><p class=pgc-img-caption></p></div><p>如果模型θ可以使用共轭先验对应于特定似然分布来建模，我们通常可以容易地和分析地解决后验。</p><p><strong>Beta分布的贝叶斯推断</strong></p><p>对于二项分布，我们可以使用beta分布对其进行建模。如果可能性是二项式或伯努利，我们将在beta分布之前选择我们的共轭。这个选择使得我们可以将后验分布为β分布，并且可以容易地分析计算计算。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/5d593f82bcae429b9c2bb5caf58a9538><p class=pgc-img-caption></p></div><p>这是关于使用β分布来寻找后验的框架，其中我们对p（data|θ）和p（θ）都使用β分布。后验p（θ|data）将是β分布，所涉及的数学只是一些补充。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/457431f176b84c77b686bfdad7d7460f><p class=pgc-img-caption></p></div><p>让我们考虑一个人接触病毒的感染率。如果我们没有先验知识，我们可以从均匀分布开始先验（如下）。贝叶斯推理中的后验与频率论的结果相似，因为我们的belief较弱。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/59179fd2b17b480b8318eda11fc409ee><p class=pgc-img-caption></p></div><p>否则，我们可以从一些基于过去经验、知识甚至直觉的先验知识开始。然而，如果我们的belief是错的，我们需要收集更多的数据来逐渐重塑后验曲线。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1ed53bd329894a848da39ccbcc593c8a><p class=pgc-img-caption></p></div><p>让我们看看贝叶斯推理与频率推断的不同之处。在贝叶斯中，我们首先认为流感感染率可以建模为B（2,6）。这将是我们下面的第一张图。假设我们只有一个实验室结果，并测试呈阳性。一个普通的频率推断者会说根据样本感染率是100％。但我们知道这在科学上是不合理的。但是对于贝叶斯来说，随着结果的逐渐出现，我们仍然可以利用贝叶斯推理得出某种结论。从某种角度来看，如果我们先验是合理的，贝叶斯推理给我们一个合理的图像。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5ca1cc01c1674e41b5c8c0b5371f5f18><p class=pgc-img-caption></p></div><p><strong>Gamma分布作为共轭先验</strong></p><p>如果似然可以用高斯分布来建模，我们可以用伽马分布作为共轭先验。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/524494bfac234085916ec6a97674d7d6><p class=pgc-img-caption></p></div><p>似然<em>p</em>（<em>x |θ</em>）的高斯分布可以用以下形式表示</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8d5a6456e145467882b8dcdc8e6a029b><p class=pgc-img-caption></p></div><p>应用贝叶斯定理，我们也可以以Gamma分布的形式推导出后验。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/48cc9cdb38ad409eb14481b57c86bf2c><p class=pgc-img-caption></p></div><p><strong>Dirichlet - 多项式的共轭先验</strong></p><p>Dirichlet分布是多项式的共轭先验。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/90942599aa6a440ea822472700715d77><p class=pgc-img-caption></p></div><p>后验是：</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e26f0f2c14df4fa795c3208afa8ea330><p class=pgc-img-caption></p></div><p>Dirichlet分布也是分类分布之前的共轭：</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8dc0280ee42145ae8fa800aeaad262b3><p class=pgc-img-caption></p></div><p><strong>共轭先验概述</strong></p><p>以下是对应于特定似然分布的一些其他共轭先验。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1b12b65e0f22458e8b00e9cd4caa164f><p class=pgc-img-caption></p></div><p><strong>预测与正则化</strong></p><p>利用bayes定理，在给定观测值的情况下，计算了θ模型的后验概率。假设模型参数θ为zero-centered高斯分布，则先验p（θ）在目标函数中转化为l2正则项。从概念上讲，p（θ）可以看作是一个正则化因子。它可以惩罚成本函数。如下图所示，如果我们事先知道θ是什么样子的，我们可以对p（θ）应用一个相当复杂的模型。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7e851ac97d2e4a609315361072aaebd4><p class=pgc-img-caption></p></div><p>为了进行新的预测，我们在训练中使用后验p（θ| X，y）作为p（θ）。然后我们通过积分θ得到边际概率p（y 0 | x 0）。这是边际推断。我们通过将其他所有内容相加来计算变量的概率。</p><h1>导数</h1><p><strong>雅可比矩阵和Hessian矩阵</strong></p><p>这些矩阵分别是f的一阶和二阶导数。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/422078a1422747418764f292daffec89><p class=pgc-img-caption></p></div><p>这种表示法称为分子布局。hessian矩阵是对称的。具有hessian矩阵和向量v的二次方程的上界是</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a1c801f4fe83445faf238da23f31357a><p class=pgc-img-caption></p></div><p>下面，我们使用分母布局。它是分子布局的转置。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d70e7a2422a94fe68338c530c70a6c90><p class=pgc-img-caption></p></div><p>这是微分一个向量和一个矩阵的结果</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6a4a8d56832c489f91253c7989946005><p class=pgc-img-caption></p></div><h1>矩阵分解</h1><p><strong>图形解释</strong></p><p>我们可以通过将x投影到x轴和y轴来表示二维向量x。因此数据点可以表示为（xᵢ，yᵢ）。我们可以选择单位向量q并计算x对q的投影。投影向量为qqᵀx，其大小等于qᵀx。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f2c1e2df0dd245cc9655a6a788616a7c><p class=pgc-img-caption></p></div><p>在机器学习（ML）中，我们将特征从高维空间提取到低维潜在空间（比如k维）。概念上，我们把x投射到k个不同的向量q ⱼ上。选择qⱼ是很重要的。如果做得正确，我们可以使用更少的成分来表示信息。例如，如果我们选择下面的q 1和q 2，我们可以忽略q 2（蓝点）。它们可能太小，我们可以忽略它们。但是，如果我们选择x轴和y轴，则情况并非如此。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/59e012dc13564f8d881618c8b0fd9cb0><p class=pgc-img-caption></p></div><p>SVD将矩阵分解为独立的成分。SVD中选取的所有q相互独立(正交)，即提取的特征不相关。从概念上讲，SVD选择第一个q，当其余成分被删除时，则最小化下面的最小平方误差</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2dea1e12a3014f85a57fe9cf42e77df8><p class=pgc-img-caption></p></div><p><em>XXᵀ</em>是对称的。<em> </em>最优<em>q</em>（命名为<em>q 1</em>）将是<em>XXᵀ</em>的特征向量，具有最大特征值<em>λ或</em>最大奇异值<em>σ</em>（<em>λ=σ²</em>）</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6b8b45d0c44641eb96395016d7bbf21b><p class=pgc-img-caption></p></div><p>然后我们基于相同的原理选择下一个组件，条件是q彼此正交。因此，所选择的q 2将具有第二大的特征值。我们可以继续这个过程，直到我们用完特征向量。</p><p><strong>奇异值分解（SVD）</strong></p><p>SVD在线性代数中的表现方式不同。任何矩阵A都可以分解为</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6490b581a124486795a23c7c2de3d8ea><p class=pgc-img-caption></p></div><p>其中U由u构成- AAᵀ和uᵢ的本征向量彼此正交。类似地，v由AᵀA的特征向量vᵢ组成，该特征向量也彼此正交。</p><p>从上面的等式，A也可以写成</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b50fd0d37f46406594348fa31be8560c><p class=pgc-img-caption></p></div><p>其中uᵢ和vᵢ是单位向量。因此，当我们评估分解成分的重要性时，我们可以忽略那些具有非常小的σᵢ的项。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e226638d9fd0412f9ecca84f1233ab82><p class=pgc-img-caption></p></div><p>如果我们仅保留具有最大σᵢ的最顶部k项，我们有效地将A的维度减小为k，即，提取的特征仅在k维度上。考虑到每个主成分的重要性，我们有效地减少了输入的维度。这就是PCA所做的。</p><p><strong>主成分分析PCA</strong></p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/63ec90c2085a419e8dd415d287dad506><p class=pgc-img-caption></p></div><p>直观地说，两个输入特征可能相互关联，因此您可以创建一个新特征来表示这两个特征。对于主成分分析，我们希望找到k个独立的特征来表示我们的数据。</p><p><strong>PCA示例</strong></p><p>在机器学习（ML）中，SVD将包含训练数据的矩阵分解为独立的特征。例如，矩阵的行包含来自用户的电影评级。列包含电影的用户评分。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7d5ef47bf81446918b0106024b90fb6d><p class=pgc-img-caption></p></div><p>如果我们选择AAᵀ的前K个特征值，其相应的特征向量等效于下面的前K个优化q k向量：</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/f8384c10c0e64179b3fa8d442c3b139b><p class=pgc-img-caption></p></div><p>回想一下，我们将x投影到这些主成分qk中。求出最上面K个优化的qk，将x的维数降为K，就可以得到投影向量是x的第K个潜在因子。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/889ac86a13d94035a4161606fc6500a7><p class=pgc-img-caption></p></div><p>我们可以连接qᵢ形成矩阵Q。我们可以通过将Qᵀ与用户的电影分级相乘得出userᵢ 的潜在特征。（qᵢ是M ×1，其中M是电影的数量，Q是M × K）</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2bb17d17faa9492c8cee9c43a2889d2b><p class=pgc-img-caption></p></div><p>SVD发现用户评级的模式（主成分）。我们可以想象一些主成分可能代表电影的类型或发行的年代。例如，zᵢ中的第一个成分可以指示用户是否喜欢喜剧。</p><p><strong>概率PCA</strong></p><p>在svd中，我们将x分解为USVᵀ。而概率pca模型X≈WZ。我们将使用em算法来学习W和Z，其中Z可以作为X的潜在特征。与svd不同，W不需要是正交的。列不需要是单位长度或彼此垂直。</p><p>首先，我们假设潜变量zᵢ是zero-centered高斯分布。利用W，我们可以通过WZ重建原始数据X，其中x也由高斯建模。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e5f91daaf4ef481191b633f01c0f24d4><p class=pgc-img-caption></p></div><p>Z是EM算法中的潜在变量θ2，W是θ1。我们的目标是</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/91e1a64910b14b07b36228cef3721ded><p class=pgc-img-caption></p></div><p>在E步骤中，我们计算<em>q</em>（<em>zᵢ</em>）的高斯分布</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2a534cf2a100415f8ca2f9b75db4e4de><p class=pgc-img-caption></p></div><p>在M步骤中，我们进行优化</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/56a1d503cbbf4247bd4b7b1342c59fa2><p class=pgc-img-caption></p></div><p>算法是：</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/adb0f03c212444e8a09a3c2de76e7e38><p class=pgc-img-caption></p></div><p><strong>Kernel PCA</strong></p><p>从一个角度来看，PCA找到一组最大化qᵀXXᵀq的向量q 。由于XXᵀ是对称的，因此q将是具有最大特征值的XXᵀ的特征向量。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/41c2895573664bf19d0c2a0b706f9c4f><p class=pgc-img-caption></p></div><p>因此，问题变为找到具有最大特征值的特征向量。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c98a690b5a69436b9ef2872fbb724167><p class=pgc-img-caption></p></div><p>我们用核（Kernel）替换XXᵀ以将输入映射到更高维度。这允许我们创建线性边界来对在低维空间中不可线性分离的数据进行分类。相反，PCA通常被认为是降维技术。所以这两种技术似乎都朝着相反的方向发展。然而，有时候，我们需要在变小之前变大。进入高维空间使我们能够以更简单明确的边界对信息进行聚类。一旦信息清晰地聚类，将更容易将其映射到较低维度的空间。这是PCA kernel背后的动机。让我们从以下等式开始</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2a2d2315cb374c068a05ecbda6d3b724><p class=pgc-img-caption></p></div><p>经过一些操作，我们得到</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/21e77b0305c3467b9f4e1a2cdfda96cb><p class=pgc-img-caption></p></div><p>因此，假设矩阵K保持核结果，我们可以通过找到K的特征向量找到aᵢ。让我们用高斯函数定义核函数。x的相应潜在因子可以计算为：</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/434c6e5b86284d01826cabb634108a42><p class=pgc-img-caption></p></div><p>下面是我们如何使用Kernel PCA 预测新输入<em>x</em> 0</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/02737320e9304c258d80c360a7a77be7><p class=pgc-img-caption></p></div><p><strong>Cholesky分解</strong></p><p>Hermitian正定矩阵A的Cholesky分解是</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f6d0307272594e8fa5240634d77bfe7d><p class=pgc-img-caption></p></div><p>Hermitian矩阵是一个等于其转置共轭的方阵。转置共轭物取每个元素的复共轭，然后转置矩阵。</p><p>协方差矩阵是对称的（如果值都是real，则是Hermitian的特殊情况）和半正定。因此，Cholesky分解通常用于机器学习（ML)，以便更容易和更稳定地操作。</p><p><strong>Moore-Penrose Pseudoinverse</strong></p><p>对于线性方程组，我们可以计算方阵<em>A</em>的倒数来求解<em>x</em>。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1d348dfa93944f528a3d5b575c4a885b><p class=pgc-img-caption></p></div><p>但并非所有矩阵都是可逆的。在机器学习（ML）中，由于数据中存在噪声，因此不太可能找到精确解。但x的解可以估算为</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bcaabfc4be6e48d1877c8fe8c4681a4e><p class=pgc-img-caption></p></div><p>其中</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e92e854844ef41af9bd92d929205b793><p class=pgc-img-caption></p></div><h1>统计显著性</h1><p>空假设H 0表示两个测量现象之间没有关系，例如，财富和幸福之间没有相关性。如果观察到的数据具有统计显著性，则拒绝零假设。例如，如果我们在100次抛硬币中看到100个正面，我们可以“否定”硬币是公平的假设。因此，备择假设 H 1（一种与H 0相矛盾的假设）可能是真的（硬币不均匀）。实际上，要量化两个变量之间的关系比计算收集到的数据只是偶然发生的概率要难得多。因此，零假设是对两种现象得出结论的较好方法。</p><p>p值(概率值)是零假设为真时观测样本的概率。一个小的p值(通常≤0.05或≤0.01)显示出与原假设相反的有力证据，即偶然发生的情况很少见。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/efd0b06930e6477988b7f65afe539ad0><p class=pgc-img-caption></p></div><p>例如，在收集100个数据点之后，我们可以基于数据计算相关系数。如上所示，如果我们收集的100个数据点的相关性为-0.25，则其对应的PDF约为0.012。只有2.5％的群体可能具有小于-0.2的相关性。因此，零假设可能是错误的。</p><p><strong>置信区间</strong></p><p>在进行实验收集样本后。我们可以使用样本数据点来估计一个像平均值这样的总体参数(称为estimator)。置信区间可以计算为这个样本均值周围的范围。95%置信水平意味着在95%的实验中，其置信区间包含总体的真实均值。换句话说，一个实验的置信区间不包含真实均值的概率是1 / 20。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3908f06aeedb4524965312d01517233b><p class=pgc-img-caption></p></div><p>这是计算样本均值的置信区间的骨架</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c30fd8cad097402dbbdfbb4ea176813d><p class=pgc-img-caption></p></div><p>样本方差：</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2bb7b9e905524c66bf5d25fc20f50bd5><p class=pgc-img-caption></p></div><p><strong>卡方检验</strong></p><p>卡方检验(Chi-square test)是一种常用的检验方法，用于测量观察到的数据之间的相关性只是偶然的可能性，而不是两个变量之间的某种相关性。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/70f0b234780543a59bf197cc524c0b8b><p class=pgc-img-caption></p></div><p>利用上述公式计算卡方统计量。我们比较样本的实际计数和假设不存在相关性的期望计数。下面是一个决定性别是否影响宠物选择的例子。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/de3bb9a96be54a4dad4803209fee14dc><p class=pgc-img-caption></p></div><p>在这个例子中，如果性别不是一个因素，我们计算了拥有汽车的男性的实际数量减去预期数量之间的差额。我们平方它，除以期望的计数然后计算相应的卡方值。在我们的表格中，我们有四种可能的组合(雄猫、雄狗、雌猫、雌狗)。因此，我们有四个自由度，我们需要把所有四个值加起来来计算卡方统计量。</p><p>对于双边检验，我们将给定的显著性水平α除以2。例如，对于α=0.05，如果卡方统计量只有0.05/2=0.025的概率是偶然的，我们可以接受相关。由于卡方分布是不对称的，我们通常会查表，看看对应的特定概率值的卡方统计量是多少。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7147c8255f09477da756b08265d37756><p class=pgc-img-caption></p></div><p>例如，当自由度为4时，如果upper-tail表卡方统计量大于11.1，我们将接受相关性。当然，我们也需要参考bottom-tail表来检查卡方值是否太小。</p><h1>探索性数据分析</h1><p>为了探索数据，我们可以计算两个变量之间的协方差，或执行如下所示的散点图来发现趋势。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/27ebd258722b427081c93bcdd0adf14d><p class=pgc-img-caption></p></div><p>例如，下面的绿点和蓝点分别是SF和NY的房子。对于海拔高度>73英尺，我们有一个决策树桩，满足这个条件的很可能是SF。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8305ffefa69f402db5a4d48d658ab782><p class=pgc-img-caption></p></div><h1>范数</h1><p><strong>L1, L2-norm</strong></p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/4bdee4819cb140559e9398a01db05163><p class=pgc-img-caption></p></div><p><strong>Lp-norm, L∞-norm (max norm) & Frobenius norm</strong></p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/09975601a17c4c8699515c9bbc9e26b0><p class=pgc-img-caption></p></div><h1>相似度</h1><p><strong>Jaccard相似度</strong></p><p>Jaccard相似度测量交集大小与并集大小之间的比率。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/836d79022150426cb1a926bc082cd8a5><p class=pgc-img-caption></p></div><p><strong>余弦相似度</strong></p><p>余弦相似度测量两个矢量之间的角度。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/07aa468aa90f490291cab5820db18575><p class=pgc-img-caption></p></div><p><strong>皮尔逊相似度</strong></p><p>Pearson相关系数ρ测量两个变量之间的相关性。</p><div class=pgc-img><img alt=机器学习总结（基础）：指数分布、矩匹配、矩阵分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9cbbe91d818a4bf6818125ee1776cc19><p class=pgc-img-caption></p></div></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'机器','学习','总结'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../cn/%E7%A7%91%E6%8A%80/a55cbbea.html alt=机器学习总结（基础）：偏差和方差、iid、分布 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../cn/%E7%A7%91%E6%8A%80/a55cbbea.html title=机器学习总结（基础）：偏差和方差、iid、分布>机器学习总结（基础）：偏差和方差、iid、分布</a></li><hr><li><a href=../../cn/%E7%A7%91%E5%AD%A6/5199ece.html alt=机器学习总结（算法）：聚类、决策树、能量模型、LSTM等 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/76d31ab63a7249a5abaeec98d8891354 style=border-radius:25px></a>
<a href=../../cn/%E7%A7%91%E5%AD%A6/5199ece.html title=机器学习总结（算法）：聚类、决策树、能量模型、LSTM等>机器学习总结（算法）：聚类、决策树、能量模型、LSTM等</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>