<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>一文了解逻辑回归和支持向量机的异同 | 极客快訊</title><meta property="og:title" content="一文了解逻辑回归和支持向量机的异同 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/1539671186198856310f156"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/62900ef0.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/62900ef0.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/62900ef0.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/62900ef0.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/62900ef0.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/62900ef0.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/62900ef0.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/62900ef0.html><meta property="article:published_time" content="2020-11-14T21:04:52+08:00"><meta property="article:modified_time" content="2020-11-14T21:04:52+08:00"><meta name=Keywords content><meta name=description content="一文了解逻辑回归和支持向量机的异同"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/62900ef0.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>一文了解逻辑回归和支持向量机的异同</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p>最近有几个朋友私信我，说是在校招的时候被问到LR与SVM之间的异同，回答的不是太满意，希望我总结一下。</p><h1>一、首先LR与SVM有什么相同点呢？</h1><p><strong>第一，LR和SVM都是监督学习算法。</strong></p><p>说白了就是带标签的学习。</p><p><strong>第二，LR和SVM都是分类算法。</strong></p><p>这里要说的是一般情况下LR与SVM的标签都是0/1这样的离散值，当然他们也是可以做回归的。</p><p><strong>第三，如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。</strong></p><p>这里要先说明一点，那就是LR也是可以用核函数的，至于为什么通常在SVM中运用核函数而不在LR中运用，后面讲到他们之间区别的时候会重点分析。总之，原始的LR和SVM都是线性分类器，这也是为什么通常没人问你决策树和LR什么区别，决策树和SVM什么区别，你说一个非线性分类器和一个线性分类器有什么区别？</p><p><strong>第四，LR和SVM都是判别模型。</strong></p><p>判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率。简单来说，在计算判别模型时，不会计算联合概率，而在计算生成模型时，必须先计算联合概率。或者这样理解：生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。</p><h1>二、LR与SVM之间有什么不同呢</h1><p><strong>第一，样本不同</strong></p><p>LR考虑的是全部样本，也就是一个样本不管决策面有多远，都会对总体的损失函数有影响（明白梯度下降算法就知道了），SVM只考虑了部分样本，也就是离分割面最近的那些样本，被称为支持向量。</p><div class=pgc-img><img alt=一文了解逻辑回归和支持向量机的异同 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1539671186198856310f156></div><p>支持向量机改变非支持向量样本并不会引起决策面的变化</p><div class=pgc-img><img alt=一文了解逻辑回归和支持向量机的异同 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15396712149530c934a2cae></div><p>逻辑回归中改变任何样本都会引起决策面的变化</p><p><strong>第二，损失函数不一样</strong></p><p>LR使用的是逻辑损失</p><div class=pgc-img><img alt=一文了解逻辑回归和支持向量机的异同 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1539671335290a244dda7da></div><p>LR使用的sigmoid函数来表示属于某一类的概率，通过极大似然估计来估计参数的值。</p><p>SVM使用的是Hinge损失</p><div class=pgc-img><img alt=一文了解逻辑回归和支持向量机的异同 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/153967137464695892675d5></div><p>SVM使用的距离的方法，寻找最优超平面。</p><p><strong>第三，在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。</strong></p><p>​ 分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM算法里只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的）。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。​</p><p><strong>第四，SVM的损失函数就自带正则，不容易过拟合，而LR容易过拟合</strong></p><div class=pgc-img><img alt=一文了解逻辑回归和支持向量机的异同 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1539671777664925a5ccb41></div><p>大概我能想到的就这些了，以后有补充的我在加上去。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'逻辑','异同','向量'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>