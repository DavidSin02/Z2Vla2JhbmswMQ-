<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>策略梯度的简明介绍 | 极客快訊</title><meta property="og:title" content="策略梯度的简明介绍 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/1527998215130569eb83799"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c932f22.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c932f22.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6c932f22.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c932f22.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c932f22.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6c932f22.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6c932f22.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c932f22.html><meta property="article:published_time" content="2020-11-14T21:08:22+08:00"><meta property="article:modified_time" content="2020-11-14T21:08:22+08:00"><meta name=Keywords content><meta name=description content="策略梯度的简明介绍"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/6c932f22.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>策略梯度的简明介绍</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p>本文旨在为强化学习 - 策略梯度中最重要的一类控制算法提供简明而全面的介绍。我将在进展中讨论这些算法，从头到尾得出众所周知的结果。它针对的是具有合理背景的读者，以及机器学习中的其他任何主题。</p><h1>介绍</h1><p>强化学习（RL）指的是学习问题和机器学习的子领域，近来出于很多原因而出现在新闻中。基于RL的系统现在已经打败Go的世界冠军，帮助更好地操作数据中心并掌握各种各样的Atari游戏。研究界看到许多更有前途的结果。有足够的动力，让我们现在看看强化学习问题。</p><p>强化学习是对学习问题的最一般的描述，其目的是最大化长期目标。系统描述包括一个代理，该代理通过其在不连续时间步骤的行为与环境交互并获得奖励。这将代理转换为新状态。典型代理 - 环境反馈回路如下图所示。</p><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1527998215130569eb83799><p class=pgc-img-caption></p></div><p>学习问题的强化学习风格与人类如何有效表现行为惊人地相似 - 体验世界，积累知识并利用学习来处理新情况。</p><h1>背景和定义</h1><p>RL背后的大量理论都是建立在奖励假设的基础上的，这个假设概括地说，一个代理的所有目标和目的都可以用一个叫做奖励的标量来解释。更正式地说，奖励假设如下</p><blockquote><p>奖励假设：所有我们所说的目标和目标都可以被认为是所接收标量信号（称为奖励）的累积和的期望值的最大化。</p></blockquote><p>作为一个RL的实践者和研究者，一个人的工作是为一个给定的问题找到一套合适的奖励，这个问题被称为奖励塑造。</p><p>代理必须通过一个称为马尔可夫决策过程的理论框架进行正式工作，该决策过程由在每个状态下要做出的决策(要采取什么行动?)组成。这就产生了一系列的状态，行为和奖励被称为轨迹，</p><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1527999114420e602ffd860><p class=pgc-img-caption></p></div><p>目标是最大化这套奖励。更正式的，我们看看马尔科夫决策过程框架。</p><blockquote><p>马尔可夫决策过程：（Discounted）马尔可夫决策过程（MDP）是一个元组（S，A，R， p， γ），例如</p></blockquote><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1527999212336cc2565aa5e><p class=pgc-img-caption></p></div><blockquote><p>其中 S_t， S_（t +1）∈S（状态空间）， A_（t +1）∈A（动作空间）， R_（t +1）， R_t∈R（奖励空间）， p定义过程的动态， G_t是折扣回报</p></blockquote><p>简单地说，MDP定义了过渡到一个新状态的概率，得到了给定当前状态和执行操作的一些奖励。这个框架在数学上是令人满意的，因为它是一阶马尔可夫。这只是一种奇特的说法，说明接下来发生的一切都只取决于现在，而不是过去。一个人如何到达当前状态并不重要，重要的是他如何到达当前状态。这个框架的另一个重要部分是贴现因子γ。将这些回报与未来回报的不同程度的重要性相加，就会得出折现回报的概念。正如你所预料的那样,γ导致更高的灵敏度更高回报的未来。然而,γ= 0的极端情况下不考虑来自未来的回报。</p><p>环境p的动态不在代理的控制之下。可以想象站在一个有风的环境中的场地，并在每秒四个方向中的一个方向迈出一步。风很强烈，你很难沿着与北，东，西，南完全一致的方向移动。在下一秒落在一个新的状态的这个概率是由有风场的动力学p给出的。这当然不在你的（代理）控制之下。</p><p>但是，如果您以某种方式了解环境的动态，并朝着除北，东，西或南以外的方向移动，该怎么办。该策略是代理控制的。当代理遵循策略π时，它会产生称为轨迹的状态，行为和奖励顺序。</p><blockquote><p>策略：策略被定义为给定状态的行为的概率分布</p></blockquote><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1527999567326018ae717ca><p class=pgc-img-caption></p></div><h1>策略梯度</h1><p>强化学习代理的目标是在遵循策略π时最大化“预期”奖励。像任何机器学习设置一样，我们定义一组参数θ（例如复杂多项式的系数或神经网络中单位的权重和偏差）来参数化这个策略 - π_θ（为简洁起见也写成π）。如果我们将给定轨迹τ的总回报表示为r（τ），我们得出以下定义。</p><blockquote><p>强化学习目标：根据参数化策略最大化“预期”奖励</p></blockquote><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1527999686366f1917db8ad><p class=pgc-img-caption></p></div><p>所有有限的MDPs至少有一个最优策略(可以给出最大的奖励)，在所有最优策略中，至少有一个是平稳和确定性的。</p><p>像任何其他机器学习的问题,如果我们能找到参数θ⋆最大化J,我们会解决这个任务。解决机器学习文献中这种最大化问题的标准方法是使用梯度上升(或下降)。在梯度上升中，我们使用以下更新规则不断地遍历参数</p><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1527999796987405811a10d><p class=pgc-img-caption></p></div><p>挑战来了，我们如何找到包含期望的目标的梯度。积分在计算环境中总是不好的。我们需要找到绕过他们的办法。第一步是从期望的扩展开始重新构造梯度</p><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1527999872005a304fcc5cb><p class=pgc-img-caption></p></div><blockquote><p>策略梯度定理：期望回报的导数是策略π_θ对数的回报和梯度乘积的 期望值</p></blockquote><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15279999278267323554711><p class=pgc-img-caption></p></div><p>现在，让我们扩展π_θ（τ）的定义</p><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1528000033417f453d3cf63><p class=pgc-img-caption></p></div><p>为了理解这个计算，让我们把它分解下来 - P代表在某些状态s_ 0 开始的遍历分布。从那时起，我们应用概率乘积法则，因为每个新的行动概率都与前一个概率无关（还记得马尔可夫吗?）。在每一步中，我们采取用策略有所行动π_θ和环境动态p决定哪些新的状态过渡到。这些乘以T时间步长表示轨迹的长度。等同地，取得日志，我们有</p><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1528000106906526d4971fd><p class=pgc-img-caption></p></div><p>这个结果本身很美，因为这告诉我们，我们并不需要知道状态P的遍历分布和环境动力学p。这是至关重要的，因为对于大多数实际目的来说，很难对这两个变量进行建模 摆脱他们，当然是很好的进展。因此，所有使用此结果的算法都被称为“无模型算法 ”，因为我们不会“模拟”环境。</p><p>“期望”（或者相当于一个积分项）仍然存在。一个简单而有效的方法是对大量轨迹进行采样并将其平均。这是一个近似值，但是是一个没有偏差的值，类似于近似在连续空间上的一个积分，其中有一组离散的点。这种技术在形式上被称为马尔可夫链蒙特卡罗（MCMC），广泛用于概率图形模型和贝叶斯网络来近似参数概率分布。</p><p>我们以上处理中未被触及的一个术语是对轨迹r（τ）的奖励。即使参数化策略的梯度不依赖于奖励，该术语也会在MCMC采样中增加很多方差。实际上，每个R_t都有T个方差来源。然而，我们可以使用回报G_t，因为从优化RL目标的角度来看，过去的回报没有任何贡献。因此，如果用折现回报G_t代替r（τ），我们得到经典算法Policy Gradient算法，称为REINFORCE。随着我们进一步讨论，这并不能完全缓解这个问题。</p><h1>REINFORCE (and Baseline)</h1><p>重申一下，REINFORCE算法计算策略梯度为</p><p><strong>REINFORCE Gradient</strong></p><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1528000451904ee628075a8><p class=pgc-img-caption></p></div><p>我们仍然没有解决采样轨迹中的方差问题。解决此问题的一种方法是重新设计上面定义的RL目标，即极大似然估计（Maximum Likelihood Estimate）。在MLE设置中，众所周知，数据压倒了以前 - 简单地说，无论初始估计多么糟糕，在数据极限内，模型都会收敛到真实参数。然而，在数据样本具有高方差的情况下，稳定模型参数可能是非常困难的。在我们的情况下，任何不稳定的轨迹都可能导致策略分配出现次优的转变。奖励的规模加剧了这个问题。</p><p>因此，我们试着通过引入另一个称为基线b的变量来优化奖励的差异。为了保持梯度估计无偏差，基线独立于策略参数。</p><p><strong>基线增强（</strong>REINFORCE with Baseline<strong>）</strong></p><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15280005274152d65f75fd3><p class=pgc-img-caption></p></div><p>要知道为什么，我们必须证明梯度与附加项保持不变</p><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1528000589847e645010013><p class=pgc-img-caption></p></div><p>在理论和实践中使用基线可以减少方差，同时保持梯度仍然无偏。一个好的基准是使用状态值当前状态。</p><blockquote><p>状态值：状态值被定义为在策略π_θ之后给定状态的预期回报 。</p></blockquote><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1528000637797bee93c57ff><p class=pgc-img-caption></p></div><h1>Actor-Critic方法</h1><p>找到一个好的基线本身就是另一个挑战，并计算它的另一个挑战。相反，让我们使用参数ω来近似以得到V ^ω_（s）。所有使用可学习的V ^ω_（s）来引导梯度的算法被称为Actor-Critic算法，因为这个值函数估计像“actor”（代理策略）对“ critic”（good v/s bad values） 。然而这一次，我们必须计算actor 和critic的梯度。</p><blockquote><p>One-Step Bootstrapped Return：单步引导回报获取即时回报，并通过使用轨迹中下一状态的引导值估计来估计回报。</p></blockquote><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15280011266342e78673c39><p class=pgc-img-caption></p></div><p><strong>Actor-Critic策略梯度</strong></p><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1528001174049828506be8c><p class=pgc-img-caption></p></div><p>不言而喻，我们还需要更新critic的参数ω。这里的目标通常被认为是均方损失（或较不严重的Huber损失）并且使用随机梯度下降来更新参数。</p><p><strong>Critic’的目标</strong></p><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1528001256935b3e28f3b3e><p class=pgc-img-caption></p></div><h1>确定性策略梯度</h1><p>很多时候，在机器人技术中，可用的可控制策略是可用的，但这些行为不是随机的。在这样的环境下，很难像以前看到的那样建立随机策略。一种方法是将噪声注入控制器。更重要的是，随着控制器越来越多的维度，先前看到的算法开始变差。由于这样的场景，我们不是直接学习大量的概率分布，而是直接了解给定状态的确定性行为。因此，以最简单的形式，贪婪的最大化目标就是我们所需要的</p><p><strong>确定性行为</strong></p><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1528002447582f43cb56440><p class=pgc-img-caption></p></div><p>然而，对于大多数实际目的而言，这种最大化操作在计算上是不可行的（因为没有其他方式比在整个空间中搜索给定的动作值函数）。相反，我们可以期望做的是构建一个函数逼近器来逼近这个argmax，因此称为确定性策略梯度（DPG）。</p><p>我们用下面的等式来总结这一点。</p><p>DPG目标</p><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/152800251423856c71a325b><p class=pgc-img-caption></p></div><p>确定性策略梯度</p><div class=pgc-img><img alt=策略梯度的简明介绍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1528002540785e93fb4daa4><p class=pgc-img-caption></p></div><p>这个数值变成了我们可以用MCMC抽样再次估计的另一个期望值。</p><h1>通用强化学习框架</h1><p>现在我们可以得到一个通用的算法，看看我们所学到的所有部分是如何结合在一起的。所有新算法通常都是下面给出的算法的变体</p><blockquote><p>Loop:</p><p>Collect trajectories (transitions - (state, action, reward, next state, terminated flag))</p><p>(Optionally) store trajectories in a replay buffer for sampling</p><p>Loop:</p><p>Sample a mini batch of transitions</p><p>Compute Policy Gradient</p><p>(Optionally) Compute Critic Gradient</p><p>Update parameters</p></blockquote><h1>Code</h1><p>对于熟悉Python的读者来说，这些代码片段旨在成为上述理论思想的更具体的表示。这些已经被从实际代码的学习循环中取出。</p><p><strong>策略梯度(同步 Actor-Critic)</strong></p><blockquote><p># Compute Values and Probability Distribution</p><p>values, prob = self.ac_net(obs_tensor)</p><p># Compute Policy Gradient (Log probability x Action value)</p><p>advantages = return_tensor - values</p><p>action_log_probs = prob.log().gather(1, action_tensor)</p><p>actor_loss = -(advantages.detach() * action_log_probs).mean()</p><p># Compute L2 loss for values</p><p>critic_loss = advantages.pow(2).mean()</p><p># Backward Pass</p><p>loss = actor_loss + critic_loss</p><p>loss.backward()</p></blockquote><p>深层确定性策略梯度</p><blockquote><p># 从轨迹获取动作的Q值</p><p>current_q = self.critic(obs_tensor, action_tensor)</p><p># 获取目标Q值</p><p>target_q = reward_tensor + self.gamma * self.target_critic(next_obs_tensor, self.target_actor(next_obs_tensor))</p><p># L2 loss for the difference</p><p>critic_loss = F.mse_loss(current_q, target_q)</p><p>critic_loss.backward()</p><p># Actor loss based on the deterministic action policy</p><p>actor_loss = - self.critic(obs_tensor, self.actor(obs_tensor)).mean()</p><p>actor_loss.backward()</p></blockquote></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'简明','介绍','梯度'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>