<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>Mini-patch：从零开始的反向传播（附详细代码） | 极客快訊</title><meta property="og:title" content="Mini-patch：从零开始的反向传播（附详细代码） - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/0b58b4457c95428a8f14c71e2da9c021"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6e5b92f2.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6e5b92f2.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6e5b92f2.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6e5b92f2.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6e5b92f2.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6e5b92f2.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6e5b92f2.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6e5b92f2.html><meta property="article:published_time" content="2020-11-14T21:07:34+08:00"><meta property="article:modified_time" content="2020-11-14T21:07:34+08:00"><meta name=Keywords content><meta name=description content="Mini-patch：从零开始的反向传播（附详细代码）"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/6e5b92f2.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>Mini-patch：从零开始的反向传播（附详细代码）</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>文章将重点介绍mini-patch逐步实现的反向传播算法。有许多教程和博客详细介绍了反向传播算法，以及演算和代数背后的所有逻辑。因此，我将跳过这一部分，并在数学和使用Python的实现中切入方程式。</p><div class=pgc-img><img alt=Mini-patch：从零开始的反向传播（附详细代码） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0b58b4457c95428a8f14c71e2da9c021><p class=pgc-img-caption></p></div><p>关于为什么我们应该从头开始实现一个算法，即使几乎所有框架都已经可以使用它，这也是一个长期存在的普遍问题。显然，在使用某些高级框架时，你甚至都不会注意到反向传播确实发挥了魔力。要完全颠倒地理解它，一次尝试肯定是不够的。反向传播在游戏中也是可以进行实验的。</p><h1 class=pgc-h-arrow-right>为什么要使用mini-patch？</h1><p>mini-patch的原理很简单。通过将数据分成小批处理，并在训练循环的每次迭代中为算法提供一部分数据集，可以节省内存和处理时间。一次馈入10000x10000矩阵不仅会消耗内存，还会花费很长时间才能运行。相反，每次迭代将其降低到50个不仅会减少内存使用量，而且可以跟踪进度。</p><p><strong>注意：</strong>这与随机方法不同，在随机方法中，我们从每个类别的数据中抽取了分层样本，并在假设模型可以推广的基础上进行训练。</p><p><strong>开始实验</strong></p><p>这是将用于实验的数据头。</p><div class=pgc-img><img alt=Mini-patch：从零开始的反向传播（附详细代码） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f10e8f150eb24c34958c203fd9aa7663><p class=pgc-img-caption></p></div><p>此处的目标变量是占用率，它是类别变量（0/1）。这将是我们将要编码的架构。</p><div class=pgc-img><img alt=Mini-patch：从零开始的反向传播（附详细代码） onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/620afa5fbaf640dc91ccdc2ad0e2a384><p class=pgc-img-caption></p></div><p><strong>算法：</strong></p><p>对于i：= 1到i：= m：</p><p>执行正向传播或正向传递以计算每一层中神经元的激活值。</p><div class=pgc-img><img alt=Mini-patch：从零开始的反向传播（附详细代码） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/560492808858411c9ea5c4014fc2d5e6><p class=pgc-img-caption></p></div><p>反向传播步骤：</p><ul><li>使用数据中的标签计算误差项（MSE或LogLoss或您的期望）：</li></ul><div class=pgc-img><img alt=Mini-patch：从零开始的反向传播（附详细代码） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2201680f6b364a1cb8633ecebbc9936a><p class=pgc-img-caption></p></div><ul><li>隐藏层中的误差项使用以下公式计算：</li></ul><div class=pgc-img><img alt=Mini-patch：从零开始的反向传播（附详细代码） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a1a9fb4797fb4247946fba1ce0e8504e><p class=pgc-img-caption></p></div><ul><li>设置梯度：<br>初始化Δ= 0</li></ul><div class=pgc-img><img alt=Mini-patch：从零开始的反向传播（附详细代码） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/31ea869e8ba64797b6315ee3f9732899><p class=pgc-img-caption></p></div><p>3.梯度下降和权重更新步骤：</p><div class=pgc-img><img alt=Mini-patch：从零开始的反向传播（附详细代码） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d9bb2bc3d5324299beea2d64e53d4a42><p class=pgc-img-caption></p></div><p>代码展示：</p><pre><code>weight_dim = [5,H,1] #[number of input features, number of hidden units, number of output units]print("Initializing using He initialization")np.random.seed(3)w1 = np.random.randn(weight_dim[1],weight_dim[0]) * np.sqrt(2/weight_dim[0]) # 0.01b1 = np.zeros((weight_dim[1],1))w2 = np.random.randn(weight_dim[2],weight_dim[1]) * np.sqrt(2/weight_dim[1]) # 0.01b2 = np.zeros((weight_dim[2],1))</code></pre><p>如前所述，这将是一个三层网络。为了使梯度和误差方程更好，更容易地识别，我们保持层的数量简洁。之后，我们将定义一个函数，该函数将用作网络中的转发器。</p><pre><code>def forward(X,w1,w2,b1,b2):    z1 = np.matmul(w1,np.transpose(np.asmatrix(X))) + b1    a1 = sigmoid(z1)    z2 = np.matmul(w2, a1) + b2    a2 = sigmoid(z2)        return z1, a1, z2, a2</code></pre><p>这里要注意的一件事是，已经将Input层视为我的第0层。可能还有其他博客/教程被认为是第一名。因此，绝对要为所需的索引编制索引。</p><p>因此，现在，在初始化权重和偏差并定义前向传播函数之后，我们将在size = data-of-setset / N的mini-patch上定义反向传播函数。通过调整N来调整所需的批次大小。</p><pre><code>def backprop(w1,b1,w2,b2,X_train,X_test):        for i in range(epoch):                no_of_batches = len(X_train) // N                for j in range(no_of_batches):                        # Initilazing gradients                        delta1 = np.zeros(w1.shape)   #(5,5)            delta2 = np.zeros(w2.shape)   #(1,5)            db1 = 0.0            db2 = 0.0                                    for row in range(j*N,(j+1)*N):                                # Drop the date column and the index column                X = X_train[row, 2:7]                Y = X_train[row, 7]                                #feed forward                z1  ,  a1  ,  z2  , a2 = forward(X,w1,w2,b1,b2)                #(5,1) (5,1) (1,1) (1,1)                                h = a2 # (1,1)                                   # initializations                d3 = a2 - Y  #(1,1)                                delta2 += d3 * np.transpose(a2)  #(1,1)                db2 += d3                d2 = np.multiply((np.transpose(w2) * d3), sigmoid_gradient(z1),dtype=float)  #(5,1)                 delta1 += d2 * np.transpose(a1)  #(5,5)                db1 += d2                                                # Gradient Descent Step            #updating weights after every batch by averaging the gradients            w1 = w1 - lr * 1.0/N * delta1 #taking the average of gradients            b1 = b1 - lr * 1.0/N * db1            w2 = w2 - lr * 1.0/N * delta1            b2 = b2 - lr * 1.0/N * db2                                            print("************************************************")        print("Train error after epoch {} is: ".format(i), np.sum(error(calc_out(X_train[:,2:7],w1,b1,w2,b2),X_train[:,7])) / len(X_train) * 100)            print("Test error after epoch {} is: ".format(i), np.sum(error(calc_out(X_test[:,2:7],w1,b1,w2,b2),X_test[:,7])) / len(X_test) * 100)        print("************************************************")        print()            train_error[i] = np.sum(error(calc_out(X_train[:,2:7],w1,b1,w2,b2),X_train[:,7])) / len(X_train) * 100        test_error[i] = np.sum(error(calc_out(X_test[:,2:7],w1,b1,w2,b2),X_test[:,7])) / len(X_test) * 100            return [w1,b1,w2,b2]</code></pre><p><strong>步骤分解</strong></p><ul><li>如前所述，Ist循环会遍历您想要使模型遍历数据的次数，只需将其放在神经网络术语“时代”中即可。</li><li>第二次循环：指定了批次数量后，此循环针对每个时期“ i”遍历每个微型批次</li><li>第三循环遍历该小批量中的每个训练示例，并计算梯度和误差值</li><li>最后，对于每个批次，都执行梯度下降步骤，并对权重矩阵进行更改。</li></ul><p>这就是mini-patch的反向传播实现。需要注意的是，此实验为网络中的每一层使用了一个矩阵变量，当网络规模扩大时，这是一种不明智的做法，我们这样做是为了了解它的实际工作原理。如果要增加“隐藏层”的数量，可以简单地使用3d矩阵进行误差和梯度计算，其中第3维将保存该层的值。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'Mini','patch','零开始'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>