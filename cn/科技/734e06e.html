<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计 | 极客快訊</title><meta property="og:title" content="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RixptB9J7gzdZC"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/734e06e.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/734e06e.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/734e06e.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/734e06e.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/734e06e.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/734e06e.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/734e06e.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/734e06e.html><meta property="article:published_time" content="2020-10-29T21:01:05+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:05+08:00"><meta name=Keywords content><meta name=description content="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/734e06e.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RixptB9J7gzdZC><p><em>内容来自将门计算机视觉主题社群</em></p><p><em>作者：黄骏杰</em></p><p>本文为<strong>将门好声音</strong><strong>第20</strong><strong>期</strong>。</p><p>论文作者是来自</p><blockquote toutiao-origin=span>将门计算机视觉主题社群、中国科学院自动化所的黄骏杰，本次他将与大家分享其一作论文：人体姿态估计中无偏的数据处理方法。</blockquote><p>在「将门好声音」</p><p toutiao-origin=span>不仅可以分享你最新的研究工作，更可以分享跟技术相关的干货观点、出坑经验等，点击</p><strong>“阅读原文”</strong><strong toutiao-origin=span>或联系将门小姐姐！只要内容合适，我"门"送你头条出道！</strong><p></p><h2 toutiao-origin=h6><strong>关于作者</strong></h2><p>黄骏杰，中国科学院自动化研究所19届硕士，算法研究员，主要的研究方向是人体姿态估计和视觉伺服。</p><p>个人主页：<strong class=highlight-text toutiao-origin=span>https://huangjunjie2017.github.io</strong></p><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RixptBl6MWnOgR><ul><li><p>论文链接:</p></li></ul><p><strong class=highlight-text toutiao-origin=span>https://ar<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-3">xi</i>v.org/abs/1911.07524</strong></p><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RWvRrxT7bPqVHJ><p class=pgc-img-caption>数据处理作为人体姿态估计算法的一个重要的组成部分，我们通过深入的研究发现现有的方法中普遍存在两个问题：一个是在测试过程中由翻转图像得到的结果和由原图得到的结果之间存在偏差，另外一个问题是现有的state-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">of</i>-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">the</i>-art方法所使用的编码解码方法存在较大的统计误差。</p><p>这两个问题耦合在一起，产生的影响包括：算法的精度较低、state-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">of</i>-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">the</i>-art指标复现困难和实验结论不可靠。本文中，我们提出<strong class=highlight-text toutiao-origin=strong>用于人体姿态估计算法无偏的数据处理方法（UDP）</strong>。</p><p>UDP适用于所有top-down的方法，以几乎可以忽略的计算代价，不仅仅把现有state-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">of</i>-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">the</i>-art算法的性能提升到前所未有的高度，同时通过详细的剖析帮助大家深入了解用于人体姿态估计数据处理，并且为社区提供一个更加可靠的baseline以促进进一步的研究。</p><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RWvRrxw8jrLrPi><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RixptCGJFQHG3P><p></p><h1 toutiao-origin=h4><strong toutiao-origin=h1>背 景</strong></h1><p>多目标的人体姿态估计以检测场景内所有人的关键点为目的，目前用于性能评估的公开数据集主要是COCO,方法主要有bottom-up(先检测所有关键点，再做实例分析) 和 top-down(先检测人，然后做单人的姿态估计)。所有计算机视觉任务都需要数据处理，同时，很多数据处理的方法在不同任务之间都是通用的，比如数据在座标系之间变换和样本增广等。这种通用性使得我们可以直接借鉴其他任务的数据处理方法，甚至复用其他任务的代码。</p><p>然而，这种通用性也往往会让人忽略不同任务对数据处理的特殊要求。根据不同任务的评测原理，我们可以总结他们对目标位置的<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">敏感</i>程度。其中分类是一个对目标位置不<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">敏感</i>的任务，而检测和分割则是对目标位置<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">敏感</i>的任务。相比之下，关键点检测是对目标位置极其<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">敏感</i>的任务，这是因为关键点评测会直接惩罚目标和真值之间的空间距离，存在于数据处理过程中的系统误差会对算法的精度产生重大的影响。</p><p>在人体姿态top-down方法中的数据处理主要包括数据变换、数据增广和编码解码。其中数据变换指的是样本数据在不同座标系（原始数据、网络输入和网络输出）之间的变换。<strong>在这个过程中已知的方法使用像素去度量图像的大小，这导致测试过程中由翻转图像得到的结果和原图得到的结果不对齐。</strong>state-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">of</i>-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">the</i>-art方法中都凭经验地在后处理中对这种不对齐进行补偿，比如MSRA在SimpleBaseline和HRNet中对由翻转图像得到的结果shift了一个像素，这方法可以有效地缓解这种不对齐带来的影响。Face++的CPN和MSPN在网络输入空间中对ensemble的结果（由翻转图像得到的结果和原图得到的结果求均值）shift了两个像素，有异曲同工之妙。</p><p>这些没有解释在paper里的操作可以对算法的性能带来巨大的提升，比如在HRNet-w32-256x192的backbone下在COCO val上可以有高达2.3AP的提升（73.3APvs75.6AP）。如此大幅度的影响使得这些技巧成为论文指标复现的关键，同时也拉开了这些方法和没有这些技巧的方法的<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-3">差距</i>，使得在并不公平的条件下得到的对比实验结果往往是不可靠的。这种不可靠使得很多很好的方法可能会被不理想的实验结果所埋没，毕竟可以在没有这些技巧之下能在性能上超越这些state-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">of</i>-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">the</i>-art的方法还是很少的。</p><p>比如在本文中介绍的基于offset的编码解码方法（Google提出的）原本是一种很好的方法，但在没有这些技巧的情况下用在HRNet-w32-256x192中，在COCO val 上的得分只有 74.5AP，和MSRA在同样backbone下的指标（75.6AP）相比，这结果就相当不如人意了。</p><p>数据增广是提高算法泛化性能的通用技巧，top-down的人体姿态估计中主要的策略有随机旋转、随机裁剪、翻转和半身监督。数据增广一般是在数据从原始数据座标系转换到网络输入座标系的过程中实现的。</p><p>编码解码是指把关键点的座标在训练过程中编码成易于网络学习的形式，然后在测试过程再从网络输出解码得到关键点座标的方法。编码解码被广泛应用在state-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">of</i>-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">the</i>-art方法中，主流的做法（Face++的CPN和MSPN，MSRA的SimpleBaseline和HRNet）是使用以座标为中心的单高斯响应图作为座标编码解码的媒介。而State-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">of</i>-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">the</i>-art们在使用这种方法时存在较大的统计误差。</p><p>Google在2017年时提出了一种基于offset设计的编码解码方法，这种方法在理想情况下（网络学习能力完美）可以做到没有统计误差，但是却没有被广泛应用在state-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">of</i>-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">the</i>-art方法中。此外，上个月电子科技大学提出Dark：一种可以达到二阶泰勒展开精度的编码解码方法。我们的实验结果显示，Dark的性能表现和Google的方法在HRNet上几乎一致，在SimpleBaseline上稍逊一筹（相差0.5AP左右）。这些方法都可以有效提高现有state-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">of</i>-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">the</i>-art算法的性能（1AP左右）。</p><p></p><h1 toutiao-origin=h4><strong toutiao-origin=h1>UDP——无偏的数据变换</strong></h1><p>我们在文章中详细推导了在数据转换过程中，使用像素作为图像大小度量尺度（有偏的）以及使用对应座标系单位长度作为图像大小度量尺度（无偏的）两种情况的过程、结果以及影响，得到以下主要的结论。</p><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RixptPjCX6Oymh><p>首先，值得注意的一点是使用和不使用像素作为图像大小的度量尺度，得到的用于网络训练的样本在语义上都是和原样本对齐的，因此如果在测试过程中不使用翻转的图像作ensemble的话，这两种做法在性能表现上应该是一样，对比实验中得到的结果分别是74.5AP和74.4AP也验证了这一点。</p><p>然而，我们以MSRA的SimpleBaseline和HRNet为例子，经过推导发现如果在测试过程中使用翻转图像作ensemble的话，由翻转图像得到的结果和由原图得到的结果在<em>O</em><em>o</em><em>-</em><em>X</em><em>o</em>方向上存在<em>-(s-1)/s</em>大小的偏差。此处的s代表网络输入和输出的大小比值（步长）。在CPN、MSPN、SimpleBaseline和HRNet中使用的步长都是4，对应的偏差是-0.75个单位距离，其均值误差为0.375单位距离会对结果造成2.5AP的影响（75.8APvs73.3AP）；而MSRA把由翻转图像得到的结果在<em>O</em><em>o</em><em>-</em><em>X</em><em>o</em>方向上shift了一个像素，使得两个结果的偏差缩小为0.25个单位距离。使得取均值后的误差缩小为0.125个单位距离，对比实验的结果显示，这技巧可以把2.5AP的影响缩小为仅仅0.2AP的影响（75.8AP vs 75.6AP)。虽然在MSRA结果上直接补偿这个0.125单位距离的剩余误差可以使得结果无偏，但相比之下使用<em>O</em><em>o</em><em>-</em><em>X</em><em>o</em>座标系中的单位长度去度量图像的大小，可以直接得到无偏的估计结果，并且免去复杂的后处理环节。</p><p>此外，当我们把误差映射回原图的时候可以得到在原始样本的座标系 <em>O</em><em>s</em><em>-</em><em>X</em>s<em>-</em><em>Y</em><em>s</em>中实际的误差大小为:</p><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RixptPy9kD89pN><p>测试过程中边界框的大小是固定的，使用更大的网络输入可以有效减少上述误差带来的影响。这也是为什么现有的state-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">of</i>-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">the</i>-art都在非常大的输入分辨率上做报告，低分辨率会使得现有state-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">of</i>-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">the</i>-art的性能大幅下降。</p><p></p><h1 toutiao-origin=h4><strong toutiao-origin=h1>无偏的编码解码方法</strong></h1><p>我们以MSRA所使用的编码解码方法为例子在网络输出空间中进行量化分析，在训练过程中，他们把关键点的座标编码为以关键点为中心的高斯响应图：</p><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RixptQEDCl6AMa><p>然后在解码的时候以相应图的响应最大点的座标为基础，通过梯度稍作正得到关键点的估计座标。该方法在理想情况下（网络的学习能力完美），估计结果和真值之间的关系为：</p><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RixptQW2CHa8Lo><p>其中<strong>F </strong>和<strong>C</strong>函数分别表示向上和向下取整，m表示关键点在<em>O</em><em>o</em><em>-</em><em>X</em><em>o</em>方向上的座标值，离散的估计结果和真值间存在一定的统计误差。</p><p>在这里我们介绍两种在理想情况下，误差为零或者几乎为零的编码解码方法。第一种是电子科技大学最近新提出的Dark，Dark也是使用高斯分布的响应图编码关键点的座标，但是在推理解码的时候，并非简单的利用一阶梯度作一个简单修正。Dark利用响应图分布已知的特点，通过在响应最大值点附近利用二阶泰勒展开求极值点的方法，去求取真正的中心点（根据高斯响应<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">最高</i>值点梯度为零）。这种方法理想情况下可以达到二阶泰勒展开的精度。</p><p>另外一种是我们在论文中介绍的Google早在2017年就已经提出的基于offset的方法。这方法在训练时把关键点座标编码成三个通道的响应图：</p><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RixptRG9vEw4rC><p>在解码时通过在H中寻找响应最大值点k，并与offset图中对应响应相加，即可在理想情况下达到零误差的效果。</p><p>我们也做了这两种方法的对比实验，其中在COCOval上，使用HRNet-w32的backbone时，两种方法的性能表现几乎一样。使用SimpleBaseline-R50的backbone时，Dark的性能稍逊一筹。当然，Google的方法因为监督增加会带来少量的计算增量以及对IO传输要求更多。</p><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RixptdUC6CVfon><p>在test-dev上Google的方法在性能上会比Dark要好：</p><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RixptdpCxK40q6><p><em>*没有使用无偏的数据转换，据我们论文推理以及实验经验，这个在256x192的分辨率上会有0.2AP的性能损失，384x288的分辨率上损失会少于0.2AP</em></p><p></p><h1 toutiao-origin=h4><strong toutiao-origin=h1>实 验</strong></h1><p>首先我们在COCOval上使用一个更好的人体检测结果，重新跑了一遍SimpleBaseline和HRNet的结果，和使用UDP后的方法作公平对比，重新跑的结果会比原文的在AP上高一个点左右。</p><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Rixpte7CEP3S9S><p>对比之下有两点值得注意的是：</p><p>1.UDP在不同backbone上的提升相近，表明UDP具备通用性；</p><p>2.UDP在低输入分辨率时带来的提升更大，分辨率越低提升越显著，UDP可以有效缩小高分辨率和低分辨率之间的性能差异，使得算法在性能和计算量之间取得更优的权衡。</p><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RixpteY6lBRigH><p>UDP在COCO test-dev2017上也有两个值得注意的点：</p><p>1.UDP在test-dev上带来的提升毫不逊色于其在val上带来的提升，表明UDP所带来的性能提升并非过拟合val数据集；</p><p>2.使用UDP和HRNet-W48-384x288的backbone，test-dev上的指标达到76.5AP，是已公开方法中<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">最高</i>的。</p><p></p><h1 toutiao-origin=h4><strong toutiao-origin=h1>总结</strong></h1><p>无偏的数据处理对于人体姿态估计任务非常重要，其重要性不仅体现在可以帮现有的方法取得更高的精度和更好的指标，还体现在是指标复现的关键点之一以及可以促进后续的研究。如果想要对文章中的理论细节和代码实现有更深入的了解，请查看原文和作者主页：</p><p>论文链接: <strong class=highlight-text toutiao-origin=span>https://ar<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-3">xi</i>v.org/abs/1911.07524</strong></p><p>个人主页: <strong class=highlight-text toutiao-origin=span>https://huangjunjie2017.github.io</strong></p><p>项目代码：<strong class=highlight-text toutiao-origin=span>https://github<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">.com</i>/HuangJunJie2017/UDP-Pose</strong></p><p></p><h2 toutiao-origin=h6>参考文献</h2><p><em>[1]: Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer V<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">is</i>ion, pages 740–755. Springer, 2014.</em></p><p><em>[2]: Bin Xiao, Haiping Wu, and Yichen Wei. <i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">Simple</i> baselines for human pose estimation and tracking. In European Conference on Computer V<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">is</i>ion, pages 466–481, 2018.</em></p><p><em>[3]: Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In IEEE Conference on Computer V<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">is</i>ion and Pattern Recognition, 2019.</em></p><p><em>[4]: Yilun Chen, Zhicheng Wang, Yu<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-3">xi</i>ang Peng, Zhiqiang Zhang, Gang Yu, and Jian Sun. Cascaded pyramid network for multi-person pose estimation. In IEEE Conferenceon Computer V<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">is</i>ion and Pattern Recognition, pages 7103–7112, 2018.</em></p><p><em>[5]: George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chr<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">is</i> Bregler, and Kevin Murphy. Towards accurate multi-person pose estimation in <i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">the</i> wild. In IEEE Conference on Computer V<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">is</i>ion and Pattern Recognition, pages 4903–4911, 2017.</em></p><p><em>[6]: Zhang F , Zhu X , Dai H , et al. D<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">is</i>tribution-Aware Coordinate Representation for Human Pose Estimation[J]. 2019.</em></p><p>来扫我呀</p><p>-<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">The</i> End-</p><p><strong>将门</strong>是一家以专注于<strong>发掘、加速及投资技术驱动型<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">创业</i>公司</strong>的新型<strong>创投机构</strong>，旗下涵盖</p><blockquote toutiao-origin=span>将门创新服务、将门技术社群以及将门创投基金。将门成立于2015年底，创始团队由微软创投在中国的创始团队原班人马构建而成，曾为微软优选和深度孵化了126家创新的技术型<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">创业</i>公司。</blockquote><p><strong>将门创新服务</strong></p><blockquote toutiao-origin=span>专注于使创新的技术落地于真正的应用场景，激活和实现全新的商业价值，服务于行业领先企业和技术创新型<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">创业</i>公司。</blockquote><p><strong>将门技术社群</strong></p><blockquote toutiao-origin=span>专注于帮助技术创新型的<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">创业</i>公司提供来自产、学、研、创领域的核心技术专家的技术分享和学习内容，使创新成为持续的核心竞争力。</blockquote><p><strong>将门创投基金</strong></p><p toutiao-origin=span>专注于投资通过技术创新激活商业场景，实现商业价值的初创企业，<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">关注</i>技术领域包括</p><strong toutiao-origin=span>机器智能、物联网、自然人机交互、企业计算。</strong><p toutiao-origin=span>在三年的时间里，将门创投基金已经投资了包括量化派、码隆科技、禾赛科技、宽拓科技、杉数科技、迪英加科技等数十家具有高成长潜力的技术型<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">创业</i>公司。</p><p></p><blockquote toutiao-origin=span>如果您是技术领域的初创企业，不仅想获得投资，还希望获得一系列持续性、有价值的投后服务，</blockquote>欢迎发送或者推荐项目给我“门”: bp@thejiangmen<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">.com</i><img alt="将门好声音 | 中科院自动化所：无偏数据处理方法助你实现SOTA人体姿态估计" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/R6RrkaV5T8rQhn><p>点击右上角，把文章朋友圈</p><p><strong>将门创投</strong></p><p>让创新获得认可！</p><p><i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">微信</i>：thejiangmen</p><p>bp@thejiangmen<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">.com</i></p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'将门','声音','自动化'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>