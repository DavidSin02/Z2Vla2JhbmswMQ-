<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>广义线性模型② | 极客快訊</title><meta property="og:title" content="广义线性模型② - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/7f891d17860b4875a43b94bdd9622312"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79abcaa.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79abcaa.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/79abcaa.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79abcaa.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79abcaa.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/79abcaa.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/79abcaa.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79abcaa.html><meta property="article:published_time" content="2020-10-29T21:01:05+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:05+08:00"><meta name=Keywords content><meta name=description content="广义线性模型②"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/79abcaa.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>广义线性模型②</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><h1 class=pgc-h-arrow-right>1.1.10. 贝叶斯回归</h1><p style=text-align:start>贝叶斯回归可以用于在预估阶段的参数正则化: 正则化参数的选择不是通过人为的选择，而是通过手动调节数据值来实现。</p><p>上述过程可以通过引入 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">无信息先验</span> 到模型中的超参数来完成。 在 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">岭回归</span>中使用的</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7f891d17860b4875a43b94bdd9622312><p class=pgc-img-caption></p></div><p>正则项相当于在</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d989ff3f11e45d49df6f0aa4ca47040><p class=pgc-img-caption></p></div><p>为高斯先验条件，且此先验的精确度为</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4fec947be31643eca3e3641b7889b167><p class=pgc-img-caption></p></div><p>时，求最大后验估计。在这里，我们没有手工调参数 lambda ，而是让他作为一个变量，通过数据中估计得到。</p><p>为了得到一个全概率模型，输出</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9389b34b296e4c67a6e9561c89799486><p class=pgc-img-caption></p></div><p>也被认为是关于</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3bc2793bb7104e77ae2cafe4bbc82eef><p class=pgc-img-caption></p></div><p>的高斯分布。</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/6fff88982c5e49b6ae66a1e99572f424><p class=pgc-img-caption></p></div><p style=text-align:start>Alpha 在这里也是作为一个变量，通过数据中估计得到。</p><p style=text-align:start>贝叶斯回归有如下几个优点:</p><ul><li>它能根据已有的数据进行改变。</li><li>它能在估计过程中引入正则项。</li></ul><p style=text-align:start>贝叶斯回归有如下缺点:</p><ul><li>它的推断过程是非常耗时的。</li></ul><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">参考资料</span></strong></p><p><span style="color:#858585;--tt-darkmode-color: #858585">一个对于贝叶斯方法的很好的介绍 C. Bishop: Pattern Recognition and Machine learning详细介绍原创算法的一本书 Bayesian learning for neural networks by Radford M. Neal</span></p></blockquote><h1 class=pgc-h-arrow-right>1.1.10.1. 贝叶斯岭回归</h1><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">BayesianRidge</span></span> 利用概率模型估算了上述的回归问题，其先验参数</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d989ff3f11e45d49df6f0aa4ca47040><p class=pgc-img-caption></p></div><p>是由以下球面高斯公式得出的：</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7f0545c81efe4d7bb7a9ede236fe8c79><p class=pgc-img-caption></p></div><p>先验参数</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/40b73f2759314b0fbce47e264b9e127a><p class=pgc-img-caption></p></div><p>和</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d023166191b4df49d76795141085e0a><p class=pgc-img-caption></p></div><p>一般是服从 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">gamma 分布</span> ，这个分布与高斯成共轭先验关系。 得到的模型一般称为 <strong>贝叶斯岭回归</strong>，并且这个与传统的 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">Ridge</span></span> 非常相似。</p><p>参数</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d989ff3f11e45d49df6f0aa4ca47040><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/40b73f2759314b0fbce47e264b9e127a><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d023166191b4df49d76795141085e0a><p class=pgc-img-caption></p></div><p>是在模型拟合的时候一起被估算出来的，其中参数</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/40b73f2759314b0fbce47e264b9e127a><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d023166191b4df49d76795141085e0a><p class=pgc-img-caption></p></div><p>通过最大似然估计得到。scikit-learn的实现是基于文献（Tipping，2001）的附录A，参数</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/40b73f2759314b0fbce47e264b9e127a><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d023166191b4df49d76795141085e0a><p class=pgc-img-caption></p></div><p>的更新是基于文献（MacKay，1992）。</p><p>剩下的超参数</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/44bd145ebf8a4802a266f4922344ed2a><p class=pgc-img-caption></p></div><p><br></p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d0a1ceb5a85b4871a9d797bec1339e00><p class=pgc-img-caption></p></div><p><br></p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a86441336667463ebc22c705e0f1b01b><p class=pgc-img-caption></p></div><p>以及</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2132c49b5edf4d6697f7ac08c2a77b75><p class=pgc-img-caption></p></div><p><br></p><p>是关于</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a67a17368688474d8b7242a771d59919><p class=pgc-img-caption></p></div><p>和</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d023166191b4df49d76795141085e0a><p class=pgc-img-caption></p></div><p>的 gamma 分布的先验。 它们通常被选择为 <strong>无信息先验</strong> 。默认</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/174a6774427c4ebd92be78a298d748ec><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/52e0e61c530644daa0f6665e022ab686><p class=pgc-img-caption></p></div><p style=text-align:start>贝叶斯岭回归用来解决回归问题:</p><pre><code>&gt;&gt;&gt; from sklearn import linear_model&gt;&gt;&gt; X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]&gt;&gt;&gt; Y = [0., 1., 2., 3.]&gt;&gt;&gt; reg = linear_model.BayesianRidge()&gt;&gt;&gt; reg.fit(X, Y)BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300, normalize=False, tol=0.001, verbose=False)Copy</code></pre><p style=text-align:start>在模型训练完成后，可以用来预测新值:</p><pre><code>&gt;&gt;&gt; reg.predict ([[1, 0.]])array([ 0.50000013])Copy</code></pre><p>权值</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d989ff3f11e45d49df6f0aa4ca47040><p class=pgc-img-caption></p></div><p>可以被这样访问:</p><pre><code>&gt;&gt;&gt; reg.coef_array([ 0.49999993,  0.49999993])Copy</code></pre><p style=text-align:start>由于贝叶斯框架的缘故，权值与 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">普通最小二乘法</span> 产生的不太一样。 但是，贝叶斯岭回归对病态问题（ill-posed）的鲁棒性要更好。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">示例</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">:</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">贝叶斯岭回归</span></p><p><strong>参考资料</strong></p><p>Section 3.3 in Christopher M. Bishop: Pattern Recognition and Machine Learning, 2006David J. C. MacKay, <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Bayesian Interpolation</span>, 1992.Michael E. Tipping, <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Sparse Bayesian Learning and the Relevance Vector Machine</span>, 2001.</p></blockquote><h1 class=pgc-h-arrow-right>1.1.10.2. 主动相关决策理论 - ARD</h1><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">ARDRegression</span></span> （主动相关决策理论）和 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">Bayesian Ridge Regression</span></span> 非常相似，但是会导致一个更加稀疏的权重w<span style="color:#4183c4;--tt-darkmode-color: #4183C4">[1][2]</span> 。</p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">ARDRegression</span></span> 提出了一个不同的</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d989ff3f11e45d49df6f0aa4ca47040><p class=pgc-img-caption></p></div><p>的先验假设。具体来说，就是弱化了高斯分布为球形的假设。</p><p>它采用</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d989ff3f11e45d49df6f0aa4ca47040><p class=pgc-img-caption></p></div><p>分布是与轴平行的椭圆高斯分布。</p><p>也就是说，每个权值</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1a0dd4578ec045a2a27d80888cc57b7f><p class=pgc-img-caption></p></div><p>从一个中心在 0 点，精度为</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/913bf994b7b34a7aad8f49e1f9355a54><p class=pgc-img-caption></p></div><p>的高斯分布中采样得到的。</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b07c725118fd4356afb3360e90cc20de><p class=pgc-img-caption></p></div><p>并且</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a9aafdf578984780a1a7a68f1813485e><p class=pgc-img-caption></p></div><p>.</p><p>与 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">Bayesian Ridge Regression</span></span><span style="color:#4183c4;--tt-darkmode-color: #4183C4">_</span> 不同， 每个</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7c7e8bf3b8e640a7ac0216fa2fea3368><p class=pgc-img-caption></p></div><p>都有一个标准差</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/735deea541154c8591dd9cbbbc0a2236><p class=pgc-img-caption></p></div><p>。所有</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/735deea541154c8591dd9cbbbc0a2236><p class=pgc-img-caption></p></div><p>的先验分布 由超参数</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/51c6ae64e9bd46de948d042ad32c76aa><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0849346be1134bcbbb6f4720ea16116c><p class=pgc-img-caption></p></div><p>确定的相同的 gamma 分布确定。</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dfe3fd43d46b4591b85ac14733714a16><p class=pgc-img-caption></p></div><p style=text-align:start>ARD 也被称为 <strong>稀疏贝叶斯学习</strong> 或 <strong>相关向量机</strong> [3][4]。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">示例</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">:</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">Automatic Relevance Determination Regression (ARD)</span></p><p><strong>参考资料</strong>:</p><p>[1] Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1[2] David Wipf and Srikantan Nagarajan: <span style="color:#4183c4;--tt-darkmode-color: #4183C4">A new view of automatic relevance determination</span>[3] Michael E. Tipping: <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Sparse Bayesian Learning and the Relevance Vector Machine</span>[4] Tristan Fletcher: <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Relevance Vector Machines explained</span></p></blockquote><h1 class=pgc-h-arrow-right>1.1.11. logistic 回归</h1><p style=text-align:start>logistic 回归，虽然名字里有 “回归” 二字，但实际上是解决分类问题的一类线性模型。在某些文献中，logistic 回归又被称作 logit 回归，maximum-entropy classification（MaxEnt，最大熵分类），或 log-linear classifier（对数线性分类器）。该模型利用函数 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">logistic function</span> 将单次试验（single trial）的可能结果输出为概率。</p><p style=text-align:start>scikit-learn 中 logistic 回归在 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LogisticRegression</span></span> 类中实现了二分类（binary）、一对多分类（one-vs-rest）及多项式 logistic 回归，并带有可选的 L1 和 L2 正则化。</p><blockquote><p><span style="color:#858585;--tt-darkmode-color: #858585">注意，scikit-learn的逻辑回归在默认情况下使用L2正则化，这样的方式在机器学习领域是常见的，在统计分析领域是不常见的。正则化的另一优势是提升数值稳定性。scikit-learn通过将C设置为很大的值实现无正则化。</span></p></blockquote><p style=text-align:start>作为优化问题，带 L2罚项的二分类 logistic 回归要最小化以下代价函数（cost function）：</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8286139a0e2b420aa65682a72eceb9e9><p class=pgc-img-caption></p></div><p style=text-align:start>类似地，带 L1 正则的 logistic 回归解决的是如下优化问题：</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/558a8665edef4757b4c7768b7590a510><p class=pgc-img-caption></p></div><p style=text-align:start><br></p><p style=text-align:start>Elastic-Net正则化是L1 和 L2的组合，来使如下代价函数最小:</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c50b8563ffb640fa93094d477c6bb2e0><p class=pgc-img-caption></p></div><p style=text-align:start>其中ρ控制正则化L1与正则化L2的强度(对应于<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">l1_ratio</span>参数)。</p><p style=text-align:start>注意，在这个表示法中，假定目标y_i在测试时应属于集合[-1,1]。我们可以发现Elastic-Net在ρ=1时与L1罚项等价,在ρ=0时与L2罚项等价</p><p style=text-align:start>在 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LogisticRegression</span></span> 类中实现了这些优化算法: <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">liblinear</span>， <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">newton-cg</span>， <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">lbfgs</span>， <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">sag</span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">saga</span>。</p><p style=text-align:start><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">liblinear</span>应用了座标下降算法（Coordinate Descent, CD），并基于 scikit-learn 内附的高性能 C++ 库 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">LIBLINEAR library</span> 实现。不过 CD 算法训练的模型不是真正意义上的多分类模型，而是基于 “one-vs-rest” 思想分解了这个优化问题，为每个类别都训练了一个二元分类器。因为实现在底层使用该求解器的 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LogisticRegression</span></span> 实例对象表面上看是一个多元分类器。 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">sklearn.svm.l1_min_c</span></span> 可以计算使用 L1时 C 的下界，以避免模型为空（即全部特征分量的权重为零）。</p><p style=text-align:start><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">lbfgs</span>, <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">sag</span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">newton-cg</span> 求解器只支持 L2罚项以及无罚项，对某些高维数据收敛更快。这些求解器的参数 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">multi_class</span>设为 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">multinomial</span> 即可训练一个真正的多项式 logistic 回归 [5] ，其预测的概率比默认的 “<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">one-vs-rest</span>” 设定更为准确。</p><p style=text-align:start><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">sag</span> 求解器基于平均随机梯度下降算法（Stochastic Average Gradient descent） [6]。在大数据集上的表现更快，大数据集指样本量大且特征数多。</p><p style=text-align:start><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">saga</span> 求解器 [7] 是 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">sag</span> 的一类变体，它支持非平滑（non-smooth）的 L1 正则选项 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">penalty="l1"</span> 。因此对于稀疏多项式 logistic 回归 ，往往选用该求解器。<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">saga</span>求解器是唯一支持弹性网络正则选项的求解器。</p><p style=text-align:start><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">lbfgs</span>是一种近似于Broyden–Fletcher–Goldfarb–Shanno算法[8]的优化算法，属于准牛顿法。<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">lbfgs</span>求解器推荐用于较小的数据集，对于较大的数据集，它的性能会受到影响。[9]</p><p style=text-align:start>总的来说，各求解器特点如下:</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fda3d048a91549dfbc09e693f9cc3e80><p class=pgc-img-caption></p></div><p style=text-align:start>默认情况下，<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">lbfgs</span>求解器鲁棒性占优。对于大型数据集，<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">saga</span>求解器通常更快。对于大数据集，还可以用 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">SGDClassifier</span></span> ，并使用对数损失（<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">log</span> loss）这可能更快，但需要更多的调优。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">示例</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">：</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">Logistic回归中的L1罚项和稀疏系数L1罚项-logistic回归的路径多项式和OVR的Logistic回归newgroups20上的多类稀疏Logistic回归使用多项式Logistic回归和L1进行MNIST数据集的分类</span></p><p><strong>与 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">liblinear</span> 的区别:</strong></p><p>当 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">fit_intercept=False</span> 拟合得到的 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">coef_</span> 或者待预测的数据为零时，用 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">solver=liblinear</span> 的 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LogisticRegression</span></span> 或 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LinearSVC</span> 与直接使用外部 liblinear 库预测得分会有差异。这是因为， 对于 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">decision_function</span> 为零的样本， <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LogisticRegression</span></span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LinearSVC</span> 将预测为负类，而 liblinear 预测为正类。 注意，设定了 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">fit_intercept=False</span> ，又有很多样本使得 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">decision_function</span> 为零的模型，很可能会欠拟合，其表现往往比较差。建议您设置 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">fit_intercept=True</span> 并增大 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">intercept_scaling</span> 。</p><p><strong>注意:利用稀疏 logistic 回归进行特征选择</strong></p><p>带 L1罚项的 logistic 回归 将得到稀疏模型（sparse model），相当于进行了特征选择（feature selection），详情参见 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">基于 L1 的特征选取</span>。</p></blockquote><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LogisticRegressionCV</span></span> 对 logistic 回归 的实现内置了交叉验证（cross-validation），可以找出最优的 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">C</span>和<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">l1_ratio</span>参数 。<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">newton-cg</span>， <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">sag</span>， <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">saga</span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">lbfgs</span> 在高维数据上更快，这是因为采用了热启动（warm-starting）。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">参考资料</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">：</span></p><p><span style="color:#858585;--tt-darkmode-color: #858585">[5] Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 4.3.4[6] Mark Schmidt, Nicolas Le Roux, and Francis Bach: </span><span style="color:#4183c4;--tt-darkmode-color: #4183C4">Minimizing Finite Sums with the Stochastic Average Gradient.</span>[7] Aaron Defazio, Francis Bach, Simon Lacoste-Julien: <span style="color:#4183c4;--tt-darkmode-color: #4183C4">SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives.</span>[8]<span style="color:#4183c4;--tt-darkmode-color: #4183C4">https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm</span>[9] <span style="color:#4183c4;--tt-darkmode-color: #4183C4">“Performance Evaluation of Lbfgs vs other solvers”</span></p></blockquote><h1 class=pgc-h-arrow-right>1.1.12. 随机梯度下降， SGD</h1><p style=text-align:start>随机梯度下降是拟合线性模型的一个简单而高效的方法。在样本量（和特征数）很大时尤为有用。 方法 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">partial_fit</span> 可用于 online learning （在线学习）或基于 out-of-core learning （外存的学习）</p><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">SGDClassifier</span></span> 和 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">SGDRegressor</span></span> 分别用于拟合分类问题和回归问题的线性模型，可使用不同的（凸）损失函数，支持不同的罚项。 例如，设定 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">loss="log"</span> ，则 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">SGDClassifier</span></span> 拟合一个逻辑斯蒂回归模型，而 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">loss="hinge"</span> 拟合线性支持向量机（SVM）。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">参考资料</span></strong></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">随机梯度下降</span></p></blockquote><h1 class=pgc-h-arrow-right>1.1.13. Perceptron（感知器）</h1><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">Perceptron</span></span> 是适用于大规模学习的一种简单算法。默认情况下：</p><ul><li>不需要设置学习率（learning rate）。</li><li>不需要正则化处理。</li><li>仅使用错误样本更新模型。</li></ul><p style=text-align:start>最后一点表明使用合页损失（hinge loss）的感知机比 SGD 略快，所得模型更稀疏。</p><h1 class=pgc-h-arrow-right>1.1.14. Passive Aggressive Algorithms（被动攻击算法）</h1><p style=text-align:start>被动攻击算法是大规模学习的一类算法。和感知机类似，它也不需要设置学习率，不过比感知机多出一个正则化参数 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">C</span> 。</p><p style=text-align:start>对于分类问题， <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">PassiveAggressiveClassifier</span></span> 可设定 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">loss='hinge'</span> （PA-I）或 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">loss='squared_hinge'</span>（PA-II）。对于回归问题， <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">PassiveAggressiveRegressor</span></span> 可设置 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">loss='epsilon_insensitive'</span> （PA-I）或 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">loss='squared_epsilon_insensitive'</span> （PA-II）。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">参考资料</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">：</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">“Online Passive-Aggressive Algorithms”</span> K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR 7 (2006)</p></blockquote><h1 class=pgc-h-arrow-right>1.1.15. 稳健回归（Robustness regression）: 处理离群点（outliers）和模型错误</h1><p style=text-align:start>稳健回归（robust regression）特别适用于回归模型包含损坏数据（corrupt data）的情况，如离群点或模型中的错误。</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4d3415e71155405586f6f8f9510108c7><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>1.1.15.1. 各种使用场景与相关概念</h1><p style=text-align:start>处理包含离群点的数据时牢记以下几点:</p><ul><li><strong>离群值在 X 上还是在 y 方向上</strong>?离群值在 y 方向上离群值在 X 方向上</li></ul><ul><li><strong>离群点的比例 vs. 错误的量级（amplitude）</strong>离群点的数量很重要，离群程度也同样重要。低离群点的数量高离群点的数量</li></ul><p style=text-align:start>稳健拟合（robust fitting）的一个重要概念是崩溃点（breakdown point），即拟合模型（仍准确预测）所能承受的离群值最大比例。</p><p style=text-align:start>注意，在高维数据条件下（ <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">n_features</span>大），一般而言很难完成稳健拟合，很可能完全不起作用。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">寻找平衡： 预测器的选择</span></strong></p><p><span style="color:#858585;--tt-darkmode-color: #858585">Scikit-learn提供了三种稳健回归的预测器（estimator）: </span><span style="color:#4183c4;--tt-darkmode-color: #4183C4">RANSAC</span> ， <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Theil Sen</span> 和 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">HuberRegressor</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">HuberRegressor</span> 一般快于 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">RANSAC</span> 和 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Theil Sen</span> ，除非样本数很大，即 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">n_samples</span> >> <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">n_features</span> 。 这是因为 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">RANSAC</span> 和 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Theil Sen</span> 都是基于数据的较小子集进行拟合。但使用默认参数时， <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Theil Sen</span> 和 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">RANSAC</span> 可能不如 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">HuberRegressor</span> 鲁棒。</p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">RANSAC</span> 比 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Theil Sen</span> 更快，在样本数量上的伸缩性（适应性）更好。<span style="color:#4183c4;--tt-darkmode-color: #4183C4">RANSAC</span> 能更好地处理y方向的大值离群点（通常情况下）。<span style="color:#4183c4;--tt-darkmode-color: #4183C4">Theil Sen</span> 能更好地处理x方向中等大小的离群点，但在高维情况下无法保证这一特点。 实在决定不了的话，请使用 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">RANSAC</span></p></blockquote><h1 class=pgc-h-arrow-right>1.1.15.2. RANSAC： 随机抽样一致性算法（RANdom SAmple Consensus）</h1><p style=text-align:start>随机抽样一致性算法（RANdom SAmple Consensus， RANSAC）利用全体数据中局内点（inliers）的一个随机子集拟合模型。</p><p style=text-align:start>RANSAC 是一种非确定性算法，以一定概率输出一个可能的合理结果，依赖于迭代次数（参数 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">max_trials</span>）。这种算法主要解决线性或非线性回归问题，在计算机视觉摄影测绘领域尤为流行。</p><p style=text-align:start>算法从全体样本输入中分出一个局内点集合，全体样本可能由于测量错误或对数据的假设错误而含有噪点、离群点。最终的模型仅从这个局内点集合中得出。</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/582cb03c289f45ecb6f92aebd2093241><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>1.1.15.2.1. 算法细节</h1><p style=text-align:start>每轮迭代执行以下步骤:</p><ol start=1><li>从原始数据中抽样 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">min_samples</span> 数量的随机样本，检查数据是否合法（见 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">is_data_valid</span> ）。</li><li>用一个随机子集拟合模型（ <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">base_estimator.fit</span> ）。检查模型是否合法（见 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">is_model_valid</span> ）。</li><li>计算预测模型的残差（residual），将全体数据分成局内点和离群点（ <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">base_estimator.predict(X) - y</span>）。绝对残差小于 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">residual_threshold</span> 的全体数据认为是局内点。</li><li>若局内点样本数最大，保存当前模型为最佳模型。以免当前模型离群点数量恰好相等（而出现未定义情况），规定仅当数值大于当前最值时认为是最佳模型。</li></ol><p style=text-align:start>上述步骤或者迭代到最大次数（ <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">max_trials</span> ），或者某些终止条件满足时停下（见 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">stop_n_inliers</span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">stop_score</span> )。最终模型由之前确定的最佳模型的局内点样本（一致性集合，consensus set）预测。</p><p style=text-align:start>函数 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">is_data_valid</span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">is_model_valid</span> 可以识别出随机样本子集中的退化组合（degenerate combinations）并予以丢弃（reject）。即便不需要考虑退化情况，也会使用 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">is_data_valid</span> ，因为在拟合模型之前调用它能得到更高的计算性能。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">示例</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">：</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">基于RANSAC的稳健线性模型估计稳健线性估计拟合</span></p><p><strong>参考资料</strong>：</p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">https://en.wikipedia.org/wiki/RANSAC“Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography”</span> Martin A. Fischler and Robert C. Bolles - SRI International (1981)<span style="color:#4183c4;--tt-darkmode-color: #4183C4">“Performance Evaluation of RANSAC Family”</span> Sunglok Choi, Taemin Kim and Wonpil Yu - BMVC (2009)</p></blockquote><h1 class=pgc-h-arrow-right>1.1.15.3. Theil-Sen 预估器: 广义中值估计器（generalized-median-based estimator）</h1><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">TheilSenRegressor</span></span> 估计器：使用中位数在多个维度泛化，对多元异常值更具有鲁棒性，但问题是，随着维数的增加，估计器的准确性在迅速下降。准确性的丢失，导致在高维上的估计值比不上普通的最小二乘法。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">示例</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">:</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">广义中值估计器回归稳健线性估计拟合</span></p><p><strong>参考资料</strong>:</p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator</span></p></blockquote><h1 class=pgc-h-arrow-right>1.1.15.3.1. 算法理论细节</h1><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">TheilSenRegressor</span></span> 在渐近效率和无偏估计方面足以媲美 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Ordinary Least Squares (OLS)</span> （普通最小二乘法（OLS））。与 OLS 不同的是， Theil-Sen 是一种非参数方法，这意味着它没有对底层数据的分布假设。由于 Theil-Sen 是基于中值的估计，它更适合于损坏的数据即离群值。 在单变量的设置中，Theil-Sen 在简单的线性回归的情况下，其崩溃点大约 29.3% ，这意味着它可以容忍任意损坏的数据高达 29.3% 。</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4d3415e71155405586f6f8f9510108c7><p class=pgc-img-caption></p></div><p style=text-align:start>scikit-learn 中实现的 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">TheilSenRegressor</span></span> 是多元线性回归模型的推广 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">[8]</span> ，利用了空间中值方法，它是多维中值的推广 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">[9]</span> 。</p><p style=text-align:start>关于时间复杂度和空间复杂度，Theil-Sen 的尺度根据</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cb6c6fd1756843ca96a312158150661d><p class=pgc-img-caption></p></div><p style=text-align:start>这使得它不适用于大量样本和特征的问题。因此，可以选择一个亚群的大小来限制时间和空间复杂度，只考虑所有可能组合的随机子集。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">示例</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">:</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">广义中值估计器回归</span></p><p><strong>参考资料</strong>:</p><p>[10] Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Theil-Sen Estimators in a Multiple Linear Regression Model.</span> |[11] Kärkkäinen and S. Äyrämö: <span style="color:#4183c4;--tt-darkmode-color: #4183C4">On Computation of Spatial Median for Robust Data Mining.</span></p></blockquote><h1 class=pgc-h-arrow-right>1.1.15.4. Huber 回归</h1><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">HuberRegressor</span></span> 与 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">Ridge</span></span> 不同，因为它对于被分为异常值的样本应用了一个线性损失。如果这个样品的绝对误差小于某一阈值，样品就被分为内围值。 它不同于 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">TheilSenRegressor</span></span> 和 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">RANSACRegressor</span></span> ，因为它没有忽略异常值的影响，并分配给它们较小的权重。</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a5d884276c584c159a1e4a221c45aca5><p class=pgc-img-caption></p></div><p style=text-align:start>这个 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">HuberRegressor</span></span> 最小化的损失函数是：</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/da62517d44a2478da467443e7f74fb96><p class=pgc-img-caption></p></div><p style=text-align:start>其中</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9b51492aff884fa787a9fe7dbe6694ab><p class=pgc-img-caption></p></div><p style=text-align:start>建议设置参数 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">epsilon</span> 为 1.35 以实现 95% 统计效率。</p><h1 class=pgc-h-arrow-right>1.1.15.5. 注意</h1><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">HuberRegressor</span></span> 与将损失设置为 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">huber</span>的 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">SGDRegressor</span></span> 并不相同，体现在以下方面的使用方式上。</p><ul><li><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">HuberRegressor</span></span> 是标度不变性的. 一旦设置了 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">epsilon</span> ， 通过不同的值向上或向下缩放 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">X</span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">y</span> ，就会跟以前一样对异常值产生同样的鲁棒性。相比 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">SGDRegressor</span></span> 其中 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">epsilon</span> 在 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">X</span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">y</span> 被缩放的时候必须再次设置。</li><li><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">HuberRegressor</span></span> 应该更有效地使用在小样本数据，同时 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">SGDRegressor</span></span> 需要一些训练数据的 passes 来产生一致的鲁棒性。</li></ul><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">示例</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">:</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">强</span><span style="color:#4183c4;--tt-darkmode-color: #4183C4">异常数据集上的huberregression与 Ridge</span></p><p><strong>参考资料</strong>:</p><p>Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale estimates, pg 172</p></blockquote><p style=text-align:start>另外，这个估计是不同于 R 实现的 Robust Regression (<span style="color:#4183c4;--tt-darkmode-color: #4183C4">http://www.ats.ucla.edu/stat/r/dae/rreg.htm</span>) ，因为 R 实现加权最小二乘，权重考虑到每个样本并基于残差大于某一阈值的量。</p><h1 class=pgc-h-arrow-right>1.1.16. 多项式回归：用基函数展开线性模型</h1><p style=text-align:start>机器学习中一种常见的模式，是使用线性模型训练数据的非线性函数。这种方法保持了一般快速的线性方法的性能，同时允许它们适应更广泛的数据范围。</p><p style=text-align:start>例如，可以通过构造系数的 <strong>polynomial features</strong> 来扩展一个简单的线性回归。在标准线性回归的情况下，你可能有一个类似于二维数据的模型:</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/22f0755614784fd799ccff85e5deb319><p class=pgc-img-caption></p></div><p style=text-align:start>如果我们想把抛物面拟合成数据而不是平面，我们可以结合二阶多项式的特征，使模型看起来像这样:</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/78d856f2a8d24ed3ad550e7e996077c7><p class=pgc-img-caption></p></div><p style=text-align:start>观察到这 <em>还是一个线性模型</em> （这有时候是令人惊讶的）: 看到这个，想象创造一个新的变量</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d41b7573b9bb48b78caefae54a9d5370><p class=pgc-img-caption></p></div><p style=text-align:start>有了这些重新标记的数据，我们可以将问题写成</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5654a29f26f5448bba6bbd58dd4ee659><p class=pgc-img-caption></p></div><p>我们看到，所得的 <em>polynomial regression</em> 与我们上文所述线性模型是同一类（即关于</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d989ff3f11e45d49df6f0aa4ca47040><p class=pgc-img-caption></p></div><p>是线性的），因此可以用同样的方法解决。通过用这些基函数建立的高维空间中的线性拟合，该模型具有灵活性，可以适应更广泛的数据范围。</p><p style=text-align:start>这里是一个示例，使用不同程度的多项式特征将这个想法应用于一维数据:</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1875b0d429454d87ad3db365d055b36e><p class=pgc-img-caption></p></div><p style=text-align:start>这个图是使用 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">PolynomialFeatures</span></span> 预创建。该预处理器将输入数据矩阵转换为给定度的新数据矩阵。使用方法如下:</p><pre><code>&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; X = np.arange(6).reshape(3, 2)&gt;&gt;&gt; Xarray([[0, 1], [2, 3], [4, 5]])&gt;&gt;&gt; poly = PolynomialFeatures(degree=2)&gt;&gt;&gt; poly.fit_transform(X)array([[  1.,   0.,   1.,   0.,   0.,   1.], [  1.,   2.,   3.,   4.,   6.,   9.], [  1.,   4.,   5.,  16.,  20.,  25.]])Copy</code></pre><p><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">X</span> 的特征已经从</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6a66a12f73b541a1984debde8ad11e4d><p class=pgc-img-caption></p></div><p>转换到</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/245df5305fe14b3c89ceb4ae622fa77a><p class=pgc-img-caption></p></div><p>, 并且现在可以用在任何线性模型。</p><p style=text-align:start>这种预处理可以通过 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Pipeline</span> 工具进行简化。可以创建一个表示简单多项式回归的单个对象，使用方法如下所示:</p><pre><code>&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; from sklearn.linear_model import LinearRegression&gt;&gt;&gt; from sklearn.pipeline import Pipeline&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; model = Pipeline([('poly', PolynomialFeatures(degree=3)),...                   ('linear', LinearRegression(fit_intercept=False))])&gt;&gt;&gt; # fit to an order-3 polynomial data&gt;&gt;&gt; x = np.arange(5)&gt;&gt;&gt; y = 3 - 2 * x + x ** 2 - x ** 3&gt;&gt;&gt; model = model.fit(x[:, np.newaxis], y)&gt;&gt;&gt; model.named_steps['linear'].coef_array([ 3., -2.,  1., -1.])Copy</code></pre><p style=text-align:start>利用多项式特征训练的线性模型能够准确地恢复输入多项式系数。</p><p>在某些情况下，没有必要包含任何单个特征的更高的幂，只需要相乘最多</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/66a5cddd27b14b1088a524c58d9a7364><p class=pgc-img-caption></p></div><p>个不同的特征即可，所谓 <em>interaction features（交互特征）</em> 。这些可通过设定 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">PolynomialFeatures</span></span> 的 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">interaction_only=True</span> 得到。</p><p>例如，当处理布尔属性，对于所有</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3cb406b96b60424a8f82adbc5a9f37d0><p class=pgc-img-caption></p></div><p></p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/12abc4634a82402f8a4edc34d0b56a7e><p class=pgc-img-caption></p></div><p>，因此是无用的；但</p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bf939892acf945a88cab56a882084922><p class=pgc-img-caption></p></div><p>代表两布尔结合。这样我们就可以用线性分类器解决异或问题:</p><pre><code>&gt;&gt;&gt; from sklearn.linear_model import Perceptron&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])&gt;&gt;&gt; y = X[:, 0] ^ X[:, 1]&gt;&gt;&gt; yarray([0, 1, 1, 0])&gt;&gt;&gt; X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)&gt;&gt;&gt; Xarray([[1, 0, 0, 0], [1, 0, 1, 0], [1, 1, 0, 0], [1, 1, 1, 1]])&gt;&gt;&gt; clf = Perceptron(fit_intercept=False, max_iter=10, tol=None,...                  shuffle=False).fit(X, y)Copy</code></pre><p style=text-align:start>分类器的 “predictions” 是完美的:</p><pre><code>&gt;&gt;&gt; clf.predict(X)array([0, 1, 1, 0])&gt;&gt;&gt; clf.score(X, y)1.0Copy</code></pre><p style=text-align:start><br></p><hr><p>一直在努力!<br></p><div class=pgc-img><img alt=广义线性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a6020647f68344fda2e5d786f154a01a><p class=pgc-img-caption></p></div><p><br></p><p>最后，小编想说：我是一名python开发工程师，</p><p>整理了一套最新的python系统学习教程，</p><p>想要这些资料的可以关注私信小编“1或者6”即可（免费分享哦）希望能对你有所帮助.</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'广义线性','模型'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>