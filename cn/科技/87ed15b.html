<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>ACL 2019论文｜为知识图谱添加注意力机制 | 极客快訊</title><meta property="og:title" content="ACL 2019论文｜为知识图谱添加注意力机制 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RVvkOoB1KZNTom"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/87ed15b.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/87ed15b.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/87ed15b.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/87ed15b.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/87ed15b.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/87ed15b.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/87ed15b.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/87ed15b.html><meta property="article:published_time" content="2020-10-29T21:05:26+08:00"><meta property="article:modified_time" content="2020-10-29T21:05:26+08:00"><meta name=Keywords content><meta name=description content="ACL 2019论文｜为知识图谱添加注意力机制"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/87ed15b.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>ACL 2019论文｜为知识图谱添加注意力机制</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><blockquote><h5>注意力机制（Attention）是近些年来提出的一种改进神经网络的方法，在图像识别、自然语言处理和图网络表示等领域都取得了很好的效果，可以说注意力机制的加入极大地丰富了神经网络的表示能力。</h5></blockquote><p>论文原文：</p><p>https://arxiv.org/pdf/1906.01195.pdf</p><p><strong>01</strong></p><p><strong>论文背景</strong></p><p>知识图谱（Knowledge Graph）可以用结构化的方式来描述真实世界的知识，如一个三元组（小罗伯特·唐尼，国籍，美国）便能够说明小罗伯特·唐尼的国籍是美国这样一件事实。然而这样的结构化表示并不是很利于计算，因而近些年来知识图谱表示学习受到了广泛的研究。</p><p>ConvE[1]和ConvKB[2]等基于卷积神经网络（CNN）的表示学习方法可以为知识图谱生成更丰富和表现力更强的向量表示，也取得了很好的实验效果。但是这些模型仅仅独立地表示每一个三元组，而忽略了三元组周围的邻居中蕴含的复杂语义信息，这在一定程度上限制了这类模型的性能。为此，今天我们要介绍的这篇论文将注意力机制引入到了每个三元组的邻居中，每个实体的表示都与其邻居息息相关。值得注意的是，传统的表示学习模型在进行实体链接预测时，是没有解释性的，仅仅可以给出结果。但是加入了注意力机制后，我们便可以使用每个邻居的注意力权重为模型的预测结果做出一定的解释。</p><p><strong>02</strong></p><p><strong>论文模型</strong></p><p>本篇论文的模型大致可以分为两部分，第一部分为加入注意力机制的编码器（Encoder），第二部分为解码器（Decoder）。论文的重点和主要贡献便是第一部分，加入注意力机制的编码器。</p><p>该模型的注意力机制是基于之前的图的注意力网络（GAT[3]）,GAT是GCN[4]的一个改进版本，它解决了GCN平等地从邻居中获取信息的缺点。更详细的说，GAT会为一个节点的每条边都学习一个注意力权重，然后按照这些权重从邻居中获取信息：</p><img alt="ACL 2019论文｜为知识图谱添加注意力机制" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RVvkOoB1KZNTom><p>在上式中，alpha_ij即是连接i和j两个节点的边的权重，N_i是i节点的邻居节点集合，<strong>W</strong>为一个线性映射矩阵，最终该式输出的便是i节点的邻居化表示。</p><img alt="ACL 2019论文｜为知识图谱添加注意力机制" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RVvkOobI7WyX5m><p>虽然GAT模型在传统的网络表示学习中很成功，但是还是不太能直接地应用到知识图谱中，因为它显然地忽略了边所蕴含的信息。在知识图谱中，一个实体在链接不同的边时往往有着不同的含义，如上图所示，Christopher Nolan在链接不同的边时可以有兄长或导演的含义。因而该篇论文在GAT的基础上进行了改进，加入了关系的信息。对于每一个三元组，学习一个表示如下：</p><img alt="ACL 2019论文｜为知识图谱添加注意力机制" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RVvkOoyI3NoYD7><p>其中h_i，h_j和g_k分别为头尾实体以及关系的向量表示，<strong>W</strong>_1为线性变换矩阵。之后经过一个LeakyRelu非线性层，可以得到：</p><img alt="ACL 2019论文｜为知识图谱添加注意力机制" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RVvkOpE9OW5epz><p>之后可以计算每个三元组的注意力权重：</p><img alt="ACL 2019论文｜为知识图谱添加注意力机制" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RVvkOpW6w88h5j><p>这里N_i为实体i的邻居节点，R_in为链接实体i和n的关系。此外该模型还加入了multi-head attention机制，可以使学习过程更加稳定，并且学习更多的邻居信息。最终每个实体的邻居表示为：</p><img alt="ACL 2019论文｜为知识图谱添加注意力机制" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RVvkP18CNbVh0z><p>这里M便是代表有M个独立的注意力计算机制，对于关系向量，原论文仅将其乘以一个线性变换矩阵，未再加入注意力机制。</p><p>此外，在学习实体的邻居表示时，我们损失了实体初始的向量表示，因而在最终输出之前，会将初始的向量表示乘以一个变换矩阵加实体的邻居表示中。最终模型图如下，其中黄色的圆圈代表初始的实体向量，绿色圆圈代表初始的关系向量。原作者的模型图画的有些不清晰明了，若要详细了解还需阅读论文：</p><img alt="ACL 2019论文｜为知识图谱添加注意力机制" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RVvkP1VGJwcbI6><p>论文使用ConvKB作为模型的解码器（Decoder），对于每一个三元组，在上述编码器得到实体和关系向量的基础上，做出如下打分：</p><img alt="ACL 2019论文｜为知识图谱添加注意力机制" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RVvkP1s51u8r87><p>其中omega^m为第m个卷积核，模型最终使用soft-margin loss进行训练。</p><p><strong>03</strong></p><p><strong>论文实验</strong></p><p>该论文在FB15K-237、WN18RR、NELL-995和Kinship等数据集上进行了链接预测实验，实验结果如下所示。从表中可以看出，在多数情况下该模型可以达到目前最佳的实验效果，这说明融合邻居信息的表示能够很好的提升模型的性能，注意力机制对于知识图谱也是很有效果的。</p><img alt="ACL 2019论文｜为知识图谱添加注意力机制" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RVvkP27FrAWat5><img alt="ACL 2019论文｜为知识图谱添加注意力机制" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RVvkP2R8oGeiGQ><p><strong>参考文献</strong></p><p>[1] Dettmers T, Minervini P, Stenetorp P, et al. Convolutional 2d knowledge graph embeddings[C]//Thirty-Second AAAI Conference on Artificial Intelligence. 2018.</p><p>[2] Nguyen D Q, Nguyen T D, Nguyen D Q, et al. A novel embedding model for knowledge base completion based on convolutional neural network[J]. arXiv preprint arXiv:1712.02121, 2017.</p><p>[3] Veličković P, Cucurull G, Casanova A, et al. Graph attention networks[J]. arXiv preprint arXiv:1710.10903, 2017.</p><p>[4] Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907, 2016.</p><img alt="ACL 2019论文｜为知识图谱添加注意力机制" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RV4ztwHCSi5YfA><p>学术头条已建立微信交流群，想进群的同学请加学术君微信：AMiner308，记得备注：名字+单位/学校噢！</p><img alt="ACL 2019论文｜为知识图谱添加注意力机制" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RDdPwEE8kwJa88><p><strong>分享干货</strong></p><p>AMiner迄今为止已发布18期AI系列研究报告，您可在后台回复<strong>对应数字</strong>获取报告。</p><p><strong>推荐阅读</strong>（点击查看↓）</p><p></p><h2>✦基于 GraphSAGE 的结构特征学习，了解一下</h2><h2>✦TuckER：基于张量因式分解的知识图谱补全</h2><h2>✦ICML2019论文 | 炼丹？找到神经网络的全局最优解</h2><h2>✦ 用户画像: 信息抽取方法概览</h2><h2>✦ Google Brain最新论文：标签平滑何时才是有用的？</h2><h2>✦ AI Time 第三期激辩|知识图谱的构建主要靠人工还是机器？</h2><h2>✦ CHI2019论文| 如何使用前置摄像头玩出手机交互新花样</h2><h2>微信公众号菜单栏为大家设置了<strong>“论文推荐”</strong>和<strong>“优质分享”</strong>专栏，欢迎大家关注。</h2><img alt="ACL 2019论文｜为知识图谱添加注意力机制" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RV05a8eGTM3bO9><p><strong>您的转发就是我们最大的动力</strong></p><p>点击阅读原文查看更多AMiner学术文章</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'ACL','2019','论文'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>