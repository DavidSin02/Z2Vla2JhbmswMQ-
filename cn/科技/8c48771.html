<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>Scrapy爬虫框架新手入门教程 | 极客快訊</title><meta property="og:title" content="Scrapy爬虫框架新手入门教程 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/c75cdad38aac4ed1b4e854b3ae6104ec"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8c48771.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8c48771.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/8c48771.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8c48771.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8c48771.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/8c48771.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/8c48771.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8c48771.html><meta property="article:published_time" content="2020-10-29T20:59:31+08:00"><meta property="article:modified_time" content="2020-10-29T20:59:31+08:00"><meta name=Keywords content><meta name=description content="Scrapy爬虫框架新手入门教程"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/8c48771.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>Scrapy爬虫框架新手入门教程</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>​Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p><hr><p><br></p><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c75cdad38aac4ed1b4e854b3ae6104ec><p class=pgc-img-caption></p></div><p><br></p><p><strong>目录</strong></p><p>安装</p><p>测试</p><p>新建工程</p><p>创建spider文件（以豆瓣电影为例）</p><p>架构(绿线是数据流向)</p><p>运作流程（个人理解）</p><p>制作步骤</p><p>在item中指明爬取字段</p><p>编写spider/movie.py</p><p>数据存至数据库</p><p>其他</p><p>最后</p><hr><h1 class=pgc-h-arrow-right>安装</h1><p>pip安装，可能会报错：</p><pre><code>pip install Scrapy</code></pre><p>anaconda安装，推荐：</p><pre><code>conda install -c conda-forge scrapy</code></pre><h1 class=pgc-h-arrow-right>测试</h1><pre><code>scrapy</code></pre><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b5920e05b8574135b7044f4b5fbca52e><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>新建工程</h1><pre><code>scrapy startproject &lt;工程名&gt;</code></pre><p>如</p><pre><code>scrapy startproject douban</code></pre><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b9071a00f59e45bf9c8d62169df9bd41><p class=pgc-img-caption></p></div><p>创建的目录结构</p><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/efe20c6cc0514f958379e888daf0b107><p class=pgc-img-caption></p></div><ul><li><strong>scrapy.cfg: </strong>项目配置文件</li><li><strong>douban/: </strong>项目python模块, 代码将从这里导入</li><li><strong>douban/items.py: </strong>项目items文件，存要爬取的字段信息，可以插入数据库、写入txt等</li><li><strong>douban/pipelines.py: </strong>项目管道文件，将爬取的数据进行持久化存储</li><li><strong>douban/settings.py: </strong>项目配置文件，可以配置数据库等</li><li><strong>douban/spiders/: </strong>放置spider的目录，也就是你要写逻辑代码的地方</li><li><strong>douban/middlewares：</strong>中间件，请求和响应都将经过他，可以配置请求头、代理、cookie、会话维持等</li></ul><p><br></p><h1 class=pgc-h-arrow-right>创建spider文件（以豆瓣电影为例）</h1><pre><code>scrapy genspider &lt;项目名&gt; &lt;爬取域&gt;</code></pre><p>如</p><pre><code>cd doubanscrapy genspider movie movie.douban.com</code></pre><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f35e37b30fd149218ab9fdebae5cfec4><p class=pgc-img-caption></p></div><p>将在spiders文件夹下自动创建movie.py，并自动生成内容：</p><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b469298e97654ab496a0b929bf8b023e><p class=pgc-img-caption></p></div><p>可以看出，要建立一个Spider， 你必须用scrapy.Spider类创建一个子类，并确定了<strong>三个强制的属性 和 一个方法</strong>。</p><ul><li><strong>name = "" ：</strong>爬虫的识别名称，必须是唯一的，在不同的爬虫必须定义不同的名字。</li><li><strong>allow_domains = [] ：</strong>是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的URL会被忽略。</li><li><strong>start_urls = () ：</strong>爬取的URL元祖/列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。</li><li><strong>parse(self, response) ：</strong>解析的方法，每个初始URL完成下载后将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数，主要作用如下：负责解析返回的网页数据(response.body)，提取结构化数据(生成item)；生成需要下一页的URL请求。</li></ul><h1 class=pgc-h-arrow-right>架构(绿线是数据流向)</h1><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f718301285ea49f092e274e89191abcc><p class=pgc-img-caption></p></div><p><br></p><ul><li><strong>Scrapy Engine(引擎)</strong>: 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。</li><li><strong>Scheduler(调度器)</strong>: 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。</li><li><strong>Downloader（下载器）</strong>：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理，</li><li><strong>Spider（爬虫）</strong>：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器).</li><li><strong>Item Pipeline(管道)</strong>：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方。</li><li><strong>Downloader Middlewares（下载中间件）</strong>：你可以当作是一个可以自定义扩展下载功能的组件。</li><li><strong>Spider Middlewares（Spider中间件）</strong>：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests）</li></ul><h1 class=pgc-h-arrow-right>运作流程（个人理解）</h1><ul><li>用户编写spider并运行</li><li>将第一个URL传给引擎</li><li>引擎将URL对应的request传给调度器</li><li>调度器将request排序入队</li><li>调度器将处理好的request返回到引擎</li><li>引擎将request按照下载中间件的设置传给下载器</li><li>下载器执行request并获得response（如果下载失败，然后引擎告诉调度器，这个request下载失败了，你记录一下，待会儿再下载）</li><li>下载器将response返回到引擎</li><li>引擎将request返回到spider用户这（默认交到def parse()这个函数处理）</li><li>spider处理完数据后，将需要跟进的URL和要保存的item传给引擎</li><li>引擎将item传给管道进行处理保存，并将URL进入下一轮循环</li><li>只有当调度器中不存在任何request了，整个程序才会停止，（也就是说，对于下载失败的URL，Scrapy也会重新下载。）</li></ul><h1 class=pgc-h-arrow-right>制作步骤</h1><ol start=1><li>新建项目 (scrapy startproject xxx)：新建一个新的爬虫项目</li><li>明确目标 （编写items.py）：明确你想要抓取的目标</li><li>制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页</li><li>存储内容 （pipelines.py）：设计管道存储爬取内容</li></ol><h1 class=pgc-h-arrow-right>在item中指明爬取字段</h1><p>如“名称”、“评分”、“简介”</p><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/946bd1a27ff1403d8cf5d0ecd9db9925><p class=pgc-img-caption></p></div><ul><li>Item 定义结构化数据字段，用来保存爬取到的数据，有点像 Python 中的 dict，但是提供了一些额外的保护减少错误。</li><li>可以通过创建一个 scrapy.Item 类， 并且定义类型为 scrapy.Field 的类属性来定义一个 Item（可以理解成类似于 ORM 的映射关系）。</li></ul><p>在item.py中修改为：</p><pre><code>class DoubanItem(scrapy.Item):    name = scrapy.Field()    rating_num = scrapy.Field()    quote = scrapy.Field()</code></pre><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ba8952f814be45ec9ef85ef9ba7106d1><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>编写spider/movie.py</h1><p>1、选择目标的xpath（也可以css等其他选择器）</p><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/48bfd07533b34b5e8a41e60d6e4b9d86><p class=pgc-img-caption></p></div><p>2、提取出公共部分</p><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/73c0f9f64e364d37a3da1210519ffbbf><p class=pgc-img-caption></p></div><p><br></p><p>3、由于豆瓣有反爬验证，因此需要加上header</p><pre><code>def start_requests(self):    url = 'http://movie.douban.com/top250/'    yield scrapy.Request(url, headers=self.headers)</code></pre><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/08d08902857f47e3a95a59f7dc18af1c><p class=pgc-img-caption></p></div><p>通过start_requests函数，对于运行后第一次访问请求，就加上了请求头。因此，start_urls其实也可以不加。</p><p><br></p><p>4、为了方便调试，新建spider/main.py，并写入</p><pre><code>from scrapy.cmdline import executeexecute(["scrapy", "crawl", "movie", "-o", "item.json"])</code></pre><p><br></p><p>5、测试一下效果</p><pre><code>class MovieSpider(scrapy.Spider):    name = 'movie'    allowed_domains = ['movie.douban.com/top250']    start_urls = ['http://movie.douban.com/top250/']    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}    def start_requests(self):        url = 'http://movie.douban.com/top250/'        yield scrapy.Request(url, headers=self.headers)    def parse(self, response):        for each in response.xpath('//*[@id="content"]/div/div[1]/ol/li'):            print(each.xpath('./div/div[2]/div[1]/a/span[1]').extract())</code></pre><p>运行main.py</p><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/abd8cb828e584c878d0147524d14c8c8><p class=pgc-img-caption></p></div><p>要提取中间的文字，则在xpath后面再添加<strong>“/text()”</strong></p><p><br></p><p>6、类似地，完善parse()函数</p><pre><code>from ..items import DoubanItem</code></pre><pre><code>def parse(self, response):    # 将得到的数据封装到一个DoubanItem对象，就是在items.py里的    item = DoubanItem()    for each in response.xpath('//*[@id="content"]/div/div[1]/ol/li'):        name = each.xpath('./div/div[2]/div[1]/a/span[1]/text()').extract_first()        rating_num = each.xpath('./div/div[2]/div[2]/div/span[2]/text()').extract_first()        quote = each.xpath('./div/div[2]/div[2]/p[2]/span/text()').extract_first()        item['name'] = name        item['rating_num'] = rating_num        item['quote'] = quote        yield item</code></pre><p><br></p><p>7、运行main.py后，在spider/item.json里将看到爬取的数据，以Unicode字符形式。</p><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/03d8a192ab7d4e96a9bdcdf350d113e7><p class=pgc-img-caption></p></div><p><br></p><p>8、还可以保存为其他形式，如csv、xml，只需将item.json改为item.csv等。</p><p><br></p><p><strong>数据存至数据库</strong></p><p>1、建库建表</p><pre><code>mysql -uroot -p Enter password:create database scrapy;use scrapy;create table movie(id int auto_increment primary key, name varchar(255),rating varchar(10), quote varchar(255))default charset=utf8;</code></pre><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ad9ca9c36f6a4472b167874a171c920a><p class=pgc-img-caption></p></div><p><br></p><p>2、在setting.py中配置数据库连接</p><pre><code>mysql_movie = {    'host': "127.0.0.1",    "port": 3306,    "user": "root",    "password": "pwd",    "db": "scrapy"}</code></pre><p><br></p><p>3、在setting.py中将以下内容取消注释</p><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5296fb80e3c34e8cba818c3ddf648b57><p class=pgc-img-caption></p></div><p><br></p><p>4、在pipelines.py中连接数据库存储数据</p><pre><code>pip install pymysql</code></pre><pre><code># -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.htmlfrom .settings import mysql_movieimport pymysqlclass DoubanPipeline:    def __init__(self):        self.host = mysql_movie["host"]        self.port = mysql_movie["port"]        self.user = mysql_movie["user"]        self.password = mysql_movie["password"]        self.db = mysql_movie["db"]        self.conn = pymysql.connect(host=self.host, port=self.port, user=self.user, password=self.password, db=self.db, charset='utf8')        self.cursor = self.conn.cursor()    def process_item(self, item, spider):        sql ='''insert into movie(name, rating, quote)values('%s','%s','%s')''' % (item["name"], item["rating_num"], item["quote"])        try:            self.cursor.execute(sql)            self.conn.commit()        except:            self.conn.rollback()        return item</code></pre><p><br></p><p>5、运行main.py后，查询数据库</p><pre><code>select * from movie;</code></pre><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3f412e60b99b439ab4ff555f68c92dfc><p class=pgc-img-caption></p></div><p><br></p><h1 class=pgc-h-arrow-right>其他</h1><p>URL跟进（翻页）；在parse函数最后，跟新以下URL，scrapy框架会自动发起下一次请求</p><pre><code>def parse(self, response):    # 将得到的数据封装到一个DoubanItem对象    item = DoubanItem()    for each in response.xpath('//*[@id="content"]/div/div[1]/ol/li'):        name = each.xpath('./div/div[2]/div[1]/a/span[1]/text()').extract_first()        rating_num = each.xpath('./div/div[2]/div[2]/div/span[2]/text()').extract_first()        quote = each.xpath('./div/div[2]/div[2]/p[2]/span/text()').extract_first()        item['name'] = name        item['rating_num'] = rating_num        item['quote'] = quote        yield item    next_url = response.xpath('//*[@id="content"]/div/div[1]/div[2]/span[3]/link/@href').extract()   if next_url:       next_url = 'https://movie.douban.com/top250' + next_url[0]       print(next_url)       yield scrapy.Request(next_url, headers=self.headers)</code></pre><p>为了做一个乖爬虫，且避免面向监狱编程，建议在setting.py至少开启以下两项：</p><div class=pgc-img><img alt=Scrapy爬虫框架新手入门教程 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/75952e69dd7b4a5c900de4d0a0761d6c><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>最后</h1><p>相信你跟我一样，过完本文，对scrapy已经有了一个大致的了解。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'Scrapy','爬虫','入门'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>