<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>反向传播：学习因素 | 极客快訊</title><meta property="og:title" content="反向传播：学习因素 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p9.pstatp.com/large/pgc-image/9e50c59f72074b2090cb85bb714fcd53"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/92d85e1.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/92d85e1.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/92d85e1.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/92d85e1.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/92d85e1.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/92d85e1.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/92d85e1.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/92d85e1.html><meta property="article:published_time" content="2020-10-29T21:05:34+08:00"><meta property="article:modified_time" content="2020-10-29T21:05:34+08:00"><meta name=Keywords content><meta name=description content="反向传播：学习因素"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/92d85e1.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>反向传播：学习因素</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p>反向传播算法最初是在20世纪70年代引入的，但直到1986年由David Rumelhart，Geoffrey Hinton和Ronald Williams撰写的著名论文才重视其重要性。本文描述了几种神经网络，其中反向传播的工作速度远远快于早期的学习方法，因此可以使用神经网络来解决以前不可解决的问题。今天，反向传播算法是神经网络学习的主力。</p><p>尽管反向传播是一种广泛使用且最成功的训练神经网络的算法，但有几个因素会影响误差反向传播训练算法。这些因素如下。</p><h1>1.初始权重</h1><p>要训​​练的神经网络的权重初始化有助于最终解决方案。最初在训练之前，网络的网络权重分配给小的随机均匀值。如果所有权重都以相等的权重值开始，那么如果所需的最终权重不相等，就有很大的可能会有不好的解决方法。类似地，如果初始随机均匀权重不均匀（在0到1之间），则存在陷入全局最小值的机会，并且因为我们的学习速率很小，所以朝向全局最小值的步骤非常小。如果初始权重远离误差图的全局最小值，则网络学习非常慢，因为网络的误差是网络权重的函数。为了更快地学习神经网络的网络初始权重，应该更接近误差图的全局最小值。该算法的许多实证研究指出，超过某一低误差平台的继续训练导致不希望的权重漂移，这导致误差增加，并且由网络实现的映射函数继续减少。为了解决这个问题，应该使用新初始化的随机均匀权重重新开始训练。</p><h1>2.累积权重调整与增量更新</h1><p>误差反向传播学习技术基于single 模式错误检测，其中需要对训练数据中的每个训练模式进行小的权重调整。当应用于网络的模式在每个步骤调整权重的这种技术称为增量更新。误差反向传播或梯度下降技术还实现了在模式的完整周期内计算的整个误差函数的梯度下降最小化，只要学习常数足够小。该方案称为累积权重调整，并且使用以下表达式计算该技术的误差。</p><div class=pgc-img><img alt=反向传播：学习因素 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/9e50c59f72074b2090cb85bb714fcd53><p class=pgc-img-caption></p></div><p>虽然这两种技术都能带来令人满意的解决方案，但应注意训练在随机条件下最佳运行的事实。对于增量更新，应该从训练集中随机选择不同类别的模式，以便网络不应该过度匹配相同的类模式。</p><h1>3.激活函数λ的陡度</h1><p>梯度下降学习算法使用连续类型的激活函数，最常用的激活函数是sigmoid函数（unipolar and bipolar）。该sigmoid函数的特征在于称为陡度因子λ的因子。激活函数的导数作为构建神经元项的误差信号项的多个因素。激活函数的选择和形状都会影响网络学习的速度。激活函数的导数（unipolar sigmoid）由下式给出，</p><p>'= =×λ×❪-❫/ [+❪-❫]</p><p>下图显示了激活函数的斜率函数，并说明了刚度λ如何影响网络的学习。</p><div class=pgc-img><img alt=反向传播：学习因素 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/5141e294ba5e470197ec7e234323eed5><p class=pgc-img-caption>不同λ值的激活函数的导数</p></div><p>对于固定学习常数，所有权重调整都与陡度系数λ成比例。当我们使用λ的大值时，我们在使用大学习常数η时得到类似的结果。</p><h1>4.学习常数η。</h1><p>误差反向传播的有效性和收敛性基于学习常数value的值。更新的网络权重与学习因子directly成正比，因此它在神经元的误差信号项中起重要作用。</p><p>Δ=-η**'</p><p>其中是神经元的误差信号项，'是激活函数的导数。</p><p>当我们使用更大的value值时，我们的网络会采取更广泛的步骤来达到误差图的全局最小值。由于ƞ值较大，如果误差图产生较短的全局最小值，则有可能丢失全局最小值。类似地，如果我们使用较小的η值，我们的网络采用较短的步骤来达到误差图的全局最小值，但在这种情况下，有可能卡在误差图的局部最小值。为了克服这些情况，使用小的随机值重新初始化网络的权重并重新训练网络。</p><h1>5.Momentum法</h1><p>Momentum法处理梯度下降学习算法的收敛性。其目的是加速学习的融合。该方法使用最近的权重调整的分数来补充当前的权重调整。这通常使用以下公式完成，</p><p>Δ❪=-η×❪+α*Δ-</p><p>其中t和t-1表示当前和前一步骤，α是用户选择Momentum常数（应为正）。等式右边的第二项代表Momentum项。对于使用Momentum法的总N步，权重变化可表示为</p><p>Δ❪=-η×Σα*-❫</p><p>通常α选自0.1至0.8。通过添加Momentum项，通过前一步骤的权重调整的分数来增强权重调整。这种方法导致误差曲线的全局最小值有更大的步骤。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'传播','学习','反向'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>