<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>多任务学习-Multitask Learning概述 | 极客快訊</title><meta property="og:title" content="多任务学习-Multitask Learning概述 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/aa014844e7cf4914b7893404eebc468b"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/93da11a.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/93da11a.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/93da11a.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/93da11a.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/93da11a.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/93da11a.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/93da11a.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/93da11a.html><meta property="article:published_time" content="2020-10-29T21:00:04+08:00"><meta property="article:modified_time" content="2020-10-29T21:00:04+08:00"><meta name=Keywords content><meta name=description content="多任务学习-Multitask Learning概述"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/93da11a.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>多任务学习-Multitask Learning概述</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>1、单任务学习VS多任务学习</p><p>单任务学习：一次只学习一个任务（task），大部分的机器学习任务都属於单任务学习。</p><p>多任务学习：把多个相关（related）的任务放在一起学习，同时学习多个任务。</p><p>多任务学习（multitask learning）产生的原因？</p><p>获取最新消息链接：获取最新消息快速通道 - lqfarmer的博客 - 博客频道 - CSDN.NET</p><p>现在大多数机器学习任务都是单任务学习。对于复杂的问题，也可以分解为简单且相互独立的子问题来单独解决，然后再合并结果，得到最初复杂问题的结果。这样做看似合理，其实是不正确的，因为现实世界中很多问题不能分解为一个一个独立的子问题，即使可以分解，各个子问题之间也是相互关联的，通过一些共享因素或共享表示（share representation）联系在一起。把现实问题当做一个个独立的单任务处理，忽略了问题之间所富含的丰富的关联信息。多任务学习就是为了解决这个问题而诞生的。把多个相关（related）的任务（task）放在一起学习。这样做真的有效吗？答案是肯定的。多个任务之间共享一些因素，它们可以在学习过程中，共享它们所学到的信息，这是单任务学习所具备的。相关联的多任务学习比单任务学习能去的更好的泛化（generalization）效果。</p><p>单任务与多任务对比如图1所示：</p><div class=pgc-img><img alt="多任务学习-Multitask Learning概述" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/aa014844e7cf4914b7893404eebc468b><p class=pgc-img-caption>图1 单任务学习与多任务学习对比</p></div><p><br></p><p>从图1中可以发现，单任务学习时，各个任务之间的模型空间（Trained Model）是相互独立的（图1上）。多任务学习时，多个任务之间的模型空间（Trained Model）是共享的（图1下）。</p><p>假设用含一个隐含层的神经网络来表示学习一个任务，单任务学习和多任务学习可以表示成如图2所示。</p><div class=pgc-img><img alt="多任务学习-Multitask Learning概述" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/daaa3af209284be0aa1af89a5de8fdd0><p class=pgc-img-caption>图2 基於单层神经网络的单任务和多任务学习对比</p></div><p><br></p><p>从图二可以发现，单任务学习时，各个task任务的学习是相互独立的，多任务学习时，多个任务之间的浅层表示共享（shared representation）。</p><p>获取最新消息链接：获取最新消息快速通道 - lqfarmer的博客 - 博客频道 - CSDN.NET</p><p>2、多任务学习的定义</p><p>多任务学习（Multitask learning）定义：基于共享表示（shared representation），把多个相关的任务放在一起学习的一种机器学习方法。</p><p>多任务学习（Multitask Learning）是一种推导迁移学习方法，主任务（main tasks）使用相关任务（related tasks）的训练信号（training signal）所拥有的领域相关信息（domain-specific information），做为一直推导偏差（inductive bias）来提升主任务（main tasks）泛化效果（generalization performance）的一种机器学习方法。多任务学习涉及多个相关的任务同时并行学习，梯度同时反向传播，多个任务通过底层的共享表示（shared representation）来互相帮助学习，提升泛化效果。简单来说：多任务学习把多个相关的任务放在一起学习（注意，一定要是相关的任务，后面会给出相关任务（related tasks）的定义，以及他们共享了那些信息），学习过程（training）中通过一个在浅层的共享（shared representation）表示来互相分享、互相补充学习到的领域相关的信息（domain information），互相促进学习，提升泛化的效果。</p><p>共享表示shared representation：</p><p>共享表示的目的是为了提高泛化（improving generalization），图2中给出了多任务学习最简单的共享方式，多个任务在浅层共享参数。MTL中共享表示有两种方式：</p><p>（1）、基于参数的共享（Parameter based）：比如基于神经网络的MTL，高斯处理过程。</p><p>（2）、基于约束的共享（regularization based）：比如均值，联合特征（Joint feature）学习（创建一个常见的特征集合）。</p><p>3、多任务学习有效的原因</p><p>为什么把多个相关的任务放在一起学习，可以提高学习的效果？关于这个问题，有很多解释。这里列出其中一部分，以图2中由单隐含层神经网络表示的单任务和多任务学习对比为例。</p><p>（1）、多人相关任务放在一起学习，有相关的部分，但也有不相关的部分。当学习一个任务（Main task）时，与该任务不相关的部分，在学习过程中相当于是噪声，因此，引入噪声可以提高学习的泛化（generalization）效果。</p><p>（2）、单任务学习时，梯度的反向传播倾向于陷入局部极小值。多任务学习中不同任务的局部极小值处于不同的位置，通过相互作用，可以帮助隐含层逃离局部极小值。</p><p>（3）、添加的任务可以改变权值更新的动态特性，可能使网络更适合多任务学习。比如，多任务并行学习，提升了浅层共享层（shared representation）的学习速率，可能，较大的学习速率提升了学习效果。</p><p>（4）、多个任务在浅层共享表示，可能削弱了网络的能力，降低网络过拟合，提升了泛化效果。</p><p>还有很多潜在的解释，为什么多任务并行学习可以提升学习效果（performance）。多任务学习有效，是因为它是建立在多个相关的，具有共享表示（shared representation）的任务基础之上的，因此，需要定义一下，什么样的任务之间是相关的。</p><p>4、相关（relate）定义</p><p>相关（related）的具体定义很难，但我们可以知道的是，在多任务学习中，related tasks可以提升main task的学习效果，基于这点得到相关的定义：</p><p>Related（Main Task，Related tasks，LearningAlg）= 1</p><p>LearningAlg（Main Task||Related tasks）> LearningAlg（Main Task） （1）</p><p>LearningAlg表示多任务学习采用的算法，公式（1）：第一个公式表示，把Related tasks与main tasks放在一起学习，效果更好；第二个公式表示，基于related tasks，采用LearningAlg算法的多任务学习Main task，要比单学习main task的条件概率概率更大。特别注意，相同的学习任务，基于不同学习算法，得到相关的结果不一样：</p><p>Related（Main Task，Related tasks，LearningAlg1）不等于 Related（Main Task，Related tasks，LearningAlg2）</p><p>5、多任务学习中的相关关系（task relationship）</p><p>多任务学习并行学习时，有5个相关因素可以帮助提升多任务学习的效果。</p><p>（1）、数据放大（data amplification）。相关任务在学习过程中产生的额外有用的信息可以有效方法数据/样本（data）的大小/效果。主要有三种数据放大类型：统计数据放大（statistical data amplification）、采样数据放大（sampling data amplification），块数据放大（blocking data amplification）。</p><p>（2）、Eavesdropping（窃听）。假设</p><p>（3）、属性选择（attribute selection）</p><p>（4）、表示偏移（representation bias）</p><p>（5）、预防过拟合（overfitting prevention）</p><p>所有这些关系（relationships）都可以帮助提升学习效果（improve learning performance），关系具体定义可以参考文献[1]</p><p>6、Multitask Learning方法</p><p>浅层隐含层节点共享神经网络是最简单MTL，如图2所示。还有基于特征（feature table）共享MTL。</p><p>基于特征的共享MTL（联合特征学习，Joint feature learning），通过创建一个常见的特征集合来实现多个任务之间基于特征（features）的shared representation，其共享表示如图3所示。</p><div class=pgc-img><img alt="多任务学习-Multitask Learning概述" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/860b32a935504e288853bf57b005c150><p class=pgc-img-caption>图3 基于特征的共享表示示意图</p></div><p><br></p><p>基于特征共享的MTL输入输出关系如图4所示，其中采用L1正则来保证稀疏性。</p><div class=pgc-img><img alt="多任务学习-Multitask Learning概述" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cc120fd31ed0485ab4dc30d2d0dff07a><p class=pgc-img-caption>图4 基于joint feature的MTL示意图</p></div><p><br></p><p>如图4，等号左边nxp的输入矩阵，乘上一个由k个任务构成的pxk的共享特征矩阵，加上一个nxk的噪声矩阵（不相关部分），就得到了左边的输出矩阵。</p><p>其他多任务学习方法还有均值约束 MTL：基于均值来约束所有的task，参数共享的高斯处理MTL，低秩约束MTL，交替结构优化MTL等等。</p><p>获取最新消息链接：获取最新消息快速通道 - lqfarmer的博客 - 博客频道 - CSDN.NET</p><p>7、多任务学习与其他学习算法之间的关系</p><p>多任务学习（Multitask learning）是迁移学习算法的一种，迁移学习之前介绍过。定义一个一个源领域source domain和一个目标领域（target domain），在source domain学习，并把学习到的知识迁移到target domain，提升target domain的学习效果（performance）。</p><p>多标签学习（Multilabel learning）是多任务学习中的一种，建模多个label之间的相关性，同时对多个label进行建模，多个类别之间共享相同的数据/特征。</p><p>多类别学习（Multiclass learning）是多标签学习任务中的一种，对多个相互独立的类别（classes）进行建模。这几个学习之间的关系如图5所示：</p><div class=pgc-img><img alt="多任务学习-Multitask Learning概述" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/046217b8e3e845c595559e1985031718><p class=pgc-img-caption></p></div><p>图5 多任务学习与其他机器学习方法之间的关系</p><p>8、多任务学习应用概述</p><p>基于神经网络的多任务学习，尤其是基于深度神经网络的多任务学习（DL based Multitask Learning），适用于解决很多NLP领域的问题，比如把词性标注、句子句法成分划分、命名实体识别、语义角色标注等任务，都可以采用MTL任务来解决。</p><p>其他MTL的应用还有，网页图片和语音搜索[Zhou et. al. KDD’11]，疾病预测[Zhang et. al. NeuroImage 12]等等</p><p>参考文献：</p><p>[1] Caruana, R. (1997). Multitask Learning. Machine Learning, 28(1), 41–75. doi: 10.1023/A:1007379606734</p><p>[2] Collobert, R., & Weston, J. (2008). A unified architecture for natural language processing: Deep neural networks with multitask learning.Presented at the Proceedings of the 25th international conference ….</p><p>[3] Lounici, K., Pontil, M., Tsybakov, A. B., & van de Geer, S. (2009, March 8). Taking Advantage of Sparsity in Multi-Task Learning. arXiv.org.</p><p>[4] Zhang, Y., & Yeung, D.-Y. (2012, March 15). A Convex Formulation for Learning Task Relationships in Multi-Task Learning. arXiv.org.</p><p>[5] Zhou, J., Chen, J., & Ye, J. (2012) Multi-Task Learning , Theory, Algorithms, and Applications, SDM</p><p><br></p><p>文章转自：https://zhuanlan.zhihu.com/p/27421983</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'任务学习','Multitask','Learning'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>