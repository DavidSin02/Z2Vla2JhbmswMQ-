<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>CVPR2019！谷歌最新成果，YouTube视频数据驱动生成深度图 | 极客快訊</title><meta property="og:title" content="CVPR2019！谷歌最新成果，YouTube视频数据驱动生成深度图 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/4630f7fc786d4e2294b1f21003ae6c0d"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/95e24da3.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/95e24da3.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/95e24da3.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/95e24da3.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/95e24da3.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/95e24da3.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/95e24da3.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/95e24da3.html><meta property="article:published_time" content="2020-11-14T21:04:21+08:00"><meta property="article:modified_time" content="2020-11-14T21:04:21+08:00"><meta name=Keywords content><meta name=description content="CVPR2019！谷歌最新成果，YouTube视频数据驱动生成深度图"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/95e24da3.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>CVPR2019！谷歌最新成果，YouTube视频数据驱动生成深度图</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p>地址：https://arxiv.org/pdf/1904.11111.pdf</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube视频数据驱动生成深度图 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4630f7fc786d4e2294b1f21003ae6c0d><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><h1 class=ql-align-center><strong>摘要</strong></h1><p class=ql-align-justify>本文提出了一种在自由移动的场景中预测密集深度的方法。现有的从单目视频中恢复动态非刚性物体深度的方法对物体的运动有很强的假设，只能恢复稀疏深度。<strong>在本文中采用数据驱动的方法，从一个新的数据源中学习人类的深度先验信息：成千上万的模仿人体模型的人的互联网视频，即冻结在不同的自然姿势，而手持摄像机巡视现场。</strong>由于人是静止的，训练数据可以通过多视图立体重建生成。<strong>在推理时，使用来自场景静态区域的运动视差提示来指导深度预测</strong>。我们演示了移动手持相机捕捉到的复杂人体动作的真实序列的方法，展示了对最先进的<strong>单目深度预测方法的改进</strong>，并展示了使用我们的预测深度产生的各种三维效果。</p><h1 class=ql-align-center><strong>简介</strong></h1><p class=ql-align-justify>手持摄像机观看动态场景是现代摄影中常见的场景。在这种情况下，恢复密集的几何图形是一项具有挑战性的任务：移动对象违反了三维视觉中使用的极线约束，并且为了可视化的目的，经常被视为现有结构中的噪波或异常值，并将其称为深度图:从运动（SFM）和多视图立体（MVS）方法。然而，人类的深度感知并不容易被物体运动所愚弄，相反，即使物体和观察者都在移动，<strong>即使只有一只眼睛观察到场景，我们仍然可以对物体的几何结构和深度顺序进行可行的解释</strong>[11]。在这项工作中，我们朝着计算上实现这一能力迈出了一步。</p><p class=ql-align-justify><strong>我们专注于从普通视频中预测准确、密集的深度</strong>，在普通视频中，摄像机和场景中的人都是自然移动的。我们关注人类有两个原因：i）在许多应用中（例如，增强现实），人类构成了场景中的突出物体；ii）人类的运动是有关节的，难以建模。通过采用数据驱动的方法，我们可以避免对人的形状或变形进行明确的假设，而是从数据中学习这些先验。</p><p class=ql-align-justify>我们从哪里获取数据来训练这种方法？生成高质量的合成数据，在这些数据中，摄像机和场景中的人都是自然移动的，这是非常具有挑战性的。深度传感器（如Kinect）可以提供有用的数据，但此类数据通常仅限于室内环境，需要在捕获和处理过程中进行大量的人工工作。此外，很难将不同年龄和性别、不同姿势的人聚集在一起。相反，<strong>我们从一个令人惊讶的来源获得数据：YouTube视频中，人们模仿人体模型，即，在精心制作的自然姿势中冻结，而手持摄像机则在现场巡视（图2）</strong>。</p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube视频数据驱动生成深度图 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/02dd9f59b4a142419228b70bacc7f21e><p class=pgc-img-caption></p></div><p class=ql-align-justify>这些视频包括我们的新人体模型（MC）数据集，我们计划为研究社区发布，因为整个场景，包括人，都是静止的，我们使用SFM和MVS估计相机的姿势和深度，并使用这个衍生的三维数据作为训练的监督。</p><p class=ql-align-justify>特别是，<strong>我们设计并训练了一个深度神经网络，它接收输入的RGB图像、人类区域的遮罩和环境的初始深度（即非人类区域），并在整个图像上输出一个密集的深度图，包括环境和人（见图1）</strong>。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube视频数据驱动生成深度图 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f7d52548f1ce4be89b0c5ba52decc53a><p class=pgc-img-caption></p></div><p class=ql-align-justify>请注意，<strong>使用视频两帧之间的运动视差计算环境的初始深度，为网络提供单帧中不可用的信息。一旦经过训练，我们的模型可以处理自然视频与任意摄像机和人体运动。</strong></p><p class=ql-align-justify>我们展示了我们的方法在各种真实的互联网视频上的适用性，这些视频是用手持摄像机拍摄的，描绘了复杂的人类行为，如行走、跑步和跳舞。<strong>我们的模型预测深度的精度比最先进的单目深度预测和运动立体方法更高</strong>。我们还将进一步展示我们的深度贴图如何用于产生各种3D效果，例如合成景深、深度感知绘制以及将虚拟对象插入具有正确遮挡的3D场景中。</p><p class=ql-align-justify>贡献是：<strong>i）一个新的深度预测数据源，包括大量互联网视频，其中摄像头以自然姿势围绕“冻结”的人移动，以及生成精确深度图和摄像头姿势的方法；ii）一个基于深度网络的模型，设计和训练用于预测密集度深度图在同时相机运动和复杂人体运动的挑战情况下。</strong></p><h1 class=ql-align-center><strong>相关工作</strong></h1><p class=ql-align-justify>基于学习的深度预测。许多基于有监督和无监督学习的算法最近被提出用于预测单个RGB图像的密集深度。<strong>一些最新的基于学习的方法也考虑多个图像，要么假设已知的相机姿势，要么同时预测相机姿势和深度。然而，它们都不能用来预测动态物体的深度，这是我们工作的重点。</strong></p><p class=ql-align-justify><strong>动态场景的深度估计。</strong>RGBD数据已广泛用于动态场景的三维建模，但只有少数几种方法试图估计单眼相机的深度。提出了几种方法来重建动态场景的稀疏几何体。罗素等]和Ranftl等人建议基于运动/对象分割的算法将动态场景分解为分段刚性部分。然而，这些方法对物体的运动施加了强烈的假设，而这种假设是人类关节运动所违反的。Konstantinos等人利用国际足联视频游戏的综合训练数据预测足球运动员的移动深度。然而，他们的方法仅限于足球运动员，不能处理一般人在野外。</p><p class=ql-align-justify><strong>用于学习深度的RGBD数据。</strong>有许多室内场景的RGBD数据集，使用深度传感器或综合渲染捕获。然而，这些数据集都不能为在自然环境中移动的人提供深度监控。有几种动作识别方法使用深度传感器来捕捉人类的动作，但大多数都使用静态摄像机，只提供有限数量的室内场景。Refresh是一个最新的半合成场景流数据集，通过将动画人物叠加到nyuv2图像上创建。在这里，数据也局限于室内，由人造人组成，他们被放置在与周围不现实的配置中。</p><p class=ql-align-justify><strong>人体形状和姿势预测。</strong>从单个RGB图像中恢复构成的三维人体网格引起了极大的关注。最近的方法在跨越各种姿势的自然图像上取得了令人印象深刻的效果。然而，这种方法只是模拟人体，不理会头发、衣服和非人体部位的场景。最后，这些方法中的许多依赖于正确地检测人体关键点，要求身体的大部分都在框架内。</p><h1 class=ql-align-center><strong>MannequinChallenge Dataset</strong></h1><p class=ql-align-justify>人体模型挑战[42]是一种流行的视频趋势，在这种趋势中，当摄像机操作员在场景周围移动拍摄人体模型时，人们通常会以有趣的姿势原地不动（如图2）。自2016年底以来，已有数千个此类视频被创建并上传至YouTube。如果人们在视频中成功地保持静止，我们可以假设场景是静态的，并通过使用SFM和MVS算法对其进行处理来获得准确的摄像机姿态和深度信息。我们发现大约2000个候选视频，这个处理是可能的。这些视频包括我们的新的（MC）数据集，它涵盖了不同年龄的人的广泛场景，自然呈现在不同的组配置中。接下来，我们将详细描述如何处理这些视频并导出我们的培训数据。</p><p class=ql-align-justify><strong>估计相机姿态。</strong>采用与周等类似的方法。我们使用ORB-SLAM2[24]来识别每个视频中的可跟踪序列，并估计每个帧的初始摄像机姿势。在这个阶段，我们处理低分辨率的视频以提高效率，并将视场设置为60度（现代手机摄像头的典型值）。然后，我们使用视觉SFM系统[32]以更高的分辨率重新处理每个序列，该系统优化了初始摄像机姿态和固有参数。该方法提取并匹配跨帧的特征，然后执行全局束调整优化。最后，利用周等的技术去除了摄像机运动不平稳的序列。</p><p class=ql-align-justify><strong>用MVS计算密集深度。</strong>一旦对每个片段的相机姿势进行了估计，我们就可以重建每个场景的密集几何体。特别是，我们使用最先进的MVS系统colmap恢复每帧密集深度图。</p><p class=ql-align-justify><strong>过滤剪辑。</strong>有几个因素会使视频剪辑不适合训练。例如，人们可能在视频中的某一点“解冻”（开始移动），或者视频可能在背景中包含合成图形元素。动态对象和合成背景不受多视图几何约束，因此被视为异常值，并被MVS过滤掉，可能只留下少量有效像素。因此，在经过两次清洗阶段后，我们删除了小于20%像素具有有效MVS深度的帧。</p><p class=ql-align-justify>此外，我们移除了估算径向畸变系数k1>0.1（表示鱼眼相机）或估算焦距小于等于0.6或大于等于1.2（相机参数可能不准确）的帧。我们保持的序列至少有30帧长，纵横比为16:9，并且具有超过1600像素的宽度。最后，我们手动检查剩余序列的轨迹和点云，并消除明显不正确的重建。删除的图像示例显示在补充材料中。</p><p class=ql-align-justify><strong>经过处理，得到4690个序列，有效图像深度对超过170K。我们用80:3:17的片段将我们的mc数据集分解为培训、验证和测试集。</strong></p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube视频数据驱动生成深度图 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ed65129ccb2448aabc4e44e6570e0098><p class=pgc-img-caption></p></div><h1 class=ql-align-center><strong>模型</strong></h1><p>我们以有监督的方式对人体模型数据集的深度预测模型进行训练，即通过回归到MVS管道生成的深度。一个关键的问题是如何构造网络的输入，以允许对冻结的人进行训练，但对自由移动的人进行推理。一种选择是从单个RGB图像回归到深度，但是这种方法忽略了有关场景静态区域的几何信息，这些静态区域通过考虑多个视图而可用。为了从这些信息中获益，我们向网络输入一张静态非人类区域的深度图，根据运动视差w.r.t.另一个场景视图进行估算。</p><h1 class=ql-align-center><strong>结果</strong></h1><p>我们对我们的方法进行了定量和定性的测试，并将其与几种最先进的单视图和基于运动的深度预测算法进行了比较。我们对复杂的人体运动和自然的摄像机运动的挑战性互联网视频显示了额外的定性结果，并演示了我们预测的深度图如何用于几种视觉效果。</p><p class=ql-align-center><strong>MCtest set 和TUM RGBD dataset</strong></p><ul><li class=ql-align-center><strong>MCtest set</strong></li></ul><p class=ql-align-justify>为了量化我们设计的模型输入的重要性，我们比较了几个模型的性能，每个模型在我们的mc数据集上训练，具有不同的输入配置。两种主要配置是：（i）单视图模型（输入为RGB图像）和（ii）我们的全双帧模型，其中输入包括参考图像、初始遮罩深度图DPP、置信图C和人体遮罩M。我们还通过将输入深度替换为光流F、从输入中删除C以及添加人类关键点图。</p><p class=ql-align-justify>定量评价如表1所示。通过比较第（i）、（iii）和（iv）行，可以清楚地看出，添加初始环境深度和置信度图可以显著提高人类和非人类区域的性能。在网络输入中添加人工关键点位置可以进一步提高性能。请注意，如果我们向网络输入光流场而不是深度（ii），则性能仅与单视图方法相当。从二维光流到深度的映射取决于摄像机的相对姿态，而这些姿态并没有提供给网络。结果表明，该网络不能隐式学习相对姿态，不能提取深度信息。</p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube视频数据驱动生成深度图 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c3ac57b5620544d0bced40d3e6a6aa1b><p class=pgc-img-caption></p></div><p class=ql-align-justify>图4显示了我们的单视图模型（I）和我们的完整模型（IDPPCMK）之间的定性比较。我们的全模型结果在人类区域（例如，第一列）和非人类区域（例如，第二列）中更准确。此外，在所有例子中，人们与周围环境之间的深度关系都得到了改善。</p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube视频数据驱动生成深度图 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a7e79abf3cc64dde8c065b4638b69cb6><p class=pgc-img-caption></p></div><p class=ql-align-justify>我们使用了tum-rgbd数据集[38]的一个子集，其中包含执行复杂动作的人的室内场景，这些场景是从不同的相机姿势捕获的。该数据集的样本图像如图5（a-b）所示。</p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube视频数据驱动生成深度图 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d70c39eb8ba2466da9936631714bd111><p class=pgc-img-caption></p></div><p class=ql-align-justify>为了运行我们的模型，我们首先使用ORB-SLAM2 3估计相机姿态。</p><p class=ql-align-justify>定量比较如表2所示，其中我们报告了5种不同的尺度不变性误差测量，以及标准的RMSE和相对误差；最后两种是通过应用一个单一的比例因子来计算的，该比例因子在最小二乘意义上对齐预测和地面真值深度。我们的单视图模型已经优于其他单视图模型，证明了用于培训的MC数据集的好处。注意，由于挑战性的摄像机和物体运动，Viopopop[ 31 ]未能产生有意义的结果。我们的完整模型，通过使用初始（掩蔽）深度图，显著地改善了所有错误度量的性能。与我们的MC测试集结果一致，当我们使用光流作为输入（而不是初始深度图）时，性能只比单视图网络稍好。最后，我们展示了我们提出的“深度清理”方法的重要性，该方法应用于培训数据（见等式1）。与同一模型相比，仅使用原始MVS深度预测作为监督进行培训，我们发现性能下降了约15%。</p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube视频数据驱动生成深度图 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/94622093a67e44e1b8347ab7d8f9e230><p class=pgc-img-caption></p></div><p class=ql-align-justify>图5显示了不同方法之间的定性比较。我们的模型的深度预测（图5（f-g））与地面情况非常相似，显示出高度的细节和尖锐的深度不连续。这一结果是对竞争方法的无表改进，这通常会在两个人类区域（如图5第二行中的腿）和非人类区域（如最后两行中的桌子和天花板）中产生显著错误。</p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube视频数据驱动生成深度图 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/94622093a67e44e1b8347ab7d8f9e230><p class=pgc-img-caption></p></div><p class=ql-align-justify>如图6所示，我们的深度预测明显优于基线方法。我们预测的深度图描绘了场景中人与其他物体之间（如人与建筑物之间、图6第四行）以及人类区域内（如图6前三行中人的胳膊和腿）的精确深度排序。</p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube视频数据驱动生成深度图 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eb78e98d51af486e81da53e176dc4bff><p class=pgc-img-caption></p></div><p class=ql-align-justify>基于深度的视觉效果。我们的深度可以用来应用一系列基于深度的视觉效果。图7显示了基于深度的散焦、插入合成的3D图形，以及去除附近的人。更多示例，包括单声道到立体声转换，请参见补充材料。</p><p class=ql-align-justify>随着时间的推移，深度估计值足够稳定，可以从视频中的其他帧进行绘制。为了使用帧进行修复，我们从深度图、纹理与视频帧的高程场构造三角高程场，并使用相对摄像机变换渲染目标帧的高程场。图7（D，F）显示了修复两个街道场景的结果。摄像机附近的人是用人脸M移除的，在视频的后期，洞里充满了200帧的颜色。一些人工制品可以在人体面具漏掉的地方看到，例如地上的阴影。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'CVPR2019','YouTube','视频'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>