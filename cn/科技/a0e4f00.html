<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>可视化解释11种基本神经网络架构 | 极客快訊</title><meta property="og:title" content="可视化解释11种基本神经网络架构 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/47236e0671d34822b0349114536afb85"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a0e4f00.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a0e4f00.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a0e4f00.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a0e4f00.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a0e4f00.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a0e4f00.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a0e4f00.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a0e4f00.html><meta property="article:published_time" content="2020-10-29T21:08:16+08:00"><meta property="article:modified_time" content="2020-10-29T21:08:16+08:00"><meta name=Keywords content><meta name=description content="可视化解释11种基本神经网络架构"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/a0e4f00.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>可视化解释11种基本神经网络架构</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div class=pgc-img><img alt=可视化解释11种基本神经网络架构 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/47236e0671d34822b0349114536afb85><p class=pgc-img-caption>> Source: Pixabay</p></div><p></p><h1 class=pgc-h-arrow-right>标准，循环，卷积和自动编码器网络</h1><p>随着深度学习的飞速发展，已经创建了完整的神经网络体系结构主机，以解决各种各样的任务和问题。 尽管有无数的神经网络架构，但对于任何深度学习工程师来说，这里有11种必不可少的知识，它们分为四大类：标准网络，递归网络，卷积网络和自动编码器。</p><p>作者创建的所有图。</p><h1 class=pgc-h-arrow-right>标准网络</h1><h1 class=pgc-h-arrow-right>1 | 感知器</h1><p>感知器是所有神经网络中最基础的，是更复杂的神经网络的基本构建块。 它仅连接输入单元和输出单元。</p><div class=pgc-img><img alt=可视化解释11种基本神经网络架构 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/e1d1a5c2dcac4d8d973f6f78acbf5bc7><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>2 | 前馈网络</h1><p>前馈网络是感知器的集合，其中存在三种基本类型的层-输入层，隐藏层和输出层。 在每个连接期间，来自上一层的信号乘以权重，加到偏置上，并通过激活函数。 前馈网络使用反向传播来迭代更新参数，直到达到理想的性能为止。</p><div class=pgc-img><img alt=可视化解释11种基本神经网络架构 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/19a278b1c8da42fca02c6fe97aaea1fe><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>3 | 残留网络（ResNet）</h1><p>深度前馈神经网络的一个问题称为消失梯度问题，即当网络太长而无法在整个网络中反向传播有用信息时。 随着更新参数的信号通过网络传播，它逐渐减小，直到网络前端的权重完全没有改变或被利用为止。</p><p>为了解决这个问题，残差网络采用了跳过连接，可以跨"跳跃的"层传播信号。 通过使用不太容易受到影响的连接，可以减少消失的梯度问题。 随着时间的流逝，网络在学习特征空间时会学习恢复跳过的图层，但由于其不易受到梯度消失的影响并且需要探索较少的特征空间，因此训练效率更高。</p><div class=pgc-img><img alt=可视化解释11种基本神经网络架构 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/68846b1d00464763aa662f64cd4c31fa><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>循环网络</h1><h1 class=pgc-h-arrow-right>4 | 递归神经网络（RNN）</h1><p>递归神经网络是一种特殊类型的网络，它包含循环并在其自身上递归，因此称为"递归"。 RNN允许将信息存储在网络中，使用先前训练中的推理来做出有关即将发生的事件的更好，更明智的决策。 为此，它将先前的预测用作"上下文信号"。 由于其性质，RNN通常用于处理顺序任务，例如逐个字母生成文本或预测时间序列数据（例如股票价格）。 他们还可以处理任何大小的输入。</p><div class=pgc-img><img alt=可视化解释11种基本神经网络架构 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/8b683c9191f740f592294e1732f4d046><p class=pgc-img-caption>> Two RNN visualization methods.</p></div><p></p><h1 class=pgc-h-arrow-right>5 | 长期短期记忆网络（LSTM）</h1><p>RNN存在问题，因为实际上上下文信息的范围非常有限。 给定输入对隐藏层（因此对网络输出）的影响（反向传播错误），要么呈指数级爆发，要么随着绕网络连接循环而消失为零。 解决这个逐渐消失的梯度问题的方法是使用长短期内存网络或LSTM。</p><p>这种RNN架构是专门为解决消失的梯度问题而设计的，将结构与存储块配合在一起。 这些模块可以看作是计算机中的存储芯片-每个模块都包含几个循环连接的存储单元和三个门（输入，输出和忘记，相当于写入，读取和重置）。 网络只能通过每个门与单元交互，因此门学会了智能地打开和关闭，以防止梯度爆炸或消失，而且还可以通过"恒定错误轮播"传播有用的信息，并丢弃无关的存储内容。</p><div class=pgc-img><img alt=可视化解释11种基本神经网络架构 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/43a3098143b24f2f8227d495d44f9fdb><p class=pgc-img-caption></p></div><p>在标准RNN无法得知输入事件与目标信号之间存在大于五到十个时间步长的时滞的情况下，LSTM不会受到影响，并且可以通过强制执行有用的恒定错误流来学习将时滞甚至连成1,000个时步。</p><h1 class=pgc-h-arrow-right>6 | 回声状态网（ESN）</h1><p>回声状态网络是递归神经网络的一种变体，具有非常稀疏的隐藏层（通常为百分之一的连接性）。 神经元的连通性和权重是随机分配的，并且忽略层和神经元的差异（跳过连接）。 学习输出神经元的权重，以便网络可以产生和再现特定的时间模式。 该网络背后的理由来自这样一个事实：尽管它是非线性的，但训练过程中唯一修改的权重是突触连接，因此可以将误差函数区分为线性系统。</p><div class=pgc-img><img alt=可视化解释11种基本神经网络架构 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7a7ec4868d364827a9c6de8b2fdeed3d><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>卷积网络</h1><h1 class=pgc-h-arrow-right>7 | 卷积神经网络（CNN）</h1><p>图像具有很高的维数，因此训练一个标准的前馈网络来识别图像将需要成千上万的输入神经元，除了公然高昂的计算费用外，还可能引起许多与神经网络的维数诅咒相关的问题 。 卷积神经网络（CNN）通过使用卷积层和池化层来帮助降低图像的维数，从而提供了解决方案。 由于卷积层是可训练的，但是比标准隐藏层具有更少的参数，因此它能够突出显示图像的重要部分并将它们向前传递。 传统上，在CNN中，最后几层是隐藏层，用于处理"压缩图像信息"。</p><div class=pgc-img><img alt=可视化解释11种基本神经网络架构 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6f20ad9279c14130b8632b783c0706c5><p class=pgc-img-caption></p></div><p>卷积神经网络在基于图像的任务上表现出色，例如将图像分类为狗或猫。</p><h1 class=pgc-h-arrow-right>8 | 反卷积神经网络（DNN）</h1><p>顾名思义，反卷积神经网络的作用与卷积神经网络相反。 DNN不是执行卷积来减小图像的维数，而是利用反卷积来创建图像，通常是根据噪声来进行的。 这是一项固有的艰巨任务。 考虑CNN的任务是为奥威尔（Orwell）1984年的整本书写一个三句摘要，而DNN的任务是从三句结构写整个本书。</p><div class=pgc-img><img alt=可视化解释11种基本神经网络架构 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0a58996da46946f3ba1043f480ae6326><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>9 | 生成对抗网络（GAN）</h1><p>生成对抗网络是一种专门设计用于生成图像的特殊类型的网络，它由两个网络（一个鉴别器和一个生成器）组成。 区分者的任务是区分是从数据集中提取图像还是由生成器生成图像，而生成者的任务是生成足够有说服力的图像，以使区分器无法区分其是否真实。</p><p>随着时间的流逝，经过精心的监管，这两个对手彼此竞争，互相推动，成功地改善了彼此。 最终结果是训练有素的生成器，可以吐出逼真的图像。 鉴别器是一个卷积神经网络，其目的是最大程度地提高识别真实/伪造图像的准确性，而生成器是一个反卷积神经网络，其目的是最小化鉴别器的性能。</p><div class=pgc-img><img alt=可视化解释11种基本神经网络架构 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/28b12e41b1ea4d638bf0d0149f66c9cf><p class=pgc-img-caption>> Generator diagram.</p></div><p></p><h1 class=pgc-h-arrow-right>自动编码器</h1><h1 class=pgc-h-arrow-right>10 | 自动编码器（AE）</h1><p>自动编码器的基本思想是获取原始的高维数据，将其"压缩"为高度信息化的低维数据，然后将压缩后的形式投影到新的空间中。 自动编码器有许多应用，包括降维，图像压缩，去噪数据，特征提取，图像生成和推荐系统。 它既可以作为无监督方法也可以作为有监督方法，可以非常洞悉数据的性质。</p><div class=pgc-img><img alt=可视化解释11种基本神经网络架构 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/39c0ef0da5c14088bc4b8cd7ab2d757d><p class=pgc-img-caption></p></div><p>隐藏的单元可以用卷积层替换以适应处理图像。</p><h1 class=pgc-h-arrow-right>11 | 可变自动编码器（VAE）</h1><p>自动编码器学习输入的压缩表示形式，例如可以是图像或文本序列，方法是压缩输入然后将其解压缩以匹配原始输入，而变分自动编码器（VAE）学习概率分布的参数 代表数据。 它不仅仅是学习表示数据的函数，还获得了更详细，细致的数据视图，从分布中采样并生成新的输入数据样本。 从这个意义上讲，它更像是一种纯粹的"生成"模型，例如GAN。</p><div class=pgc-img><img alt=可视化解释11种基本神经网络架构 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/22a5f5e7f716474481200faa7573f40f><p class=pgc-img-caption></p></div><p>VAE使用概率隐藏单元格，该单元格将径向基函数应用于测试用例与单元格均值之间的差异。</p><p><br></p><p>(本文翻译自Andre Ye的文章《11 Essential Neural Network Architectures, Visualized & Explained》，参考：https://towardsdatascience.com/11-essential-neural-network-architectures-visualized-explained-7fc7da3486d8)</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'可视','11','神经'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>