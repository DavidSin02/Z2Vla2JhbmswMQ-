<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>万字长文带你了解变分自编码器VAEs | 极客快訊</title><meta property="og:title" content="万字长文带你了解变分自编码器VAEs - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/0f0d438dcd3f4085902cc8c9214d8655"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a1fb61f.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a1fb61f.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a1fb61f.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a1fb61f.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a1fb61f.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a1fb61f.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a1fb61f.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a1fb61f.html><meta property="article:published_time" content="2020-10-29T21:05:25+08:00"><meta property="article:modified_time" content="2020-10-29T21:05:25+08:00"><meta name=Keywords content><meta name=description content="万字长文带你了解变分自编码器VAEs"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/a1fb61f.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>万字长文带你了解变分自编码器VAEs</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>原文标题：Understanding Variational Autoencoders (VAEs)</p><p>原文链接：https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</p><p>原文作者：Joseph Rocca & Baptiste Rocca</p><p><br></p><p>最近比较关注文本生成任务，除了用到传统的seq2seq模型，还涉及到一些GAN和VAE的知识。这篇是看的资料里介绍VAE比较好的一篇文章，就翻译了一下，推荐给大家。</p><p><br></p><ul><li>简介</li><ul><li>大纲</li></ul><li>降维，PCA和自编码器</li><ul><li>什么是降维？</li><li>主成分分析（PCA）</li><li>自编码器</li></ul><li>变分自编码器（VAE）</li><ul><li>自编码器用于内容生成的局限性</li><li>变分自编码器的定义</li><li>关于正则化的直观解释</li></ul><li>VAE的数学细节</li><ul><li>概率框架和假设</li><li>变分推理公式</li><li>将神经网络引入模型</li></ul><li>总结</li></ul><p><br></p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0f0d438dcd3f4085902cc8c9214d8655><p class=pgc-img-caption></p></div><p><br></p><h2 class=pgc-h-arrow-right>简介</h2><p>在过去的几年中，由于一些惊人的进步，基于深度学习的生成模型越来越受到关注。依靠大量数据，精心设计的网络结构和训练技术，深度生成模型已经显示出了令人难以置信的能力，可以生成高度逼真的各种内容，例如图像，文本和声音。在这些深度生成模型中，有两个类别脱颖而出，值得特别关注：生成对抗网络（GAN）和变分自编码器（VAE）。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b11c1020985e48df93a030bcecea49b4><p class=pgc-img-caption></p></div><p>VAE生成的人脸图片</p><p>在今年1月发布的文章中，我们深入讨论了生成对抗网络（GANs），并展示了对抗训练如何对抗两个网络（生成器和鉴别器），以推动这两个网络同时迭代。在这篇文章中，我们将介绍另一种主要的深度生成模型：变分自编码器（VAE）。简而言之，VAE是一种自编码器，在训练过程中其编码分布是规范化的，以确保其在隐空间具有良好的特性，从而允许我们生成一些新数据。术语“变分”源自统计中的正则化和变分推理方法。</p><p>虽然最后两句话很好地概括了VAE的概念，但是它们也会引出很多问题。什么是自编码器？什么是隐空间，为什么要对其进行规范化？如何用VAE生成新数据？VAE与变分推理之间有什么联系？为了尽可能详细地描述VAE，我们将尝试回答所有这些问题（以及许多其他问题！），并为读者提供尽可能多的知识（从基本直觉到更高级的数学细节）。因此，本文的目的不仅是讨论变分自编码器所依赖的基本概念，而且还要逐步构建出导致这些概念的推理过程。</p><p>事不宜迟，让我们一起（重新）发现VAE！</p><h3 class=pgc-h-arrow-right>大纲</h3><p>在第一部分中，我们将回顾一些有关降维和自编码器的重要概念，这些概念将有助于理解VAE。在第二部分中，我们将说明为什么不能使用自编码器来生成新数据，并将介绍变分自编码器，它们是自编码器的规范化版本，使生成数据成为可能。在最后一节中，我们将基于变分推论对VAE进行更数学的描述。</p><blockquote><p>注意：在最后一节中，我们试图使数学推导尽可能完整和清楚，以弥合直觉和方程之间的差距。但是，不想深入了解VAE的数学细节的读者可以跳过本节，而不会影响对主要概念的理解。还要注意，在本文中，我们将大量使用以下符号：对于随机变量，我们将用表示该随机变量的分布（或密度，取决于上下文）。</p></blockquote><h2 class=pgc-h-arrow-right>降维，PCA和自编码器</h2><p>在这部分中，我们将从讨论与降维有关的一些概念开始。特别地，我们将简要回顾主成分分析（PCA）和自编码器，以展示这两种思想之间的相互关系。</p><h3 class=pgc-h-arrow-right>什么是降维？</h3><p>在机器学习中，<strong>降维是减少描述某些数据的特征数量的过程</strong>。可以通过<strong>选择</strong>（仅保留一些现有特征）或通过<strong>提取</strong>（基于旧特征来生成数量更少的新特征）来进行降维。降维在许多需要低维数据（数据可视化，数据存储，繁重的计算...）的场景中很有用。尽管存在许多不同的降维方法，但是我们可以构建一个适用于大多数方法都的总体框架。</p><p>首先，我们称编码器为从“旧特征”表示中产生“新特征”表示（通过选择或提取）的过程，然后将逆过程称为解码。降维可以理解为为数据压缩，其中编码器压缩数据（从初始空间到编码空间，也称为隐空间），而解码器将其解压缩。当然，根据初始数据分布、隐空间大小和编码器定义，压缩可能是有损的，这意味着一部分信息在编码过程中丢失，并且在解码时无法恢复。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d0fb1a2723924daba71ae6eab89bb543><p class=pgc-img-caption></p></div><p>编码器和解码器</p><p>降维方法的主要目的是在给定候选中找到最佳的编码器/解码器对。换句话说，对于给定的一组可能的编码器和解码器，我们希望编码时保持信息量最大，从而在解码时具有尽可能小的重构误差。如果我们分别用和表示我们正在考虑的编码器和解码器，则降维问题可以表示为</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/68506e5ceb8f45878bbfe53765196d31><p class=pgc-img-caption></p></div><p>其中</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5d104718072d4b1ab61ab22e810f4442><p class=pgc-img-caption></p></div><p>定义输入数据和编码解码数据之间的重构误差量度。需要注意，在下面我们将用表示数据数量，表示初始（解码）空间的维数，表示降维后（编码）空间的维数。</p><h3 class=pgc-h-arrow-right>主成分分析（PCA）</h3><p>谈到降维时，首先想到的方法就是主成分分析（PCA）。为了展示它如何符合我们刚刚描述的框架并建立与自编码器的联系，我们将对PCA的工作方式进行的概述（忽略大部分细节）。</p><p>PCA的想法是构建个新的独立特征，这些特征是个旧特征的<strong>线性组合</strong>，使得这些新特征所定义的子空间上的数据投影尽可能接近初始数据（就欧几里得距离而言）。换句话说，PCA寻找初始空间的最佳线性子空间（由新特征的正交基来描述），以使通过其在该子空间上的投影近似数据的误差尽可能小。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1586e653399f4a528d4bbcb527e0fcec><p class=pgc-img-caption></p></div><p>PCA是在寻找最佳子空间</p><p>用我们的框架来描述，我们正在搜寻一个矩阵表示的编码器（线性变换），该变换的各行是正交的（特征独立性），并且在寻找一个矩阵表示的相应解码器。可以证明，与协方差特征矩阵的个最大特征值（在范数上）相对应的单位特征向量是正交的（可以构造出正交的），并且组成了维数的最佳子空间，使得数据投影到子空间的误差最小。因此，可以选择这个特征向量作为我们的新特征，因此，降维问题可以表示为特征值/特征向量问题。此外，还可以推出，在这种情况下，解码器矩阵是编码器矩阵的转置。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f31a4337c70e46c8a1e0ff4e13c9510c><p class=pgc-img-caption></p></div><p>PCA与我们的总体框架是兼容的</p><h3 class=pgc-h-arrow-right>自编码器</h3><p>现在我们讨论<strong>自编码器</strong>，以及如何使用神经网络进行降维。自编码器的总体思路非常简单，主要包括用神经网络来作为编码器和解码器，并使用迭代优化学习最佳的编码-解码方案。因此，在每次迭代中，我们向自编码器体系结构（编码器后跟解码器）提供一些数据，我们将编码解码后的输出与初始数据进行比较，并通过反向传播误差来更新网络的权重。</p><p>因此，直观地讲，整个自编码器体系结构（编码器+解码器）会构造出数据瓶颈（bottleneck），从而确保只有信息的主要结构部分可以通过瓶颈并进行重构。从我们的总体框架来看，考虑的编码器系列由编码器网络体系结构定义，考虑的解码器系列由解码器网络体系结构定义，而重构误差的减小则通过对编码器和解码器参数进行梯度下降来进行。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5238bbe0b694487cadbb17b721dfccfb><p class=pgc-img-caption></p></div><p>带有损失函数的自编码器</p><p>首先，假设我们的编码器和解码器体系结构都只有一层且没有非线性（线性自编码器）。这样的编码器和解码器是简单的线性变换，可以表示为矩阵。在这种情况下，某种意义上，我们可以看到与PCA的明显的关联，就像PCA一样，我们正在寻找最佳的线性子空间来投影数据，并且使这样做时信息损失尽可能少。用PCA获得的编码和解码矩阵自然地定义了梯度下降所能满足的一种解决方案，但是我们应该指出这不是唯一的解决方案。实际上，<strong>可以选择几组不同的基向量来描述相同的最佳子空间</strong>，因此，几个编码器/解码器对都可以提供最小的重构误差。此外，与PCA不同，对于线性自编码器，我们最终获得的新特征不必是独立的（神经网络中没有正交性约束）。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cff7e327454441d98a528fc028c3bb0a><p class=pgc-img-caption></p></div><p>线性自编码器和PCA的联系</p><p>现在，假设编码器和解码器都是深度非线性网络的。在这种情况下，网络结构越复杂，自编码器就可以进行更多的降维，同时保持较低的重构损失。直观地讲，如果我们的编码器和解码器具有足够的自由度，则可以将任何初始维数减小为1。实际上，具有“无限大能力”的编码器理论上可以将我们的N个初始数据编码为1、2、3， …最多N个（或更一般地说，为实轴上的N个整数），相关的解码器再进行逆变换，在这种过程中不会造成任何损失。</p><p>但是我们应该牢记两点。首先，在没有重建损失的情况下进行重要的降维通常会带来一个代价：隐空间中缺乏可解释和可利用的结构（缺乏规则性，lack of regularity）。其次，大多数时候，降维的最终目的不仅是减少数据的维数，而是要在减少维数的同时将数据结构信息的主要部分保留在简化的表示中。出于这两个原因，必须根据降维的最终目的来仔细控制和调整隐空间的大小和自编码器的“深度”（深度定义压缩的程度和质量）。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/8d4c097f640248fcbfc6920f52b2e307><p class=pgc-img-caption></p></div><p>降维时希望保留主要信息</p><h2 class=pgc-h-arrow-right>变分自编码器（VAE）</h2><p>到目前为止，我们已经讨论了降维问题，并介绍了自编码器，它们是可以通过梯度下降训练的编码器-解码器结构。现在，让我们考虑内容生成问题，看一下自编码器对此问题的局限性，并介绍变分自编码器。</p><h3 class=pgc-h-arrow-right>自编码器用于内容生成的局限性</h3><p>此时，自然会想到一个问题：“自编码器和内容生成之间的联系是什么？”。确实，一旦对自编码器进行了训练，我们既有编码器又有解码器，但是仍然没有真正的方式来产生任何新内容。乍一看，我们可能会认为，如果隐空间足够规则（在训练过程中编码器很好地“组织”了），我们可以从该隐空间中随机取一个点并将其解码以获得新的内容。然后，解码器将或多或少地像生成对抗网络的生成器一样工作。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8d26406a0a6c46e99ba4d5839c3c1b54><p class=pgc-img-caption></p></div><p>但是，正如我们在上一节中讨论的那样，自编码器的隐空间的规则性是一个难点，它取决于初始空间中数据的分布、隐空间的大小和编码器的结构。因此，很难（如果不是不可能）先验地确保编码器将以与我们刚刚描述的生成过程兼容的方式智能地组织隐空间。</p><p>为了说明这一点，让我们再看一遍之前给出的示例，在该示例中，我们描述了一种强大的编码器和解码器，可以将任何N个初始训练数据放到实轴上（每个数据点都被编码为实值）并可以没有任何损失地解码。在这种情况下，自编码器的高自由度使得可以在没有信息损失的情况下进行编码和解码（尽管隐空间的维数较低）但会导致严重的过度拟合，这意味着隐空间的某些点将在解码时给出无意义的内容。尽管这种精挑细选的一维示例太过极端，但可以注意到自编码器的隐空间规则性问题是普遍的，值得特别注意。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f117d52f1f004ced803d3ffac0a24e8d><p class=pgc-img-caption></p></div><p>进入隐空间的编码数据之间缺乏结构是很正常的。的确，在自编码器的训练任务中，没有什么东西保证能够得到这种结构：无论隐空间如何组织，自编码器都仅以尽可能少的损失为目标进行训练。因此，如果我们对架构的定义不小心，那么在训练过程中，网络很自然地会利用任何过拟合的可能性来尽可能地完成其任务……除非我们明确对其进行规范化！</p><h3 class=pgc-h-arrow-right>变分自编码器的定义</h3><p>因此，为了能够将我们的自编码器的解码器用于生成目的，我们必须确保隐空间足够规则。获得这种规律性的一种可能解决方案是在训练过程中引入显式的正规化（regularisation）。<strong>因此，正如我们在这篇文章的简介中提到的那样，变分自编码器可以定义为一种自编码器，其训练经过正规化以避免过度拟合，并确保隐空间具有能够进行生成过程的良好属性。</strong></p><p>就像标准自编码器一样，变分自编码器是一种由编码器和解码器组成的结构，经过训练以使编码解码后的数据与初始数据之间的重构误差最小。但是，为了引入隐空间的某些正则化，我们对编码-解码过程进行了一些修改：<strong>我们不是将输入编码为隐空间中的单个点，而是将其编码为隐空间中的概率分布</strong>。然后对模型进行如下训练：</p><ul><li>首先，将输入编码为在隐空间上的分布；</li><li>第二，从该分布中采样隐空间中的一个点；</li><li>第三，对采样点进行解码并计算出重建误差；</li><li>最后，重建误差通过网络反向传播。</li></ul><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/4d87e23328714179a9ca719239208f79><p class=pgc-img-caption></p></div><p>自编码器（确定性）和变分自编码器（概率性）之间的差异</p><p>实践中，选择正态分布作为编码的分布，使得可以训练编码器来返回描述高斯分布的均值和协方差矩阵。将输入编码为具有一定方差而不是单个点的分布的原因是它可以非常自然地表达隐空间规则化：编码器返回的分布被强制接近标准正态分布。在下一节中，我们将通过这种方式确保隐空间的局部和全局正则化（局部是由于方差控制，而全局是由于均值控制）。</p><p>因此，在训练VAE时最小化的损失函数由一个“重构项”（在最后一层）组成，“重构项”倾向于使编码解码方案尽可能地具有高性能，而一个“正则化项”（在隐层）通过使编码器返回的分布接近标准正态分布，来规范隐空间的组织。该正则化项为返回的分布与标准高斯之间的<strong>Kulback-Leibler散度[1]</strong>，这将在下一节中进一步说明。我们可以注意到，两个高斯分布之间的Kullback-Leibler散度具有封闭形式，可以直接用两个分布的均值和协方差矩阵表示。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cd850df1343e467293065952575303b2><p class=pgc-img-caption></p></div><p>在变分自动编码器中，损失函数由一个重构项（优化编码-解码）和一个正则化项（使隐空间规则化）组成。</p><h3 class=pgc-h-arrow-right>关于正则化的直观解释</h3><p>为了使生成过程成为可能，我们期望隐空间具有规则性，这可以通过两个主要属性表示：连续性（continuity，隐空间中的两个相邻点解码后不应呈现两个完全不同的内容）和完整性（completeness，针对给定的分布，从隐空间采样的点在解码后应提供“有意义”的内容）。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/59f1dde9fce048e7b816469e783ac3ef><p class=pgc-img-caption></p></div><p>不规范的隐空间vs规范的隐空间</p><p>VAE将输入编码为分布而不是点不足以确保连续性和完整性。如果没有明确定义的正则化项，则模型可以学习最小化其重构误差，从而“忽略”要返回一个分布并表现得几乎像经典自编码器一样（导致过度拟合）。为此，编码器可以返回具有微小方差的分布（这往往是点分布punctual distributions），或者返回具有巨大均值差异的分布（在隐空间中彼此实际上相距很远）。在这两种情况下，返回分布的限制都没有取得效果，并且不满足连续性和/或完整性。</p><p>因此，为了避免这些影响，<strong>我们必须对协方差矩阵和编码器返回的分布均值进行正则化</strong>。实际上，通过强制分布接近标准正态分布（集中和简化）来完成此正则化。这样，我们要求协方差矩阵接近於单位阵，防止出现单点分布，并且均值接近于0，防止编码分布彼此相距太远。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7718165368954f0491bb9043118bef0e><p class=pgc-img-caption></p></div><p>必须对返回的VAE分布进行正则化，以获得具有良好属性的隐空间。</p><p>使用此正则化项，我们可以防止模型在隐空间中的编码相互远离，并鼓励尽可能多的返回分布“重叠”，从而满足预期的连续性和完整性条件。自然地，对于任何正则化项，这都是以训练数据上更高的重建误差为代价的。然而，可以调整重建误差和KL散度之间的权重，我们将在下一节中看到如何从形式推导中自然得出平衡的表达。</p><p>总结这一小节，我们可以观察到，通过正则化获得的连续性和完整性往往会在隐空间中编码的信息上产生“梯度”。例如，应将隐空间中位于来自不同训练数据的两个编码分布的均值的中间点解码为提供第一个分布的数据和提供第二个分布的数据之间的某个数据，如下所示：在两种情况下，它都可以由自编码器采样。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/14df8d3d3df14c8dbd2b77dadee04b93><p class=pgc-img-caption></p></div><p>正则化倾向于在潜在空间中编码的信息上创建一个“梯度”。</p><blockquote><p><strong>注意</strong>：顺带一提，我们提到的第二个问题（网络使分布彼此远离）当规模变大时几乎与第一个潜在问题（网络趋向于返回单点分布）等效：在两种情况下，分布方差的差别都相对小于均值之间的差别。</p></blockquote><h2 class=pgc-h-arrow-right>VAE的数学细节</h2><p>在上一节中，我们给出了以下直观的概述：VAE是将输入编码为分布而不是点的自编码器，并且其隐空间结构通过将编码器返回的分布约束为接近标准高斯而得以规范化。在本节中，我们将对VAE进行更数学的介绍，从而使我们可以更严格地证明正则化项的合理性。为此，我们将建立一个明确的概率框架，并将使用变分推理技术。</p><h3 class=pgc-h-arrow-right>概率框架和假设</h3><p>让我们首先定义一个概率图形模型来描述我们的数据。我们用表示代表我们的数据变量，并假定是由未直接观察到的潜在变量（编码表示,the encoded representation）生成的。因此，对于每个数据点，假定以下两个步骤生成过程：</p><ul><li>首先，从先验分布中采样一个隐空间表示；</li><li>第二，按条件概率采样数据</li></ul><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/de543f03a926476681618a4e3f1247b0><p class=pgc-img-caption></p></div><p>数据生成过程的概率图模型。</p><p>在这种概率模型下，我们可以重新定义编码器和解码器的概念。实际上，与考虑使用确定的编码器和解码器的简单自编码器不同，我们现在将考虑这两个对象的概率版本。自然地，“概率解码器”由定义，描述给定已编码变量的解码变量的分布，而“概率编码器”由定义，描述的分布。根据解码后的变量给出编码后变量的分布。</p><p>此时，我们已经注意到，在简单的自编码器中所缺乏的隐空间的正则化自然出现在数据生成过程的定义中：我们假设隐空间中的编码表示遵循先验分布。否则，我们还可以提一下著名的贝叶斯定理，该定理在先验，似然性和后验之间建立联系</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/fc475eecc1d2499f9f454b3884f03566><p class=pgc-img-caption></p></div><p>现在让我们假设是标准的高斯分布，是高斯分布，其均值由变量的确定函数定义，并且协方差矩阵的形式为正常数乘以单位矩阵的。假定函数属于记为的函数族，对于我们暂不指定。因此，我们有</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f6a708bca21a4d99a6192d8178ee6ec9><p class=pgc-img-caption></p></div><p>现在，让我们考虑。从理论上讲，我们知道和，便可以使用贝叶斯定理来计算：这是经典的<strong>贝叶斯推理问题[2]</strong>。但是，正如我们在前一篇文章中所讨论的那样，这种计算通常是棘手的（由于分母处的积分），需要使用诸如变分推理之类的近似技术。</p><blockquote><p><strong>注意</strong>：在这里我们可以提到和都是高斯分布。因此，如果我们有，则意味着也应遵循高斯分布，并且从理论上讲，我们可以“仅”尝试表示均值以及的协方差矩阵相对于和x|z)$的均值和协方差矩阵。但是，实际上不满足此条件，因此我们需要使用近似算法（如变分推理），使该方法能够通用，并且对模型假设的某些更改更加健壮。</p></blockquote><h3 class=pgc-h-arrow-right>变分推理公式</h3><p>在统计中，变分推论（VI）是一种近似复杂分布的技术。这个想法是要设置一个参数化的分布族（例如高斯族，其参数是均值和协方差），并在该族中寻找目标分布的最佳近似。该族中最好的对象是使给定的近似误差测量值最小化的元素（大多数情况下是近似分布与目标分布之间的Kullback-Leibler散度），并通过对该族的参数进行梯度下降来发现。有关更多详细信息，请参阅有关<strong>变分推理的文章[3]</strong>及其中的参考。</p><p>在这里，我们将通过高斯分布来近似，其均值和协方差由参数的两个函数和定义。这两个函数应该分别属于函数族和，这将在以后指定，但是应该对其进行参数化。因此我们可以写出如下表示</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/989658a89769433c955bd7d902ff08d9><p class=pgc-img-caption></p></div><p>好了，我们已经以这种方式定义了一个变分推论的候选族，现在需要通过优化函数和（实际上是它们的参数）以最小化近似分布和真实分布间的Kullback-Leibler散度，从而找到该族中的最佳近似。换句话说，我们正在寻找最优的和，使得</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/38ef33cac1724f7791dfe663cc833089><p class=pgc-img-caption></p></div><p>在倒数第二个方程中，我们可以观察到在近似后验时存在一个权衡点——最大化“观测”的可能性（第一项，预期对数似然概率的最大）与接近先验分布（第二项，和之间的KL散度最小）。这种折衷对于贝叶斯推理问题是很自然的，体现了在数据的置信度与先验分布置信度之间的平衡。</p><p>到目前为止，如果假设函数是已知且固定的，则我们可以使用变分推理技术来近似后验。但是实际上，定义解码器的函数是未知的，而且也需要求解。但我们最初的目标是找到一种性能良好的编码/解码方案，其隐空间又足够规则，可以用于生成目的。如果规则性主要由在隐空间上假定的先验分布所决定，则整个编码-解码方案的性能高度取决于函数的选择。确实，由于可以从和近似（通过变分推论），而是简单的标准高斯模型，仅存的两个需要我们优化的对象是参数(决定了分布的协方差)和函数(决定了分布的均值)。</p><p>因此，让我们考虑一下，正如我们之前讨论的那样，对于中的任何函数（每个函数都定义一个不同的概率解码器），我们都可以得到的最佳近似。无论他的特性如何，但我们正在寻找一种尽可能高效的编码-解码方案，然后，当给定从采样的时，我们希望选择函数使的期望对数似然概率最大化。<strong>换句话说，对于给定的输入，当我们从分布采样然后从分布采样时，我们希望最大化的概率。因此，我们正在寻找最优</strong></p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ebd76e8633f84525a07cfeb2a0dd44cf><p class=pgc-img-caption></p></div><p>其中取决于函数，获得方法如前所属。将所有部分聚集在一起，我们正在寻找最优的，和，以便</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/961fc36eea8b4bdb8834fc09ddc8342d><p class=pgc-img-caption></p></div><p>我们可以在该目标函数中对应前面章节里对VAE的直观描述中引入的元素：和之间的重构误差以及和（这是标准的高斯）之间由KL散度给出的正则项。我们还可以注意到常数，它决定了前两个条件之间的平衡。越高，我们对模型中的概率解码器假设周围的方差就越大，我们也就越关注正则化项（如果低，则相反）。</p><h3 class=pgc-h-arrow-right>将神经网络引入模型</h3><p>目前为止，我们已经建立了一个依赖于三个函数，和的概率模型，并使用变分推理表示要解决的优化问题，以便获得能够给出最优值的，和。此模型的编解码方案。由于我们无法轻松地在函数的整个空间上进行优化，因此我们限制了优化域，并决定将，和表示为神经网络。因此，，和分别对应于网络体系结构定义的函数族，并且将对这些网络的参数进行优化。</p><p>实际上，和不是由两个完全独立的网络定义的，而是共享它们的一部分结构和权重，因此我们可以</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d12ce99240d0416d8634d13068219e52><p class=pgc-img-caption></p></div><p>因为它定义了的协方差矩阵，所以被假定为方阵。但是，为了简化计算并减少参数的数量，我们做出了额外的假设，即的近似值是具有对角协方差矩阵的多维高斯分布（变量独立性假设）。在此假设下，只是协方差矩阵对角元素的向量，因此其大小与相同。但是，我们以这种方式减少了我们用于变分推断的分布族，因此，对的近似可能不太准确。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f7507eb154264fb28b97d4190a040805><p class=pgc-img-caption></p></div><p>VAE的编码器部分</p><p>编码器对建模时将均值和协方差均视关于为(和）的高斯函数，解码器则与之不同。我们的模型假设具有固定的高斯协方差。函数定义关于的高斯分布均值，由神经网络建模，可以表示如下</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d4513d5b24754fb394f952bc838f9dee><p class=pgc-img-caption></p></div><p>VAE的解码器部分</p><p>然后，通过将编码器和解码器部分串联在一起，可以获得总体架构。但是，在训练过程中，我们仍然需要非常小心从编码器返回的分布中进行采样。采样过程必须以允许误差通过网络反向传播。尽管有随机采样发生在模型的中间，但有一个简单的称为重参数化技巧（reparametrisation trick），使梯度下降成为可能。它利用以下事实：如果是遵循均值的高斯分布的随机变量，则与协方差可以表示为</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/345d58395d044fcd9bb7f7a35d21f3f3><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/82a0372225cd42e8809c4fcb587f9eac><p class=pgc-img-caption></p></div><p>reparametrisation示意图</p><p>最后，这种方式获得的变分式自编码器架构的目标函数由上一小节的最后一个方程式给出，其中理论期望由蒙特卡洛近似代替，该近似值在大多数情况下仅进行单次采样。因此，考虑这种近似并表示（），我们可以获得上一节中直观得出的损失函数，该函数由一个重构项，一个正则项和一个常数来定义这两个项的相对权重。</p><div class=pgc-img><img alt=万字长文带你了解变分自编码器VAEs onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/febaee24996c4e39b0165377ad5e0479><p class=pgc-img-caption></p></div><p>变分自编码器的表示形式</p><h2 class=pgc-h-arrow-right>总结</h2><p>本文的主要方法是：</p><ul><li>降维是减少描述某些数据的特征量的过程（通过仅选择初始特征的子集或通过将它们组合成数量更少的新特征），因此可以看作是编码过程</li><li>自编码器是由编码器和解码器组成的神经网络体系结构，它们会创建瓶颈以处理数据，并经过训练以在编码-解码过程中损失最少的信息（通过梯度下降迭代进行训练，目的是减少重建错误）</li><li>由于过度拟合，自编码器的隐空间可能极为不规则（隐空间中的临近点可能产生截然不同的解码数据，而且解码后，隐空间中的某些点可能产生无意义的内容），因此，我们无法定义了一个生成过程，该过程仅包含从隐空间中采样一个点，然后使其经过解码器以获取新数据</li><li>变分自编码器（VAE）是自编码器，它通过使编码器返回隐空间中的分布而不是单个点，并在损失函数中添加一个对返回的分布的正则项来解决隐空间不规则性的问题，以确保更好地组织隐空间</li><li>假设有一个简单的基本概率模型来描述我们的数据，则可以详细推导由重构项和正则项组成的非常直观的VAE损失函数，尤其是使用变分推理的统计技术（因此称为“变分”自编码器）</li></ul><p>总而言之，在过去的几年中，GAN获得的关注远超过VAE。除其他原因外，与GAN对抗训练的简洁概念相比，VAE的理论基础（概率模型和变分推论）的复杂程度较高。希望我们能像今年早些时候为GAN所做的那样，通过本篇文章分享宝贵的直觉和强大的理论基础，以使VAE更易于新人使用。</p><h3 class=pgc-h-arrow-right>参考资料</h3><p>[1]</p><p>KL散度: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence</p><p>[2]</p><p>贝叶斯推理问题: https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29</p><p>[3]</p><p>变分推理文章: https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'万字长','文带','了解'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>