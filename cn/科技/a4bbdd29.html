<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>针对深度神经网络的简单黑盒对抗攻击 | 极客快訊</title><meta property="og:title" content="针对深度神经网络的简单黑盒对抗攻击 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/b9ec712cd33442338496141ebfcecb45"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a4bbdd29.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a4bbdd29.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a4bbdd29.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a4bbdd29.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a4bbdd29.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a4bbdd29.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a4bbdd29.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a4bbdd29.html><meta property="article:published_time" content="2020-10-29T21:09:20+08:00"><meta property="article:modified_time" content="2020-10-29T21:09:20+08:00"><meta name=Keywords content><meta name=description content="针对深度神经网络的简单黑盒对抗攻击"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/a4bbdd29.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>针对深度神经网络的简单黑盒对抗攻击</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><div class=pgc-img><img alt=针对深度神经网络的简单黑盒对抗攻击 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b9ec712cd33442338496141ebfcecb45><p class=pgc-img-caption></p></div><blockquote><p><strong>论文：</strong>Narodytska, Nina, and Shiva Prasad Kasiviswanathan. "Simple Black-Box Adversarial Attacks on Deep Neural Networks." CVPR Workshops. 2017.</p><p><u>http://www.shivakasiviswanathan.com/CVPR17W.pdf</u></p></blockquote><h1>摘要：</h1><p>深度神经网络是一个功能强大且广泛流行的学习模型，在很多计算机视觉、语音和语言处理任务都能实现最先进的模式识别效果。然而这些网络很容易受到对抗性扰动的影响，迫使模型对输入错误分类，从而导致不良后果，在模型实际部署的时候甚至会带来安全风险。</p><p>在本篇论文中，作者们主要针对深度卷积神经网络开展研究，并证明了即使对目标网络内部一无所知，攻击者也很容易制造对抗样本。他们将神经网络看作一个黑盒，并且只假设可以在探测的输入上观察神经网络的输出。然后使用一种新颖的局部搜索技术来构建数值近似的网络梯度，然后将其用在构造图片的像素扰动上，并展示了如何调整这个潜在的想法，以实现那些强烈的错误分类概念。由于这种攻击方法的简单性和有效性，所以作者认为它可以服务于设计网络鲁棒性。</p><h1>技术介绍：</h1><p>卷积神经网络（CNN）是处理计算机视觉任务的主流技术之一，在图像识别，本地化，视频跟踪以及图像和视频分割等任务中表现出良好的性能，但也特别容易受到输入图像的对抗性扰动，甚至可能在实际应用中导致不良后果。这篇文章中，作者研究了最先进的卷积神经网络（CNN）的鲁棒性，以图像作为简单的黑盒对抗性攻击的输入。在这种情况下，对抗性攻击的粗略目标如下：给定一个由卷积神经网络正确分类的图像，通过构建图片的变换（比如小规模的像素扰动），从而导致网络分类不正确。</p><p>现在关于关于对抗性攻击的主要研究方向分为两种，包括基于网络的梯度构建扰动和基于黑盒的扰动，这篇文章采用的是第二种，即在限制了被攻击网络的内部信息，不了解网络结构、参数或训练数据的情况下进行对抗样本的生成。但作者通过实验证明，在这种黑盒模式下的访问级别和少量查询也能够提供足够的信息来构建对抗性图像。</p><p>和利用对抗样本的迁移性来进行黑盒攻击不同，作者认为他们的攻击方法更加简单直接，更有有效，还有其他一些计算优势，能够在不同训练集上训练的多个网络架构上构建对抗样本。实验采用了CIFAR10，MNIST，SVHN，STL10和ImageNet1000五类数据集，和Network-in-Network，以及VGG两种流行的网络架构。表1是ImageNet1000的四个示例，上排是原始图像，下排是生成的用来使模型错误分类的扰动图像。</p><div class=pgc-img><img alt=针对深度神经网络的简单黑盒对抗攻击 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/43bf2ad350d54c7e930acd256a85e677><p class=pgc-img-caption>表1</p></div><h1>初步计划：</h1><p>（1) 符号和正则化</p><p>（2) 定义神经网络的输入和输出</p><p>（3) 定义错误分类概念</p><p>（a) K-错误分类，即神经网络输出正确标签的可能性至少在k个其他标签之下</p><p>（b) 针对目标的错误分类，即在给定目标标签T的情况下，使识别错误分类图像产生的真实标签c(I) 满足：T ≠ c(I)</p><p>（4) 确定对抗目标</p><h1>对抗图片的生成：</h1><p>在对抗图片的生成上，作者提出他们的攻击方法使用局部搜索技术来构建网络梯度的隐式近似，然后用于指导扰动图像的生成。另外关于图片的扰动，也有很多方法，在这篇文章中，作者使用了一个简单的符号保留扰动函数PERT(, p, x, y), 其中L表示图片，p是一个扰动参数，x，y是图片中像素点的位置。输出的图片满足</p><div class=pgc-img><img alt=针对深度神经网络的简单黑盒对抗攻击 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ba154b9ec6f2449abe6d03364791e47c><p class=pgc-img-caption></p></div><p>过程中除了特定位置上根据扰动参数产生简单的扰动，不改变图片的其他地方，即：</p><div class=pgc-img><img alt=针对深度神经网络的简单黑盒对抗攻击 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/9e67a00c0ad9485c9c201ea9816dd03e><p class=pgc-img-caption>注：（b, u, v）是图片l在b通道（u, v）位置上的像素值。</p></div><p>至于如何指导扰动的生成，作者则是根据两种不同的错误分类概念分别来进行阐述。</p><p>1）k-错误分类的对抗攻击</p><p>在K-错误分类中，作者使用局部搜索程序，生成和原始图像仅存在几个像素差异的扰动图片，来最小化使网络输出原始标签的概率。在每一轮中，局部搜索过程会通过理解输出上的几个像素的影响来计算当前图像的梯度的隐式近似，并用于更新该图像。</p><div class=pgc-img><img alt=针对深度神经网络的简单黑盒对抗攻击 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8f2c2b4a2e654aac8083f3b84725ef08><p class=pgc-img-caption></p></div><p>其次，定义了局部搜索中图片的邻域集，即以图片和图片像素点的差异个数作为彼此之间的距离，对于同个领域集，到当前迭代中的扰动图片的距离，都应该是一样的（为1），所以可以用一系列像素位置来明确领域集的数学含义。令(PX , PY )i 表示第i次迭代中像素位置的集合，前后迭代中应满足：</p><div class=pgc-img><img alt=针对深度神经网络的简单黑盒对抗攻击 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0dfa55a2f18f4ba597a57aab62ff8abd><p class=pgc-img-caption></p></div><p>第三，描述了像素位置集的转化函数g，它将图像l'和 像素位置集作为输入，参数t表示g扰动像素的个数，以及两个扰动参数p和r， r的值一般控制在[0,2]范围内，同时在搜索过程中自动调整p的值。 在局部搜索的第i轮迭代中，构造扰动图片集如下：</p><div class=pgc-img><img alt=针对深度神经网络的简单黑盒对抗攻击 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4a62ef8013ad44338acde8d8f9b7f7b5><p class=pgc-img-caption></p></div><p>然后计算出扰动图片集中每一张图片的成本函数，并降序排列，得到排序集。使成本函数f产生较大下降的图片很有可能能构造对抗候选图片，通过选择排序集前t张图片构造像素位置集，即 (PX∗ , PY∗ )i =</p><div class=pgc-img><img alt=针对深度神经网络的简单黑盒对抗攻击 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2066e082e02742b59da3d651d5bbb42b><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=针对深度神经网络的简单黑盒对抗攻击 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/28efb71afc08426c9bb35ba39cfb74c4><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=针对深度神经网络的简单黑盒对抗攻击 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/79596c53ac1641eca89f3a01418998d5><p class=pgc-img-caption>算法1</p></div><p>算法LOCSEARCHADV提供了本地搜索过程的完整伪代码（参见算法2）。它将图像作为输入，并在每一轮迭代中，使用定义的目标函数找到一些需要扰动像素位置，然后将上面定义的变换函数应用于这些选定的像素以构建新的扰动图像。如果在任何一轮中产生的图片都将真实标签的概率低于结果向量的其他k个分类，则终止算法，不然，则进入下一轮（最多R轮）。算法LOCSEARCHADV扰动的像素数理论上最多为t×R，实际上会更少，跟对每个单独的像素应用相同的扰动的对抗攻击方案形成鲜明的对比。</p><div class=pgc-img><img alt=针对深度神经网络的简单黑盒对抗攻击 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3fa2d803bc2045b49635a3c7fd5f8085><p class=pgc-img-caption>算法2</p></div><p>2）针对目标错误分类的对抗攻击</p><p>对于特定目标的错误分类，作者提出只要改进LOCSEARCHADV算法的成本函数，即最大化图片 属于目标类的概率，然后换以递增的顺序排序成本函数即可，其余的部分和LOCSEARCHADV原算法保持一致。</p><h1>本文主要贡献：</h1><p>论文展示了在不了解网络架构或其参数的情况下，为现代深度CNN生成对抗图像的简易性。攻击策略基于贪婪局部搜索的概念，即迭代搜索过程，其中在每一轮中使用局部邻域来细化当前图像并且在过程中优化取决于网络输出的一些目标函数。由于是一个黑盒操作，因此无法获得网络损耗函数的真实梯度，因此依赖于梯度的数值近似。</p><p>作者考虑了更强的错误分类概念，将其称为k-错误分类，其目标是改变输入，使得即使放宽到k个标签范围，网络也无法识别真实标签。同时还考虑了针对目标错误分类的概念，其目标是获取输入并对其进行更改以使网络将其分类为与真实类别标签不同的任何选定目标类别标签。</p><p>同时他们对多个图像数据集进行了广泛的实验评估，并表明基于局部搜索的方法可以可靠地生成对抗图像，几乎没有任何扰动。另一个特征是，通过设计，和大多数可能需要扰动所有像素的方法相比，这篇论文提出的方法仅在对抗图像生成过程中扰乱非常小部分的像素（例如，在ImageNet1000上，平均扰动每个图像仅约0.5％的像素）。</p><h1><strong>致谢</strong></h1><p class=ql-align-justify>此文由南京大学软件学院2018级硕士周赛翻译转述。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'针对','神经','网络'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>