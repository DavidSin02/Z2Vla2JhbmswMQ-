<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>深度学习之图像分类-----VGG | 极客快訊</title><meta property="og:title" content="深度学习之图像分类-----VGG - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/93a1ccf97a2f4277b9c68a06f44e4ccc"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a8622e86.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a8622e86.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a8622e86.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a8622e86.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a8622e86.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a8622e86.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a8622e86.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a8622e86.html><meta property="article:published_time" content="2020-11-14T21:03:08+08:00"><meta property="article:modified_time" content="2020-11-14T21:03:08+08:00"><meta name=Keywords content><meta name=description content="深度学习之图像分类-----VGG"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/a8622e86.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>深度学习之图像分类-----VGG</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><h1><strong>1 简介</strong></h1><p>VGGNet由牛津大学的视觉几何组（Visual Geometry Group）提出，获得了2014年ILSVRC竞赛的分类任务第二名和定位任务第一名，主要贡献在于证明了使用3x3小卷积核，增加网络深度可以有效提升模型性能，并且对于其他数据集也有很好的泛化性能。</p><h1><strong>2 网络结构</strong></h1><p>论文中一共提供了6种网络配置，层数从浅到深分别为11层、13层、16层和19层。其中11层时，主要比较了Local Response Normalisation（LRN）的作用，结果是LRN并没有提升网络性能。</p><div class=pgc-img><img alt=深度学习之图像分类-----VGG onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/93a1ccf97a2f4277b9c68a06f44e4ccc><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=深度学习之图像分类-----VGG onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4024d89dc0eb4be7b03cd9d2f6b4ca02><p class=pgc-img-caption></p></div><p><strong>网络结构的特点</strong></p><p>网络有5段卷积，每段卷积内分别有22333个卷积层，每段后面跟着个最大池化层</p><p>每段内的卷积核数量一样，越后边段内卷积核数量越多，依次为64-128-256-512-512</p><p><strong>使用多个3*3叠加的原因：</strong></p><p>（两个3*3相当于5*5的感受野）</p><p>保持感受野相同的情况下，参数更少</p><p>增加了非线性，使网络对特征的学习能力更强</p><p><strong>使用1*1的卷积核</strong></p><p>在不影响感受野的情况下，增加非线性</p><p>用来调整特征图的channel的维度（这里没有使用）</p><h1>3 VGGNet网络的参数：</h1><div class=pgc-img><img alt=深度学习之图像分类-----VGG onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/230053a4bc454d9fb53196d835e12123><p class=pgc-img-caption></p></div><p>现象：网络逐渐变深，参数量并没有增加很多。</p><p>原因：参数量主要消耗在最后的3个全连接层，前面的卷积层数虽多，但参数消耗量不大。（但卷积层训练比较耗时，因为计算量大）</p><p><strong>参数和计算量的对比</strong></p><p>在计算量这里，为了突出小卷积核的优势，我拿同样conv3x3、conv5x5、conv7x7、conv9x9和conv11x11，在224x224x3的RGB图上（output_channel=96）做卷积，卷积层的参数规模和得到的feature map的大小如下：</p><div class=pgc-img><img alt=深度学习之图像分类-----VGG onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c773f86156704bbd9e0ef40e8f3ad93f><p class=pgc-img-caption></p></div><p>计算量：思考角度为特征图，特征图的B个参数，每一个参数都是经过单层的卷积核计算而来，每个单层卷积层有kernel_param个参数，需要经过kernel_param个计算。前项和后向共两次，所以再乘以2。</p><p>对于该例子：卷积层本身的参数占少数，不同的卷积层出来的特征图参数400w；conv3*3的计算量达到2亿，conv11*11的计算量达到30亿。 所以，使用两个conv3*3代替conv5*5，网络本身参数由7200-->‭5184‬(2592+2592)，计算量由7亿-->5亿(2.6亿+2.6亿）</p><h1>4 VGGNet的训练</h1><p><strong>超参数的设置</strong></p><p>训练使用加动量的小批基于反向传播的梯度下降法来优化多项逻辑回归目标</p><p>batch size为256，momentum设为0.9，权重衰减系数为0.0005</p><p>前两层的全连接后面跟着Dropout，设置为0.5</p><p>学习率初始为0.01，当验证集的准确率停止提高时，学习率除以10。最终学习率降低了三次以后，网络就收敛了</p><p>对于深度网络来说，网络的权重初始化十分重要。为此，论文中首先训练了一前程的网络结构A（第一个图中），训练折后网络时，随机初始化他的权重就足够得到比较好的结果。然后，当训练深层的网络时，前四层卷积核最后三个全连接层使用的是学习好的A网络的权重来进行初始化，其余层则随机初始化。（即某些层的预初始化）</p><p><strong>图片的处理</strong></p><p>（1）首先将图片保持高宽比进行缩放，使最短边缩放至S(S>=224)。由于要求输入224*224大小，再对缩放后的图片进行crop。</p><p>S的取值方法：</p><p>固定的尺寸。对所有的图片，S都是固定的值。论文考察了S=256和S=384两种情况下分别训练得到的网络性能。为了加速，训练S=384的网络时，使用的训练好的S=256的网络来初始化权重，并且学习率更小，为0.001</p><p>可变的尺寸。设置一个范围[Smin, Smax]，每一张图片都从这个范围中随机选取一个数作为它的S。论文中使用的范围是[256, 512]。这个方法叫做尺度抖动scal jittering，有利于训练集增强，这使得训练在一个很大范围的图像尺度之上进行。为了加速，通过微调S=384的固定尺寸的网络来训练得到可变尺度的网络</p><p>（2）为了更进一步的增加训练集，对每张图片进行水平翻转以及进行随机RGB色差调整</p><h1><strong>5 VGGNet的测试</strong></h1><p><strong>将全连接换成全卷积</strong></p><div class=pgc-img><img alt=深度学习之图像分类-----VGG onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fc15146c714d4668aabd3d4a174e9d0c><p class=pgc-img-caption></p></div><p>相同处：</p><ul><li>两种方式，网络的参数是相同的。对于输入固定的输入，效果也是相同的。</li></ul><p>不同处：</p><ul><li>全连接的多次输入时，必须保持输入维度不变。</li><li class=ql-indent-1>需要保证网络的测试的图片保持相同的大小。</li><li class=ql-indent-1>一张大图裁剪成多张224*224小图，重复的部分就会多次进入网络进行计算，产生很多冗余计算</li></ul><p><strong>卷积是可以接受不同尺寸的信息</strong>。这样：</p><p>图片输入时，不需要提前裁剪成224*224。</p><p>当图片较大的情况下，网络输出的feature map尺寸就不是1*1*1000。网络的输出还保留了得分的位置信息。在输出channel上求平均后，在输入softmax层，得到预测结果。减少冗余操作（下图例子）</p><div class=pgc-img><img alt=深度学习之图像分类-----VGG onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/897be6152f294c70b3e9624d31f901ce><p class=pgc-img-caption></p></div><p><strong>多重裁剪评估方式</strong></p><p>在GoogleLeNet中描述的详细过程如下：</p><ul><li>将图片缩放到不同的4种尺寸(纵横比不变，GoogleLeNet使用的4种尺寸为：缩放后的最短边长度分别为：256,288,320和352)。</li><li>对于得到的每个尺寸的图像，取左、中、右三个位置的正方形图像(边长就是最短边的长度。对于纵向图像来说，则取上、中、下三个位置)，因此每个尺寸的图像得到3个正方形图像；</li><li>然后再在每个正方形图像的4个crop顶点和中心位置处crop处224*224的图像，此外再加上将这个正方形图像缩放到224*224大小的图像，因此每个正方形图像得到6个224*224的图像；</li><li>最后，再将所有得到的224*224的图像水平翻转。因此，每个图像可以得到4*3*6*2=144个224*224大小的图像。</li><li>将这些图像分别输入神经网络进行分类，最后取平均，作为这个图像最终的分类结果。</li><li>而VGG中则使用的是3种尺寸，每个尺寸在5个位置处取正方形图像，每个正方形图像crop出5个224*224大小的图像，最后水平翻转，即3*5*5*2=150。</li></ul><p>（个人觉得，在实际应用中这样操作是不必要的。实际应用中我们注重效果的同时还有速度）</p><h1>6 VGGNet实验结果</h1><p>数据集 在本章，我们讲述了卷积神经网络在ILSVRC2012数据集上的分类结果。数据集包含1000个类别，被分为三部分：训练集（1.3M张图片），验证集（50K张图片），测试集（100K张图片，没有标签）。分类性能使用两个办法评估：top-1和top-5 error。前者是一个多类分类错误率，即错误分类图像的比例；后者是在ILSVRC上的主要评估标准，即真实类别不在top-5预测类别之中的图像的比例。</p><p><strong>5.1 单尺度评估</strong></p><p>对上面提出的模型进行测试。测试时，对于固定的S，去Q=S；对于可变的S，取Q=0.5*(Smin + Smax)。实验结果如下：</p><div class=pgc-img><img alt=深度学习之图像分类-----VGG onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3911df3e902a4db3bc6bc482cdec6ad8><p class=pgc-img-caption></p></div><ul><li>A和A-LRN对比：后者性能并没有提升，LRN没有好的效果</li><li>B和C对比（C优）：C有多的1*1的卷积。说明额外的非线性可以提升网络的性能</li><li>C和D对比（D优）：C有些1*1的卷积，D相应的是3*3卷积。说明抓取空间局部信息同样重要</li><li>总的看出，误差随着网络的深度的增加而减低。</li><li>训练时，使用可变的尺度训练要比固定尺度的效果要好</li></ul><p><strong>5.2 多尺度评估</strong></p><p>固定S训练的网络，考虑到训练与测试尺度之间的巨大差异会导致性能的下降，测试使用的Q={S-32， S， S+32}。</p><p>尺度S抖动训练的网络，则使用Q={Smin, 0.5(Smin, Smax), Smax}。</p><div class=pgc-img><img alt=深度学习之图像分类-----VGG onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f2e4f1c47bec4805a0074a5ec90593f7><p class=pgc-img-caption></p></div><p>通过和单尺度的结果对比，可以发现在测试时使用多尺度(尺度抖动)，可以提升准确率</p><p><strong>5.3 多重裁剪评估</strong></p><div class=pgc-img><img alt=深度学习之图像分类-----VGG onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/411653238f4d493a816fdeee2e5417e0><p class=pgc-img-caption></p></div><ul><li>可以看到，多重裁剪要比密集评估（多尺度）的结果要好。</li><li>论文结论，两种方法结合的效果会更好（softmax输出求均值），两种方法是互补的。</li><li class=ql-indent-1>原因：卷积层提取特征时的padding方式不同。多重裁剪：补0；多尺度：补的是它们附近的像素</li></ul><p><strong>5.4 多个网络结合</strong></p><p>论文还将训练的多个网络模型的结果相结合得到最终的分类结果，显然，这会提升分类的准确率以及稳定性，结果如下：</p><div class=pgc-img><img alt=深度学习之图像分类-----VGG onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4d5720c6a3e942428fe0b51294285781><p class=pgc-img-caption></p></div><p><strong>5.5 多种方法的对比</strong></p><div class=pgc-img><img alt=深度学习之图像分类-----VGG onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/485d6626fcec439c8ce3060b30eeb812><p class=pgc-img-caption></p></div><h1>私信我:“学习”，可免费领取更多相关学习资料 （免费的哦）。</h1><div class=pgc-img><img alt=深度学习之图像分类-----VGG onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a4cd665f2c8d4384903e183831ca4e17><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=深度学习之图像分类-----VGG onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/9062ededf4bc4783a8a5f83020ddaf58><p class=pgc-img-caption></p></div></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'学习','之图','分类'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>