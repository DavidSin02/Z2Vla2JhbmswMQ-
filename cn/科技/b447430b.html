<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 | 极客快訊</title><meta property="og:title" content="信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/007b4884a10342e39988c1777d5965c0"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b447430b.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b447430b.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/b447430b.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b447430b.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b447430b.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/b447430b.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/b447430b.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b447430b.html><meta property="article:published_time" content="2020-11-14T21:00:20+08:00"><meta property="article:modified_time" content="2020-11-14T21:00:20+08:00"><meta name=Keywords content><meta name=description content="信息抽取算法从入门到精通，一文了解自然语言处理的前世今生"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/b447430b.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>信息抽取算法从入门到精通，一文了解自然语言处理的前世今生</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><blockquote><p>本文整理自达观数据联合创始人高翔 7 月 11 日的公开课——《智能文本信息抽取算法的进阶和应用》。从“人工智能中的明珠”自然语言处理的历史讲起，由基础到前沿，循序渐进地介绍各种信息抽取相关算法，并在文末给出了练习建议。<em>全文上万字，建议收藏阅读。</em></p></blockquote><h1><strong>文本挖掘简介</strong></h1><p>我们开始介绍一下文本挖掘。我们可以把人工智能分为三类——图像、文本和语音。达观是主要在文本智能领域的公司。文本相对于图像和语言来说更难处理，因为文本数据需要做一些逻辑分析。图像和语音属于感知智能，而文本属于认知智能，所以号称是“人工智能的明珠”，难度很大。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/007b4884a10342e39988c1777d5965c0><p class=pgc-img-caption></p></div><p>自然语言处理的任务是什么？简单来说就是让机器知道怎么看、要么写。我们一般把“看”叫自然语言理解（NLU），包括自动化审核、自动文本比对、信息纠错，搜索推荐等等，它可以大幅度减轻人工的负担。自动写作叫自然语言生成（NLG），包括自动填表、生成摘要，文本润色，还有大家看到的“自动生成股市”、“自动生成对联”等等。目前我们主要还是在解决自然语言理解的问题。语言生成因为一些限制，实际落地的效果仍然有待提高的。所以我们今天主要讨论自然语言理解这部分。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5f52180472f44cc89c3794cc703a00e7><p class=pgc-img-caption></p></div><p>其实自然语言处理的历史非常悠久，甚至出现在“AI”这个概念之前，最早叫“符号主义”。刚开始的时候人们选择了一个很不好的场景：机器翻译。机器翻译是一个难度很大的任务，因为涉及了语义的理解和不同语种语法的规则。所以早期自然语言处理不是很成功。过了20-30年，到上世纪80年代开始，我们使用了语法规则，基于自然语言处理的一些基本原理，再通过人工在这些语法的规则上进行修订，做了一些问答、翻译和搜索方面的尝试。</p><p>自然语言处理真正的黄金时期是从上世纪90年代开始，那时候我们搞了统计学，做了很多基于统计机器学习的算法。从下图中我们可以发现，统计模型的效果让自然语言处理的应用领域更加广泛，产生了很大进步。其实在上世纪90年代的时候，自然语言处理已经可以在很多场景表现得很不错了，比之前的技术要先进很多。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/634e4c08f083443b8e8176946bcd647b><p class=pgc-img-caption></p></div><p>从2006年到现在，深度学习已经开始起步。之前“神经网络”这个概念已经有了，只是当时受限于各种各样的算法和硬件，没法做得很好。但现在各方面都成熟之后，大家发现深度学习是一个神器。其实深度学习最早的时候在图像领域的应用较多，但目前自然语言处理也逐渐开始过渡到深度学习的阶段。尤其是去年像BERT这样的模型出来之后，我们发现自然语言处理的评测经常被屠榜，这说明神经网络非常有效，但也说明数据也很重要，后文中我们会解释数据的重要性。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5a67199795574f55a2751766f4bd082f><p class=pgc-img-caption></p></div><p>我们对比一下人类和计算机之间的差异。其实我们人类短时间内阅读理解文字的能力还不错，但是时间久了很容易遗忘。但计算机基本不会忘，只要硬盘不坏。人脑难以长期记忆，但我们对内容的推理能力比计算机强。因此，我们可以请计算机来做一些比较细节的工作。例如文字比对，我们检查错误要逐字逐句地看，非常累。计算机能做到秒看，却很难做复杂的逻辑和推理。</p><p>此外，虽然人类阅读速度很快，但写作速度很慢。大家高考的时候都要留几十分钟来写作。这是因为写的时候，我们手速有限。而且在写的过程中还要进行很多思考。写作本质是把脑中的很多语义信息压缩到一个点，也就是文章的主题。有了主题后我们还要再把作文展开，所以要花很多时间构思大纲、设计章节结构和文章主线，非常耗时。所以大家不要催那些网文作者了，他们每天写得其实挺辛苦的。要把整个大的流程串起来，其实是一件比较难的事情。</p><p>我们在接受信息时能很快地理解整体，但是难以记住细节。我们看完一个东西立刻能知道它的中心思想。例如，我们浏览了一个企业的信息之后，就能做出“这个企业比较靠谱，愿意投资”的判断。但是企业收入、竞争利润、负债这些具体数字很难全部记清楚。所以人去寻找局部信息的能力和计算机比非常慢。计算机的优点就是找这种局部信息，越细的东西它找得越快。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e7104cf3c12046f49e8f0aa65046438e><p class=pgc-img-caption></p></div><p>什么场景比较适合让计算机去做？基于现阶段的技术，现在大部分场景计算机还是无法取代人。我们可以看到，很多行业，包括法律，包括企业合同、客户意见、产品手册、新闻、问答资料的数据是需要我们亲自来看。虽然这些行业领域不同，但做的事情都类似。审一个企业合同的时候，需要看一些关键的信息，如甲方、乙方，以及这些东西是否合规，总金额是否正确。在法律行业，法官判案时也要看整个案由，包括被告和原告的相关信息，案件的时间、地点等等。这些都是信息抽取，在很多应用场景下都需要信息抽取。无论我们做了什么决策，判断是否投资，是否通过合同，如何进行法律判决，都需要先从文字中提取信息。</p><p><strong>其实在一些比较固定的，相对简单，不需要特别复杂的逻辑推理的场景中，机器学习算法已经可以完成信息抽取任务。</strong>我们正努力让计算机在这些场景落地，这不仅仅是算法的问题，也是应用的问题。这也是我们一直在思考的问题。</p><h1><strong>抽取算法概述</strong></h1><p>现在我们具体讲讲信息抽取的几种最主流的算法。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/40e339ae8b9342e4982a9127325fd2e6><p class=pgc-img-caption></p></div><p>什么是<strong>信息抽取？其实就是从文本中找到指定类型的实体。</strong>大家应该听过命名实体识别（NER），其实命名实体识别只是抽取中的一种。广义上的信息抽取，除了命名实体识别之外，还包括关系抽取、事件抽取等。其实在我看来，关系抽取和事件抽取比命名实体识别的应用层次更高级一点。因为这两个抽取同需要做NER，只是在做NER的基础之上，还要做一些其他的工作，来满足场景需求。</p><p>我们先从最简单的NER开始。命名实体一般是指人物、地点、机构、时间等内容。现在我们以公司抽取为例详细说明一下。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0c9b241a1cdf427c8e35669ff0a1efd8><p class=pgc-img-caption></p></div><p>如果从历史的角度来说，识别公司的任务就是所谓的“符号主义”任务，简单来说就是穷举所有公司的名称做词典匹配。这样就是一个命名实体。但是，这么做场景其实有限。为什么？因为上市公司的集合是有限的，所以直接拿公司字典可能比训练模型更快。</p><p>但是你会发现这种场景并不常见。比如，如果抽取所有公司（不仅限于上市公司）就不能用这种办法，因为公司实在太多了。十年前如果你看到“饿了么”，如果没有上下文，你不会觉得这是一个公司，但因为现在大家经常点“饿了么”，都知道这是一个公司的名字。而且，每天都有大量新公司产生，所以整体的公司是一个没法穷尽的集合。在这种情况下，我们没办法用字典很好地完成绝大多数任务。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bc355a80c48c432382b65c0b9f3ccb30><p class=pgc-img-caption></p></div><p>之前我们提到了上下文。那我们现在加入上下文信息，是不是可以知道某个实体是一个公司呢？最直接的方法是通过语法规则来做，例如“A是一家公司”、“B作为一家公司”等等。你会看到这样的一些模板，然后再去分析。如果说得学术/技术一点，相当于把这个任务提炼成一个比较复杂的句法依赖和语法规则。但从代码角度可能会比较简单，比如把模板中间的东西抠掉，然后去做匹配，做完匹配再去做填空，填空的内容就是你要的这些公司。</p><p>但这样做也有很大的问题，因为我们语言表述的方法太多了。例如，“我是A公司的”，“我来自B公司”以及很多种其他不同的表述都是一个意思，我们无法穷尽所有的表述方法。甚至周星弛的电影也能增加这种做法的难度。我们以前说“我先走了”，现在会说“我走了先”、“我吃了先”，这其实跟我们传统的语法都不太一样，但现实生活中就有这么多表述。不过，和上面的字典类似，在特定的场合，比如一些特定领域的公文等文书文章，还是有套路或者标准写法，也许可以用这种方法。总的来说这种方法比较简单。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4f1367d915254356bb38fb5b38a20e81><p class=pgc-img-caption></p></div><p>更高级的是基于统计机器学习的方法，从算法上来说是用序列标注的方式来做。这种方法要求我们标注数据，例如上图中我们标注了一句话：“达观数据是人工智能公司”。现在它会预测“上海的虚拟数据”中的“虚拟数据”也是一家公司。它是怎么做到的？后文会详细介绍。这种做法就跟模板匹配完全不一样了。在图中，可能第一个预测“虚拟数据是人工智能公司”还有模板的性质，但后面两个表述和前面完全不同，所以这种基于统计机器学习的方式有了一定的预测能力。</p><p>但问题是什么？它需要两个条件。<strong>首先是数据。大部分的机器学习都是监督学习，要做数据标注。</strong>而且我们传统机器学习经常要做特征工程。甚至在很多任务中，一个特征工程可能要占到我们项目时间和精力的90%。我们之前参加CIKM评测并拿到冠军的任务中，就耗费了大量时间构建特征。举个例子，我们实际工作中完成文本分类任务的时候，仅仅把文字的长度这个特征加进去，效果一下子提升了很多。这种特征我们很难想到。特征的选择可能有时候还有一定的逻辑推理，但有的时候就是拍脑袋。所以特征工程做好是很难的，需要很多的经验，还需要有扩散性的思维。</p><p><strong>此外训练和预测需要很多计算资源。</strong>某些机器学习（尤其是传统的机器学习）的训练过程中，特征有时候会特别耗费内存，可能不一定训练得完，所以对机器有一定的限制。当然，现在做深度学习，限制可能是GPU。深度学习相对于传统机器学习，对数据量地要求更高。因为传统的机器学习模型的各种参数没有深度学习这么多。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b6c5da0c3ce9424f9aeb5a6ad3fe573a><p class=pgc-img-caption></p></div><p>虽然深度学习的可解释性经常被人诟病，但也有些模型实际上可以给我们一些解释。尤其是一些基于Attention机制的模型。这里就是一个Attention分类器。图中可以看到它能从句子级别和词级别告诉你，对一个分类模型来说，哪句话最重要，哪个词最重要。这些词和句子都是有权重的。因为有Attention这样的权重，我们就能把它拿出来做可视化。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/098634eab8f1463c8d5ad5f57918bbb5><p class=pgc-img-caption></p></div><p>所以整体来说还是要通过序列标注来做。上图有一个序列标注的例子：分词。要分词的句子是“它来自达观数据”。我们有一个叫Label Set，也就是标签集。图中我们用的是BMES这个很经典的标签集，这个标签集其实对应的英文Begin、Middle、End、Single，大家一看就知道是什么意思。对于分词来说，每个字可能组成一个词（单字成词），也可能是一个词的开始、的中间或结尾。</p><p>上图还可以看到，在分词之外，命名实体我们用另外一个标签集。我们做词性分析可能用不同的标签集。可以看到，不同的标签集可以用来做不同的事情。所以无论是传统的机器学习，还是深度学习，我们都是在解决一个叫做“序列标注”的问题。所以标签集和标注方式都是基础的、几乎是一样的。有什么样不同？后文会具体讨论。</p><h1><strong>传统抽取算法介绍</strong></h1><p>其实传统抽取算法有很多，这里会介绍一些大家比较常用，也比较好理解的模型。<strong>第一个模型叫生成式模型。生成式模型的一个代表就是隐马尔科夫模型（HMM）。另外一个是判别式模型，代表是条件随机场（CRF）。</strong>这两个模型都结合了概率论还有图论的一些内容，也都基于统计机器学习的算法。它们都能根据训练集训练出不同的结果。下面我们详细介绍一下这两个模型。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cf6ca9b6480748368db86925a8930ad0><p class=pgc-img-caption></p></div><p>我人生第一次做序列标注任务的时候，用的就是HMM模型。马尔可夫这个名字一听就像是个数学很厉害的俄国人，但其实HMM模型并不难。大家只要记住两部分内容：两个序列、三个矩阵。如下图所示。我们要做的就就是把这五个部分定义好，整个模型和要解决的问题就定义清楚了。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/31fa8f6c2eff4ad39a9e3ac0de8a4075><p class=pgc-img-caption></p></div><p>首先是观察序列。上图中“他来自达观数据”，就是我们人看得到的观察序列，但它背后隐藏了分词。“他”是一个词，“来自”是一个词，“达观数据”是一个词，这个是我们说“隐藏序列”，没有写到明面上，但需要我们模型预测。怎么预测？下图画了预测模型的示意图。图中，X_1、X_2、X_3就是我们说的隐藏内容，人能看到的是y_1、y_2、y_3、y_4，也就是观察序列。但其实不同状态是可以不停地转换的。比如X_1到X_2之间有一条连线说明X_1和X_2之间可以通过概率a_12做转换；X_2到X_3之间通过概率a_23做转换。所以这个模型其实比链式的HMM还要更复杂一点，因为它有X_2到X_1这样的转换。所有的X都可以转换到y_1、y_2、y_3、y_4这样的观察序列，没对转换关系都有对应的概率。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d96f0ff1f26841f78fa325f9b5736f07><p class=pgc-img-caption></p></div><p>这样我们就把模型定义好了。我们只需要求模型的哪几个部分呢？主要是这三个矩阵：初始状态矩阵，发射状态矩阵，以及状态转移矩阵。</p><p>第一个是初始状态矩阵。我们现在举的例子都是有序列标注，例如多轮分词。下图是一个真实的多轮分词模型里面的图，这是我们自己训练的一个模型。可以看到，初始状态只可能是S(ingle)或B（egin），因为不可能从代表词结尾的标记开始一个句子。所以我们要从所有的语料中统计，单字词S和多字词B开始的概率是多少。仅仅统计这两个矩阵就可以，因为其他两个标记M（iddle）和E（en）是不可能出现在句首的。图中的概率有负数，是因为经过log和相关处理，从而可以方便后续的计算，但本质的含义还是概率。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/420faf8a2ab745c4af1d1aadce428702><p class=pgc-img-caption></p></div><p>第二个矩阵是发射状态矩阵。什么是发射状态矩阵？简单来说就是我们在分词里每个字变成任何一个标签的概率（如下图所示）。例如“他”这个字如果来自“他来自达观数据”这句话，就是一个单字词S（ingle）；但如果在“他”出现在“他们”等多字词里，标签就是B(egin)；在“关心你我他”里，“他”的标签可能就是E（end）。所以你会在训练语料看到“他”有不同的标签。发射状态矩阵就是把“他”到每一个标签的概率集合起来。发射状态矩阵非常重要，它说明了每一个字到不同标签的概率。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e98436fc80c449848a0b8b0608292a4d><p class=pgc-img-caption></p></div><p>第三个是状态转移矩阵。什么是状态转移矩阵？其实状态转移矩阵也是统计出来的，也就是刚才说的X_1和X_2之间的概率。我们训练语料里面已经有了SB、BMME这样的标签。其实我们可以观察到一些现象，例如S（ingle）后面不可能跟E（nd）和M（iddle）。这些就是状态转移矩阵描述的内容，如下图所示。它说明E后面跟着S的概率是多少，E后面跟着B的概率又是多少等等。这些值其实都是从语料库中训练出来的。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7261bebfe1ea412d81a883195c581280><p class=pgc-img-caption></p></div><p>下面讨论两类学习算法：一种是<strong>“监督学习”</strong>，通过极大似然估计就可以得到这些值，非常好算，简单地说就是统计次数：统计这个标签一共有多少，相关概率又是多少，就可以得出结果了。还有是一个非监督学习Baum-Welch，这个算法我们用得比较少，因为根据我们自己的经验，它的整体效果会比做统计差很多。而且监督学习有个好处是因为有了训练集和相关的数据，所以很容易去查错。</p><p>解码算法基本是用Viterbi来做。当然你也可以把当前最好的状态输出来，找到在当前序列下能够输出的最大标签，通过自己的一些解码逻辑（比如B后面一定是M或者E，不可能是S）优化一些内容。但我们经常还是用Viterbi去做整体的解码，取得最优路径的概率。Viterbi解码算法大家一定要掌握，因为后面有有不少算法与它类似。只要把Viterbi学会了，后面的很多东西就很好理解了。</p><p>HMM是我个人学的第一个模型，但是我现在基本上不用这个模型。为什么不用？因为它的效果还是相对差一点。但它也有优点。因为做极大似然估计就是简单的统计，速度非常快。所以这个模型的更新可以做到秒级。你做一个数据的修改，跑一遍立刻把数据统计出来，修改矩阵以后很快就对这个模型做一个更新。所以在项目的初始阶段，我们可以快速地用这个方法来做baseline或者动态的修改。尤其在实际业务中，可能客户做了一些修改后他需要实时知道反馈，这时候可以用HMM，虽然可能不能保证有好的效果。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a382ca13bacb4bf7b6da6523419b251d><p class=pgc-img-caption></p></div><p>在实际应用中我们用的最多还是<strong>条件随机场（CRF）</strong>。因为CRF往往效果更好。下图说明了HMM和CRF的关系是什么，我们可以看到一个HMM是链式传递，但加上一个条件就是我们最常见的链式条件随机场。通用CRF就是下图中右下角的图，但是我们做序列标注的话可能是最下面一行中间的这个图，也就是链式的CRF。它跟上面一行的图的区别是什么？大家可以看到下面一行图中有好多小的黑色正方形，这就是我们说的条件。我们是如何得出条件的？下面我们就来介绍一下如何通过真实训练得到条件。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7cfc4a6e4f264262b3c277fc55afa3a5><p class=pgc-img-caption></p></div><p>我们先看下面这张图。图中nz在词性里表示是一个“其他”类型的实体。这种类型很难归入时间、地点、人物等常见的实体类型，比如“苹果手机”可能就可以算是一个nz。我们把所有不太好分类的实体都归入到nz里。在这里，标签集还是BMES，但是加了一个“O”。标签后面的后缀其实就是类型。刚才提到的“其他”是nz，还可以有其他类型（如地名、时间、机构等）可以用其他字符串表示，比如nr、ns、nt。定义好这套标签集后，我们就开始定义特征函数。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/13dcf6f4e25a4f7bbe9010c12ab280fd><p class=pgc-img-caption></p></div><p>下图是我们是用CRF++、CRFPP做的特征模板。大家可以看到，图里有U00到U08，最后还有一个字母“B”，B说明它会学习标签间的转移。U00到U08都是特征，U00表示第一个特征，U01是第二个特征。此外还有一个x%，它代表了前面特征的内容。</p><p>首先看第一个特征：U00: %X[-3,0]。U00表示把我们要研究的字左边的第三个字作为特征，向量后一个数0表示我们没有添加人工特征。我们把这些拼接起来就是一个最终的特征。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c9cffd6a6cae45bbbfafcfd06519ada5><p class=pgc-img-caption></p></div><p>下图中包括了特征函数的权重（weight）。我们可以看到“U06：径”，这表示当前的字右边第三个字是一个“径”字。我们会给出每个标签的得分。可选的标签就是BEMOS。这里的数字代表得分（不是概率），有正有负。我们最终就是要把训练集所有的数据先通过这个特征模板变成一个特征。对于每个字，都有8个特征，第一个特征就是当前字左边的第三个字，第二个特征是左边第二个字，U03就是当前字本身。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e42f5135228a471f8f54ea7b1e75c713><p class=pgc-img-caption></p></div><p>所以大家可以看到CRF和HMM最大的不同。我们定义了这样一个特征函数（或者特征模板）。我们还可以人工设置一些特征影响特征模板。比如在研究当前字时，如果用了这样的模板，我就知道前三个字和后三个字会对当前这个字的标签的输出产生影响。除此之外，还可以用前一个字和当前字，或者当前字和后一个字的组合作为特征。有了这些特征，我们就要计算特征的结果。这时可以迭代训练模型，CRF使用了L-BFGS来训练。最终训练出来的模型可以告诉我们每个特征值对于不同的标签的值是多少，相当于是一个全局最优的值。</p><p>下面这张图代表了标签之间的转移，这跟HMM非常像，也可以算出来。所以CRF最终在一个全局最优的情况下达到了一个最优点。我们可以存储这个最优点情况下每一个特征的值，用来解码。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/276f2ed346cc4ca3a99ae16649b83488><p class=pgc-img-caption></p></div><p>CRF的解码较为简单，我们根据当前序列的位置，根据特征的模板生成很多特征函数，直接去查我们的模型，找到其对应的特征函数权重，之后每一个特征函数权重加起来。查到这个特征函数就把相应的权重取出来，加起来，没有查到就是0，就不用去做了，最终有一个得分，这样每一个标签都会有相关的得分。这个字生成的Score会有BEMOS相对应的，最终得到一个图，我们就用Viterbi解码，跟前面一样就能解出来了。</p><p>为什么CRF效果好？因为我们可以定义特征模板，包括了很多上下文比较远的特征。CRF的特征是人工选择的，可以选择前两个、前三个，甚至更多，所以可以让模型学到更多上下文，而且是远距离的上下文，辅助我们判断，提升整体效果。但条件随机场需要迭代优化，根据梯度下降的方向去找最优点，所以整体速度相对较慢，算出来的模型也不会小。很多时候必须要筛选或裁剪标签。</p><p>以上内容就是HMM和CRF这两个传统的算法。</p><h1><strong>基于深度学习的抽取算法</strong></h1><p>经典机器学习的很多算法需要比较强的数学功底，通过数学公式做出优美完整的论证。但现在经典机器学习算法的收益已经没有以前大了。原因如下图所示，图中列出了文本挖掘领域中，经典的机器学习和深度学习的对比。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/7b2e13adad3a4b5da492a69739b646f7><p class=pgc-img-caption></p></div><p>最大的区别就是紫色的框：特征工程。其实算法并不多，但特征工程五花八门，包括我们做文本处理时经常遇到的TF-IDF、互信息、信息增益、期望交叉熵等等。其实这些提取特征的方式都有一些科学依据，但很多场景下我们需要靠直觉。特征工程往往占到项目时间的90%。</p><p>而深度学习不在乎特征。模型定好之后只管输入，有了输入就能输出一个最好的结果。基本不用改代码的，只需要调参。如果数据小，还需要修改一下过拟合方面的东西就可以了。但是用经典机器学习做特征工程可能要改很多代码才能做出一个非常好的特征，这就是传统机器学习和深度学习最大的区别。</p><p>用深度学习做文本处理基本绕不开LSTM。虽然现在有很多模型，但也采用LSTM做baseline。下面是一篇著名的介绍LSTM的文章的截图，建议大家看一下原文。文章中最精华的就是下面四张图，展示了LSTM的工作原理。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e127d593c40b4e68bbb13f6bb66c3a25><p class=pgc-img-caption></p></div><p>第一个步骤是单元状态丢弃（如下图），图中有两个量x_t和h_t-1。x_t就是当前的输入，h_t-1是上一时刻的隐层的输出。这个公式求出来一个0-1之间的值，决定要留下多少东西。（任何东西乘以0-1其实就是计算要留多少东西，乘以0什么都留不了，乘以1就都留下，乘0.8就留80%。）</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4ed40ccab6b046b6a3b2d3ef7ed78560><p class=pgc-img-caption></p></div><p>第一步：单元状态丢弃</p><p>第二步是新信息的选择。当前输入包括上一时刻隐层的输出和当前的输入。这一步骤判断应该留下来多少内容。它还是计算两个系数，一个i_t，这也是一个0-1之间的值。第二个是C_t，表示当前cell的状态。计算完毕后需要把这两个系数的值保存下来。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ef45c96409ac41a5b8707c3f3793cdb7><p class=pgc-img-caption></p></div><p>第二步：新信息选择</p><p>第三步是更新状态。上面一步已经决定可以留下的新内容和老内容。这一步要决定如何组合新老内容。老内容可以乘以第一步计算出的f_t，新内容可以乘以第二步算出来的i_t，然后把新老内容相加，就是最新的状态了。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/19509524d8c649f5a2c6f1deaa2061bc><p class=pgc-img-caption></p></div><p>第三步：单元状态更新</p><p>第四步是得出最后的输出值。Cell不会一股脑输出，而是计算出了系数o_t和状态相关的函数结果相乘后得出输出。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5fd34666557642289b2bb673f19e0bde><p class=pgc-img-caption></p></div><p>第四步：确定输出</p><p>以上四步定义了LSTM基本的原理。LSTM其实提出来已经很多年了，在很多场景下都经受了考验。所以希望大家一定要把上面介绍的基础原理了解好。</p><p>下图显示了基于深度学习的信息抽取技术Bi-LSTM+CRF的原理。这个方法代表了深度学习和传统的机器学习一个很好的结合。传统CRF最大的问题是特征很稀疏，想做一个很好的特征要花费很多时间。我们可能会有几套比较经典的特征，但不一定保证效果最好，特别是训练数据发生变化以后。而词向量和Bi-LSTM可以做很多的特征提取工作。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4e35a770c9fa4dbd85bcb52c6016e958><p class=pgc-img-caption></p></div><p>为什么要用Bi-LSTM而不是简单的LSTM？举个例子，“华为发布了新一代的麒麟处理X”这句话中，“X”一看就是处理器的“器”。因为我们都知道前文“麒麟处理”后面肯定跟着“器”。类似地，根据“X鲜和美国签订了新一轮的谅解备忘录”很容易猜出X是“朝鲜”的“鲜”，这是根据后文做出的判断。天然的语言中存在前后文的信号，都会影响当前字的选择。Bi-LSTM可以兼顾前后文的影响，所以是从理论上来说是个很符合人类直觉的工具。</p><p>如果不用CRF，可能整体效果还不错，但会出现很多bad case。比如B后面出现S，S后面出现O。因为算法只考虑当前的最优输出，没有考虑整个序列的最优结果。而CRF是一个考虑全局的算法，也考虑到标签间的转移概率。所以用CRF会得到一个比较可控的结果。</p><p>总得来说，上图介绍的Bi-LSTM+CRF方法，结合了CRF和Bi-LSTM，把“小明去达观数据开会”这几个字变成向量，通过中间的Bi-LSTM隐层，提取出来高维的特征，输入CRF层，CRF最后就会给出标签和结果。</p><p>下面我们会介绍这篇文章最重要的部分：<strong>预训练模型</strong>。深度学习除了不用做大量的特征工程，还可以对文本做非常好的表示。这里的例子是用Word2Vec做出词向量，然后用TensorBoard可视化，如下图所示。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2fd1d42e4ce24bf099f9b8498115cfbd><p class=pgc-img-caption></p></div><p>在图中“威海”、“潍坊”、“枣庄”这三个山东的城市的词汇，被转化成了三个低维向量，向量中的数都是浮点数，有正数也有负数。如果从空间的角度来看这三个向量，可以发现它们距离很近，说明从语义角度来看它们的含义很接近。而且我们还可以直接对这些词向量进行计算，例如山东-威海=广东-佛山，皇帝-皇后+女人=男人，所以词向量是很优秀的自然语言的表征方式。</p><p>上图用的是Word2Vec模型。下图还有一些其他的模型，比如Glove。这两个模型都是静态表示。静态表示有天然的缺陷，例如它们很难区分“苹果好吃”和“苹果手机”中的两个“苹果”。就好像我们学技术的时候什么都想学，但因为时间是有限，所以每种技术学得都不够深入。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1ea3b14253ce4aaf84c5523f34272b11><p class=pgc-img-caption></p></div><p>所以从2018年开始，出现了很多新的预训练模型，不少模型都用《芝麻街》里怪物的名字命名，比如ELMO、BERT和ERNIE。除此之外还有微软的MASS，Google最新的XLNet等等。这些模型本质上都用深度学习的神经网络做表示，虽然有的用Attention，有的用Transform，但本质差别不大。</p><p>这些模型和Word2Vec/Glove最大的区别在于它们是动态模型。下图是一个真实的例子。输入“苹果好吃”和“苹果手机”后，用BERT对每个字建模，发现前两个字的向量很不一样。这说明BERT可以根据不同的上下文语境编码每个字，或者说可以根据上下文语境对同一个字做出不同的表示。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3adc620f8cac4bb092d2ac497c655070><p class=pgc-img-caption></p></div><p>BERT可以根据上下文，对同一个字做出不同的表示。</p><p>如何选择预训练模型呢？我建议大家可以都尝试一下。大部分同学都可以训练ELMO，它的结构和LSTM很像，我们可以自己训练一个语言模型。BERT训练的成本就要高很多，但现在已经有一些其他的框架或语言做处理。我们自己用中文维基百科训练BERT只用了几天，也没有用很多显卡，当然我们也做了不少优化工作。可以先试着用Word2Vec看看效果，有可能效果已经很不错。关键在于要找到在能力范围内按时训练完的模型。</p><h1><strong>抽取算法在达观的具体实践</strong></h1><p>下面我们分享一下在达观的实践中完成抽取任务的一些经验和教训。</p><p>首先我们要注重场景。应用场景一般就是客户提供的文档，包括财务报表、基金合同等等。文档处理的核心是自然语言处理，特别是抽取技术。我们也需要考虑实际应用，结合一些其他的工程技术，比如外部系统、分布式技术、数据库技术等等。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/806dc0cbf9f04f6a8be9f099111d3d7d><p class=pgc-img-caption></p></div><p>第二是要解决数据不足的问题。尤其是序列标注比文本分类需要更多的标注成本，所以很可能数据量不够。虽然目前有一些通用的数据（比如《人民日报》的数据），但针对具体的业务场景可能没有足够多的语料和标注数据。这时候我们就要做数据增强。数据增强是一种通用的方法，可以应用于传统的机器学习和深度学习中。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3925a8ac51174f97a08c27a08d10e968><p class=pgc-img-caption></p></div><p>在上图中，我们可以看到标注数据只有三句话，黄色表示要做机构识别。怎么增加标注数据的量？我们可以直接暴力地把它们两两随机组合。初听起来可能会觉得有点不可理喻，但确实有效果。上图中右边的三段话中，前两段是两两随机组合，最后一段是把三句话全部混合到一起。把这些新生成的数据加入原数据起去做模型，就会发现效果的确好了很多。数据增强为什么有效？从模型的角度简单地说，这样可以看到更多上下文，特别是可以跨句子看到上下文，所以会有帮助。基本上写5-10行代码就能产生一些收益。</p><p>还有一种方法是非监督的Embeddin的学习。下图是我们的一个真实的例子。当时登贝莱刚转会到巴塞罗那俱乐部。我们用标准语料去训练，发现“登贝莱”这个名字一定会被切开，无论怎么训练分词都不行。潜在的解决方法之一是增加很多登贝莱相关的标注数据，但是这么做收益不足。所以我们就找了很多外部的语料做嵌入。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b3fa8aee794a4a1f80e5be93285792fc><p class=pgc-img-caption></p></div><p>如上图所示，我们在网上找了一些登贝莱的新闻补充到《人民日报》等语料里一起训练。在完全没有修改，只是重新训练了预训练模型的情况下，“登贝莱”就成了一个词。这说明深度学习的预训练模型，可以非常好地捕捉到上下文，而且我们知道大部分的神经网络的语言模型训练是非监督学习，所以不需要很多标注数据。可以有很大数据量。总体来说数据越多，模型会学得越准，效果越好。BERT训练了一两千万的中文后，可以达到非常好的效果，我觉得这是个大力出奇迹的模型。</p><p>除了NER，还可以抽取别的内容。例如知识图谱就要做关系抽取。输入一句话，“美国总统特朗普将考察苹果公司，该公司由乔布斯创立”，怎么抽取关系？有两种方法。一种方式是把实体抽出来，然后两两实体做一些分类，分到一些关系里面。另一种依靠序列标注，也就是基于联合标注的方法。这么做的好处是不用修改标注框架。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b7b48965108a440f9b44ca5c596818fe><p class=pgc-img-caption></p></div><p>我们总结一下本文内容。在实际工作中，到底怎么来用深度学习挖掘文本？最重要的一点是要用预训练模型，通过非监督数据训练向量，提升泛化能力。虽然中间步骤难以分解，但因为深度学习有端到端的能力，所以对中间步骤要求较低。而且，深度学习能克服一些传统模型的缺点，例如LSTM的上下文依赖就比CRF强。</p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/fecea4b442a240599b38cd1a53f826bd><p class=pgc-img-caption></p></div><p>但是深度学习也有一些缺点，它在小数据集上的效果难以保证，很可能会过拟合或者难以收敛。例如大家看到TensorBoard经常在抖，就是有这样的问题。而且大家现在把深度学习调参的工作叫炼丹室，你也不知道好坏就在反复调。有时候调参的工作量不亚于特征工程，特征工程至少知道在做什么，而想分析调参结果更加困难。另外深度学习对计算资源的要求更高。</p><p>所以我们最终的思考是：第一要尽可能地收集数据、理解数据，这是所有做机器学习的同学第一步就应该做的事情。我们应该去分析数据、看数据，而不是一开始就上模型。如果不做数据清洗，好数据、乱数据、脏数据都在里面，模型是做不好的。就像教孩子一样，如果好的坏的都教，他就不知道什么是好坏了。而且我们要分析问题的本质，选择合适的模型。例如，对于已有数据的数据量，选先进模型有用吗？如果没有用，就要赶紧去收集数据。</p><p>而且在任务一开始的阶段，我比较推荐大家做传统的机器学习，因为这些模型比较现成，也比较通用。在做了一个非常好的baseline之后，你就知道底线在哪，然后再引用深度学习。去年的达观杯我们就发现很多参赛者一上来就在用深度学习，结果做了各种调参，效果还不如我们自己20行代码的传统的机器学习。所以刚开始的时候一定要让传统机器学习帮助你，这样你更有信心做后面的事情。另外，这句话一定要送给大家：“数据决定效果上限，模型逼近此上限”，所以大家一定要重视数据清理，数据的分析真的比调参调模型收益更大。</p><p>如果遇到疑难杂症，端到端技术经常会有惊喜，但不能保证每次都有惊喜。大家在学习的过程中一定要关心最前沿的技术。</p><p>做机器学习肯定会遇到失败和挫折，重要的是从挫折中总结规律才是最重要的，不要被同一个坑绊。这样的经验很难依靠别人教会，因为所处的环境、场景、场合、数据不可能完全一致，所以需要有自己的思考。</p><p>大家看完这篇文章可以用我们介绍的内容做一些实践。我们为大家提供了海量数据的练手题目，今年biendata平台上的“达观杯”题目，提交入口是一直开放的。今年的数据很有意思，<strong>文字经过了加密，每个字都做了一个随机的映射。这么做的好处是可以更多地关注算法的本身，而不用去想如何补充数据。</strong></p><p>数据有两部分，一部分是有标注的数据，另外一部分是一个规模达到上百万的非标注的数据。比赛的关键就是如何利用这些非标注的数据来提升整个模型的效果。而这就是我们最终在实际生活和工作中遇到的问题：只有少量标注数据，但是有大量的未标注数据。</p><p><strong>作者简介</strong></p><div class=pgc-img><img alt=信息抽取算法从入门到精通，一文了解自然语言处理的前世今生 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f0a9c299581d46cc9079fd6e93551f65><p class=pgc-img-caption></p></div><p>高翔，达观数据联合创始人，上海交通大学硕士，负责达观数据产品技术相关开发管理工作。曾任职于盛大文学、盛大创新院，在搜索引擎、自然语言处理、机器学习及前端技术有着丰富的经验。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'从入','门到','语言'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>