<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>hdfsæºç åˆ†æç¬¬ä¸€å¼¹ | æå®¢å¿«è¨Š</title><meta property="og:title" content="hdfsæºç åˆ†æç¬¬ä¸€å¼¹ - æå®¢å¿«è¨Š"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/cf716b17efaf49ae85d773a3f0ac1959"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b5561fb3.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b5561fb3.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/b5561fb3.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b5561fb3.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b5561fb3.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/b5561fb3.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/b5561fb3.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b5561fb3.html><meta property="article:published_time" content="2020-11-14T21:03:23+08:00"><meta property="article:modified_time" content="2020-11-14T21:03:23+08:00"><meta name=Keywords content><meta name=description content="hdfsæºç åˆ†æç¬¬ä¸€å¼¹"><meta name=author content="æå®¢å¿«è¨Š"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/b5561fb3.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>ğŸ¤“ æå®¢å¿«è®¯ Geek Bank</a></h1><p class=description>ä¸ºä½ å¸¦æ¥æœ€å…¨çš„ç§‘æŠ€çŸ¥è¯† ğŸ§¡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>çŒœä½ å–œæ­¡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=ç§‘æŠ€>ç§‘æŠ€</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=éŠæˆ²>éŠæˆ²</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=ç§‘å­¸>ç§‘å­¸</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>hdfsæºç åˆ†æç¬¬ä¸€å¼¹</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>ç§‘æŠ€</a></span></div><div class=post-content><div><p><strong>1. hdfså®šä¹‰</strong></p><pre>HDFS is the primary distributed storage used by Hadoop applications. A HDFS cluster primarily consists of a NameNode that manages the file system metadata and DataNodes that store the actual data.</pre><p><strong>2. hdfsæ¶æ„</strong></p><div class=pgc-img><img alt=hdfsæºç åˆ†æç¬¬ä¸€å¼¹ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cf716b17efaf49ae85d773a3f0ac1959><p class=pgc-img-caption></p></div><p><strong>3. hdfså®ä¾‹</strong></p><p>ä½œä¸ºæ–‡ä»¶ç³»ç»Ÿï¼Œæ–‡ä»¶çš„è¯»å†™æ‰æ˜¯æ ¸å¿ƒï¼š</p><pre>/** * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * "License"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */import java.io.File;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.Path;public class HadoopDFSFileReadWrite {  static void usage () { System.out.println("Usage : HadoopDFSFileReadWrite &lt;inputfile&gt; &lt;output file&gt;"); System.exit(1); }  static void printAndExit(String str) { System.err.println(str); System.exit(1); } public static void main (String[] argv) throws IOException { Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); if (argv.length != 2) usage();  // Hadoop DFS deals with Path Path inFile = new Path(argv[0]); Path outFile = new Path(argv[1]);  // Check if input/output are valid if (!fs.exists(inFile)) printAndExit("Input file not found"); if (!fs.isFile(inFile)) printAndExit("Input should be a file"); if (fs.exists(outFile)) printAndExit("Output already exists"); // Read from and write to new file FSDataInputStream in = fs.open(inFile); FSDataOutputStream out = fs.create(outFile); byte buffer[] = new byte[256]; try { int bytesRead = 0; while ((bytesRead = in.read(buffer)) &gt; 0) { out.write(buffer, 0, bytesRead); } } catch (IOException e) { System.out.println("Error while copying file"); } finally { in.close(); out.close(); } }}</pre><p>ä¸Šè¿°ç¤ºä¾‹ï¼Œå°†ä¸€ä¸ªæ–‡ä»¶çš„å†…å®¹å¤åˆ¶åˆ°å¦ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œå…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š</p><p>ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºä¸€ä¸ªæ–‡ä»¶ç³»ç»Ÿå®ä¾‹ï¼Œç»™è¯¥å®ä¾‹ä¼ é€’æ–°çš„é…ç½®ã€‚</p><pre>Configuration conf = new Configuration();FileSystem fs = FileSystem.get(conf);</pre><p>ç¬¬äºŒæ­¥ï¼šè·å–æ–‡ä»¶è·¯å¾„</p><pre>// Hadoop DFS deals with Path Path inFile = new Path(argv[0]); Path outFile = new Path(argv[1]);  // Check if input/output are valid if (!fs.exists(inFile)) printAndExit("Input file not found"); if (!fs.isFile(inFile)) printAndExit("Input should be a file"); if (fs.exists(outFile)) printAndExit("Output already exists");</pre><p>ç¬¬ä¸‰æ­¥ï¼šæ‰“å¼€æ–‡ä»¶è¾“å…¥è¾“å‡ºæµï¼Œå°†è¾“å…¥æµå†™åˆ°è¾“å‡ºæµä¸­ï¼š</p><pre> // Read from and write to new file FSDataInputStream in = fs.open(inFile); FSDataOutputStream out = fs.create(outFile); byte buffer[] = new byte[256]; try { int bytesRead = 0; while ((bytesRead = in.read(buffer)) &gt; 0) { out.write(buffer, 0, bytesRead); } } catch (IOException e) { System.out.println("Error while copying file"); } finally { in.close(); out.close(); }</pre><p>ä¸Šé¢æ–‡ä»¶è¯»å†™åŠŸèƒ½æ¶‰åŠåˆ°äº†æ–‡ä»¶ç³»ç»ŸFileSystemã€é…ç½®æ–‡ä»¶Configurationã€è¾“å…¥æµ/è¾“å‡ºæµFSDataInputStream/FSDataOutputStream</p><p>4. <strong>åŸºæœ¬æ¦‚å¿µåˆ†æ</strong></p><p>4.1 æ–‡ä»¶ç³»ç»Ÿ</p><p>æ–‡ä»¶ç³»ç»Ÿçš„å±‚æ¬¡ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼š</p><div class=pgc-img><img alt=hdfsæºç åˆ†æç¬¬ä¸€å¼¹ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/16bcaf79d7ea40dfa7d5ccb4c564f773><p class=pgc-img-caption></p></div><p>æ–‡ä»¶ç³»ç»Ÿæœ‰ä¸¤ä¸ªé‡è¦çš„åˆ†æ”¯ï¼Œä¸€ä¸ªæ˜¯åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼Œå¦ä¸€ä¸ªæ˜¯â€œæœ¬åœ°â€ï¼ˆæ˜ å°„åˆ°æœ¬åœ°è¿æ¥çš„ç£ç›˜ï¼‰æ–‡ä»¶ç³»ç»Ÿï¼Œæœ¬åœ°ç£ç›˜é€‚ç”¨äºæ¯”è¾ƒå°‘çš„hadoopå®ä¾‹å’Œæµ‹è¯•ã€‚ç»å¤§éƒ¨åˆ†æƒ…å†µä¸‹ä½¿ç”¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼Œhadoop åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿä½¿ç”¨å¤šä¸ªæœºå™¨çš„ç³»ç»Ÿï¼Œä½†å¯¹ç”¨æˆ·æ¥è¯´åªæœ‰ä¸€ä¸ªç£ç›˜ã€‚å®ƒçš„å®¹é”™æ€§å’Œå¤§å®¹é‡æ€§ä½¿å®ƒéå¸¸æœ‰ç”¨ã€‚</p><p>4.2 é…ç½®æ–‡ä»¶</p><p>é…ç½®æ–‡ä»¶çš„å±‚æ¬¡ç»“æ„å¦‚ä¸‹ï¼š</p><div class=pgc-img><img alt=hdfsæºç åˆ†æç¬¬ä¸€å¼¹ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/99a7174e973c40c2b474bc029224c93c><p class=pgc-img-caption></p></div><p>æˆ‘ä»¬å…³æ³¨çš„æ˜¯HdfsConfigurationï¼Œå…¶æ¶‰åŠåˆ°çš„é…ç½®æ–‡ä»¶æœ‰hdfs-default.xmlå’Œhdfs-site.xmlï¼š</p><pre> static { addDeprecatedKeys(); // adds the default resources Configuration.addDefaultResource("hdfs-default.xml"); Configuration.addDefaultResource("hdfs-site.xml"); }</pre><p>4.3 è¾“å…¥/è¾“å‡ºæµ</p><p>è¾“å…¥/è¾“å‡ºæµå’Œæ–‡ä»¶ç³»ç»Ÿç›¸å¯¹åº”ï¼Œå…ˆçœ‹ä¸€ä¸‹è¾“å…¥æµï¼š</p><div class=pgc-img><img alt=hdfsæºç åˆ†æç¬¬ä¸€å¼¹ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/6d24688e3e634053bd5f2cd8c7907b09><p class=pgc-img-caption></p></div><p>å…¶ä¸­ï¼ŒHdfsDataInputStreamæ˜¯FSDataInputStreamçš„å®ç°ï¼Œå…¶æ„é€ å‡½æ•°ä¸ºï¼š</p><pre> public HdfsDataInputStream(DFSInputStream in) throws IOException { super(in); }DFSInputStreamå±‚æ¬¡ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</pre><div class=pgc-img><img alt=hdfsæºç åˆ†æç¬¬ä¸€å¼¹ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b8fa8afd587f40c28d0db158d11fcfcb><p class=pgc-img-caption></p></div><p>åœ¨äº†è§£ä¸€ä¸‹è¾“å‡ºæµï¼š</p><div class=pgc-img><img alt=hdfsæºç åˆ†æç¬¬ä¸€å¼¹ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/50411e1fbaf849f68aa7faed88332f78><p class=pgc-img-caption></p></div><p>å…¶ä¸­ï¼Œé‡ç‚¹æ˜¯HdfsDataOutputStreamï¼Œå…¶æ„é€ å‡½æ•°ä¸ºï¼š</p><pre> public HdfsDataOutputStream(DFSOutputStream out, FileSystem.Statistics stats, long startPosition) throws IOException { super(out, stats, startPosition); }DFSOutputStream çš„å±‚æ¬¡ç»“æ„ä¸ºï¼š</pre><div class=pgc-img><img alt=hdfsæºç åˆ†æç¬¬ä¸€å¼¹ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cbd8229d0ea042a499c86aa10001f4a0><p class=pgc-img-caption></p></div><pre></pre><p>å‚è€ƒæ–‡çŒ®ï¼š</p><p>ã€1ã€‘http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html</p><p>ã€2ã€‘http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html</p><p>ã€3ã€‘http://wiki.apache.org/hadoop/HadoopDfsReadWriteExample</p><p>ã€4ã€‘http://blog.csdn.net/gaoxingnengjisuan/article/details/11177049</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'hdfs','æºç ','ç¬¬ä¸€'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=æœç´¢>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>ğŸ”</button></form></section><section class=widget><h3 class=widget-title>æœ€æ–°æ–‡ç«  âš¡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>å…¶ä»–</h3><ul class=widget-list><li><a href=TOS.html>ä½¿ç”¨æ¢æ¬¾</a></li><li><a href=CommentPolicy.html>ç•™è¨€æ”¿ç­–</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>è¯çµ¡æˆ‘å€‘</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>æå®¢å¿«è¨Š</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>