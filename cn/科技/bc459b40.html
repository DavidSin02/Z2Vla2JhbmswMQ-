<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>物体检测新方法 | 极客快訊</title><meta property="og:title" content="物体检测新方法 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/3986195e31c34412b8b0b0a8e9b78b6b"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/bc459b40.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/bc459b40.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/bc459b40.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/bc459b40.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/bc459b40.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/bc459b40.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/bc459b40.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/bc459b40.html><meta property="article:published_time" content="2020-10-29T21:11:33+08:00"><meta property="article:modified_time" content="2020-10-29T21:11:33+08:00"><meta name=Keywords content><meta name=description content="物体检测新方法"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/bc459b40.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>物体检测新方法</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><blockquote><p>作者：Libor Vanek</p><p>编译：ronghuaiyang</p></blockquote><h1 class=pgc-h-arrow-right>导读</h1><blockquote><p>CenterNet（Objects as Points）和TTFNet的简单介绍，以及基于TensorFlow2.2+的实现。</p></blockquote><div class=pgc-img><img alt=物体检测新方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3986195e31c34412b8b0b0a8e9b78b6b><p class=pgc-img-caption></p></div><p>首先，我将简要介绍一下不同的物体检测方法。在介绍了传统和新方法之后，你可以阅读有关CenterNet和TTFNet的最重要的部分。这两个模型中的许多想法是相似的，因此它们将被一起引入。我们实现了一个受这两个网络启发的包。如果你有兴趣，请访问我们的GitHub：https://github.com/ximilarcom/xcenternet。</p><h1 class=pgc-h-arrow-right><strong>传统方法</strong></h1><p>就像在计算机科学中经常发生的那样，当我们遇到一个没有解决方案的难题时，我们试图把它变成一个我们已知解决方案或更简单的问题。所谓的两阶段检测模型就是一个很好的例子。在这种情况下，更简单的问题是图像分类。(将给定的图像放入一个类别中或为给定的图像分配标签)简单地说，我们可以将图像划分为多个区域然后对它们进行分类，对吗？是的，但是要花很多时间。因此，你需要聪明一点。使用这种方法的算法的一个例子是R-CNN(2014)。之后发展为Fast R-CNN(2015)和Faster R-CNN(2016)。</p><p>虽然这些模型的性能相当好，但研究人员显然在问自己，这个过程是否可以变得简单，从而提高效率。在一个阶段中完成，不需要建议区域。一个可能的答案是YOLO — You Only Look Once(2015)。它现在在YOLOv4(2020)，或SSD - Single Shot multibox Detector(2015)。最后，RetinaNet(2017)也应该被提到，特别是因为它被用来引入focal loss用于物体检测，这在现在是非常常用的。</p><h1 class=pgc-h-arrow-right><strong>新方法</strong></h1><p>近年来，另一种观点越来越受欢迎。物体可以转换为一组点。检测任务可以看作是一个关键点估计问题。这种方法在CornerNet: Detecting Objects as Paired Keypoints中介绍。顾名思义，物体被表示为一对关键点，左上角和右下角。</p><p>类似的想法在Objects as Points一篇介绍CenterNet的文章中也有探讨。在这里，我们使用热图检测边界框的中心点。其他属性，如边界框的大小，直接使用回归来预测。</p><div class=pgc-img><img alt=物体检测新方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a26ee8c82fc34a9cb6ec03cc8afc3eb2><p class=pgc-img-caption></p></div><p>这种方法的缺点是训练速度较慢。为了解决这个问题，提出了TTFNet(Training-Time-Friendly Network)。它遵循相同的基本思想，因此我们决定在一个包中实现来自两个网络的思想。</p><h1 class=pgc-h-arrow-right><strong>神经网络结构</strong></h1><p>我们深入一下。下面我就从头开始，给大家介绍一下网络布局。然后，将讨论个别重要的部分，主要是热图和不同的损失函数。</p><p>我们可以使用一些专门为这类任务设计的网络。例如Hourglass network。或者，正如我们决定的那样，将标准图像分类CNNs中的一个进行修改，以满足我们的需要。我们选择ResNet(18, 50)和EfficientNet(b0, b1, b2)进行测试。</p><p>在所有的标准迁移学习任务中，我们丢弃了顶端的dense层。但是，最上面的那一层根本不符合我们需要的输出。因此，有必要进行上采样。除此之外，来自底层的连接还可以提高性能。当我们的网络末端有一个尺寸正确的层时，我们可以将它“分割”成所需的head。</p><div class=pgc-img><img alt=物体检测新方法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3f8a7cd2f8134432bbb11f34eefe2e8c><p class=pgc-img-caption>使用ResNet18的CenterNet的简单可视化，使用了上采样和拼接。(黄色:卷积层，红色:最大池化，蓝色:上采样)</p></div><p>为了使网络运行得更快，热图端仅为输入图像的1/4。每个类别都有一张热图。然后，在CenterNet(用于基本目标检测)和TTFNet中还有另外两个head。</p><p>对于<strong>CenterNet</strong>，有</p><ul><li>一个预测大小的head，包含包围框的<strong>宽和</strong>高</li><li>预测偏移的head，包含使用下采样热图时产生的中心的<strong>x和y偏移</strong>。</li></ul><p>两者都只有两个filters — 在热图的任何给定点上都只能有一个物体。如果你对一个物体的其他属性感兴趣，可以添加更多head。</p><p>在<strong>TTFNet</strong>中，只有一个附加的head带有<strong>四个</strong>filters来计算尺寸 — <strong>到物体两侧的距离</strong>。</p><h1 class=pgc-h-arrow-right><strong>热图</strong></h1><p>那么，热图是什么样的呢？它是一个填充了从0.0到1.0的值的矩阵。这张图上的峰值表示某个物体的存在。</p><p>下面，你会看到一些生成的训练热图。只有一个点正好是1.0。围绕着这一点，置信度在慢慢变小。</p><div class=pgc-img><img alt=物体检测新方法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3cd9385add5d453fb534d3d9d36249d9><p class=pgc-img-caption>CenterNet (左)和TTFNet (右)的热图</p></div><h1 class=pgc-h-arrow-right>可变形卷积</h1><p>网络的上采样部分可以通过多种方式实现。我们已经向你展示了使用拼接、上采样和标准卷积层的简单方法。为了提高性能，使用了可变形卷积。</p><p>毫无疑问，卷积神经网络给深度学习带来了一场伟大的革命。它们使我们能够提取出很难从全连接的层中得到的特征。再加上另一种布局上的改进，我们的网络将变得非常深。尽管如此，基本思想还是一样的。特别是滤波器的形状总是矩形的。Deformable Convolutions正在尝试对此进行改进。它们学习标准网格的偏移量，并使用这个“变形的”卷积核执行卷积。</p><div class=pgc-img><img alt=物体检测新方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/17f8fe38341c47dfa32f475bcd2c8bb0><p class=pgc-img-caption>标准卷积和可变形卷积</p></div><p>遗憾的是，在TensorFlow和TensorFlow Addons (TFA)中还没有实现可变形的卷积层。我们正在使用TFA的一个分支：https://github.com/smallsunsun1/addons/tree/feature/deformable_ops，支持可变形的Conv2d，并希望它能很快被合并。</p><h1 class=pgc-h-arrow-right><strong>损失函数</strong></h1><p>现在，我们有了网络布局和输出，只有一个关键的东西是缺失的。损失函数。</p><h1 class=pgc-h-arrow-right><strong>Focal Loss</strong></h1><p>热图有一个问题 —— 它们非常稀疏。大多数情况下没有检测(零)，只有非常偶然的情况下我们看到一个物体(1，周围的值递减)。标准度量方法在这种情况下并不能很好地工作。幸运的是，有一个解决方案 —— focal loss。它被用于CenteNet和TTFNet。</p><h1 class=pgc-h-arrow-right><strong>基于IoU的损失</strong></h1><p>为了优化包围框的大小，CenterNet使用L1损失。它是真实包围框座标和预测包围框座标之间差异的简单求和。它似乎是合理的，但另一方面，我们并不经常使用它来评估。我们使用IoU来度量。因此，当我们对改进这个指标感兴趣时，为什么不将它也用于优化呢？我们可以用1减去IoU值，将其变成损失。</p><p>不幸的是，如果没有交集，IoU是0。因此在这种情况下损失总是1。因此，又提出了两种基于IoU的损失函数。在TTFNet中使用的GIoU，解决了这个问题。DIoU还关注于在函数中添加距离信息，换句话说，就是我们离边框中心有多远。</p><h1 class=pgc-h-arrow-right><strong>最后</strong></h1><p>你可以在GitHub：https://github.com/Ximilar-com/xcenternet上找到我们的实现。</p><p><br>英文原文：https://towardsdatascience.com/new-approaches-to-object-detection-f5cbc925e00e</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'物体','检测','方法'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>