<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法 | 极客快訊</title><meta property="og:title" content="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/R6Ieh75DBRtmcY"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c29afbb.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c29afbb.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/c29afbb.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c29afbb.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c29afbb.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/c29afbb.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/c29afbb.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c29afbb.html><meta property="article:published_time" content="2020-10-29T20:59:02+08:00"><meta property="article:modified_time" content="2020-10-29T20:59:02+08:00"><meta name=Keywords content><meta name=description content="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/c29afbb.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>《测绘学报》</p><p><strong class=highlight-text toutiao-origin=span>构建与学术的桥梁 拉近与权威的距离</strong></p><p><strong class=highlight-text toutiao-origin=span>复制链接，关注《测绘学报》抖音！</strong></p><p><strong class=highlight-text toutiao-origin=p>【测绘学报的个人主页】长按复制此条消息，长按复制打开抖音查看TA的更多作品##7NsBSynuc88##[抖音口令]</strong></p><p><strong class=highlight-text toutiao-origin=span>本文内容来源于《测绘学报》2020年第8期，审图号GS（2020）4062号。</strong></p><p><strong toutiao-origin=span>遥感影像地物分类多注意力融和U型网络法</strong></p><p>李道纪<img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/R6Ieh75DBRtmcY>,郭海涛,卢俊<img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/R6W0QpMHySg0Qb>,赵传, 林雨准,余东行</p><p>信息工程大学地理空间信息学院, 河南 郑州 450001</p><p><strong>基金项目：</strong>国家自然科学基金（41601507）</p><p><strong>摘要</strong>：经典的卷积神经网络在对遥感影像进行地物分类的过程中，由于影像中的地物尺寸和光谱特征差异较大、待分类目标背景环境复杂等问题，经典影像分类方法很难得到理想的分类结果。针对这些问题，本文借鉴U型卷积神经网络多层次特征融和的思想，提出了多注意力融和U型网络（MAFU-Net）。该网络利用注意力模块提取和处理不同层次的语义信息，强化不同位置像素和不同特征图之间的相关性，进而提高网络在复杂背景条件下的分类性能。为了验证本文提出的网络在遥感影像地物分类中的效果，分别在ISPRS上的Vaihingen数据集以及北京、河南两地区高分二号数据集上进行了试验，并与目前主流的语义分割网络进行了对比。试验结果表明，相比其他网络，本文提出的MAFU-Net在不同特点的数据集上均可以得到最佳的地物分类结果。同时，该网络结构简单、计算复杂度低、参数量少，具有很强的实用性。另外，本文充分利用特征可视化手段进行MAFU-Net和其他网络的分类性能对比分析，试验结果表明，目前多数深度学习网络模型的深层次原理和作用机制较为复杂，无法准确解释特定网络为何在某种数据集中会失效。这需要研究人员进一步通过更加高级的可视化表达方法和量化准则来对特定深度学习模型及网络性能进行分析评价，进而对更加高级的模型结构进行设计。</p><p><strong>关键词：</strong>地物分类 遥感影像 注意力机制 U型卷积神经网络 语义分割</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RrAUsFz6Oa0Su9><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RrAUsGE2zceNSA><p>引文格式：李道纪, 郭海涛, 卢俊, 等. 遥感影像地物分类多注意力融和U型网络法. 测绘学报，2020，49(8)：1051-1064. DOI: 10.11947/j.AGCS.2020.20190407.</p><p><strong>阅读全文：</strong>http://xb.sinomaps.com/article/2020/1001-1595/2020-8-1051.htm</p><p><strong toutiao-origin=span>全文概述</strong></p><p>遥感影像地物分类技术作为遥感影像理解的基石，在土地数据更新、地物观测、变化检测等领域都有着十分重要的作用<sup>[1</sup><sup>]</sup>。不同于遥感影像场景分类与目标检测，遥感影像地物分类的目的是对影像中的每个像素点进行类别归属，即把影像中所包含的每个类别都准确地从原图中标记出来。</p><p>早期遥感影像中的地物分类结果主要依靠人工判读得到。然而，针对目前海量的遥感影像数据，完全依赖人工解译不切实际，一方面效率低、成本高，分类质量难以保证；另一方面还容易造成判读人员视力疲劳和损伤等问题。因此，为减少人力物力资源，同时提高遥感系统智能化水平，适应大数据时代的应用需求，遥感影像地物分类自动化逐渐成为遥感领域的重要课题。然而，遥感影像成像机理和背景环境复杂，在进行地物类别区分的过程中很容易发生混淆<sup>[2</sup><sup>]</sup>。目前，如何对遥感影像地物实现高精度的类别划分，始终是一个亟待解决的难题。</p><p>遥感影像地物分类的方法按照是否使用先验知识，可以分为两大类：非监督分类法与监督分类法。常用的非监督分类法有ISODATA分类法和k-means分类法等<sup>[3</sup><sup>]</sup>。该类方法不需要任何先验知识，主要依靠地物光谱或空间的统计特性进行分类，节省了人工标注的时间和精力。然而，该类方法所产生的集群组需要大量的分析及后处理，加上“同物异谱”与“异物同谱”的现象，很难与地物类别之间做到一一对应的关系。比较而言，监督分类法可以充分利用先验知识，预先确定类别，避免了非监督分类对光谱集群组的重新归类，因此常用于地物分类任务当中。监督分类方法有很多种，当前的研究主要集中于基于机器学习方法的地物分类，常用的有支持向量机<sup>[4</sup><sup>]</sup>、决策树<sup>[5</sup><sup>]</sup>和集成模型等分类方法。这些机器学习方法都需要人为预先创建特征，进而对特征之间的关系进行分析，寻找最佳分类阈值和参数。然而，创建特征往往需要大量的专业知识，不恰当的特征反而会降低分类精度。因此，机器学习中的深度学习技术，被逐渐应用到了各个领域。深度学习不需要人工设计特征，而是通过对训练样本的学习自动进行特征提取。近年来，深度学习技术在计算机视觉领域快速发展，加之卷积神经网络参数共享的特点，使得图像解译、分类、识别的精度及效率均得到了明显的提升<sup>[6</sup><sup>]</sup>。</p><p>在计算机视觉领域，类似地物分类这种逐像素分类任务又被称为语义分割<sup>[7</sup><sup>]</sup>。全卷积网络(fully convolutional network, FCN)在语义分割领域具有开创性的意义，FCN利用反卷积层(上采样)替换全连接层，大幅提高了分类效率<sup>[8</sup><sup>]</sup>，并在建筑物检测等方面得到了较多的应用<sup>[9</sup><sup>-10</sup><sup>]</sup>。在FCN之后，大量性能优异的语义分割网络被相继提出，典型的网络包括U-Net<sup>[11</sup><sup>]</sup>、SegNet<sup>[12</sup><sup>]</sup>和DeconvNet<sup>[13</sup><sup>]</sup>。其中，U-Net对高低级语义信息进行了融和，改善了物体边界语义细节的分类效果，提升了网络的分类性能。此后，很多语义分割网络均借鉴了U-Net的语义信息融和思想，如Refinenet<sup>[14</sup><sup>]</sup>、Encnet<sup>[15</sup><sup>]</sup>、Denseaspp<sup>[16</sup><sup>]</sup>和Deeplabv2<sup>[17</sup><sup>]</sup>等，这些网络通过对整体结构的精心设计，并采用多尺度特征融和策略，在公开的自然影像数据集<sup>[18</sup><sup>-20</sup><sup>]</sup>上取得了很好的分类效果。不同于自然影像，遥感影像涉及的场景范围更广，其中包含的地物往往不具有固定尺寸和特征，因此直接将一些高精度语义分割网络应用于遥感数据集，通常难以达到预期的精度和效果。</p><p>随着深度学习技术的研究和发展，相应的注意力模块被逐渐应用到语义分割网络当中。注意力模块借鉴了人类的注意力机制，即利用有限的注意力资源从大量信息中快速筛选出高价值信息。目前已有大量研究将注意力机制应用于U型网络当中，以最大程度发挥注意力特征处理性能和U-Net网络结构的优势。例如，Attention U-Net(Att-UNet)结构(arXiv preprint arXiv: 1804.03999, 2018)在U-Net网络高低级语义信息融和的过程中，加入了注意力控制模块(Attention Gates)，强化了有效信息的传递，并对无效信息的传输进行抑制。文献[21]在Att-UNet的基础上加入了多级监督策略，并重新对损失函数进行设计，显著提高了医学影像二分类精度。RAU-Net (arXiv preprint arXiv: 1811.01328, 2018)将残差注意力模块和U-Net结构进行结合，同时设计了二维和三维卷积网络，提高了医学影像肿瘤分割准确率。此外，很多学者设计了类似于Non-Local的注意力结构<sup>[22</sup><sup>]</sup>，该结构利用矩阵转置相乘的方法，达到大范围信息相关性计算的目的。例如，文献[23]提出的CCNet通过在像素周围十字交叉的区域内进行相关性计算，进而提取出强依赖性特征，减少了冗余信息。文献[24]为降低模型复杂度，提出一种期望最大化注意力模型，该模型通过期望最大化算法，极大程度地减少了计算量，同时提高了特征的稳健性。文献[25]提出新型双注意力分割网络DANet，该网络嵌入了两种特殊的自注意力特征融和结构，分别在空间维度和通道维度上捕获视觉特征的依赖关系，将所有位置的特征值进行加权，各个通道中的特征相互依赖。文献[26]提出的RAFCN结构同样是在通道维度和空间维度对特征进行处理，不同于DANet，该网络分别对3种不同层次的抽象信息进行注意力强化和反卷积，并最终对处理后的特征进行融和，输出类别。</p><p>基于上述分析，本文提出了一种用于遥感影像地物分类的多注意力融和U型网络(multilevel attention fusion U-Net, MAFU-Net)，并以不同分辨率的遥感数据集作为研究对象，将该网络与目前主流的语义分割网络进行了对比试验。本文的贡献主要有3点：①本文将注意力机制应用于不同层次语义信息当中，充分发挥注意力机制的优势，在不显著增加参数量的情况下，提高地物分类精度；②将注意力机制内部的原理进行了剖析，揭示了注意力机制内部相关性计算的本质和作用；③提出了U型网络语义融和的模式，利用上采样中多层次语义信息进行综合评价，以加强网络在地物细节方面的分类精度，进一步提升性能。</p><p><strong toutiao-origin=span>1 理论背景与方法</strong></p><p><strong class=highlight-text toutiao-origin=span>1.1 双注意力模块</strong></p><p>注意力模块本质上是通过矩阵转置相乘进行相关性计算，增大依赖性较强的特征权重，降低噪声干扰，提高对有效信息的利用率。为增强网络的特征处理和场景理解能力，需要在局部特征表示上建立丰富的上下文关系模型，并加入全局相关性特征。为此本文引入位置注意模块(position attention module, PAM)<sup>[25</sup><sup>]</sup>。PAM的结构如图 1所示。</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/SBUy93HFKMmABG><p>图 1 PAM结构 Fig. 1 The illustration of the basis PAM structure</p><p>图选项</p><p>图 1中，<em>A</em>为输入特征图，<em>A</em>∈<em>R</em><em>C</em>×<em>H</em>×<em>W</em>。通过对特征图<em>A</em>进行卷积，得到特征图<em>B</em>、<em>C</em>和<em>D</em>，{<em>B</em>,<em>C</em>,<em>D</em>}∈<em>R</em><em>C</em>×<em>H</em>×<em>W</em>。将特征图<em>B</em>和<em>C</em>进行维度重排，并在像素维度上进行转置相乘，得到空间注意力特征图像<em>S</em>，尺寸为(<em>H</em>×<em>W</em>)×(<em>H</em>×<em>W</em>)。具体操作如式(1)</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUy9Pp2UXkX2E><p>(1)</p><p>式中，s<sub>ji</sub>∈<em>S</em>，为第<em>i</em>个像素和第<em>j</em>个像素之间的相关性，二者特征越相似，则转置相乘得到的相关性越大，在注意力特征图像上具有更高的特征值。</p><p>之后，将注意力特征图<em>S</em>与特征图<em>D</em>进行矩阵相乘，并与特征图<em>A</em>进行加和操作，得到空间注意力特征图E<sub>p</sub>。具体操作如式(2)</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUy9QBGAxdo2T><p>(2)</p><p>式中，E<sub>j</sub>∈E<sub>p</sub>；<em>α</em>为可训练的权重系数。</p><p>由式(2)可知，空间注意力特征图E<sub>p</sub>中每个特征值均为所有位置特征值的加权与原始特征值之和，因此，E<sub>p</sub>中既包含局部语义特征信息，也包含全局语义特征信息。这种加权策略在经过权重系数调整后，可以减少多余特征的干扰，降低噪声，加快对重要信息的提取和训练。</p><p>此外，特征图中每个通道特征都可以看作是网络模型对特定语义信息的响应，不同的语义信息之间相互关联。通过建立通道映射之间的相互依赖关系，可以增强特定语义信息的表达。据此，本文在所提出的网络中引入通道注意力模块(channel attention module, CAM)，其结构如图 2所示。CAM的注意力特征图X通过在通道维度上转置相乘得到，其余结构与PAM相同。</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUy9QQJJ0iYj1><p>图 2 CAM结构 Fig. 2 The illustration of the basis CAM structure</p><p>图选项</p><p>在对特征图进行处理的过程中，PAM和CAM并行操作，并将所得到的空间注意力特征图E<sub>p</sub>和通道注意力特征图E<sub>c</sub>进行加和运算，得到最终的注意力特征图。具体操作如图 3所示。为了降低特征维度减少计算量，首先对输入特征图进行降维卷积操作，并将降维后的特征图并行输入到两个注意力模块和卷积层中进行注意力机制处理和信息整合，最终将得到的特征图进行加和融和并输出。</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUy9QnEx7dOI0><p>图 3 PAM及CAM并行特征处理 Fig. 3 The parallel feature maps convolution processing in PAM and CAM model</p><p>图选项</p><p><strong class=highlight-text toutiao-origin=span>1.2 MAFU-Net原理及网络结构</strong></p><p>语义分割网络一般是通过多组卷积层对影像进行特征提取，进而在深层次特征图上对类别进行划分，而不同层次的语义特征往往会有很大差别。图 4(a)为原始光谱影像及地物类别标签，图 4(b)、(c)分别为浅层语义特征及深层语义特征的可视化。可以发现，图 4(b)中的每张特征图均保留了原始影像的细节信息，并且所呈现的特征皆不相同；图 4(c)中的每张特征图所呈现的信息更加抽象，几乎无法从中理解原图像的含义。对比分析可知，浅层语义特征保留了地物在细节方面的信息，而深层语义特征则更多地捕获了地物位置及类别语义信息。因此，将不同层次的特征图进行融和，可以使得特征互补，进而改善分类精度。</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUy9RCFvukSB6><p>图 4 U型网络的深层特征与浅层语义特征可视化对比 Fig. 4 The visualization of the deep features and shallow features in U-shape network</p><p>图选项</p><p>基于上述讨论，本文提出的MAFU-Net整体结构如图 5所示，主要分为两部分：U型卷积网络和注意力特征提取网络。图 5左侧的U型卷积网络主要用于深层次特征的提取和上采样，是MAFU-Net的主要网络骨架。为了最大限度地保留编码网络在降采样过程中的重要特征和地物细节信息，不同层次的特征图之间通过跳跃结构<sup>[11</sup><sup>]</sup>(skip connection)进行连接，并在通道维度上进行叠合操作。在编码路径中，每两个3×3卷积层后接一个2×2的最大池化层，并且每个卷积层之后都进行批量归一化和线性整流函数(RELU)激活操作。除此之外，每一次降采样，特征图的通道数会增加一倍。在解码路径中，选用双线性函数对特征图进行上采样，每两个3×3卷积层后接一个2×2的双线性上采样层，每一步的上采样都会加入来自相对应编码路径的特征图。</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUyAGWBObRQKo><p>图 5 嵌入双注意力模块的U型网络结构 Fig. 5 The structure of the U-Net with attention module</p><p>图选项</p><p>图 5右侧为注意力特征提取网络，目的是对高低层次叠合得到的特征图进行注意力机制处理和融合。由于卷积层的随机初始化，所有特征图包含的信息是等价的，需要在学习的过程中不断对卷积层参数和权重进行调整。然而，不同层次的语义信息差别较大，融合得到的特征图并不利于卷积层的学习。如果选择注意力模块来对特征图进行处理，将会使得网络对特征的学习有所侧重，提高网络的学习能力。卷积层与注意力模块的作用机制如图 6所示，卷积层是对所有特征进行提取，包括冗余特征，如图 6(a)所示；而注意力模块则是有选择地对某些相关性较大的特征进行提取，如图 6(b)所示。</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyAH26LmIc2W><p>图 6 卷积层与注意力模块的作用机制 Fig. 6 The mechanism diagram of convolutional layer and attention module</p><p>图选项</p><p>MAFU-Net注意力特征提取网络的具体操作如式(3)</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyAHPDBBoV31><p>(3)</p><p>式中，<em>U</em><sub>1</sub>、<em>U</em><sub>2</sub>和<em>U</em><sub>3</sub>为U型网络上采样叠合后的3组深层次特征图，其中包含了不同层次的语义信息，每组特征图的尺寸和通道维度各不相同；<em>C</em><em>x</em>,<em>y</em>(·)代表卷积操作；<em>x</em>代表卷积核尺寸；<em>y</em>代表卷积操作的输出通道数；<em>A</em>(·)代表双注意力模块操作；<em>S</em>(·)代表上采样操作；<em>F</em>为3个层次语义信息融合后得到的特征图，<em>F</em>∈<em>R</em><em>N</em>×<em>H</em>×<em>W</em>，<em>N</em>为地物类别数。</p><p>在得到特征图<em>F</em>之后，对其进行softmax函数激活，输出最终类别概率图<em>P</em>，具体操作如式(4)</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyAHfDiyShC8><p>(4)</p><p>式中，f<sub>ij</sub>∈<em>F</em>；p<sub>ij</sub>∈<em>P</em>。</p><p>考虑到第1次上采样得到的特征图尺寸小、语义特征抽象、通道维度高，不适于进行特征相关性检测，故MAFU-Net没有对其进行处理。此外，适当减少特征图的融合可以降低网络复杂度和计算内存的占用。</p><p><strong toutiao-origin=span>2 数据集及训练细节</strong></p><p><strong class=highlight-text toutiao-origin=span>2.1 数据集</strong></p><p>为了对MAFU-Net进行测试，试验选取了3组遥感影像数据集，分别是ISPRS的Vaihingen航空影像数据、北京和河南地区高分二号数据集<sup>[27</sup><sup>]</sup>，如图 7所示。</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUyAJ5A7h9jXT><p>图 7 Vaihingen，北京及河南地区高分二号数据集 Fig. 7 The Vaihingen datasets, Beijing and Henan datasets of GF 2</p><p>图选项</p><p>ISPRS航空影像数据集包含16张已标记的假彩色航空影像(即包含近红外、红、绿3个通道)，影像尺寸介于1388×1281像素到2995×3007像素之间，地面分辨率均为0.09 m。每张影像均分为6类地物，即地面、高大植被、建筑物、车辆、低矮植被和杂类，杂类中主要包含水域和集装箱等。经过统计，该数据集中各地物像素占比差别较大，例如水域等杂类的像素数量仅为地面像素数量的1/38。同时，各地物的尺寸也不尽相同，车辆类似于点状地物，植被、建筑物和地面等则接近面状地物。</p><p>北京和河南地区遥感影像为红、绿、蓝3通道的真彩色影像，地面分辨率为4 m，影像尺寸均为6800×7200像素。河南地区数据集共包含4类地物，分别为水域、居民地、植被和背景区域；北京地区数据集在这4类地物基础上额外标注了土路和公路共计6类地物，由于道路在影像中宽度仅为2~3个像素，近似为线状，因此在地物分类当中检测难度较高。</p><p>各数据集训练、验证和测试的比例分配及具体的地物类别见表 1。</p><p>表 1 3组数据集详情Tab. 1 The details of the three datasets used in performance testing</p><table><thead><tr><td>数据集</td><td>ISPRS</td><td>北京</td><td>河南</td></tr></thead><tbody><tr><td>空间分辨率/m</td><td>0.09</td><td>4</td><td>4</td></tr><tr><td>地物类别</td><td>地面、杂类(水域)、高大植被、建筑物、汽车、低矮植被</td><td>背景、植被、水域、居民地、公路、土路</td><td>背景、植被、水域、居民地</td></tr><tr><td>类别占比</td><td>38:1:32:33:1.6:26</td><td>54:35:10:10:1:1</td><td>22:27:1:8.7</td></tr><tr><td>训练、验证和<br>测试集分配</td><td>1.5:1:1.3</td><td>7:1:2</td><td>7:1:2</td></tr></tbody></table><p>表选项</p><p><strong class=highlight-text toutiao-origin=span>2.2 训练方法及环境配置</strong></p><p>为扩充训练样本，本文对Vaihingen数据集、北京和河南遥感数据集进行随机旋转，加噪和<em>γ</em>变换<sup>[28</sup><sup>]</sup>等图像增强操作，分别生成了13 000、7000和6000张训练样本。此外，对验证集进行随机裁切分别生成5000、1300和2000张验证样本。鉴于设备条件限制，输入样本尺寸被设定为128×128像素。在将样本输入到网络之前，对其进行了归一化处理，以加快收敛和优化的进程。</p><p>在神经网络训练的过程中，损失函数直接决定着最终的训练效果。多分类交叉熵损失函数在图像语义分割任务当中经常被采用，然而该损失函数在样本类别比例偏差较大的情况下，无法对小比例样本进行兼顾。实际上，在图像多分类任务当中，采用交并比(intersection-over-union，IoU)和平均交并比(mean intersection-over-union，mIoU)进行效果评价是一种更为合适的选择。交并比反映的是预测值与真值二者的契合程度。对于<em>n</em>类地物，可以得到IoU<sub>1</sub>，IoU<sub>2</sub>，…，IoU<sub>n</sub>。之后，对其进行加和平均，求出平均交并比mIoU。</p><p>本文选择Lovasz Softmax损失函数<sup>[29</sup><sup>]</sup>间接对IoU进行优化，IoU评价指标计算如式(5)</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyB8CIXq7vel><p>(5)</p><p>式中，<img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/SBUyB8eHd8pdTg><em>y</em><sup>*</sup>为标签值；<em>c</em>为对应类别的编码集合；∩为取交集操作；∪为取并集操作。定义Jaccard损失为</p><p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyB8t4JG2IfV>(6)</p><p>本文采用的Lovasz Softmax损失函数计算如式(7)</p><p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyB97AoKr0JH>(7)</p><p>式中，Jaccard损失函数为子模函数，<em>△</em>J<sub>c</sub>相当于把原子模函数的输出值作为基进行插值；<em>m</em>(<em>c</em>)函数性质如下</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyB9H5yctEKD><p>(8)</p><p>试验中，初始学习率<em>lr</em><sub>base</sub>设为0.000 5，设定学习率变化如下</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUyBtGBO3AmoW><p>(9)</p><p>式中，epoch为当前迭代次数；EPOCH为总迭代次数；试验设置EPOCH=150，power=0.9。</p><p>神经网络的训练不仅需要对损失函数进行设计，还需要对超参数和优化算法进行选择。经过试验探索，本文采用Adam优化器(arXiv preprint arXiv: 1412.6980, 2014)对网络进行训练，对应的优化超参数为<em>β</em><sub>1</sub>=0.9，<em>β</em><sub>1</sub>=0.999，<em>ε</em>=10<sup>-8</sup>，批处理大小(batch size)为8。试验中采用Windows下Pytorch机器学习框架，硬件环境为CPUInter(R)Xeon(R)E2176G，GPU RTX2080Ti，11 GB显存。</p><p><strong toutiao-origin=span>3 试验及结果分析</strong></p><p>为了验证MAFU-Net的有效性，将其与Refinenet<sup>[14</sup><sup>]</sup>、Encnet<sup>[15</sup><sup>]</sup>、Denseaspp<sup>[16</sup><sup>]</sup>、Atlention-Unet、RAV-Net和DANet<sup>[27</sup><sup>]</sup>进行对比，其中文献[27]中的DANet选用了两种结构进行对照，即单一损失结构和辅助损失结构，分别以DANet和DANet<sup>*</sup>表示。此外，本文加入了消融试验用来探究PAM模块和CAM模块对MAFU-Net的分类结果影响。</p><p><strong class=highlight-text toutiao-origin=span>3.1 数据集测试及试验结果定性分析</strong></p><p>本文提出的MAFU-Net与上述文献中的网络在3组数据集上的预测结果及细节如图 8所示。为了更加全面地比较各种方法的分类结果，从Vaihingen数据集中分别选出两个典型场景进行分析，分别记为场景1~2。其中，场景1为包含水域的影像，用来探究各网络在地物类别不均衡条件下的分类性能；场景2包含大型建筑物和车辆，用来检测网络对大尺寸地物的区分能力以及对小尺寸地物的细节分割能力，每种场景影像、标签及各方法预测结果如图 8(a)和(b)列所示。</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyBtlO3B7Ks><p>图 8 8种网络在Vaihingen及北京、河南地区高分二号数据集上的结果对比 Fig. 8 The predicted results of the eight networks on Vaihingen, Beijing and Henan datasets</p><p>图选项</p><p>对于场景1，DANet、Denseaspp、Encnet和Att-UNet 4种方法均未检测出水域部分，而DANet<sup>*</sup>、Refinenet、RAU-Net和MAFU-Net则成功检测出水域。这说明MAFU-Net使用的注意力模块和Refinenet、RAU-Net所使用的残差单元缓解了过拟合，提高了地物类别检测的稳健性。对于场景2，Encnet和Denseaspp在大型建筑物上的检测效果明显优于MAFU-Net，这主要是因为Encnet和Denseaspp中使用了扩张卷积和多尺度策略，这种卷积操作可以明显增大感受野，同时多尺度策略可以提高不同尺寸地物的检测精度。MAFU-Net仅仅利用网络输出的卷积特征图进行操作，没有对感受野范围进行扩充，因此对于大型地物的检测在完整性上有所欠缺。但是对于小尺寸或难区分地物的检测，MAFU-Net具有明显的优势。例如场景2中的圆圈区域，有两辆汽车，由于光线问题，一辆汽车被阴影遮挡，很难进行分辨，其余方法均未成功检测。但MAFU-Net则较为完整地将该车辆预测并标记，说明注意力模块可以很好地剔除冗余信息、抵抗干扰，通过强相关性找到有价值的地物特征线索。图 9(a)为该幅影像的局部放大示意图，通过亮度调整和对比度拉伸，可以明显看到该处有车辆，若是单纯通过人眼进行辨别，则很容易将该车辆漏检。值得一提的是，DANet<sup>*</sup>在两个场景中的检测效果均明显优于DANet，这主要是由于DANet<sup>*</sup>中的辅助监督策略强化了细节方面的特征，在一定程度上维持了DANet下采样所丢失的信息。</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUyBvN6AR8Og3><p>图 9 场景2和场景5中的局部放大细节示意 Fig. 9 The local magnification details in scene 2 and scene 5</p><p>图选项</p><p>图 8(c)—(d)为北京数据集中选取的两幅场景及对应检测结果，分别记为场景3、场景4。从场景3可以发现，RAU-Net、Att-UNet和MAFU-Net在道路的检测上效果明显优于其他方法，这说明以U-Net结构作为基础骨架，可以有效地检测线状地物，这也证实了高低级语义信息融和在细节特征提取中的重要性。此外，该场景中方形框选区域中的居民地，大部分方法均有误检，而MAFU-Net检测结果则相对准确。场景4中的背景环境和纹理特征相对复杂，给水域和公路的检测带来很大难度。例如，Encnet对于该场景中水域区域有很大程度的漏检，DANet<sup>*</sup>对于横跨水域的公路也检测失效。Refinenet网络在该数据集上的检测效果最差。相比之下，MAFU-Net对于公路和水域的检测效果较为稳健，提取的分类图较为完整清晰。</p><p>对于河南数据集，由于影像中地物细节少、纹理特征相对单一、地物类别划分简单，各方法在该数据集上的分类精度要明显高于前两组数据集。对于水域部分，几种方法得到的结果差别并不明显；对于植被部分，由于其分布范围广，很难在影像中进行客观的评价。因此，本文从中选择了一幅包含居民区的场景进行分析，记为场景5，相应的影像、标签及分类结果如图 8(e)所示。</p><p>对于场景5，参考文献中的方法均将圆圈中的背景区域误检为居民区，而MAFU-Net则可以准确地将该区域预测为背景，其放大细节示意图如图 9(b)所示。从放大细节中会发现，该部分地物与其他居民区差别较大，经过实地勘察，该区域为一家奶牛场，属于工业用地，并不属于居民区范畴。</p><p><strong class=highlight-text toutiao-origin=span>3.2 试验结果定量分析</strong></p><p>为了对各分割结果进行定量评价，本文选用交并比作为类别评价指标，选择平均交并比作为总体评价指标。3组数据集上的评价结果分别见表 2、表 3和表 4。</p><p>表 2 不同方法在Vaihingen数据集上的交并比和平均交并比Tab. 2 The experimental results of IoU and mIoU on Vaihingen dataset in different method</p><table><thead><tr><td>方法</td><td>地面</td><td>高大植被</td><td>建筑物</td><td>车辆</td><td>低矮植被</td><td>水域等杂类</td><td>mIoU</td></tr></thead><tbody><tr><td>DANet</td><td>71.61</td><td>71.67</td><td>80.34</td><td>—</td><td>58.15</td><td>—</td><td>46.96</td></tr><tr><td>DANet<sup>*</sup></td><td>74.20</td><td>71.80</td><td>80.39</td><td>53.92</td><td>59.62</td><td>35.89</td><td>62.62</td></tr><tr><td>Denseaspp</td><td>76.07</td><td>71.97</td><td><strong>83.66</strong></td><td>58.37</td><td>59.64</td><td>—</td><td>58.28</td></tr><tr><td>Encnet</td><td><strong>76.48</strong></td><td><strong>72.60</strong></td><td><strong toutiao-origin=span>83.07</strong></td><td><strong toutiao-origin=span>64.96</strong></td><td>59.88</td><td>—</td><td>59.50</td></tr><tr><td>Refinenet</td><td>74.96</td><td>71.31</td><td>80.70</td><td>51.21</td><td>59.52</td><td><strong>43.41</strong></td><td><strong toutiao-origin=span>63.52</strong></td></tr><tr><td>RAU-Net</td><td>75.10</td><td>71.87</td><td>82.01</td><td>50.20</td><td>58.53</td><td><strong toutiao-origin=span>39.45</strong></td><td>62.86</td></tr><tr><td>Att-UNet</td><td>73.64</td><td>71.67</td><td>80.66</td><td>49.46</td><td><strong toutiao-origin=span>60.54</strong></td><td>—</td><td>55.99</td></tr><tr><td>MAFU-Net</td><td><strong toutiao-origin=span>76.14</strong></td><td><strong toutiao-origin=span>72.17</strong></td><td>82.20</td><td><strong>65.46</strong></td><td><strong>60.58</strong></td><td>30.19</td><td><strong>64.46</strong></td></tr></tbody><tfoot><tr><td colspan=8>注：加粗字体为每列最优值，加下划线字体为每列次优值，—代表预测失效。</td></tr></tfoot></table><p>表选项</p><p>表 3 不同方法在北京数据集上的交并比和平均交并比Tab. 3 The experimental results of IoU and mIoU on Vaihingen dataset in different method</p><table><thead><tr><td>方法</td><td>植被</td><td>水域</td><td>居民地</td><td>公路</td><td>小路</td><td>背景</td><td>mIoU</td></tr></thead><tbody><tr><td>DANet</td><td>67.46</td><td>88.33</td><td>71.11</td><td>22.75</td><td>35.89</td><td>58.90</td><td>57.41</td></tr><tr><td>DANet<sup>*</sup></td><td><strong toutiao-origin=span>71.58</strong></td><td>89.02</td><td><strong toutiao-origin=span>73.29</strong></td><td>23.31</td><td>35.87</td><td>59.77</td><td>58.80</td></tr><tr><td>Denseaspp</td><td>63.58</td><td><strong toutiao-origin=span>90.45</strong></td><td>70.18</td><td>28.23</td><td>45.13</td><td>56.48</td><td>59.01</td></tr><tr><td>Encnet</td><td><strong>73.14</strong></td><td>89.15</td><td>73.56</td><td>27.38</td><td>44.00</td><td><strong toutiao-origin=span>62.21</strong></td><td><strong toutiao-origin=span>61.57</strong></td></tr><tr><td>Refinenet</td><td>66.53</td><td>86.28</td><td>65.39</td><td>9.58</td><td>8.64</td><td>48.98</td><td>47.58</td></tr><tr><td>RAU-Net</td><td>62.22</td><td>88.30</td><td>52.51</td><td>32.75</td><td>42.03</td><td>51.29</td><td>54.85</td></tr><tr><td>Att-UNet</td><td>55.65</td><td>79.08</td><td>73.02</td><td><strong toutiao-origin=span>47.70</strong></td><td><strong toutiao-origin=span>49.54</strong></td><td>54.14</td><td>59.01</td></tr><tr><td>MAFU-Net</td><td>71.35</td><td><strong>91.20</strong></td><td><strong>76.00</strong></td><td><strong>47.91</strong></td><td><strong>50.36</strong></td><td><strong>62.39</strong></td><td><strong>66.53</strong></td></tr></tbody><tfoot><tr><td colspan=8>注：加粗字体为每列最优值，加下划线字体为每列次优值。</td></tr></tfoot></table><p>表选项</p><p>表 4 不同方法在河南数据集上的交并比和平均交并比Tab. 4 The experimental results of IoU and mIoU on Henan dataset of GF2 in different method</p><table><thead><tr><td>方法</td><td>植被</td><td>水域</td><td>居民区</td><td>背景</td><td>mIoU</td></tr></thead><tbody><tr><td>DANet</td><td>82.91</td><td>89.79</td><td>68.45</td><td>52.86</td><td>73.50</td></tr><tr><td>DANet<sup>*</sup></td><td>82.55</td><td>89.38</td><td>68.73</td><td>51.32</td><td>72.99</td></tr><tr><td>Denseaspp</td><td>84.92</td><td><strong>91.90</strong></td><td>69.02</td><td>56.83</td><td>75.67</td></tr><tr><td>Encnet</td><td>84.54</td><td>91.69</td><td><strong>71.00</strong></td><td>58.16</td><td>76.35</td></tr><tr><td>Refinenet</td><td>82.39</td><td>90.26</td><td>68.27</td><td>53.34</td><td>73.57</td></tr><tr><td>RAU-Net</td><td>83.51</td><td>90.48</td><td>68.93</td><td>52.28</td><td>73.80</td></tr><tr><td>Att-UNet</td><td><strong>86.88</strong></td><td>90.37</td><td>70.21</td><td><strong>62.54</strong></td><td><strong toutiao-origin=span>77.45</strong></td></tr><tr><td>MAFU-Net</td><td><strong toutiao-origin=span>86.67</strong></td><td><strong toutiao-origin=span>91.74</strong></td><td><strong toutiao-origin=span>70.72</strong></td><td><strong toutiao-origin=span>60.98</strong></td><td>77.53</td></tr></tbody><tfoot><tr><td colspan=6>注：加粗字体为每列最优值，加下划线字体为每列次优值。</td></tr></tfoot></table><p>表选项</p><p>表 2为不同方法在Vaihingen数据集上的评价指标，分析可知，DANet<sup>*</sup>在引入了辅助损失对深度特征进行监督后，相比于DANet，各类地物的检测精度均有所提升；Denseaspp对建筑物的分类交并比达到了83.66%，高于其他方法，但对于小比例样本预测仍然会失效；Encnet在地面和高大植被的检测上达到了不错的精度，在建筑物和车辆上的检测也达到了次优值，但在小比例样本的预测上却全部漏检；Refinenet在总体的分类性能上达到了次优值，mIoU达到了63.52%，并且成功将杂类进行了区分；Att-Unet在低矮植被的检测上达到了次优值，但水域等杂类仍被漏检。本文提出的MAFU-Net在所有地物类别的分类上较为均衡。其中，对于地面和高大植被的分类交并比都达到了次优值，在车辆和低矮植被的检测上则达到了最优值，其交并比分别为65.46%和60.58%，说明MAFU-Net对于小尺寸地物检测具有较好的效果，同时对于难检测和难区分的地物仍然可以达到不错的分类精度。</p><p>表 3为各方法在北京地区高分二号数据集上的评价指标。总体而言，本文提出的MAFU-Net在该数据集上取得了较为理想的分类结果，尤其在水域、居民地、公路和小路等典型地物和难检测地物的分类上，均取得了最优值，总体的精度高于次优值近5%。除此之外，可以发现基于U-Net改进的网络在线状地物的检测上，精度普遍高于其他方法。</p><p>表 4为各方法在河南地区高分二号数据集上的评价指标。在该数据集中，DANet和DANet*的总体精度较低，不及其他方法。Denseaspp在水域的检测中达到了比较好的效果，而Encnet在居民区的检测中达到了最优值。Refinenet网络在该数据集上的表现相对较差，每种地物的检测精度都偏低。相比之下，MAFU-Net对各类地物的分类结果均达到了次优值，并且总体精度mIoU达到了77.53%，高于其他方法。</p><p>经过以上分析，可以得出如下结论：①高低级语义信息的融和对遥感影像地物分类是极其重要的，尤其对细节特征较多的地物，多级语义信息融和更为关键；②注意力模块可以提高小尺寸地物和难区分地物的检测能力；③基于U-Net改进的注意力机制网络，在线状地物的检测上具有明显的优势；④MAFU-Net可以适应不同分辨率的遥感影像，并且对于不同类型地物的分类都具有很好的稳健性。</p><p><strong class=highlight-text toutiao-origin=span>3.3 模型复杂度及轻量化分析</strong></p><p>理论上，适当增加网络模型的深度，可以提取到更深层次的语义特征，对于网络的分类往往更加有利。然而，当网络模型加深的同时，参数量会不断增大，模型的计算复杂度(计算复杂度是指在单个样本的输入下，模型进行一次完整的前向传播所发生的浮点运算个数，本试验中选择尺寸为128×128像素的影像作为单个样本进行复杂度计算)也会大幅增加，即在预测的过程中会消耗更多的资源。因此，在实际使用中，需要在模型的分类精度与计算复杂度上进行权衡。实际上，计算复杂度的大小并不代表预测时间的长短，网络前向传播一次的时间还取决于计算平台的带宽上限和平台算力，本文试验中选用的计算平台为RTX2080Ti。本文中模型的参数量及计算复杂度是通过Github开源的PyTorch-OpCounter模块进行统计得到，预测时间是在计算硬件和平台完全相同的情况下，以Vaihingen测试集预测为任务进行统计。</p><p>表 5列出了各模型的参数指标，包括参数量、计算复杂度和在Vaihingen测试集上的预测时间。可以看出，Denseaspp的轻量化水平最高，参数量和计算复杂度都明显低于其他方法，但预测时间较长；RAU-Net的轻量化水平低于Denseaspp，但预测时间短；本文提出的MAFU-Net在轻量化水平上与RAU-Net相当，预测效率高于RAU-Net近10%。由于PyTorch-OpCounter模块在统计计算复杂度的过程中，仅统计了加乘运算操作，并未统计模型中通道重排和通道叠加操作，而通道重排和通道叠加虽然不需要进行浮点运算，但却需要计算机中的内存数据交换，导致计算复杂度与预测时间不成正比。因此，在实际使用中，不能仅仅依靠计算复杂度来对模型进行轻量化判定，还需要在实际任务中，对模型预测效率进行评价。</p><p>表 5 不同网络的模型参数量、计算复杂度及Vaihingen测试集的预测时间Tab. 5 GFLOPS-parameters-inference time of different models on Vaihingen test dataset</p><table><thead><tr><td>模型</td><td>参数量<br>/MB</td><td>计算复杂度<br>(GFLOPS)</td><td>预测时间<br>/s</td></tr></thead><tbody><tr><td>DANet</td><td>49.49</td><td>12.48</td><td>671.03</td></tr><tr><td>DANet<sup>*</sup></td><td>49.49</td><td>12.48</td><td>650.71</td></tr><tr><td>Denseaspp</td><td><strong>10.20</strong></td><td><strong>3.11</strong></td><td>1 403.78</td></tr><tr><td>Encnet</td><td>38.03</td><td>9.81</td><td>1 115.38</td></tr><tr><td>Refinenet</td><td>113.88</td><td>16.05</td><td>1 570.16</td></tr><tr><td>RAU-Net</td><td><strong toutiao-origin=span>13.40</strong></td><td><strong toutiao-origin=span>8.64</strong></td><td>507.75</td></tr><tr><td>Att-UNet</td><td>34.88</td><td>18.76</td><td><strong>326.92</strong></td></tr><tr><td>MAFU-Net</td><td>13.89</td><td>8.84</td><td><strong toutiao-origin=span>466</strong></td></tr></tbody><tfoot><tr><td colspan=4>注：加粗字体为每列最优值，加下划线字体为每列次优值。</td></tr></tfoot></table><p>表选项</p><p>各个网络的参数指标可视化如图 10所示，其中圆圈的半径大小代表预测时间的长短，圆圈颜色的深浅程度代表参数量大小。DANet由于缺乏对多级语义信息的处理，丢失了细节，导致杂类及车辆类别误判，整体精度远不及其他方法，但当加入了辅助损失后，分类精度大幅提升。Refinenet、MAFU-Net、RAU-Net和DANet<sup>*</sup>虽然在精度上大体相同，但是Refinenet在计算复杂度、参数量和预测时间上则与后者有着较大的差距。总体而言，本文提出的MAFU-Net具有较低的参数量和计算复杂度，同等情况下可以利用少量的资源达到较高的精度，更具实用性。</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyBvr4kT7OxC><p>图 10 模型精度mIoU-GFLOPS-参数量-预测时间关系图(Vaihingen数据集) Fig. 10 GFLOPS-parameters-inference time-mIoU performance of different networks on Vaihingen dataset</p><p>图选项</p><p><strong class=highlight-text toutiao-origin=span>3.4 消融试验及可视化分析</strong></p><p>为了探究PAM和CAM模块对MAFU-Net的影响以及作用机制，本文额外进行了消融试验和特征可视化分析。</p><p>从Vaihingen分类结果中选取两幅场景进行分析，其光谱图像和标签如图 11(a)、(b)所示，CAM模块和PAM模块单独作用得到的分类结果如图 11(c)、(d)所示，图 11(e)为MAFU-Net得到的分类结果，即CAM和PAM模块共同作用得到的分类图。通过对分类图进行对比，可以发现PAM作用得到的分类结果轮廓较为清晰，例如图 11第1幅场景中的建筑物区域，PAM作用得到的分类结果较为完整，无过多误检，而CAM作用得到的结果则较为杂乱，误检率较高；第2幅场景中车辆较多，分类难度相对较大，CAM单独作用的结果明显优于PAM，说明CAM模块在小尺寸地物检测上的作用大于PAM。MAFU-Net结合了CAM和PAM模块的优势，因此达到了最佳的分类效果。</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUyBwLD4iesC><p>图 11 消融试验分类结果对比 Fig. 11 The classification results of ablation study</p><p>图选项</p><p>以上是通过最终的分类结果对CAM和PAM模块的作用进行的初步分析，至于其在特征处理上的具体功能还无法得知。为了进一步分析两种注意力模块的作用机制，需要分别对CAM和PAM模块处理后的特征图进行可视化。为了更直观地体现特征图的特点，应选取尺寸较大的特征图组进行分析。由MAFU-Net网络结构可知，特征图组U<sub>2</sub>经过注意力特征操作后得到的特征图尺寸较大，更利于理解和分析。</p><p>试验选用图 4中的光谱影像进行前向传播生成特征图，PAM和CAM模块分别处理后可得到两组特征图，每组16张，尺寸为64×64。可视化结果如图 12所示。图 12(a)为CAM模块处理后得到的特征图，可以发现CAM模块主要是对地物类别信息进行了强化，各个通道间的信息相互关联，语义更加明确；图 12(b)为PAM模块处理得到的特征图，该特征图对地物的边缘信息进行了强化，这说明PAM模块通过像素注意力相关性处理，成功提取了地物的边缘轮廓，使得地物分类结果更加清晰。这也从另一方面说明了同种地物边缘有着相似特征，在进行像素相关性计算的过程中，边缘像素在理论上会产生相近的响应强度。</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUyCuJDFHcqt3><p>图 12 CAM和PAM模块特征处理可视化 Fig. 12 The visualization of the processed feature by CAM and PAM module</p><p>图选项</p><p>经过以上分析可知，CAM模块通过通道相关性处理，强化了地物类别语义信息；PAM模块通过长距离像素相关性处理，更多地提取了类别边缘特征，使得地物分类结果更加精细。</p><p><strong toutiao-origin=span>4 结论</strong></p><p>本文结合U型网络的不同层次语义信息融和思想和注意力机制，提出了一种高效轻量化语义分割网络MAFU-Net，提高了遥感影像高低级语义特征融和的性能和语义信息相关性处理能力。笔者分别在Vaihingen数据集以及北京、河南地区高分二号数据集上测试，结果表明，MAFU-Net总体分类性能要优于目前主流的语义分割网络，并且在参数量、计算复杂度以及预测时间上都有较大的优势。此外，通过消融试验和可视化分析，发现双注意力机制的CAM和PAM模块是通过对语义信息及边缘信息分别进行了强化，从而达到改善分类精度的目的。</p><p>在语义分割网络中加入注意力机制可以大幅改善影像的分类性能，但MAFU-Net使用的双注意力模块在一定程度上会增加计算量，这主要是模块内部特征相关性计算所导致。因此，未来可以对双注意力模块进行改进，即将通道维度和像素维度的相关性计算放到同一个模块中进行，进一步降低网络的整体参数量和计算复杂度，减少计算过程中内存的占用。此外，可以将空洞卷积和多尺度结构加入网络中，增大感受野范围，以适应不同尺寸的遥感地物，提升遥感影像地物分类的能力。</p><p>最后，本文提出的MAFU-Net在影像分类方面的大量试验结果及分析表明，深度学习网络虽然有其优势，但也有不足之处。首先，深度学习方法对样本依赖性较强；其次，深度学习网络的搭建需要不断地尝试才能找到适合某任务的网络结构，例如经典的ResNet系列，VGG系列网络都是通过提出者不断尝试和分析后才找到的最佳网络结构；最后，深度学习内部的特征提取较为繁杂，很难用准确的数学原理进行推导，换句话说，神经网络对于使用者，甚至于设计者来说，相当于一个“黑匣子”，很难说清网络内部的学习机制，也就无法解释某网络为何在某种数据集中会失效。因此，神经网络所表现出的分类效果，在一定程度上，反映了网络的某些特性，但如果深究其内部原因，可能还需要一定时间进行探索，或者通过更加高级的可视化表达或者量化准则来进行分析评价。</p><p><strong toutiao-origin=span>作者简介</strong></p><p>第一作者简介：李道纪(1994-), 男, 硕士生, 研究方向为遥感影像地物分类。E-mail:wang111@alumni.sjtu.edu.cn</p><p>通信作者：卢俊, E-mail：ljhb45@126.com</p><p><strong toutiao-origin=span>团队简介</strong></p><p>郭海涛副教授团队长期从事数字摄影测量、机载激光雷达点云处理、遥感影像目标检测、变化检测等方面的研究，有100余篇论文发表在《IEEE Geoscience and Remote Sensing Letters》、《International Journal of Remote Sensing》、《Journal of Applied Remote Sensing》、《测绘学报》、《遥感学报》、《中国图象图形学报》、《武汉大学学报•信息科学版》、《光学精密工程》、《测绘科学技术学报》等中英文刊物上。</p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RN4Licj7bGr0PK><p></p><h1 toutiao-origin=h1>《测绘学报（英文版）》（JGGS）专刊征稿：LiDAR数据处理</h1><p><strong>论文推荐 | 郑鑫,潘斌,张健：可变形网络与迁移学习相结合的电力塔遥感影像目标检测法</strong></p><p><strong>资讯 | 2020年度海洋科学技术奖拟提交终评项目名单/海洋优秀科技图书拟提交终评项名单</strong></p><p><strong>院士论坛 | 郭毅可院士：人工智能的热望与冷思考</strong></p><img alt="论文推荐 | 李道纪，郭海涛，卢俊，赵传，林雨准，余东行：遥感影像地物分类多注意力融和U型网络法" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RwfZ0qO5B9LQb6><p>权威 | 专业 | 学术 | 前沿</p><p>微信、抖音小视频投稿邮箱 | song_qi_fan@163.com</p><p>欢迎加入《测绘学报》作者QQ群：<strong> 751717395</strong></p><p>进群请备注：姓名+单位+稿件编号</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'论文','推荐','李道纪'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>