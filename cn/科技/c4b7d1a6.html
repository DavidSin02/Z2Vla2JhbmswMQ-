<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>CAAI AIDL 第七期 演讲实录丨章国锋：视觉SLAM技术与AR应用 | 极客快訊</title><meta property="og:title" content="CAAI AIDL 第七期 演讲实录丨章国锋：视觉SLAM技术与AR应用 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/098283f570024390af0681b956fa4d9b"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c4b7d1a6.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c4b7d1a6.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/c4b7d1a6.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c4b7d1a6.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c4b7d1a6.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/c4b7d1a6.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/c4b7d1a6.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c4b7d1a6.html><meta property="article:published_time" content="2020-11-14T21:03:55+08:00"><meta property="article:modified_time" content="2020-11-14T21:03:55+08:00"><meta name=Keywords content><meta name=description content="CAAI AIDL 第七期 演讲实录丨章国锋：视觉SLAM技术与AR应用"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/c4b7d1a6.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>CAAI AIDL 第七期 演讲实录丨章国锋：视觉SLAM技术与AR应用</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><blockquote>8月31日-9月1日，由中国人工智能学会主办，华中科技大学电子信息与通信学院承办的主题为《计算机视觉应用技术》的AI前沿讲习班第七期在华中科技大学成功举办。浙江大学计算机辅助设计与图形学国家重点实验室教授、博士生导师、国家优秀青年科学基金获得者章国锋发表了主题为《视觉SLAM技术与AR应用》的精彩演讲。</blockquote><div class=pgc-img><img alt="CAAI AIDL 第七期 演讲实录丨章国锋：视觉SLAM技术与AR应用" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/098283f570024390af0681b956fa4d9b><p class=pgc-img-caption>章国锋 浙江大学计算机辅助设计与图形学 国家重点实验室教授博士生导师、国家优秀青年科学基</p></div><p>以下是章国锋的演讲实录：</p><p>章国锋：非常感谢也荣幸有机会跟大家分享一下我们课题组最近一些年在视觉SLAM方面做的一些工作，以及在AR和自动驾驶方面做的一些应用。</p><p>我们来看一下，首先这样一个非常复杂的城市的场景，地上有无人车在开，天上有无人机在飞，大家戴着VR或AR的头盔在看，你会发现这里面都需要用到对设备的实时定位和对环境的三维感知，这就需要用到SLAM技术，所谓SLAM就是能够实现在未知环境中定位自身的方位，并同时去构建这个环境的三维地图，无论像增强现实、虚拟现实、机器人、无人驾驶、航天航空领域都需要用到实时的定位和三维地图的构建，所以需要到SLAM技术。SLAM技术的类型有很多，根据不同的传感器方法也是不一样的，甚至差别非常大，比如一些深度的传感器、激光雷达等等之类的，还有摄像头，分为单目、双目或者多目，还有惯性传感器，就是我们智能手机里面都会有的传感器。</p><p>我们先看一下SLAM的运行结果，根据传感器的信息它可以实时计算设备在空间中的位置和朝向，并且同时构建三维地图；根据地图的稀疏性可以分为左边的稀疏SLAM，只恢复一些稀疏的三维顶云，或者右边非常稠密的点云甚至是三维网格。SLAM经过几十年的发展，它的整个框架已经趋于定型了，这个是目前主流的框架，最早是2007年PTAM这个工作提出来的，整个计算分为两类线程，即前台线程和后台线程（后台线程可能不止一个），前台线程是实时计算的，根据输入传感器的信息，在完成初始化之后就可以进行实时的位姿恢复。后台线程通过对局部或者全局的地图优化来减少误差累积，如果有回路把回路检测出来通过全局优化闭合掉，还有如果跟踪丢了可以通过重定位来恢复跟踪。</p><p>我下面简要地把这其中的几个关键模块讲一下。首先是初始化，SFM翻译成中文叫运动恢复结构，它跟SLAM其实是非常类似的。SFM一般是离线处理的，实时的SFM其实就是视觉SLAM。很多时候我们需要处理相机内参未知的情况，比如互联网上找的一些照片可能没有内参信息，我们可以通过SFM技术恢复出它的内参以及外参（即位姿）。但是SLAM通常一般认为相机的内参已经事先标定好，这样它的初始化是在内参已知的情况下进行的，常用的方法有五点法。如果是双目或者多目的SLAM的话那就更加简单了。这里介绍一下SLAM常见的几种初始化策略。比如PTAM需要用户指定两个关帧进行初始化，但是这在AR应用里体验肯定是不好的；ORBSLAM在PTAM的框架上做了一些改进，它可以自动选帧来做初始化，还有一些其他的方法，比如说单帧的初始化，但是它要假设对着地平面或者基本对着地平面。还有一种比较常见的方式就是通过检测出一个已知的Marker来完成初始化。初始化完了以后要进行特征跟踪，先检测特征，然后进行特征的匹配，通过跟地图里的三维特征点建立对应关系之后，就可以根据若干3D-2D的对应点把当前帧的位姿求解出来。完成了当前帧的位姿估计之后再三角化出更多的三维点，进行地图扩展，基本上是这样一个过程。跟踪主要有两类方法，一类是基于关键帧的特征跟踪，地图点一般是依附于关键帧的，当前帧和关键帧（当然关键帧可能不止一个）匹配的时候，可以基于一定的运动预测（比如假设相机做一个平滑运动），来初略估计当前帧的初始位姿，再把地图点投影过来进行局部的搜索来实现快速准确的匹配。这里特征的匹配，还要考虑空间的均匀分布，避免大部分匹配点集中在一小块区域。这是基于关键帧的匹配，另一类方法就是连续帧的匹配跟踪。比如光流法，先在第一帧提取特征点直接到下一帧的局部区域去搜索对应点，找到了之后基于下一帧的对应点继续跟下下一帧去匹配，一般只会在相邻帧之间进行匹配，不会再跟之前的间隔比较远的某一帧进行匹配。连续帧的跟踪方法一般比较简单，一般直接用光流法就可以了，不需要用基于描述量的方法。但是它比较适合连续帧的跟踪，如果某一个特征点由于遮挡的原因跟踪断了之后，后面即使重复提取出来也会被认为是新的特征点，不会再跟之前的匹配，因此它比较难去处理非连续帧之间的匹配和回路的闭合。此外，在运动比较快的情况下，连续帧跟踪比较丢掉，一旦有一个遮挡就跟踪断掉了，而且容易产生误差累积。基于关键帧的跟踪和基于连续帧的跟踪有他们的优缺点，关键帧跟踪方法不容易有误差累积，因为你总是跟关键帧匹配，所以它不会产生慢慢漂移的情况，但是它的缺点是在弱纹理和重复纹理的情况下容易匹配错误，因为它总是跟关键帧进行匹配，在弱纹理或重复纹理的情况下，关键帧相比于连续帧来说还是颜色差异比较大些，而且位姿变换也比较大，很容易造成误匹配。相比而言连续帧的跟踪在弱纹理和重复纹理的情况下要好一些，因为相邻帧之间的图像颜色变化一般不是那么大，而且位姿变换也不大，所以它的稳定性要好。但是基于连续帧的跟踪很容易有漂移和误差累积问题，而且很难实现回路的闭合。所以比较好的方式是将这两个方法结合起来，在关键帧跟踪的基础上再结合连续帧跟踪来加强跟踪的稳定性。</p><p>我们跟踪得到了一些匹配点之后，通过最小化目标函数，就是优化三维点到对应的二维特征点的距离，就可以把当前帧相机的位姿给解出来。这其实是一个PnP问题，有很多方法，最简单就是构造一个线性的矩阵去求解，还有一些其他的方法，方法非常多，我就不细讲了。</p><p>前台的跟踪基本是这样的，那么后台线程需要不停地对地图进行优化。后台优化最主要的就是集束调整，集束调整里面的变量有三维点和相机位姿，放在一起进行全局的优化。它的复杂度是非常高的，在规模比较大的情况下很难保证实时性。所以有一些策略，比如说采用局部集束调整法来进行高频次的优化，而全局集束调整比较低频次的调用，往往只有在回路闭合的时候才会用到。另外是回路闭合，回路闭合需要先检测出回路之后再通过全局优化来闭合，比较耗时，因此一般放在后端。还有重新定位、稠密的三维重建这些模块，一般也是放在后端。如果回路没有闭合之前误差已经很大了，闭合之后可以把误差消除掉。刚才讲到后台的全局优化，因为你要用相机的状态和地图的三维点进行批量式的优化，因为它用了所有的信息，所以它的精度是最高的，当然速度就比较慢。</p><p>对于局部的窗口的优化大致可以分为两类，都是采取滑动窗口的方式，比如说滑动窗口里有十个关键帧，新的关键帧进来了以后要把老的关键帧给移出去，至于移出去的策略可以有好多种，最简单的就是新的关键帧进来，最老的一个关键帧滑出去。窗口内始终保持一定数量的关键帧，优化速度非常快，缺点是精度比较低，因为旧的关键帧的相机状态移出去以后它的信息被固化了，不会再被优化，所以一旦有误差累积就一直累积在那里，就是错误的信息不能得到纠正。所以后来提出带有状态先验的局部窗口优化，最大的不同是什么？就是我滑出去的这一帧，并不是直接把它的相机状态和三维点给固化掉，而是对它做一个边缘化的处理，所谓边缘化就是根据当前的状态值进行线性化，然后作为先验加到目标函数里。这个方法比直接对状态进行固化的方式精度要高，但是速度要慢些，因为增加了目标函数的复杂度。但是总的来说，只要滑动窗口的大小固定，增加的复杂度也是有限的，所以计算复杂度不会无限增长。</p><p>全局优化就是对所有的相机状态进行批量式的优化，理论上是最优的。由于计算复杂度很高，一般只进行低频次的调用，或者在回路闭合的时候调用。一些加速的方法也被相继提出来，比如有人提出把三维点都消元掉，只留下相机位姿参数，因为相机位姿的变量数是远远要小于三维点的数目的，所以这样它的计算复杂度会降低很多。当然它的精度也会下降，特别是如果三维不准的话，这样做的误差也是很大的。</p><p>还有一种策略是采用增量式的集束调整方法，每来一帧不是重新构造一遍，而是通过重用上次计算的结果，只要做一些局部的更新就好了。还有一个就是状态删除的策略，你要保证地图不能无限制的上涨，关键帧的数目要设一个上限。怎么删除关键帧就比较关键了，一种是直接删除，直接删除会造成信息的丢失，还有一个是边缘化删除，这样的话虽然做了一些删除，但还是保留了部分信息，当然计算复杂度会高些。</p><p>SLAM难免会有跟踪丢掉的情况，丢掉了以后需要恢复回来，这就需要用到重定位。另外一种情况是，随着运动时间和距离加长，误差难以避免会不停的累积，这时候如果有一个回路，可以通过回路闭合来把误差累积消除掉。重定位和回路闭合有相似的地方，一开始都要去寻找当前场景跟已经生成的地图的联系，也就是图像检索的过程，这是第一步。但是二者的优化目标不一样，重定位只需要得到当前帧相机的位姿，回路闭合则需要修正整个相机轨迹以及相关的三维点的座标。</p><p>刚才说到它们都是用到的图像检索，随着场景的拓展，关键帧的数量不断增加，主要的问题是如何快速鲁棒地从中找到和当前帧相似的关键帧。可以分为两类方法，一类是基于局部特征的检索方法，还有一类是全局图像的检索。局部特征大家都比较熟悉了，我就不展开来讲了。还有就是基于整张图像进行检索，比如说用Gist这种比较传统的方法，但这类方法的速度比较慢，现在普遍采用深度学习的方法来做。</p><p>前面我们快速过了一下SLAM系统中的几大模块，下面我重点讲一下视觉SLAM以及视觉SLAM的挑战和针对性的解决方法。目前它主要有两个方面的挑战，一个是精度和稳定性，因为这个场景是动态变化的，视觉特征匹配由于弱纹理或者重复纹理会导致匹配不准，从而导致优化计算不稳定。还有一个是场景的规模非常大，比如在一个城市规模的场景下，计算复杂度会很高，如何在手机或者眼镜这种低功耗设备上做到实时稳定的运行，挑战还是很大的。</p><p>针对第一个挑战，我们需要做到目标函数里的约束方程要正确，如果有大量的Outliers，会造成错误的约束，从而导致求解的不稳定，需要将它们剔除掉。还有就是要保证约束的充分性，比如在一些弱纹理场景下，没有足够的特征匹配，导致约束的方程很少，也会造成优化的不稳定，这需要增加一些约束，比如加上运动先验的约束，最典型的就是利用帧与帧之间的加速度、角速度信息来进行相邻帧的运动约束。还有就是如果场景有一些平面性的结构或垂直关系，把这些先验信息用上的话也可以提高稳定性。</p><p>关于第一个方法，就是如何检测出outliers，我们在2013年的时候做了一个工作RDSLAM，这是整个的框架，在Mapping线程我们会不停地检测哪些三维点已经改变，如果某个点发生改变那么需要从地图里把它标志为无效或者删掉，某个关键帧如果无效点太多的话也需要把它替换掉。我们采用关键帧的表达方法，每次在线匹配了之后，我们会选五个与当前帧位姿最接近的关键帧，把关键帧的点投影到当前帧来。因为我们没有法向的信息，这样投过来的话，如果没有进行角度的矫正的话，实际上误差可能会比较大，所以我们如果只是做一个简单的比较需要限制一个角度，不能改变太大。因此我们加了这样一个限制，在这种情况下如果颜色变化依然比较大，我们还要进一步排除是不是由于遮挡造成的，当然投过来的是不一样的，我们进一步排除遮挡，如果不是遮挡造成的，那就是确实改变了。这里给出了一个例子，我们故意拿一张纸在镜头前面晃来晃去，如果我不进行遮挡处理，系统会误认为这里的点都是被改变掉的，然后就会把这些点全部移除掉，这会造成不稳定，因此我们需要排除这种情况，也就是只把真正改变的点去掉。还有一个也是非常重要的，我们在有大量outliers的情况下要迭代很多次才可以选出正确的一组inliers，然后把相机位姿给解出来。比如说每次选六对点，如果inliers比例是10%的话，那么要选六对点保证都是inliers的概率是非常低的，所以在这样的情况下传统的RANSAC方法就很难工作了。因此我们针对这个问题提出了一个基于时序先验的RANSAC方法。我们首先对图像进行一个10×10的等分。为什么做10×10的等分呢？因为我们发现如果特征点匹配都集中在某一块小的区域其实意义不大，所以我们每一块区域只需要选一对点就够了，再多了其实意义不大。我们计算每一个小块的inliers的分布，然后假设当前帧和前一帧的运动是比较平滑的，我们直接把上一帧的inliers分布概率传递到当前帧。这时候我们不需要完全随机选点了，而是根据inliers分布优先去选inliers可能性比较高的帧，这样就会大大提高我们选出一组都是inliers的可能性。仅仅是这样还不够，如果这个场景中有一个刚性运动的物体，而且特征很丰富，这时候只选出inliers数目最大的一组的话，就可能会出问题了，可能会误认为这个做刚性运动物体的点是inliers，应该是静态的点。事实上，我们发现这些在刚性运动物体上的点往往集中在一个比较小的区域，而静态背景的点一般分布比较均匀，因此我们可以算一个点分布的协方差，然后跟inliers数目综合考虑起来，这样能比较有效地区分。这里有一组例子，左边显示的是有200个绿色的点，属于静态背景，然后300个紫色的点是位于刚性运动的物体上，还有另外500个是完全随机的。我们发现刚性物体上的点的数目比静态背景上的数目还要多一些，因此你如果用传统的算法只算inliers数目的话这两个是非常接近的，甚至可能弄错了。但如果我们进一步考虑了这些点的分布面积之后，就能正确区分出来。</p><p>我们来看一个例子，这本红色的书，如果是用传统的方法很可能会误认为它属于静态背景，恢复出来的相机运动是跟着这本书的。而采用了我们提出的PARSAC方法，能正确选出静态背景上的特征点，从而恢复的相机位姿不会随著书的运动而运动。最后放一下结果，这是一个很有挑战性的场景，人在整理书本，同时有手电筒在照，造成一些光照的变化，大家可以看到跟踪还是比较稳定的，这个是恢复三维的点，就是关键帧，红色的点代表的是改变的三维点。这是跟PTAM的比较。</p><p>刚才说的是怎么去通过先验去很好地选出inliers来提高SLAM的稳定性以及效率。但如果在特征不够丰富的场景下，或者匹配不是很好的情况下，这个时候怎么办呢？我们知道，视觉惯性SLAM利用IMU来弥补视觉的不足。如果是只有摄像头没有IMU，有没有办法从视觉惯性SLAM做一些借鉴来提高视觉SLAM的稳定性呢？IMU主要有加速度和角速度信息，加速度（尤其是手持设备的加速度）往往比较小，因此我们可以直接把它设为零来对相邻帧的运动做约束。但旋转的角度不能这么做，需要对它做一个比较准确的估计。我们假设位移比较小，将特征匹配和整张图像对齐结合起来只估计连续帧之间的旋转角度，这样即使在运动模糊情况下没有足够的特征匹配很多时候依然能稳定地求解旋转角度。这个是我们2016年的一个工作，也就是RKSLAM。这是整个系统的框架，首先前端是基于Multi—Homography的跟踪方法，假设这个场景可能会存在多个平面，通过拟合多平面去提高跟踪稳定性。另外就是把角速度估计出来，然后通过滑动窗口优化提高跟踪稳定性。这里有一个比较，左上角是基于我们估计出的连续帧旋转角度的跟踪结果，右上角是用了真实IMU的跟踪结果，下面两个，其中一个是直接将角速度设为零，还有一个是不加运动约束的结果。这个对比还是非常强烈的，上面两个的结果还是很接近的，有真实IMU会好一点，下面的两个跟踪结果就差距比较大了。这个是我们跟其他方法做的对比。我们也在TUM数据集上做了评估和比较。我们选了12个序列分为四组，其中D组是快速运动和强旋转，可以看到在D组序列的跟踪成功率上是明显好于ORB-SLAM的。而且我们的方法速度要快很多，是ORB-SLAM的将近5倍，在手机上可以做到实时。</p><p>刚才讲到跟踪的稳定性，我们分享了如何解决跟踪稳定性做的一些工作。还有就是如何解决求解效率的问题，这个里面最主要的模块就是集束调整，就是通过优化目标函数把所有的三维点和相机参数放在一起进行联合优化，大家可以想像这个目标函数是非常的庞大，因为三维点的个数可能非常多。如果你不利用稀疏性去解，计算复杂度是很高的。因此我们肯定得利用矩阵的稀疏性，一般常规的做法是每次迭代都会解这样一个线性方程组，上面的是相机参数的变量，下面是相应的三维点的变量，因为三维点的变量数目一般远远大于相机参数的变量数，所以我们先把相机参数解出来，把右上角的W变成零。上面部分可以独立地把相机的参数解出来，之后再回来把三维点数目解出来，这样的好处是因为相机参数的数目比较小，可以解得比较快，然后再回到下面这个线性方程组求解三维点，这时候每个点可以独立的求解，因此可以非常高效。但是即使是这样利用稀疏性去做，这是我们用常规的方法去做的，虽然关键帧数目的增长，可以看到它基本上是线性的，可能是因为稀疏性发生了改变。随着SLAM的帧数越来越多，复杂度还是会增长很快。主要有两种思路，一个是分治求解，还有一个是增量式计算。分治求解常见有两类方法，一个是基于分段的BA，就是将一个长序列分成若干段短序列，每段整体（帧和三维点）做一个7DoF的相似变换，这样变量数大幅下降，优化速度可以大幅提升，而且因为是全局优化不容易陷入局部最优解。当然自由度下降了，优化能力肯定也会有所下降，可能优化的结果误差还是比较大的，那么我们可以对段进一步分裂然后再优化，不断地重复这个过程直到不能再分裂或误差小于某一个阈值为止。分布式BA的做法也是有点类似，将整个数据集分成若干个子集，对每一个子集进行独立的局部BA优化，然后通过共享变量的方式进行全局优化。这个是基于分段BA的结果，可以看到经过几次迭代之后基本上就不变化了，也就是收敛了。这个数据集有6段视频序列将近10万帧，在一台PC上进行求解只需要16分钟，加上匹配的时间，平均下来达到17.7fps，还是相当快的。</p><p>我们再看一下增量式BA，对于SLAM来说，特别是基于关键帧的SLAM，每加一个关键帧都需要进行优化，如果每加一个关键帧整体重新优化一遍，每次优化的复杂度都是会增加的，这对于大尺度场景来说就不太可行。所以有些人就提出来，每加一帧进行优化的时候，是不是可以重复利用前面优化或计算的结果呢？代表性的增量式方法有iSAM以及iSAM2，我们也做了一些工作，也就是EIBA和ICE-BA。增量式方法的核心思想就是只更新加入或更新的变量对应的矩阵元素，也就是进行增量的更新，而不是从头到尾重新构造。比如说原有来C1、C2、C3三帧，现在加入了一个新的三维点X3以及新的帧C4，我们只需要这两个变量对应的矩阵元素就好了，其它不需要重新构造，这样就有点像一个局部BA，但精度可以达到全局BA。当然如果在有回路闭合的情况下，这种增量式BA就要退化到批量式全局BA，因为所有的变量对应的元素都要更新了。我们来看一下Incremental BA的效率，比之前的方法有一个数量级的提升。对于局部BA来说，滑动窗口里面的特征轨迹可能会比较长，导致对应的信息矩阵的复杂度会比较高。为了高效的求解，我们提出把一个很长的特征点轨迹切成若干段短的特征轨迹，这样对应的信息矩阵变得稀疏很多。如果矩阵比较大，就更明显了，求解的效率会大大提高。大家可能会问，这样会不会造成精度的下降？因为把特征轨迹切分成若干段，其实是放松了一些约束。不过我们是只在计算相机参数的时候把特征轨迹切分，而回代去求解三维点的时候并没有切分，因此这时候是没有近似的。事实上，我们发现虽然它比常规的优化方法需要更长的迭代次数，但是因为每次迭代的速度大幅提升，所以整体最后收敛的时间反而是更快的，而且我们发现最后的优化精度其实是没有下降的，基本上跟标准的方法是一样的。这是局部BA方面，我们的方法和OKVIS的对比，可以看到速度提升了一个数量级。在全局BA方面，我们的方法也比iSAM2快一个数量级。</p><p>最后讲一些应用。我们和商汤一起开发的SenseAR平台，跟其他平台比起来，它能支持单目、双目和RGB-D等多种类型的传感器，目前已经支持了AR测量、高德地图AR步行导航以及《王者荣耀》和《一起来捉妖》等游戏的应用。最近又升级到了2.0，形成了一个云与端融合的增强现实平台，通过构建视觉高精度地图以及云和端结合的方式实现室内大尺度场景的导航。再比如，基于云-端结合的AR多人共享，你和朋友各自拿一个手机可以一起来玩这样一个AR多人射击游戏。</p><p>提到AR，对于AR来说SLAM主要面临哪些挑战呢？在AR的应用场景里面其实挑战还是很大的，因为用户拿到手机不会那么小心翼翼，他可能会突然地转动，然后场景里可能有很多动态物体、高光和重复纹理、甚至弱纹理区域。对于好的AR体验来说要求三维注册要很精确，没有漂移现象，走了一圈回路要闭合起来，而且希望跟踪丢失的情况尽可能少，就算丢失了也可以尽快的恢复，也就是重定位的时间要很短。我们来看一下现有的一些SLAM方面的数据集，就是视觉加IMU，它们往往采用同步比较好的传感器，IMU一般也是比较好的。但是目前无论是手机还是AR眼镜，里面的IMU不会太好。那这样的数据集实际上并不能满足我们在AR场景下对SLAM性能的评估。因此，我们自己构建了一个新的数据集。我们当时用了两款手机，一个是iPhone X，还有一个是小米8，这是它们的图像、分辨率的参数，为了模拟AR运动的情况我们选了五种运动类型。我们分别用小米8手机录了A系列序列，iPhone X录了B系列序列。我们来看一下这些序列是怎么样的。A系列序列还是属于比较正常的运动类型，就是我刚才说的几种常规的运动，主要是用来测试跟踪精度的；而B系列序列是测试鲁棒性的，相对来说比较极端，比如说突然手去挡住相机，或者突然把图像变成黑色，强迫SLAM系统跟踪丢失进入重定位状态。</p><p>我们提出了一些新的指标，首先是Tracking Accuracy。跟以往的标准不同，除了绝对的位置精度，我们还专门提出了完整度，绝对位置误差如果小于某一个设定的阈值就认为是好的位姿，然后算一个好位姿的比例。相对于平均绝对位置精度，完整度不容易受到个别位姿误差很大的影响。还有一个很重要的指标就是初始化的质量，有两个方面：一个是初始化的时间，还有初始化完成之后尺度的精度。我们把这两方面综合起来评估初始化的质量。也就是初始化的时间越短越好（甚至做到用户无感初始化），尺度的精度越高越好。还有一个是跟踪的鲁棒性，比如是不是容易跟丢，跟丢之后重定位回来是不是准确。还有就是重定位时间，也就是跟丢之后多久能重定位回来。为了准确估计重定位的时间，我们需要知道SLAM系统什么时候真正丢掉，什么时候完成重定位。因此，我们会故意把图像变成纯黑的，对于视觉SLAM来说肯定就跟踪丢掉了，但对于VISLAM来说，虽然视觉跟踪会丢失，但它还是会持续地输出位姿信息，这个时候我们到底怎么判断重定位成功了呢？视觉SLAM的判断很简单，直到它输出一个正常的位姿就说明重定位成功了。但VISLAM就没那么容易了，不能直接看出它什么时候完成了重定位，当然如果我们可以读它的源代码根据系统内部的状态判断是可以的，但是我们不想把问题复杂化，而是希望只是输出位姿信息就能判断是不是重定位了。为此我们做了这样一个检测：当黑屏结束之后，系统应该会启动重定位，如果后面某个时刻位姿突然跳了一下就说明是重定位成功了，然后计算重定位的时间。</p><p>目前有这么多的代表性方法，我们从中选了一些开源的方法，以及我们和商汤合作研发的SenseSLAM。我们将这些方法分为VSLAM和VISLAM两类，并制定了跟踪精度、初始化质量、跟踪鲁棒性以及重定位时间来进行比较。这里，我们特别看一下重定位的结果比较。VSLAM系统（PTAM和ORB-SLAM）重定位的时间明显比VISLAM短一些，这是因为VISLAM不仅要解出来当前帧的位姿，还需要重置IMU的状态，所以一般需要好几帧的时间。不过，一般一秒以内的重定位还是可以接受的。我们这个Benchmark对应的文章已经发表了，大家有兴趣可以去看一下。对于开源的软件，我们可以通过导入一样的数据集运行输出结果来评估，但如果是不支持导入序列的商业软件，那怎么办呢？针对这种情况，我们把两个配置一样分别装了SenseSLAM和ARCore的手机并排放在一起，将位姿实时输出进行比较。可以看到，SenseSLAM 2.0和ARCore 1.9的结果在各个方面基本差不多。</p><p>最后讲一下我们在自动驾驶方面做的一些工作，结果都还比较初步。自动驾驶完全用纯视觉的SLAM技术还很困难，目前比较现实的还是视觉怎么和其他传感器进行融合，比如可以跟LiDAR融合，跟GPS融合，甚至和轮速计融合。此外，通过视觉技术生成带有语义的高精度地图，对自动驾驶来说也是很有用的，它可以帮助更好地做定位修正，消除误差累积。我们做了基于双目的里程计以及融合了LiDAR的里程计。目前基于LiDAR的单帧定位耗时10毫秒左右，精度基本上100米的误差在1%左右。另外，还可以融合IMU和GPS，比如融合IMU尤其是场景里面有体积比较大的动态物体，比如像一些大卡车开过会造成运动的偏移，有了IMU可以有效地减缓这个问题。如果有GPS也可以进一步降低误差累积，因为普通的GPS虽然精度不是很高，但没有误差累积。一般车上都有轮速计，根据轮速计也可以计算出运动轨迹，但是误差累积还是比较大的，比如在这个实验里它的尺度误差超过了10%。如果是只利用单目相机的信息，也是很难准确估计尺度，而且漂移也比较严重。但是我们把两者融合起来之后，定位精度会得到明显的提升。特别是结合带有语义的地图，通过对道路进行一些平面以及线的识别，然后再用点到线的方式进行定位修正，这样横向的误差可以明显降低。当然，这还只是非常初步的一些结果，未来我们计划将更多的语义信息融合提高定位的精度和可靠性。</p><p>刚才介绍的一些工作的可执行程序或源代代码都已经放出来了，未来我们计划开源更多的算法和数据集（http://github.com/zju3dv），欢迎大家关注和下载使用。</p><p>最后讲一下视觉SLAM技术的发展趋势。视觉SLAM最大的问题就是对特征的依赖非常明显，因此大家都在考虑如何缓解对特征的依赖，比如结合基于边、面特征的跟踪，采用直接图像跟踪或半稠密跟踪，还有结合机器学习等。当然，这些方法只能缓解但不能彻底解决特征依赖问题。每个传感器都有各自的优点和缺点，如果能把多种传感器的信息融合起来，那就可以得到一个更高可靠性和高精度的定位，这也是未来的一大发展趋势。此外，视觉SLAM也在朝着稠密三维重建的趋势发展。比如目前基於单目或多目的三维重建，已经能做到实时了；如果有深度相机，那么实时三维重建可以做得更好，甚至能做到对非刚性物体的实时三维重建。</p><p>我今天的报告就到这里，谢谢大家！</p><p>嘉宾：谢谢章老师，下面的时间大家可以提问。</p><p>提问：现在你们在无人机场景下考虑做了一些什么吗？</p><p>章国锋：暂时还没有。</p><p>提问：比如在长隧道里面GPS失效了，在隧道里面你如何用SLAM提高它的定位精度？</p><p>章国锋：在隧道里面我们没有测过，但是VSLAM我觉得还是可以工作的，当然前提是隧道里面不能太黑，如果完全黑了的话那就不行了。基于LiDAR的SLAM技术在隧道里应该也能比较好地工作。</p><p>提问：还有一个问题，就是隧道里面无法进行回环检测。</p><p>张辉：对，没有回环检测误差累积就难以消除。那么关键就是误差累积有多快了，这个其实跟传感器和运动的速度都有关系。比如图像如果比较模糊，那么误差累积一般会厉害一点；如果视觉融合IMU信息，误差累积可能就没有那么快了。当然还可以和LIDAR以及其他一些深度传感器融合，比如毫米波雷达，也能提高定位的精度，减缓误差累积。</p><p>嘉宾：毫米波雷达的定位精度有多高？</p><p>章国锋：毫米波雷达没有试过。定位精度其实跟传感器本身的精度和场景类型都有关系。比如基于纯激光雷达的里程计，在KITTI数据集上已经可以做到每100米的平均误差在1米左右，目前差不多都是这样一个级别。如果跟其他传感器融合，定位精度应该还会再高一点。</p><p>嘉宾：章老师，我想请教一下，你们的技术有没有在一些室内的场馆里面运用？</p><p>章国锋：这个其实我们已经在做了。我们为杭州国际博览中心做了定位导航，就是通过视觉的方法做的。</p><p>嘉宾：但是大型的场馆有时候会有遮挡，包括人员也很稠密，之前有团队是用无线的技术做的导航，但是有一个问题是，无线的信号是很容易被屏蔽的，就需要借助一些视觉的技术。</p><p>章国锋：结合肯定可以做，但是我们的出发点是尽可能把视觉的潜力发挥到极限。比如，国博的场景还是很有挑战的，地面都是大理石，反光很严总，而且很多区域很相似，在那样的场景下我们单帧的平均定位成功率已经达到80%以上，而且还在进一步改进中，预计未来可以做到90%的定位成功率；如果是旋转半圈拍摄视频的方式，那么定位的成功率可以达到96%，基本上可以满足实际应用的要求了。</p><p>嘉宾：您刚刚说用手机在室内导航必须要先有地图吗？</p><p>章国锋：是的，还是需要先把地图构建出来才能进行定位导航，就跟现在的室外导航一样的道理。</p><p>嘉宾：是离线构图吗？</p><p>章国锋：是的。</p><p>嘉宾：有没有可能在没有任何先验的数据环境下用SLAM就可以实时定位？</p><p>章国锋：问题是SLAM的实时定位是基于系统自己的一套三维座标系，跟地理信息座标系或场景地图的三维座标系是不一致的，因此这样的定位无法用于导航。因为你不知道自己在整个场景中位于哪个位置，也不知道目标位置在哪里，这种情况下是无法导航到目标位置的。</p><p>嘉宾：我们想先解决A点的定位问题，B点的信息用其他的算法可以解决。</p><p>章国锋：因为SLAM技术可以在未知的环境下恢复相机相对于场景的位姿，并且不断地恢复周围环境的三维结构，所以确实可以确定A点的定位信息（相对于A点所在的局部场景而言）。但是由于一开始并不知道B点相对于A点的方位，所以这种情况下还是无法导航的。对于导航来说，如果没有预先的地图信息（哪怕是比较粗糙的相对方位信息）是不可能做到的。</p><p>嘉宾：地图信息必须要下载到手机端吗？</p><p>章国锋：不需要。完整的地图信息可以放在云端，通过云端定位的方式来实现。</p><p>嘉宾：章老师我有一个问题，在自动驾驶里面跟踪很重要，如果要达到很高的精度，我希望跟踪的特征点轨迹要足够长，但是如果太长了以后会它产生漂移吗？</p><p>章国锋：这要看是基于关键帧的跟踪还是连续帧跟踪。对于连续帧跟踪，确实特征点跟踪长了会逐渐漂移。我们可以通过基于关键帧的跟踪方法来抑制漂移问题；当然如果视角变化过大，基于关键帧可能很难匹配上。</p><p>嘉宾：所以这个时候语义可以提供帮助吗？</p><p>章国锋：结合语义信息可以帮助减缓这个问题，但是目前的方法还很难做到点和点的准确对应。通过结合语义信息建立的约束可以减少误差，特别是有比较大的累积误差的时候，还是有效果的。但是如果希望得到非常高的精度，尽可能把误差累积消除掉，那么可能还是需要建立准确的点和点之间的对应，这个对于基于语义的方法来说目前还是有难度的。</p><p>嘉宾：时间有限，非常感谢章老师的精彩报告。</p><p>（本报告根据速记整理）</p><p class=ql-align-center><br></p><p class=ql-align-center>CAAI原创 丨 作者章国锋</p><p class=ql-align-center>未经授权严禁转载及翻译</p><p class=ql-align-center><strong>如需转载合作请向学会或本人申请</strong></p><p class=ql-align-center><strong>转发请注明转自中国人工智能学会</strong></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'第七期','CAAI','AIDL'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>