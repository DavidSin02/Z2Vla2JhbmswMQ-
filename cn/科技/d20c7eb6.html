<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>纯干货|Boosting家族之GBDT | 极客快訊</title><meta property="og:title" content="纯干货|Boosting家族之GBDT - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p9.pstatp.com/large/pgc-image/e8608f5a5496448bb7926da57722748a"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d20c7eb6.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d20c7eb6.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d20c7eb6.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d20c7eb6.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d20c7eb6.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d20c7eb6.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d20c7eb6.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d20c7eb6.html><meta property="article:published_time" content="2020-10-29T21:12:41+08:00"><meta property="article:modified_time" content="2020-10-29T21:12:41+08:00"><meta name=Keywords content><meta name=description content="纯干货|Boosting家族之GBDT"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/d20c7eb6.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>纯干货|Boosting家族之GBDT</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p>本文就对Boosting家族中另一个重要的算法梯度提升树(Gradient Boosting Decison Tree, 以下简称GBDT)做一个总结。GBDT有很多简称，有GBT（Gradient Boosting Tree）, GTB（Gradient Tree Boosting ）， GBRT（Gradient Boosting Regression Tree）, MART(Multiple Additive Regression Tree)，其实都是指的同一种算法，本文统一简称GBDT。GBDT在BAT大厂中也有广泛的应用，假如要选择3个最重要的机器学习算法的话，个人认为GBDT应该占一席之地。</p><h1><strong>1. GBDT概述</strong></h1><p>GBDT也是集成学习Boosting家族的成员，但是却和传统的Adaboost有很大的不同。回顾下Adaboost，我们是利用前一轮迭代弱学习器的误差率来更新训练集的权重，这样一轮轮的迭代下去。GBDT也是迭代，使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。</p><p>在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/e8608f5a5496448bb7926da57722748a><p class=pgc-img-caption></p></div><p>, 损失函数是</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7252490b4f244773a775ab4197e15181><p class=pgc-img-caption></p></div><p>, 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/cfec7fa3a4ad46bda3caad773249a15f><p class=pgc-img-caption></p></div><p>，让本轮的损失损失</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/39b006524e694376b8f7d29866855382><p class=pgc-img-caption></p></div><p>最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。</p><p>GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。</p><p>从上面的例子看这个思想还是蛮简单的，但是有个问题是这个损失的拟合不好度量，损失函数各种各样，怎么找到一种通用的拟合方法呢？</p><h1><strong>2. GBDT的负梯度拟合</strong></h1><p>必须要澄清的误区：提起决策树（DT, Decision Tree) 绝大部分人首先想到的就是C4.5分类决策树。但如果一开始就把GBDT中的树想成分类树，那就是一条歪路走到黑，一路各种坑，最终摔得都要咯血了还是一头雾水，所以说千万不要以为GBDT是很多棵分类树。决策树分为两大类，回归树和分类树。前者用于预测实数值，如明天的温度、用户的年龄、网页的相关程度；后者用于分类标签值，如晴天/阴天/雾/雨、用户性别、网页是否是垃圾页面。这里要强调的是，前者的结果加减是有意义的，如10岁+5岁-3岁=12岁，后者则无意义，如 男+男+女=到底是男是女？ GBDT的核心在于累加所有树的结果作为最终结果，每棵树学的是之前所有树结论和的残差。这也就是“Boost”思想的应用，就像前面对年龄的累加（-3是加负3），而分类树的结果显然是没办法累加的，所以GBDT中的树都是回归树，不是分类树，这点对理解GBDT相当重要（尽管GBDT调整后也可用于分类但不代表GBDT的树是分类树）。</p><p>这里的残差是怎样计算的呢？？所以“G” 就派上用场了，也就是梯度下降思想。梯度下降作用的是损失函数，使其损失函数迭代到极小值。在GBDT 中处理不同的问题，其损失函数是不一样的。</p><p>在上一节中，我们介绍了GBDT的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，大牛Freidman提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。第t轮的第i个样本的损失函数的负梯度表示为</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8c28600591ce4e9ebc4c531824dff963><p class=pgc-img-caption></p></div><p>利用</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a5f956c37f1f45d6962839e36fce662f><p class=pgc-img-caption></p></div><p>,我们可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b91b4cf2924f44fc8f6aa889d1e3d134><p class=pgc-img-caption></p></div><p>。其中J为叶子节点的个数。</p><p>针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/887badda4aeb4876bf1b6699d7e9c8f1><p class=pgc-img-caption></p></div><p>如下：</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6c2c4f4d7bf14b82a943e9159102aeb5><p class=pgc-img-caption></p></div><p>这样我们就得到了本轮的决策树拟合函数如下：</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b29e344e717e44928bd4b23e54758a59><p class=pgc-img-caption></p></div><p>从而本轮最终得到的强学习器的表达式如下：</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d34f55a6f5194626a30e07c978de35ae><p class=pgc-img-caption></p></div><p>通过损失函数的负梯度来拟合，我们找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。</p><h1><strong>3. GBDT回归算法</strong></h1><p>好了，有了上面的思路，下面我们总结下GBDT的回归算法。为什么没有加上分类算法一起？那是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们在下一节讲。</p><p>输入是训练集样本</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c079c68d1f5649bb996de366f29c04af><p class=pgc-img-caption></p></div><p>， 最大迭代次数T, 损失函数L。</p><p>输出是强学习器f(x)</p><p>1) 初始化弱学习器</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e7af2386b1b24fa48465ce18ea6a53e6><p class=pgc-img-caption></p></div><p>2) 对迭代轮数t=1,2,...T有：</p><p>a)对样本i=1,2，...m，计算负梯度</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dfd388888b5f444a9c92fb6a6ba4dc5f><p class=pgc-img-caption></p></div><p>b)利用</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/25f6d0d5c7e24944a023bfca13cf5495><p class=pgc-img-caption></p></div><p>拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/177b59f2b24c4e5183178cc60a5259f1><p class=pgc-img-caption></p></div><p>其中J为回归树t的叶子节点的个数。</p><p>c) 对叶子区域j =1,2,..J,计算最佳拟合值</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15f9fa88ea6c41cc8b314ea7ce6a3c1a><p class=pgc-img-caption></p></div><p>d) 更新强学习器</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c91804d8e84c431fbc90060afaf52e1d><p class=pgc-img-caption></p></div><p>3) 得到强学习器f(x)的表达式</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1b888f056c3e4ec1b65a738bacb04915><p class=pgc-img-caption></p></div><h1><strong>4. GBDT分类算法</strong></h1><p>这里我们再看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。</p><p>为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。</p><p><strong>4.1 二元GBDT分类算法</strong></p><p>对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为：</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e24a8e9ddd1d4b3e9ee9ab0cf88683cf><p class=pgc-img-caption></p></div><p>其中y∈{−1,+1}</p><p>。则此时的负梯度误差为</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b9d5078809654ceaaaffaf6057cf8a48><p class=pgc-img-caption></p></div><p>对于生成的决策树，我们各个叶子节点的最佳残差拟合值为</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3c95c502519a476e804a8d2f4373bd9e><p class=pgc-img-caption></p></div><p>由于上式比较难优化，我们一般使用近似值代替</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cc6a4621a77f401cb38b9daaac5172de><p class=pgc-img-caption></p></div><p>除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同。</p><p><strong>4.2 多元GBDT分类算法</strong></p><p>多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为：</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/e4a643251ede4716aa6bc1411b7565e5><p class=pgc-img-caption></p></div><p>其中如果样本输出类别为k，则</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b09c3f87143243248c90f17dc876b9f4><p class=pgc-img-caption></p></div><p>。第k类的概率</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4ac6a53edb244607ac37814bdce44458><p class=pgc-img-caption></p></div><p>的表达式为：</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5705e993bf3c4cd0a2abaa5bf2531bc2><p class=pgc-img-caption></p></div><p>集合上两式，我们可以计算出第t</p><p>t轮的第i</p><p>i个样本对应类别l</p><p>l的负梯度误差为</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/6969f46b63f5444eb88b2b0179ba2134><p class=pgc-img-caption></p></div><p>观察上式可以看出，其实这里的误差就是样本i</p><p>i对应类别l</p><p>l的真实概率和t−1</p><p>轮预测概率的差值。</p><p>对于生成的决策树，我们各个叶子节点的最佳残差拟合值为</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f35965f511c542a0a8ed474fd4a0f7e3><p class=pgc-img-caption></p></div><p>由于上式比较难优化，我们一般使用近似值代替</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1178d0a8584c4a94b45e356b4c96d300><p class=pgc-img-caption></p></div><p>除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。</p><h1><strong>5. GBDT常用损失函数</strong></h1><p>这里我们再对常用的GBDT损失函数做一个总结。</p><p>对于分类算法，其损失函数一般有对数损失函数和指数损失函数两种:</p><p>a) 如果是指数损失函数，则损失函数表达式为</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/56060f7e00ff46c2a1a2a975c27990eb><p class=pgc-img-caption></p></div><p>其负梯度计算和叶子节点的最佳残差拟合参见Adaboost原理篇。</p><p>b) 如果是对数损失函数，分为二元分类和多元分类两种，参见4.1节和4.2节。　</p><p>对于回归算法，常用损失函数有如下4种:</p><p>a)均方差，这个是最常见的回归损失函数了</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ed51ed331a3c4189be4dec2f314f183c><p class=pgc-img-caption></p></div><p>b)绝对损失，这个损失函数也很常见</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/74efba45ab5b414a9955a2909a5a7f40><p class=pgc-img-caption></p></div><p>对应负梯度误差为：</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d0526c395dee49e38616d96f591152c6><p class=pgc-img-caption></p></div><p>c) Huber损失，它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。损失函数如下：</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3a4ed48579e54d0aaa41d90abaa1b100><p class=pgc-img-caption></p></div><p>对应的负梯度误差为：</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/fa436fa98bd64854a15b64771f981202><p class=pgc-img-caption></p></div><p>d) 分位数损失。它对应的是分位数回归的损失函数，表达式为</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/586d3edc327a49478e2a4d289ebd8a4b><p class=pgc-img-caption></p></div><p>其中θ</p><p>为分位数，需要我们在回归前指定。对应的负梯度误差为：</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4c15bbf91b504287a13fe4efb18d8430><p class=pgc-img-caption></p></div><p>对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。</p><h1><strong>6. GBDT的正则化</strong></h1><p>和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。GBDT的正则化主要有三种方式。</p><p>第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为ν</p><p>ν,对于前面的弱学习器的迭代</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/67a49355bb4d4076a05d57cffbf0c02a><p class=pgc-img-caption></p></div><p>如果我们加上了正则化项，则有</p><div class=pgc-img><img alt=纯干货|Boosting家族之GBDT onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/116f83a461db4f6786a77c001b239f33><p class=pgc-img-caption></p></div><p>ν的取值范围为0&lt;ν≤1</p><p>0&lt;ν≤1。对于同样的训练集学习效果，较小的ν</p><p>ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p><p>第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。</p><p>使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。</p><p>第三种是对于弱学习器即CART回归树进行正则化剪枝。在决策树原理篇里我们已经讲过，这里就不重复了。</p><h1><strong>7. GBDT小结　</strong></h1><p>GBDT终于讲完了，GDBT本身并不复杂，不过要吃透的话需要对集成学习的原理，决策树原理和各种损失函树有一定的了解。由于GBDT的卓越性能，只要是研究机器学习都应该掌握这个算法，包括背后的原理和应用调参方法。目前GBDT的算法比较好的库是xgboost。当然scikit-learn也可以。</p><p>最后总结下GBDT的优缺点。</p><p>GBDT主要的优点有：</p><p>1) 可以灵活处理各种类型的数据，包括连续值和离散值。</p><p>2) 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。</p><p>3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</p><p>GBDT的主要缺点有：</p><p>1)由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。</p><p>参考：</p><p>1、https://www.cnblogs.com/pinard/p/6140514.html</p><p>2、https://blog.csdn.net/zbj366112/article/details/70037865</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'纯干货','Boosting','GBDT'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>