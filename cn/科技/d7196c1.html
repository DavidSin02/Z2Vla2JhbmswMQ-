<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>手工打造神经网络：透视分析 | 极客快訊</title><meta property="og:title" content="手工打造神经网络：透视分析 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/6ee200033390f3f6b2ca"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d7196c1.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d7196c1.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d7196c1.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d7196c1.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d7196c1.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d7196c1.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d7196c1.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d7196c1.html><meta property="article:published_time" content="2020-10-29T21:05:34+08:00"><meta property="article:modified_time" content="2020-10-29T21:05:34+08:00"><meta name=Keywords content><meta name=description content="手工打造神经网络：透视分析"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/d7196c1.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>手工打造神经网络：透视分析</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><h1>内容导读</h1><blockquote><p>MNIST只包含70000张28x28像素的手写数字的单通道灰度图，对于现在的算力来说是很小的数据。我定义一个模型训练函数，目标是不断优化权重和偏差得到最佳组合。现在输入层到隐藏层我选择了ReLu作为激活函数，从图形中看出它具备左侧硬饱和的特性。接下来自然就会思考，权重和偏差如何迭代优化呢? 首先正向传播计算出output_layer节点，用损失函数计算一下和标注y的差距，根据损失大小返回来修正权重和偏差，这个通过链式法则对多层复合函数求导的过程就是反向传播，其目标就是要最小化训练集上的累积误差。第二种方式是每跑训练集中的一条数据就计算损失函数并更新参数，速度比较快，但收敛性不太好，可能会出现较多毛刺在最优点附近摇摆。完成模型训练代码后可以再定义一个交叉熵损失函数来观察收敛效果，迭代到三五千次的时候其实已经差不多了，后面的几万次学习依然会不时出现一些毛刺影响准确率。</p></blockquote><p>MNIST就是机器学习的Hello World, 或者说图片处理的Lena, 是必不可少的经典初体验。它已有20多年的历史，但是到今天依然魅力不减，依然是最高引用的数据集。MNIST只包含70000张28x28像素的手写数字的单通道灰度图，对于现在的算力来说是很小的数据。</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/6ee200033390f3f6b2ca></p><p>Denise Krebs on Flickr</p><p>网上大多数的MNIST教程，包括TensorfFlow官方教程带给我们的都是一种自然主义的学习体验，给出代码示例简单教会步骤，对着代码敲一下就能运行，毕竟参与感不够。从结构主义的学习方式来看，我们应该至少尝试一次不用任何深度学习的现成框架，纯手工从零开始实现一次神经网络，并且打开其中的黑盒，以可视化的呈现方式形象理解神经网络如何工作，这就是我写此文的目的。</p><p><strong>数据准备 (Data Preparation)</strong></p><p>MNIST数据集中Train dataset有60000张图片与相应的标注，其中55000张训练集，5000张验证集(Validation)，Test dataset有10000张训练集，下载完这四个文件后我保存到MNIST-data目录下。</p><blockquote><p>train-images-idx3-ubyte.gz: training set images (9912422 bytes)<br>train-labels-idx1-ubyte.gz: training set labels (28881 bytes)<br>t10k-images-idx3-ubyte.gz: test set images (1648877 bytes)<br>t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes)</p></blockquote><p>数据集就是这么四个文件，都是特殊的Binary格式。解析起来费点功夫，不过TensorFlow已封装好了便利的接口 - input_data.read_data_sets。虽然这次我不用TensorFlow框架的神经网络，但是解析数据部分借用一下无妨。这个函数会自动尝试下载数据集到指定目录中，不过强烈建议自己手工下载好这四个文件，由于不可描述的原因，用这个方法直接下载数据基本都是以time out失败告终。第一个参数是指定的数据集存放路径，第二个参数决定是否以独热键(one-hot)形式读取标签，如果设为True则以10维向量形式代表一个数字。</p><blockquote><p>mndata = input_data.read_data_sets("MNIST-data/", one_hot=True)</p></blockquote><p>分别获取训练集和测试集的图片和标注</p><blockquote><p>X_train=mndata.train.images # training set<br>y_train=mndata.train.labels<br>X_test=mndata.test.images # testing set<br>y_test=mndata.test.labels</p></blockquote><p>然后写一个画图的函数，读取矩阵数据在表格中展示，仅展示非0数字且保留两位小数。</p><blockquote><p># visualize grid data of a matrix, zero cell shown as empty<br>def plt_grid(data):<br>fig, ax = plt.subplots()<br>fig.set_size_inches(30,30)<br>width, height = data.shape</p><p>#imshow portion<br>imshow_data = np.random.rand(width, height)<br>ax.imshow(imshow_data, cmap=plt.cm.Pastel1, interpolation='nearest')</p><p>for x in range(0, height):<br>for y in range(0, width):<br>if (data[y][x]>0):<br>ax.text(x, y, np.round(data[y][x],2), va='center', ha='center', fontsize=20)<br>plt.show()</p></blockquote><p>随便找个吉利数字88作为index, 从训练集中抽一张图片打印原始数据看看这个28x28的矩阵里到底存放了什么</p><blockquote><p>plt_grid(X_train[88].reshape(28,28))</p></blockquote><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/6ee60001d25df9767538></p><p>数字3</p><p>非0的数值本身就已经能看到数字的形状了，是个3. 数值越接近1表示颜色越白，边缘的颜色应该是比较灰的，而背景数值为0自然就是黑色。</p><p>打印相应的标签出来也是3: [ 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]</p><p>下面我再直接把图片抽出来打印对比一下，果不其然, 和上图长的一模一样。</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/6ee60001d25e98f04787></p><p><strong>网络架构 (Network Layouts)</strong></p><p>数据准备好后就开始设计神经网络了, 简单一点就三层: input layer, hidden layer, output layer</p><p>input layer(输入层)的节点就是要喂给神经网络的像素值，共784个节点。</p><p>hidden layer(隐藏层)可以让网络在抽象层次上学习特征，虽然我只放一层，但是也可以有多层。少量隐藏层会得到浅层神经网络SNN，隐藏层很多时就是深层神经网络DNN。理论上，单隐藏层神经网络也可以逼近任何连续函数，只要神经元数量够多。如果增加隐藏层或者隐藏层神经元的数量，神经网络的容量会变大，空间表达能力会变强，但如果太多的话也容易过拟合。先暂定15个节点吧。</p><p>output layer(输出层)有10个节点，因为图片要分类映射到十个数字上。</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/6ee50002bef9200dccef></p><p>Neural Network and Deep Learning</p><p><strong>权重和偏差 (Weights and Bias)</strong></p><p>上图中两个神经元之间的每一条连线都代表一个权重值，神经网络通过不断调整权重来逼近结果。首先把一组权重应用在input layer的节点上，加上偏差值后得到hidden layer的节点，然后对hidden layer的节点应用另一组权重，加上偏差值后最终得到output layer的节点。</p><p>先设置一下两组权重和偏差的初始值，后面再看如何更新这些权重和偏差。我定义一个模型训练函数，目标是不断优化权重和偏差得到最佳组合。第一组权重是一个784 x 15的矩阵，第二组权重是一个15 x 10的矩阵，用随机函数生成一堆0到1之间的浮点数值，偏差就先都设为0. 两组权重都除以5是我在调参过程中发现初始权重数值要更小一些效果比较好，随便拍的一个数。</p><blockquote><p>input_layer_size = 28 * 28<br>hidden_layer_size = 15<br>output_layer_size = 10</p><p>def train_model():<br># init weights and bias<br>np.random.seed(1)<br>W1 = np.random.random([input_layer_size, hidden_layer_size])/5 # 784 x 15<br>b1 = np.zeros((1, hidden_layer_size))<br>W2 = np.random.random([hidden_layer_size, output_layer_size])/5 # 15 x 10<br>b2 = np.zeros((1, output_layer_size))</p></blockquote><p>现在可以把权重W1也打印出来看看，我只拿输入层第一个节点和隐藏层第一个简单之间的一根线来查看。可以看到都是非常微小的随机数字，最大不会超过0.2</p><blockquote><p>plt_grid(W1.T[0].reshape(28,28))</p></blockquote><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/6ee400030db6324f2555></p><p>第一条线权重</p><p>后来在训练50000次之后可以用plt将十个数字权重的热力图可视化呈现:</p><blockquote><p>for i in range(10):<br>plt.subplot(2, 5, i+1)<br>weight = W1[:,i]<br>plt.title(i)<br>plt.imshow(weight.reshape([28,28]), cmap=plt.get_cmap('seismic'))<br>frame1 = plt.gca()<br>frame1.axes.get_xaxis().set_visible(False)<br>frame1.axes.get_yaxis().set_visible(False)</p></blockquote><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/6ee2000333910cb8c03c></p><p>Weight Heatmap</p><p><strong>激活函数 (Activation Function)</strong></p><p>Montreal 大学的 Bengio 教授在 ICML 2016 中给出了激活函数定义: 激活函数是映射 h:R→R，且几乎处处可导。</p><p>引入激活函数是为了将权值转化为分类结果，有多重选择: Sigmoid(S型), Tanh(双切正切), ReLu(只保留非零), Softmax(归一化) etc. 这些常用的激活函数多数都是非线性的，为了弥补线性函数区分度不够好的短板，而且激活函数要能保证数据输入与输出也是可微的。本来我尝试用Sigmoid作为激活函数，但是可能由于我的实现方式导致效果不好，准确率到70%多我就优化不下去了，可能它本身由于软饱和性也容易出现梯度消失的问题，只好暂时放弃。</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/6ee400030db7c8a5f18f></p><p>上面这句话我再解释一下，Sigmoid就是处处可导的S型曲线，且两侧导数趋近于0，所以它是一个软饱和函数，而且左右两侧都是软饱和。一旦落入了软饱和区f'(x)就接近于0了，无法再继续传递梯度，这就是所谓的梯度消失。</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/6ee70000b641874e0002></p><p>Sigmoid图形</p><p>现在输入层到隐藏层我选择了ReLu作为激活函数，从图形中看出它具备左侧硬饱和的特性。</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/6ee50002befac5b3752a></p><p>ReLu Formula</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/6ee60001d25f8187a1ea></p><p>ReLu 图形</p><p>代码实现如下</p><blockquote><p>def relu(x):<br>return np.maximum(x, 0)</p></blockquote><p>从隐藏层到输出层我选择了softmax作为激活函数</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/6ee70000b6427c984347></p><p>Softmax Formula</p><p>序列中最大的那个数映射的分量逼近于 1, 其他就逼近于 0，非常适合多分类问题。取指数是为了让马太效应凸显，大数进一步放大，同时也满足了可导函数的需求。</p><p>Softmax代码实现如下 (如果出现overflow的话可以参考scikit-learn源码的实现方式)</p><blockquote><p>def softmax(x):<br>e_x = np.exp(x - np.max(x))<br>return e_x / e_x.sum()</p></blockquote><p><strong>正向传播(Forward Propagation)</strong></p><p>正向传播的计算就是把输入层到隐藏层的节点与权重和偏差结合，计算出输出层节点的过程。<br>对于这个三层网络，假定x是包含一个单一训练样本的列向量。则向量化的正向传播步骤如下：(这个图我画完以后感觉用更严谨的方式来描述的话，四个标注应该是隐藏层的输入，隐藏层的输出，输出层的输入，输出层的输入，但懒得改图了)</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/6ee200033392be486202></p><p>Forward Propagation</p><p>假设我要训练50000次，train_model方法中加入正向传播的循环代码实现如下</p><blockquote><p>batch= 50000<br>for i in range(0, batch):<br>X = X_train[i]<br>y = y_train[i]</p><p>input_layer = X.dot(W1)<br>hidden_layer = relu(input_layer + b1)<br>output_layer = np.dot(hidden_layer, W2) + b2<br>output_probs = softmax(output_layer)</p></blockquote><p>我还是继续拿index为88的数字3图片为例看看各层是什么数字</p><p>input_layer是输入层矩阵与权重1矩阵相乘得到15维向量</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/6ee70000b643db70a946></p><p>input_layer</p><p>hidden_layer是上一层加上偏差1作为ReLu的输入计算出来的, 维度同上</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/6ee10003f4e8682c1c90></p><p>Hidden Layer</p><p>此时权重2是15x10的矩阵</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/6ee60001d260fa9e6fff></p><p>W2</p><p>权重2和hidden_layer矩阵相乘加上偏差2得到十维向量output_layer, 这里最大的数字是第九位的20.09</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/6ee2000333933ca14ae4></p><p>output_layer</p><p>output_layer作为softmax输入计算后得到最终结果output_probs, 第九位被转成了非常接近1的一个小数，也是序列中最大的数字。这是训练之初的数值，实际上最大的数字应该在第四位，所以此时误差比较大。大概在学习3000次之后已经能较大概率的在第四位逼近1</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/6ee50002befb6df58064></p><p>output_probs</p><p><strong>反向传播 (Backward Propagation)</strong></p><p>接下来自然就会思考，权重和偏差如何迭代优化呢? 首先正向传播计算出output_layer节点，用损失函数计算一下和标注y的差距，根据损失大小返回来修正权重和偏差，这个通过链式法则对多层复合函数求导的过程就是反向传播，其目标就是要最小化训练集上的累积误差。</p><p>在前文"<a target=_blank>人工神经元是如何模拟生物神经元的</a>"中我提到机器学习需要不断调整weight和bias来逐步逼近预期的输出值，<strong>而且必须保证weight和bias的微小变化也只会带来输出值的微小变化.</strong></p><p>调参的过程就像打高尔夫一样，目标是以最少的杆数将球打进洞，如果过于谨慎可能耗费的杆数太多，如果太过激进可能球被打进了沙池或者水坑，欲速则不达。每一杆都要让球离球洞更近，进入果岭的时候还要确保不要用力过度让球跑过头了。(见下文<strong>学习率</strong>)</p><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/6ee70000b6445aa7d4e4></p><p>在train_model方法中继续实现这个逆向过程, 计算输出层的error, 再将此error逆向传播到隐藏层，最后根据隐藏层的error来对连接权重与偏差进行调整，迭代循环下去不断更新让error收敛。核心逻辑是梯度下降的算法，这里有三种选择: 批量梯度下降(Batch Gradient Descent)，随机梯度下降(Stochastic Gradient Descent)和小批量梯度下降。</p><p>第一种方式遍历完整训练集算出一个损失函数，然后更新参数再跑一次完整训练集，如此迭代循环，所以计算量很恐怖。第二种方式是每跑训练集中的一条数据就计算损失函数并更新参数，速度比较快，但收敛性不太好，可能会出现较多毛刺在最优点附近摇摆。最后一个是前两者的这种方案，既不是跑全量数据而不是跑单个数据，而是拿一小批数据来计算损失函数更新参数。我先用SGD来跑，后面可以通过图像看到毛刺的问题。</p><p>用数学语言来描述，梯度下降算法的核心是<strong>多元函数求微</strong>，针对每一个变量都分别求微，每一次迭代都用多元函数减去多元函数的微分与学习率的乘积。下面代码中第一行设置的参数是学习率，这个参数是要在精度和速度之间找到平衡，学习率太大则训练的快但精度不够(每次击球都很大力)，学习率太小则提升精度但过于耗费时间(每次击球都小心翼翼轻轻挥杆保证精准)，这里设置的学习率是固定的，这样的静态设置显然不会是最佳选择，不过处于学习目的也够了。</p><blockquote><p>learning_rate = .01<br>reg_lambda = .01</p><p>output_error = (output_probs - y) / output_probs.shape[0]</p><p>hidden_error = np.dot(output_error, W2.T)<br>hidden_error[hidden_layer &lt;= 0] = 0</p><p># gradient layer2 weights and bias<br>g2_weights = np.dot(hidden_layer.T, output_error)<br>g2_bias = np.sum(output_error, axis = 0, keepdims = True)</p><p># gradient layer1 weights and bias<br>g1_weights = np.dot(X.reshape(input_layer_size,1), hidden_error)<br>g1_bias = np.sum(hidden_error, axis = 0, keepdims = True)</p><p># gradient descent parameter update<br>W1 -= learning_rate * g1_weights<br>b1 -= learning_rate * g1_bias<br>W2 -= learning_rate * g2_weights<br>b2 -= learning_rate * g2_bias</p></blockquote><p><strong>正则化干扰 (Regularization Terms)</strong></p><p>为了让拟合效果更好可以在error后面加入正则干扰项, 让模型和样本不要完全拟合，当出现欠拟时干扰项的影响要小，当出现过拟时干扰项的影响要大。reg_lambda就是设置的拟合参数，所以可以在更新w和b之前再加两行代码。</p><blockquote><p># add regularization terms<br>g2_weights += reg_lambda * W2<br>g1_weights += reg_lambda * W1</p></blockquote><p><strong>预测函数</strong></p><p>这个逻辑很简单，和训练集的正向传播完全一样，只不过输入参数换成测试集数据，代入已经训练好的权重和偏差，统计正确预测的比例。</p><blockquote><p>input_layer = np.dot(X_test[:10000], W1)<br>hidden_layer = relu(input_layer + b1)<br>scores = np.dot(hidden_layer, W2) + b2<br>probs = softmax(scores)<br>print ('Test accuracy: {0}%'.format(accuracy(probs, y_test[:10000])))</p></blockquote><p>完成模型训练代码后可以再定义一个交叉熵损失函数来观察收敛效果，迭代到三五千次的时候其实已经差不多了，后面的几万次学习依然会不时出现一些毛刺影响准确率。</p><blockquote><p>def cross_entropy_loss(probs, y_onehot):<br>indices = np.argmax(y_onehot, axis = 0).astype(int)<br>predicted_prob = probs[np.arange(len(probs)), indices]<br>log_preds = np.log(predicted_prob)<br>loss = -1.0 * np.sum(log_preds) / len(log_preds)<br>return loss</p></blockquote><p><img alt=手工打造神经网络：透视分析 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/6ee200033394c684d879></p><p>最后把上面的代码重构一下整合起来贴出完整代码(不包含调试打印图片和日志)，预测准确率90%, 下一篇我将改用TensorFlow对Fashion MNIST预测，并提升准确率。</p><blockquote><p>import numpy as np</p><p>from tensorflow.examples.tutorials.mnist import input_data</p><p>import matplotlib.pyplot as plt</p><p>mndata = input_data.read_data_sets("MNIST-data/", one_hot=True)</p><p>X_train=mndata.train.images # training set</p><p>y_train=mndata.train.labels</p><p>X_test=mndata.test.images # testing set</p><p>y_test=mndata.test.labels</p><p>input_layer_size = 28 * 28</p><p>hidden_layer_size = 15</p><p>output_layer_size = 10</p><p>reg_lambda = .01</p><p>learning_rate = .01</p><p># visualize grid data of a matrix, zero cell shown as empty</p><p>def plt_grid(data):</p><p>fig, ax = plt.subplots()</p><p>fig.set_size_inches(30,30)</p><p>width, height = data.shape</p><p>#imshow portion</p><p>imshow_data = np.random.rand(width, height, 2)</p><p>ax.imshow(imshow_data, cmap=plt.cm.Pastel1, interpolation='nearest')</p><p>for x in range(0, height):</p><p>for y in range(0, width):</p><p>if (data[y][x]>0):</p><p>ax.text(x, y, np.round(data[y][x],8), va='center',</p><p>ha='center', fontsize=20)</p><p>plt.show()</p><p>def softmax(x):</p><p>e_x = np.exp(x - np.max(x))</p><p>return e_x / e_x.sum()</p><p>def relu(x):</p><p>return np.maximum(x, 0)</p><p>def cross_entropy_loss(probs, y_onehot):</p><p>indices = np.argmax(y_onehot, axis = 0).astype(int)</p><p>predicted_prob = probs[np.arange(len(probs)), indices]</p><p>log_preds = np.log(predicted_prob)</p><p>loss = -1.0 * np.sum(log_preds) / len(log_preds)</p><p>return loss</p><p># init weights and bias</p><p>def init_weights_bias():</p><p>np.random.seed(1)</p><p>W1 = np.random.random([input_layer_size, hidden_layer_size])/5 # 784 x 15</p><p>b1 = np.zeros((1, hidden_layer_size))</p><p>W2 = np.random.random([hidden_layer_size, output_layer_size])/5 # 15 x 10</p><p>b2 = np.zeros((1, output_layer_size))</p><p>model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}</p><p>return model</p><p># derivative weights and bias</p><p>def derivative_weights_bias(output_error, hidden_layer, X, model):</p><p>W1, _, W2, _ = model['W1'], model['b1'], model['W2'], model['b2']</p><p>hidden_error = np.dot(output_error, W2.T)</p><p>hidden_error[hidden_layer &lt;= 0] = 0</p><p># gradient layer2 weights and bias</p><p>g2_weights = np.dot(hidden_layer.T, output_error)</p><p>g2_bias = np.sum(output_error, axis = 0, keepdims = True)</p><p># gradient layer1 weights and bias</p><p>g1_weights = np.dot(X.reshape(input_layer_size,1), hidden_error)</p><p>g1_bias = np.sum(hidden_error, axis = 0, keepdims = True)</p><p># add regularization terms</p><p>g2_weights += reg_lambda * W2</p><p>g1_weights += reg_lambda * W1</p><p>param = { 'dW1': g1_weights, 'db1': g1_bias, 'dW2': g2_weights, 'db2': g2_bias}</p><p>return param</p><p>def forward_propagation(X, model):</p><p>W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']</p><p>input_layer = np.dot(X, W1)</p><p>hidden_layer = relu(input_layer + b1)</p><p>output_layer = np.dot(hidden_layer, W2) + b2</p><p>probs = softmax(output_layer)</p><p>return probs, hidden_layer</p><p>def accuracy(predictions, labels):</p><p>preds_correct_boolean = np.argmax(predictions, 1) == np.argmax(labels, 1)</p><p>correct_predictions = np.sum(preds_correct_boolean)</p><p>accuracy = 100.0 * correct_predictions / predictions.shape[0]</p><p>return accuracy</p><p>#predict test set</p><p>def predict(X, model):</p><p>W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']</p><p>input_layer = np.dot(X_test[:10000], W1)</p><p>hidden_layer = relu(input_layer + b1)</p><p>output_layer = np.dot(hidden_layer, W2) + b2</p><p>probs = softmax(output_layer)</p><p>print ('Test accuracy: {0}%'.format(accuracy(probs, y_test[:10000])))</p><p># - batch: Size of passes through the training data for gradient descent</p><p>def train_model(batch, X, y):</p><p>model = init_weights_bias()</p><p>W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']</p><p># Gradient descent. For each batch...</p><p>for i in range(0, batch):</p><p>output_probs, hidden_layer = forward_propagation(X[i], model)</p><p>output_error = (output_probs - y[i]) / output_probs.shape[0]</p><p>param = derivative_weights_bias(output_error, hidden_layer, X[i], model)</p><p>dW1, db1, dW2, db2 = param['dW1'], param['db1'], param['dW2'], param['db2']</p><p># gradient descent parameter update</p><p>W1 -= learning_rate * dW1</p><p>b1 -= learning_rate * db1</p><p>W2 -= learning_rate * dW2</p><p>b2 -= learning_rate * db2</p><p>model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}</p><p>loss = cross_entropy_loss(output_probs, y[i])</p><p>if (i % 2000 == 0):</p><p>print('loss @ %d is %f' % (i, loss))</p><p>return model</p><p>model = train_model(50000, X_train[:50000], y_train[:50000])</p><p>predict(X_test[:10000], model)</p></blockquote><p><em>References:</em></p><p><em><a rel=nofollow target=_blank>Neural Network Vectorization</a></em></p><p><em><a rel=nofollow target=_blank>Neural Network and Deep Learning</a></em></p><p><em>Not another MNIST tutorial with TensorFlow OReilly Media</em></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'神经','网络','透视'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>