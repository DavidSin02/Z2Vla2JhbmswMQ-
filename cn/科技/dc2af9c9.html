<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 | 极客快訊</title><meta property="og:title" content="机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/dc2af9c9.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/dc2af9c9.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/dc2af9c9.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/dc2af9c9.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/dc2af9c9.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/dc2af9c9.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/dc2af9c9.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/dc2af9c9.html><meta property="article:published_time" content="2020-11-14T21:08:22+08:00"><meta property="article:modified_time" content="2020-11-14T21:08:22+08:00"><meta name=Keywords content><meta name=description content="机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/dc2af9c9.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><h1 class=pgc-h-arrow-right>一、算法概述</h1><p>逻辑回归(Logistic)虽带有回归二字，但它却是一个经典的二分类算法，它适合处理一些二分类任务，例如疾病检测、垃圾邮件检测、用户点击率以及上文所涉及的正负情感分析等等。</p><p>首先了解一下何为回归？假设现在有一些数据点，我们利用一条直线对这些点进行拟合(该线称为最佳拟合直线)，这个拟合的过程就称作回归。利用逻辑回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。</p><p>线性回归算法后面的笔记会介绍，这里简单对比一下两者，逻辑回归和线性回归的本质相同，都意在拟合一条直线，但线性回归的目的是拟合<strong>输入变量的分布</strong>，尽可能让所有样本到该条直线的距离最短；而逻辑回归的目的是拟合<strong>决策边界</strong>，使数据集中不同的样本尽可能分开，所以两个算法的目的是不同的，处理的问题也不同。</p><h1 class=pgc-h-arrow-right>二、Sigmoid函数与相关推导</h1><p>我们想要的函数应该是，能接受所有的输入并且预测出类别，比如二分类中的0或者1、正或者负，这种性质的函数被称为海维赛德阶跃函数，图像如下：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051><p class=pgc-img-caption></p></div><p>但这种函数的问题在于从0跳跃到1的过程非常难处理，比如我们常接触的多次函数，可能在某种条件下需要求导解决问题；而<strong>Sigmoid</strong>函数也具有类似的性质，并且在数学上更容易处理，其公式如下：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ce0599fb4c8c478695af3b062381f2b5><p class=pgc-img-caption></p></div><p>下图是<strong>Sigmoid</strong>函数在不同座标尺度下的两条曲线图。当x为0时，<strong>Sigmoid</strong>函数值为0.5，随着x的增大，对应的Sigmoid值将逼近于1；而随着x的减小，<strong>Sigmoid</strong>值将逼近于0。如果横座标刻度足够大，<strong>Sigmoid</strong>函数看起来就很像一个阶跃函数。</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7147661fa0314b0f8a1a82fb890f8ee4><p class=pgc-img-caption></p></div><p>若我们将<strong>Sigmoid</strong>函数的输入记作z，可推出下面公式：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b6708f9ca2d645f9b56b87d51d8f1e34><p class=pgc-img-caption></p></div><p>它表示将这两个数值向量对应元素相乘然后全部相加起来得到z值，其中向量x是分类器的输入数据，向量w就是我们要找到的能使分类器尽可能准确的最佳参数。</p><p>由上述公式就可得：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c1c7c20be795491194f3ce30afbecb4a><p class=pgc-img-caption></p></div><p>其中h_w(x)的作用就是给定输入时，输出分类为正向类(1)的可能性。例如，对于一个给定的x，h_w(x)的值为0.8，则说明有80%的可能输出为正向类(1)，有20%的可能输出为负向类(0)，二者成补集关系。</p><p>对于一个二分类问题而言，我们给定输入，函数最终的输出只能有两类，0或者1，所以我们可以对其分类。</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a6ea21e24c90440ea32f0e8e2e1cd296><p class=pgc-img-caption></p></div><p>为了运算便捷，我们将其整合为一个公式，如下：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f82ceeefb01c40f49886bd23e5e09af4><p class=pgc-img-caption></p></div><p>由于乘法计算量过大，所以可以将乘法变加法，对上式求对数，如下：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3bec757f6192428bbdd8a85bb8ae89c0><p class=pgc-img-caption></p></div><p>可以看出当y=1时，加号后面式子的值为0；当y=0时，加号前面式子的值为0，这与上文分类式子达到的效果是一样的。L(w)称为似然函数，J(w)称为对数似然函数，是依据最大似然函数推导而成。此时的应用是梯度上升求最大值，如果梯度下降求最小值，可在公式之前乘以-1/n。</p><p>为了学习嘛，这里再介绍一下另一种方式，利用<strong>损失函数</strong>推导应用于梯度下降的公式；损失函数是衡量<strong>真实值与预测值</strong>之间差距的函数，所以损失函数值越小，对应模型的效果也越好，损失函数公式如下：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/261ae2e6279d44978f0e922238b54688><p class=pgc-img-caption></p></div><p>可能只看公式理解相对抽象，通过对应的函数图像足以理解上式，如下：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c30ec56af2614a748cb2a2ee70a44f10><p class=pgc-img-caption></p></div><p>注意！！！<strong>公式后面的y*不代表纵座标</strong> ！！！</p><p>当类标签y=1时，对应的-log(x)图像越接近于1，其距离x轴越近，代表其损失越小；反之当类标签y=0时，对应的-log(1-x)图像越接近于0，其距离x轴越近，代表其损失越小，也就是预测值越接近于真实值。</p><p>将两个损失函数综合起来得：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4f0e9dc7339b413fb32ce95299893533><p class=pgc-img-caption></p></div><p>对于m个样本，总损失函数为：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/91f13091da6c49ed8309073c1db57fad><p class=pgc-img-caption></p></div><p>其中m为样本个数、yi为标签，可取0或1、i为第i个样本、p(x_i)为预测输出。</p><h1 class=pgc-h-arrow-right>三、梯度</h1><h1 class=pgc-h-arrow-right>3.1梯度上升</h1><p>上面已经列出了一大堆的公式，难道这又要开始一连串的大公式？</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dacc5717800043109d50578cbb8239af><p class=pgc-img-caption></p></div><p>心态放平，上文虽说公式有点多，但目的都是为了推出最后的对数似然函数或者总损失函数，掌握这两个是关键，梯度上升和梯度下降也会利用上面的公式做推导，所以二者之间存在关联。首先梯度上升你需要了解一下何为梯度？</p><p>如果将梯度记为▽，则函数f(x,y)的梯度可由下式表示：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/96dabd3f47fd43a1b5791a167653229f><p class=pgc-img-caption></p></div><p>通俗的说，即对多元函数的参数求偏导，并把求得的各个参数的偏导以向量的形式写出来。或者你只要理解这个梯度要沿着x的方向移动delta f(x,y)/delta x，沿着y方向移动delta f(x,y)/delta y足以，但f(x,y)必须要在待计算的点上有定义且可微。</p><p>下图为一个梯度上升的例子，梯度上升法在到达每一个点之后都会重新评估下一步将要移动的方向。从x0开始，在计算完该点的梯度，函数就会移动到下一个点x1。在x1点，梯度会重新计算，继而移动到x2点。如此循环迭代此过程，直到满足停止条件，每次迭代过程都是为了找出当前能选取到的最佳移动方向。</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/611645e9e2494e1c978d4b877c13a33d><p class=pgc-img-caption></p></div><p>之前一直在讨论移动方向，而未提到过移动量的大小。该量值称为步长，记作$\alpha$。那么就可以得出梯度上升法的迭代公式：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ed228fc3c66a44b8b06d8a1ee734c2f9><p class=pgc-img-caption></p></div><p>所以对于上文所提及的对数似然函数J(w)，我们也可以利用上述迭代的方式，一步一步移动到目标值或者无限接近于目标值，J(w)求偏导的公式如下：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e82777b03b9549b787aedbda29a69a35><p class=pgc-img-caption></p></div><p>可能有的人看到这个偏导公式有点蒙，其实这里面用到的三个函数公式都是上文所提及的，来回顾一下。</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eeaa3112bc334decb5493ce6c16ebf7d><p class=pgc-img-caption></p></div><p>求偏导过程涉及到高数知识，即最外层函数对外层函数求偏导、外层函数对内层函数求偏导、内层函数对其元素求偏导，三者相乘可得出所需偏导。推导过程有些麻烦，这里只给出推导结果，在后面运用时我们也只会运用到最终结果，如下：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/7e6b48fd1be14ff3947f1ec40cb9b899><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>3.2梯度下降</h1><p>如果利用将对数似然函数换成损失函数$J(\Theta)$，则得到的是有关计算梯度下降的公式，如下：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/344b06a5345e4ddda7e3f0441b68e031><p class=pgc-img-caption></p></div><p>两个公式中的w和Theta的含义是一样的，都代表我们所求的最佳回归系数，两个公式对比可以看出梯度上升和梯度下降只有加减号区别之分。下面这个动图就可以很好的展示梯度下降法的过程：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d6d73b3169b94e279bd692a2a4d9b851><p class=pgc-img-caption></p></div><p>公式推导部分至此结束了，基础偏好的伙伴可能一遍就懂了，但基础偏弱理解起来比较困难，偶当时也是对著书、跟着视频啃了好久，多啃几遍终归会理解的。</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ad8a721899314a41a24a545d4bc9c51f><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>四、算法应用</h1><h1 class=pgc-h-arrow-right>4.1数据概览</h1><p>有这样一份数据集，共100个样本、两个特征(X1与X2)以及一个分类标签，所绘制图像如下：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0361ebe35ff542ebafdde5842fd238a4><p class=pgc-img-caption></p></div><p>在此数据集上，我们将通过梯度下降法找到最佳回归系数，也就是拟合出Logistic回归模型的最佳参数。 该算法的伪代码如下：</p><pre><code>每个回归系数初始化为1重复R次：    计算整个数据集的梯度    使用alpha*gradient更新回归系数的向量    返回回归系数</code></pre><h1 class=pgc-h-arrow-right>4.2加载数据集</h1><pre><code>def loadDataSet():    dataMat = []    # 创建数据列表    labelMat = []    # 创建标签列表    fr = open('LRData.txt','r',encoding='utf-8')    #逐行读取全部数据    for line in fr.readlines():        #将数据分割、存入列表        lineArr = line.strip().split()        #数据存入数据列表        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])        #标签存入标签列表        labelMat.append(int(lineArr[2]))    fr.close()    return dataMat, labelMat</code></pre><p><strong>loadDataSet</strong>函数的作用是打开存储数据的文本文件并逐行读取。每行的前两个值分别对应X1和X2，第三个值是数据对应的类别标签。为了方便计算，函数还在X1和X2之前添加了一个值为1.0的X1，X1可以理解为偏置，即下图中的x0。</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b6708f9ca2d645f9b56b87d51d8f1e34><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>4.3训练算法</h1><pre><code>#sigmoid函数def sigmoid(inX):    return 1.0 / (1 + np.exp(-inX))</code></pre><p><strong>sigmoid</strong>函数就是传入一个参数(这里是一个100x1的向量)，通过公式计算并返回值。</p><pre><code>def gradAscent(dataMatIn, classLabels):    # 将列表转换成numpy的matrix(矩阵)    dataMatrix = np.mat(dataMatIn)    # 将列表转换成numpy的mat,并进行转置    labelMat = np.mat(classLabels).T    # 获取dataMatrix的行数和列数。    m, n = np.shape(dataMatrix)    # 设置每次移动的步长    alpha = 0.001    # 设置最大迭代次数    maxCycles = 500    # 创建一个n行1列都为1的矩阵    weights = np.ones((n,1))    for k in range(maxCycles):        # 公式中hΘ(x)        h = sigmoid(dataMatrix * weights)        # 误差，即公式中y-hΘ(x)        error = labelMat - h        # 套入整体公式        weights = weights + alpha * dataMatrix.T * error    return weights</code></pre><p>最后<strong>weights</strong>返回的是一个3x1的矩阵,运行截图如下：</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/54fc89ca6eb445c6a28e18c3e185b4a7><p class=pgc-img-caption></p></div><p><strong>gradAscent</strong>传入参数为<strong>loadDataSet</strong>的两个返回值，然后通过<strong>numpy</strong>的<strong>mat</strong>方法将<strong>dataMatrix</strong>和<strong>labelMat</strong> 分为转化为100x3和1x100的矩阵，但<strong>labelMat</strong> 经过T转置后变成100x1的矩阵。然后初始化权重，利用的方法就是创建一个n行1列的矩阵。整个算法的关键处于for循环中，我们先回顾一下上文的两个公式。</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c1c7c20be795491194f3ce30afbecb4a><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9ca329c5f6b0404da06fc51c68a5e719><p class=pgc-img-caption></p></div><p>其中h的计算结果即h_w(x)，权重<strong>weight</strong>为W向量，输入矩阵<strong>dataMatrix</strong>为x向量。误差<strong>error</strong>代表yi-h_w(xi)，有人可能会发现1/m没有出现在代码中，因为alpha和1/m都为常数，二者相乘也为常数，所以只需要用alpha代替即可。</p><p>公式中的加和部分又怎么体现呢？如果学过线性代数或者了解<strong>numpy</strong>运算的伙伴应该都理解矩阵的乘法，不理解也没有关系，看下图这个例子，当两个矩阵相乘时，对应元素之间会求和作为最终元素。</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d2a771c173874bb4a35575084cb9579c><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>4.4绘制决策边界</h1><pre><code>def plotDataSet(weight):    #获取权重数组    weight = weight.getA()    # 加载数据和标签    dataMat, labelMat = loadDataSet()    # 将列表转换成numpy的array数组    dataArr = np.array(dataMat)    #获取样本个数    n = np.shape(dataMat)[0]    #创建4个空列表，1代表正样本、0代表负样本    xcord1 = []; ycord1 = []    xcord0 = []; ycord0 = []    # 遍历标签列表，根据数据的标签进行分类    for i in range(n):        if int(labelMat[i]) == 1:            # 如果标签为1，将数据填入xcord1和ycord1            xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])        else:            # 如果标签为0，将数据填入xcord0和ycord0            xcord0.append(dataArr[i,1]); ycord0.append(dataArr[i,2])    #绘制图像    fig = plt.figure()    ax = fig.add_subplot(111)    ax.scatter(xcord1, ycord1, s = 20, c = 'red', marker = '*',label = 'Class1')    ax.scatter(xcord0, ycord0, s = 20, c = 'green',marker = 's',label = 'Class2')    #绘制直线，sigmoid设置为0    x = np.arange(-3.0, 3.0, 0.1)    y = (-weight[0] - weight[1] * x) / weight[2]    ax.plot(x, y)    #标题、x标签、y标签    plt.title('LRData')    plt.legend(loc='upper left')    plt.xlabel('X1'); plt.ylabel('X2')    plt.savefig("E:\machine_learning\LR03.jpg")    plt.show()</code></pre><p>这部分代码唯一需要注意的是，将<strong>sigmoid</strong>的值设置为0，可以回忆一下文章刚开始时的<strong>Sigmoid</strong>函数图像，0是两个分类的分界处。因此，我们设定0=w_0x_0+w_1x_1+w_2x_2，x_0的值为1，所以已知回归系数，就可求得x_1和x_2的关系式，从而画出决策边界。</p><div class=pgc-img><img alt=机器学习笔记(七)——初识逻辑回归、不同方法推导梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9b59d1afc5e6472193d907ad7f2e5476><p class=pgc-img-caption></p></div><p>上图可以看出分类的效果还是不错的，根据函数绘制出的直线已经很大程度上将两类样本分隔开，100个样本中，只有五个样本分类错误，其中有三个还是位于回归线上。</p><h1 class=pgc-h-arrow-right>五、文末总结</h1><p>本文所讲的梯度上升公式，属于批量梯度上升，此外还有随机梯度上升、小批量梯度上升，而批量梯度上升每次计算都要计算所有的样本，所以程序计算过程是十分复杂的，并且容易收敛到局部最优，而随机梯度上升将会对算法进行调优，下一篇文章将会介绍随机梯度上升，并分析两者之间的区别。</p><blockquote><p>私信小编可获取数据和源码供参考，感谢阅读。</p></blockquote></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'机器','学习','笔记'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../cn/%E7%A7%91%E5%AD%A6/be7a95a.html alt=机器学习笔记(九)——手撕支持向量机SVM之间隔、对偶、KKT推导 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/098e7c3fc26e4c148cd535acf6e21f94 style=border-radius:25px></a>
<a href=../../cn/%E7%A7%91%E5%AD%A6/be7a95a.html title=机器学习笔记(九)——手撕支持向量机SVM之间隔、对偶、KKT推导>机器学习笔记(九)——手撕支持向量机SVM之间隔、对偶、KKT推导</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>