<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>ICCV2019 | RankSRGAN:基于排序学习的生成对抗超分辨率重建方法 | 极客快訊</title><meta property="og:title" content="ICCV2019 | RankSRGAN:基于排序学习的生成对抗超分辨率重建方法 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RfXvDe62hf1aYu"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/de41a3ee.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/de41a3ee.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/de41a3ee.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/de41a3ee.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/de41a3ee.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/de41a3ee.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/de41a3ee.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/de41a3ee.html><meta property="article:published_time" content="2020-11-14T21:03:55+08:00"><meta property="article:modified_time" content="2020-11-14T21:03:55+08:00"><meta name=Keywords content><meta name=description content="ICCV2019 | RankSRGAN:基于排序学习的生成对抗超分辨率重建方法"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/de41a3ee.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>ICCV2019 | RankSRGAN:基于排序学习的生成对抗超分辨率重建方法</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>本文作者：肖健 在读博士</p><p>作者学校：哈尔滨工程大学</p><p>研究方向：生成对抗网络，视频处理，目标检测。</p><p>前言</p><p>本文将对ICCV2019的Oral论文《RankSRGAN：Generative Adversarial Networks with Ranker for Image Super-Resolution》进行解读。</p><p>这篇文章聚焦于利用生成对抗网络（Generative Adversarial Networks，GANs）解决单幅图像超分辨率重建（SISR）问题。为了进一步改善超分辨率重建结果的视觉质量，PIRM2018-SR挑战使用了如PI、NIQE和Ma等与人类评价等级高度相关的感知指标来评价重建的感知质量，但现有方法无法直接优化这些感知指标。为此，该文提出了带有排序器Ranker的超分辨率重建网络--RankSRGAN，用感知指标优化生成器G。</p><p>该方法首先训练一个可以学习感知指标行为的Ranker，然后引入一个新的rank-content loss（内容排序损失）来优化感知质量。最令人瞩目的是，该方法可以结合不同SR方法的优势来产生更好的结果。大量实验表明，RankSRGAN在视觉效果方面取得了令人愉悦的效果，并在感知指标方面达到了最优的性能。</p><p><strong>论文地址</strong>：https://arxiv.org/abs/1908.06382</p><p><strong>源码地址</strong>：https://wenlongzhang0724.github.io/Projects/RankSRGAN</p><p><strong>论文作者</strong>：Wenlong Zhang，Yihao Liu，Chao Dong，Yu Qiao（中国科学院深圳先进技术研究院，先进院-商汤联合实验室）</p><p>研究方法</p><p>单幅图像超分辨率重建旨在从一幅低分辨率（LR）图像重建或生成一幅高分辨率（HR）图像。近期基于CNN的SISR研究逐渐增多，主要分为两类：一类是将SR视为重建问题，利用MSE作为损失函数以获得高PSNR值；另一类方法是将SR转换为图像生成问题，以获得更好的视觉质量。第二类方法能获得更加逼真的重建图像，该文就是研究这种感知SR方法。</p><p>感知SR方法面临的最具挑战性的问题是评估，依靠人类主观判断不可靠也不公平。为了解决这个问题，该文提出了许多与人类评价高度相关的无参考图像质量评估（NR-IQA）指标，例如NIQE（相关系数0.76）和PI（相关系数0.83），PIRM2018-SR挑战已成功使用了这些指标。但是这些NR-IQA指标大多数是不可微分的，例如它们包括手工特征提取或统计回归操作，因此无法用作损失函数来优化网络。</p><p>为了使感知指标能用于优化网络，进一步提高重建质量，作者提出了一个通用且可微分的模型--Ranker，该模型可以模拟任何NR-IQA指标，并提供明确的目标（作为损失函数）以优化感知质量。Ranker是一个孪生CNN，它通过学习排序方法来模拟感知指标。Ranker与标准的SRGAN模型一起形成一个新的感知SR框架--RankSRGAN（带有Ranker的SRGAN）。所提出的框架还具有rank-content loss（内容排序损失），用训练好的Ranker来度量输出图像质量，这样SR模型可以针对特定的感知指标稳定地优化。图1显示了RankSRGAN的结果，它融合了SRGAN和ESRGAN的图像效果并获得了更好的NIQE得分（NIQE值越小越好，PSNR值越大越好）。</p><p>作者进行了全面实验证明了所提出方法的有效性。总结本文的贡献有三点。<strong>（1）提出了一个通用的感知SR框架--RankSRGAN，该框架可以利用不可微分的感知指标优化生成器，并实现了最先进的性能。</strong><strong>（2）本文首次利用其他SR方法的结果来构建训练数据集。</strong><strong>所提出的方法结合了不同SR方法的优势，并产生了更好的结果。</strong><strong>（3）所提出的SR框架具有高度的灵活性，并且在构造的不同数据集，感知指标和损失组合的情况下产生多种结果。</strong></p><img alt="ICCV2019 | RankSRGAN:基于排序学习的生成对抗超分辨率重建方法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXvDe62hf1aYu><p>图 1 RankSRGAN与其它感知SR方法的比较</p><p>RankSRGAN是在基于GAN的SR方法上建立的，它包含一个生成器和一个判别器。判别器网络区分是真实图像还是超分辨率重建的结果，训练生成器网络来愚弄判别器。为了获得更自然的纹理，作者通过利用感知指标的先验知识为标准SRGAN增加额外约束，以提高输出图像的视觉质量。RankSRGAN的整体框架如图2所示，主要包括三个阶段。</p><img alt="ICCV2019 | RankSRGAN:基于排序学习的生成对抗超分辨率重建方法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXvDeX94ZH10e><p>图 2 RankSRGAN的整体框架</p><p><strong>阶段一：</strong><strong>利用感知度量生成不同SR方法的重建图像，获得rank数据集。</strong>首先在公开的SR数据集上生成不同SR方法的超分辨率重建图像；然后在生成的图像上应用选定的感知指标（例如NIQE），之后选出内容相同的图像组成图像对（即相同图像经不同SR方法得到的HR图像组成的图像对），并根据感知指标计算出的图像质量得分对图像对进行排序。最后就获得了成对图像和对应的排序标签（标签的分配规则是：最好的NIQE值标记为1，即重建质量好的、NIQE值低的用1标记）。</p><p><strong>阶段二：</strong><strong>训练Ranker。</strong><strong>Ranker采用孪生网络结构来学习感知指标的行为，</strong>Ranker具有两个相同的网络分支，其中包含一系列的卷积、LeakyReLU激活、池化和全连接层。在特征提取器之后使用全局平均池化GAP层，这样网络可以摆脱输入大小的限制。为了得到排序得分，使用一个全连接层作为回归器来量化排序结果。注意这里并不是预测感知指标的实际值，而是只关注排名信息。最后，两个分支的输出构成margin-ranking loss，这样我们可以计算梯度并应用反向传播来更新整个网络的参数。训练好的Ranker应该具有根据图像感知得分对图像进行排序的能力。</p><p><strong>阶段三：</strong><strong>训练重建网络RankSRGAN，判别器D的损失与SRGAN中的相同，</strong>生成器G的总损失包含三部分：感知损失、对抗损失和rank-content loss。其中rank-content loss由阶段二用训练好的Ranker给出，是本文方法新引入的损失函数，能使标准SRGAN生成视觉逼真的图像。</p><p>实验</p><p>作者进行了全面的实验证明所提出方法的有效性。</p><p>1.为了验证Ranker的有效性，作者比较了两种排序策略--度量排序（metric rank，本文提出所使用的方法）和模型分类。实验证明了度量排序可以组合不同算法的优势，并超过单个算法的上界。</p><p>2.作者用DIV2K数据集训练RankSRGAN，在Set14、BSD100和PRIM-test测试不同算法的性能，评价指标有NIQE、PI和PSNR（NIQE、PI值越低表示视觉质量越好），测试结果如下表所示。在NIQE和PI两个指标上，本文的RankSRGAN都比SRGAN和ERGAN性能更好，重建图像感知质量的提升是以牺牲PSNR为代价的。图3给出了不同方法重建的结果，RankSRGAN重建的图像具有更真实的纹理而不会引入其它伪像。</p><img alt="ICCV2019 | RankSRGAN:基于排序学习的生成对抗超分辨率重建方法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXvDen4mXeZU9><img alt="ICCV2019 | RankSRGAN:基于排序学习的生成对抗超分辨率重建方法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXvDf36AUvuX4><p>图 3 不同方法超分辨率重建结果</p><p>总结</p><p>针对超分辨率重建问题，本文提出RankSRGAN来优化面向感知指标的SR模型。关键思想是引入Ranker，通过排序学习来学习感知指标的行为。RankSRGAN可以结合不同SR方法的优势并产生更好的结果。大量的实验很好地证明了RankSRGAN是一个灵活的框架，可以在感知度量指标上取得最优的性能，并且能够恢复更逼真的纹理。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'ICCV2019','RankSRGAN','学习'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>