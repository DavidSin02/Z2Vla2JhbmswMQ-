<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 | 极客快訊</title><meta property="og:title" content="PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/704bd6b96fe84583802e545aabd17adc"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e0c61c0.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e0c61c0.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e0c61c0.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e0c61c0.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e0c61c0.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e0c61c0.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e0c61c0.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e0c61c0.html><meta property="article:published_time" content="2020-10-29T21:01:05+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:05+08:00"><meta name=Keywords content><meta name=description content="PARL源码走读——使用策略梯度算法求解迷宫寻宝问题"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/e0c61c0.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>PARL源码走读——使用策略梯度算法求解迷宫寻宝问题</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p class=ql-align-justify>前不久，百度发布了基于PaddlePaddle的深度强化学习框架PARL。</p><p class=ql-align-justify>作为一个强化学习小白，本人怀着学习的心态，安装并运行了PARL里的quick-start。不体验不知道，一体验吓一跳，不愧是 NeurIPS 2018 冠军团队的杰作，代码可读性良好，函数功能非常清晰，模块之间耦合度低、内聚性强。不仅仅适合零基础的小白快速搭建DRL环境，也十分适合科研人员复现论文结果。</p><p class=ql-align-justify>废话不多说，我们从强化学习最经典的例子——迷宫寻宝(俗称格子世界GridWorld)开始，用策略梯度(Policy-Gradient)算法体验一把PARL。</p><p class=ql-align-justify><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/704bd6b96fe84583802e545aabd17adc><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-center><strong>模拟环境</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>强化学习适合解决智能决策问题。如图，给定如下迷宫，黑色方格代表墙，黄色代表宝藏，红色代表机器人；一开始，机器人处于任意一个位置，由于走一步要耗电，撞墙后需要修理，所以我们需要训练一个模型，来告诉机器人如何避免撞墙、并给出寻宝的最优路径。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/703f9dd4b31c4a36aba31fd0f622e1d1><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p class=ql-align-justify>接下来，定义强化学习环境所需的各种要素：状态state、动作action、奖励reward等等。</p><p class=ql-align-justify>state就是机器人所处的位置，用(行、列)这个元组来表示，同时可以表示墙：</p><pre>self.wallList=[(2,0),(3,2),(1,3),(4,4)]self.start=(0,4)self.end=(4,0)</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>使用random-start策略实现reset功能，以增加初始状态的随机性：</p><pre>defreset(self): for _ in range(0,1024): i=np.random.randint(self.row) j=np.random.randint(self.col) if (i,j) not in self.wallList and (i,j)!=self.end: self.pos=(i,j) break return self.pos</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>定义动作action，很显然，机器人可以走上下左右四个方向：</p><pre>action_dim=4dRow=[0,0,-1,1]dCol=[1,-1,0,0]</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>定义奖励reward，到达终点奖励为10，走其他格子需要耗电，奖励为-1：</p><pre>def reward(self, s): if s == self.end: return 10.0 else: return -1.0</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>另外，越界、撞墙需要给较大惩罚：</p><pre>if not checkBounds(nextRow, nextCol) : #越界 return self.pos, -5.0, False, {'code':-1,'MSG':'OutOfBounds!'} nextPos=(nextRow,nextCol) if meetWall(self.wallList, nextPos): #撞墙 return self.pos, -10.0, False, {'code':-1,'MSG':'MeetWall!'}</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>至此，强化学习所需的状态、动作、奖励均定义完毕。接下来简单推导一下策略梯度算法的原理。</p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/704bd6b96fe84583802e545aabd17adc><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-center><strong>策略梯度(Policy-Gradient)算法是什么？</strong></p><p class=ql-align-justify>我们知道，强化学习的目标是给定一个马尔可夫决策过程，寻找出最优策略。所谓策略是指状态到动作的映射，常用符号π表示，它是指给定状态s时，动作集上的一个分布，即：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/95dbcda3c3b3469a81abe862309e00e4><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p class=ql-align-justify>策略梯度的做法十分直截了当，它直接对求解最优策略进行参数化建模，策略p(a|s)将从一个概率集合变成一个概率密度函数p(a|s,θ)，即：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/9b3c1c72166c4caeb4acd902e2152726><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p class=ql-align-justify>这个策略函数表示，在给定状态s和参数θ的情况下，采取任何可能动作的概率，它是一个概率密度函数，在实际运用该策略的时候，是按照这个概率分布进行动作action的采样的，这个分布可以是离散(如伯努利分布)，也可以说是连续（如高斯分布）。最直观的方法，我们可以使用一个线性模型表示这个策略函数:</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d23de83872548b19010a63e9c157030><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>其中，φ(s)表示对状态s的特征工程，θ是需要训练的参数。这样建模有什么好处呢？其实最大的好处就是能时时刻刻学到一些随机策略，增强探索性exploration。</p><p class=ql-align-justify><br></p><p class=ql-align-justify>为什么可以增加探索性呢？</p><p class=ql-align-justify><br></p><p class=ql-align-justify>比如迷宫寻宝问题，假设一开始机器人在最左上角的位置，此时p(a|s,θ)可以初始化为[0.25,0.25,0.25,0.25]，表明机器人走上、下、左、右、的概率都是0.25。当模型训练到一定程度的时候，p(a|s,θ)变成了[0.1,0.6,0.1,0.2]，此时，向下的概率最大，为0.6，机器人最有可能向下走，这一步表现为利用exploitation；但是，向右走其实也是最优策略，0.2也是可能被选择的，这一步表现为探索exploration；相对0.6和0.2，向上、向左两个动作的概率就小很多，但也是有可能被选择的。如果模型继续训练下去，p(a|s,θ)很有可能收敛成[0.05,0.45,0.05,0.45]，此时，机器人基本上只走向下或者向右，选择向上、向左的可能性就极小了。这是最左上角位置(状态)的情况，其他状态，随着模型的训练，也会收敛到最优解。</p><p class=ql-align-justify>有了模型，就想到求梯度，那么，如何构建损失函数呢？标签y-Target又是什么?</p><p class=ql-align-justify>一个非常朴素的想法就是：如果一个动作获得的reward多，那么就使其出现的概率变大，否则减小，于是，可以构建一个有关状态-动作的函数 f(s,a) 作为损失函数的权重，这个权重函数可以是长期回报G(t)，可以是状态值函数V(s)，也可以是状态-行为函数Q(s,a)，当然也可以是优势函数A。但是，这个权重函数和参数θ无关，对θ的梯度为0，仅仅作为p(a|s,θ)的系数。</p><p class=ql-align-justify>现在考虑模型的输出π(a|s,θ)，它表示动作的概率分布，我们知道，智能体每执行完一轮episode，就会形成一个完整的轨迹Trajectory:</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c293a3084e564948930f84ddd4d31794><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>其中，状态</p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7767ca7d071e4e22a23e01078dcd886b><p class=pgc-img-caption></p></div><p class=ql-align-justify>和参数θ无关，状态转移概率P(s'|s,a)是由环境所决定的，和参数θ也无关。所以，我们的目标简化为：优化参数θ，使得每个动作概率的乘积</p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ee052037d2504c7098c6840c1236c502><p class=pgc-img-caption></p></div><p class=ql-align-justify>达到最大，即使得</p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b701463cd81a46679fcf1218a8dd762e><p class=pgc-img-caption></p></div><p class=ql-align-justify>这个累乘概率达到最大，可用如下公式表示：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8be9ecfdad814768804cf7cc13cc0447><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>这显然是我们熟悉的极大似然估计问题，转化为对数似然函数：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d1701610055b43edb4274ff65d299159><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>乘以权重 f(s,a)，构建如下目标函数，这个目标函数和我们平时见到的损失函数正好相反，它需要使用梯度上升的方法求一个极大值：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1bc5ff7519b645edb98a30d541ea0034><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>注意到，这里的aTrue就是标签y-Target，表示agent在状态$s_{t}$时真实采取的动作，可以根据轨迹trajectory采样得到。学过机器学习的同学都知道，一般用目标函数的均值代替求和，作为新的目标函数：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/da23ef2a4ca24f8ba6868acd93d5a1a0><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>均值，就是数学期望，所以目标函数也可以表示为：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8846e5b3be274e93b27f58f7041e6b78><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>有了目标函数，梯度就很容易计算了，由于</p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8ad09cbe329c49d796d0512ffcc1a503><p class=pgc-img-caption></p></div><p class=ql-align-justify>对于θ来说是系数，故梯度公式如下:</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1fb76b71d77e41a0b19a4ac2745b1ab3><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>那么，策略π具体的表现形式如何？前文提到，策略可以是离散的，也可以是连续的，不妨考虑离散的策略。由于我们需要求解最大值问题，也就是梯度上升问题，自然而然就想到把梯度上升问题转化为梯度下降问题，这样才能使得目标函数的相反数达到最小，而什么样的函数可以将梯度下降和对数函数关联起来呢？显然是我们熟悉的交叉熵，所以最终的损失函数确定为：</p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b48df45e15944315af3f81eb4d6141b7><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>连续策略的推导与离散策略类似，有兴趣的读者可以参考相关文献。</p><p class=ql-align-justify>自此，公式推导可以告一段落。策略梯度的基本算法就是Reinforce，也称为蒙特卡洛策略梯度，简称MCPG，PARL的官方policy-gradient就是基于以下算法框架实现的：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7dd2579d6fc94c2ca0a2e42f78cda67d><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/704bd6b96fe84583802e545aabd17adc><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-center><strong>PARL源码结构</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>在搭建模型之前，我们先分析一下PARL的主要模块：</p><p class=ql-align-justify>1. env：环境，在这里，我们的环境就是迷宫寻宝。</p><p class=ql-align-justify>2. model：模型，可以是简单的线性模型，也可以是CNN、RNN等深度学习模型。</p><p class=ql-align-justify>3. algorithm：算法，对model层进行封装，并利用模型进行predict(预测)，同时构建损失函数进行learn(学习)；具体实现形式可以是DQN、PG、DDPG等等。</p><p class=ql-align-justify>4. agent：智能体，对algorithm层进行封装，一般也包含predict、learn两个函数；同时，由于智能体要同时进行探索exploration-利用exploitation，还经常包含一个sample函数，用于决定到底是randomSelect(随机选择或者根据分布函数选择动作)，还是argmax(100%贪心，总是选择可能性最大的动作)。</p><p class=ql-align-justify>5. train：训练和测试，用于实现agent和环境的交互，当模型收敛后，可以测试智能体的准确性。</p><p class=ql-align-justify>6. utils：其他辅助功能。</p><p class=ql-align-justify>以下的架构示意图，可以帮助我们更好的理解PARL：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/ffeb24d457ef41be8890f1f5164e8e13><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/704bd6b96fe84583802e545aabd17adc><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-center><strong>代码实现&源码解读</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>在理解了框架的各个模块之后，我们就可以按照模板填代码了，学过MVC、ORM等框架的同学都知道，这是一件非常轻松愉快的事情。</p><p class=ql-align-justify>1、MazeEnv。迷宫环境，继承自gym.Env，实现了reset、step、reward、render四个主要方法，这里不再赘述。</p><p class=ql-align-justify>2、MazeModel。模型层，搭建如下全链接神经网络，输入是状态state-input，输出是策略函数action-out，由于策略函数是动作的概率分布，所以选用softmax作为激活函数，中间还有若干隐藏层。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a00b424c96b94362a6d1658403097cfc><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p class=ql-align-justify>代码实现非常的简单，让MazeModel继承官方的Model类，然后照猫画虎搭建模型即可：</p><pre>class MazeModel(Model): def__init__(self, act_dim): self.act_dim = act_dim hid1_size = 32 hid2_size = 32 self.fc1 = layers.fc(size=hid1_size, act='tanh') self.fc2 = layers.fc(size=hid2_size, act='tanh') self.fcOut = layers.fc(size=act_dim,act='softmax') defpolicy(self, obs): out = self.fc1(obs) out = self.fc2(out) out = self.fcOut(out) return out</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>3、policy_gradient。算法层；官方仓库提供了大量的经典强化学习算法，我们无需自己重复写，可以直接复用算法库(parl.algorithms)里边的 PolicyGradient 算法！</p><p class=ql-align-justify>简单分析一下policy_gradient的源码实现。</p><p class=ql-align-justify>define_predict函数，接收状态obs，调用model的policy方法，输出状态所对应的动作：</p><pre>def define_predict(self, obs): """ use policy model self.model to predict the actionprobability """ return self.model.policy(obs) </pre><p class=ql-align-justify><br></p><p class=ql-align-justify>define_learn函数，接收状态obs、真实动作action、长期回报reward，首先调用model的pocliy方法，预测状态obs所对应的动作概率分布act_prob，然后使用交叉熵和reward的乘积构造损失函数cost，最后执行梯度下降法，优化器为Adam，完成学习功能：</p><pre>def define_learn(self, obs, action, reward): """ update policy model self.model with policy gradientalgorithm """ act_prob = self.model.policy(obs) log_prob = layers.cross_entropy(act_prob, action) cost = log_prob * reward cost = layers.reduce_mean(cost) optimizer = fluid.optimizer.Adam(self.lr) optimizer.minimize(cost) return cost</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>4、MazeAgent。智能体。其中，self.pred_program是对algorithm中define_predict的简单封装，self.train_program是对algorithm中define_learn的简单封装，我们可以参考官方的CartpoleAgent实现，按照框架模板填入相应的格式代码。</p><p class=ql-align-justify>这里，仅仅分析self.pred_program，self.train_program写法类似：</p><pre>self.pred_program = fluid.Program()#固定写法with fluid.program_guard(self.pred_program): obs= layers.data( name='obs',shape=[self.obs_dim], dtype='float32')#接收外界传入的状态obsself.act_prob = self.alg.define_predict(obs)#调用algorithm的define_predict,self.act_prob为动作的概率分布</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>sample函数，注意这句话：</p><pre>act = np.random.choice(range(self.act_dim),p=act_prob)</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>这句话表示根据概率分布随机选出相应的动作；假设上、下、左、右的概率分别为[0.5,0.3,0.15,0.05]，那么上被选择的概率是最大的，右被选择的概率是最小的，所以sample函数既能exploration，又能exploitation，体现了强化学习中的探索-利用的平衡。</p><p class=ql-align-justify>predict函数，和sample函数不同的是，它总是贪心的选择可能性最大的动作，常常用于测试阶段：</p><pre>act = np.argmax(act_prob)</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>learn函数，接收obs、action、reward，进行批量梯度下降，返回损失函数cost。</p><p class=ql-align-justify>5、TrainMaze。让环境env和智能体agent进行交互，最主要的部分就是以下代码，体现了MCPG过程：</p><pre>#迭代十万个episodefor i in range(1,100001): #采样  obs_list, action_list, reward_list = run_train_episode(env, agent)  #使用滑动平均的方式计算奖励的期望  MeanReward=MeanReward+(sum(reward_list)-MeanReward)/i  batch_obs = np.array(obs_list)  batch_action = np.array(action_list)  #通过backup的方式计算G(t)，并进行归一化处理  batch_reward = calc_discount_norm_reward(reward_list, GAMMA)  #学习  agent.learn(batch_obs, batch_action, batch_reward)</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>其中，滑动平均可以选择任意一个公式，无偏估计表示真实的均值，有偏估计更加接近收敛后的平均奖励：</p><p class=ql-align-justify>无偏估计：</p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fe24f8e9b0db4a0da69bb9d06b1f89c0><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>有偏估计：</p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d53d26d12321404c8f276b8bd275ba81><p class=pgc-img-caption></p></div><p class=ql-align-justify>，α是学习率，取0.1、0.01等等</p><p class=ql-align-justify>其他代码都是辅助功能，如记录log、画图、渲染环境等等。</p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/704bd6b96fe84583802e545aabd17adc><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-center><strong>运行程序并观察结果</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>运行TrainMaze，可以看到如下输出。</p><p class=ql-align-justify>1、训练之前，机器人并不知道如何寻宝，所以越界、撞墙次数非常多，也绕了很多弯路，平均奖励比较低。</p><p class=ql-align-justify>ErrorCountBeforeTrain:25052 #越界+撞墙次数</p><p class=ql-align-justify>平均奖励曲线：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7183572cbe2b44e4befd82aade9bb389><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p class=ql-align-justify>2、训练模型。迭代十万个episode，观察如下学习曲线，纵轴表示平均奖励，可以看到，模型已经收敛了：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/bc20e059b5b04991870661e34da25f28><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p class=ql-align-justify>3、测试模型的准确性。测试阶段，我们迭代128轮，智能体几乎没有任何越界或者撞墙行为，由于是random-start，所以平均奖励有少许波动，但稳定在5-7之间。</p><p class=ql-align-justify>ErrorCountAfterTrain:0 #没有任何撞墙或者越界</p><p class=ql-align-justify>训练后的平均奖励：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/805ed7ce086c433ebc289239bb9979f1><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-center><br></p><p class=ql-align-justify>源码Git地址</p><p class=ql-align-justify>https://github.com/kosoraYintai/PARL-Sample</p><p class=ql-align-justify>参考文献：</p><p class=ql-align-justify>· CS 294-112 at UC Berkeley，DeepReinforcement Learning.</p><p class=ql-align-justify>· Deepmind，Silver.D，Reinforcement Learning Open Class.</p><p class=ql-align-justify>· 冯超. 强化学习精要[M]. 北京：电子工业出版社，2018.</p><p class=ql-align-justify>· 郭宪，方勇纯. 深入浅出强化学习[M]. 北京：电子工业出版社，2018.</p><p class=ql-align-justify></p><div class=pgc-img><img alt=PARL源码走读——使用策略梯度算法求解迷宫寻宝问题 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a478223dc63f447cb6a38e838fa412a8><p class=pgc-img-caption></p></div></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'PARL','源码','迷宫'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>