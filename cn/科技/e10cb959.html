<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>SKLearn分类树在合成数集上的表现 | 极客快訊</title><meta property="og:title" content="SKLearn分类树在合成数集上的表现 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/a30bbaa14dc44146bbcde0a85690399f"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e10cb959.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e10cb959.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e10cb959.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e10cb959.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e10cb959.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e10cb959.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e10cb959.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e10cb959.html><meta property="article:published_time" content="2020-10-29T21:09:57+08:00"><meta property="article:modified_time" content="2020-10-29T21:09:57+08:00"><meta name=Keywords content><meta name=description content="SKLearn分类树在合成数集上的表现"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/e10cb959.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>SKLearn分类树在合成数集上的表现</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p>小伙伴们大家好~o(￣▽￣)ブ，我是菜菜，这里是我的sklearn课堂</p><p>我的开发环境是Jupyter lab，所用的库和版本大家参考：</p><p>Python 3.7.1（你的版本至少要3.4以上</p><p>Scikit-learn 0.20.0 （你的版本至少要0.20</p><p>Graphviz 0.8.4 (没有画不出决策树哦，安装代码conda install python-graphviz</p><p>Numpy 1.15.3, Pandas 0.23.4, Matplotlib 3.0.1, SciPy 1.1.0</p><p>在这里，我们使用SKlearn构建三种不同分布的数据，然后在这些数据集上测试一下决策树的效果，让大家更好地理解决策树。下图就是三种表现结果，后面会详细介绍实现过程~</p><div class=pgc-img><img alt=SKLearn分类树在合成数集上的表现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a30bbaa14dc44146bbcde0a85690399f><p class=pgc-img-caption></p></div><p><strong>1. 导入需要的库</strong></p><pre>import numpy as npimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.datasets import make_moons, make_circles, make_classificationfrom sklearn.tree import DecisionTreeClassifier</pre><p><strong>2. 生成三种数据集</strong></p><p>我们先从sklearn自带的数据库中生成三种类型的数据集：1）月亮型数据，2）环形数据，3）二分型数据</p><pre>#make_classification库生成随机的二分型数据X, y = make_classification(n_samples=100, #生成100个样本 n_features=2, #包含2个特征，即生成二维数据 n_redundant=0, #添加冗余特征0个 n_informative=2, #包含信息的特征是2个 random_state=1, #随机模式1 n_clusters_per_class=1 #每个簇内包含的标签类别有1个 )</pre><p>在这里可以查看一下X和y，其中X是100行带有两个2特征的数据，y是二分类标签</p><p>也可以画出散点图来观察一下X中特征的分布</p><pre>plt.scatter(X[:,0],X[:,1]); </pre><div class=pgc-img><img alt=SKLearn分类树在合成数集上的表现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ac590e83ff2b4d198935a4824c9ce5e1><p class=pgc-img-caption></p></div><p>从图上可以看出，生成的二分型数据的两个簇离彼此很远，这样不利于我们测试分类器的效果，因此我们使用np生成随机数组，通过让已经生成的二分型数据点加减0~1之间的随机数，使数据分布变得更散更稀疏。</p><p>【注意】这个过程只能够运行一次，因为多次运行之后X会变得非常稀疏，两个簇的数据会混合在一起，分类器的效应会继续下降</p><pre>rng = np.random.RandomState(2) #生成一种随机模式X += 2 * rng.uniform(size=X.shape) #加减0~1之间的随机数linearly_separable = (X, y) </pre><p>生成了新的X，依然可以画散点图来观察一下特征的分布</p><pre>plt.scatter(X[:,0],X[:,1]);</pre><div class=pgc-img><img alt=SKLearn分类树在合成数集上的表现 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3289cbba61fa4ceb9e3c8a4045aef116><p class=pgc-img-caption></p></div><pre>#用make_moons创建月亮型数据，make_circles创建环形数据，并将三组数据打包起来放在列表datasets中datasets = [make_moons(noise=0.3, random_state=0), make_circles(noise=0.2, factor=0.5, random_state=1), linearly_separable]</pre><p><strong>3. 画出三种数据集和三棵决策树的分类效应图像</strong></p><pre>#创建画布，宽高比为6*9figure = plt.figure(figsize=(6, 9))#设置用来安排图像显示位置的全局变量ii = 1​#开始迭代数据，对datasets中的数据进行for循环​for ds_index, ds in enumerate(datasets):  #对X中的数据进行标准化处理，然后分训练集和测试集 X, y = ds X = StandardScaler().fit_transform(X)  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)  #找出数据集中两个特征的最大值和最小值，让最大值+0.5，最小值-0.5，创造一个比两个特征的区间本身更大一点的区间 x1_min, x1_max = X[:, 0].min() - .5, X[:, 0].max() + .5 x2_min, x2_max = X[:, 1].min() - .5, X[:, 1].max() + .5  #用特征向量生成网格数据，网格数据，其实就相当于座标轴上无数个点 #函数np.arange在给定的两个数之间返回均匀间隔的值，0.2为步长 #函数meshgrid用以生成网格数据，能够将两个一维数组生成两个二维矩阵。 #如果第一个数组是narray，维度是n，第二个参数是marray，维度是m。那么生成的第一个二维数组是以narray为行，m行的矩阵，而第二个二维数组是以marray的转置为列，n列的矩阵 #生成的网格数据，是用来绘制决策边界的，因为绘制决策边界的函数contourf要求输入的两个特征都必须是二维的 array1,array2 = np.meshgrid(np.arange(x1_min, x1_max, 0.2), np.arange(x2_min, x2_max, 0.2))​ #接下来生成彩色画布 #用ListedColormap为画布创建颜色，#FF0000正红，#0000FF正蓝 cm = plt.cm.RdBu cm_bright = ListedColormap(['#FF0000', '#0000FF'])  #在画布上加上一个子图，数据为len(datasets)行，2列，放在位置i上 ax = plt.subplot(len(datasets), 2, i)  #到这里为止，已经生成了0~1之间的座标系3个了，接下来为我们的座标系放上标题 #我们有三个座标系，但我们只需要在第一个座标系上有标题，因此设定if ds_index==0这个条件 if ds_index == 0: ax.set_title("Input data")  #将数据集的分布放到我们的座标系上 #先放训练集 ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train,  cmap=cm_bright,edgecolors='k') #放测试集 ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test,  cmap=cm_bright, alpha=0.6,edgecolors='k')  #为图设置座标轴的最大值和最小值，并设定没有座标轴 ax.set_xlim(array1.min(), array1.max()) ax.set_ylim(array2.min(), array2.max()) ax.set_xticks(()) ax.set_yticks(())  #每次循环之后，改变i的取值让图每次位列不同的位置 i += 1  #至此为止，数据集本身的图像已经布置完毕，运行以上的代码，可以看见三个已经处理好的数据集  #############################从这里开始是决策树模型##########################  #迭代决策树，首先用subplot增加子图，subplot(行，列，索引)这样的结构，并使用索引i定义图的位置 #在这里，len(datasets)其实就是3，2是两列 #在函数最开始，我们定义了i=1，并且在上边建立数据集的图像的时候，已经让i+1,所以i在每次循环中的取值是2，4，6 ax = plt.subplot(len(datasets),2,i)  #决策树的建模过程：实例化 → fit训练 → score接口得到预测的准确率 clf = DecisionTreeClassifier(max_depth=5) clf.fit(X_train, y_train) score = clf.score(X_test, y_test)  #绘制决策边界，为此，我们将为网格中的每个点指定一种颜色[x1_min，x1_max] x [x2_min，x2_max] #分类树的接口，predict_proba，返回每一个输入的数据点所对应的标签类概率 #类概率是数据点所在的叶节点中相同类的样本数量/叶节点中的样本总数量 #由于决策树在训练的时候导入的训练集X_train里面包含两个特征，所以我们在计算类概率的时候，也必须导入结构相同的数组，即是说，必须有两个特征 #ravel()能够将一个多维数组转换成一维数组 #np.c_是能够将两个数组组合起来的函数 #在这里，我们先将两个网格数据降维降维成一维数组，再将两个数组链接变成含有两个特征的数据，再带入决策树模型，生成的Z包含数据的索引和每个样本点对应的类概率，再切片，且出类概率 Z = clf.predict_proba(np.c_[array1.ravel(),array2.ravel()])[:, 1]  #np.c_[np.array([1,2,3]), np.array([4,5,6])]  #将返回的类概率作为数据，放到contourf里面绘制去绘制轮廓 Z = Z.reshape(array1.shape) ax.contourf(array1, array2, Z, cmap=cm, alpha=.8)  #将数据集的分布放到我们的座标系上 # 将训练集放到图中去 ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors='k') # 将测试集放到图中去 ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, edgecolors='k', alpha=0.6)  #为图设置座标轴的最大值和最小值 ax.set_xlim(array1.min(), array1.max()) ax.set_ylim(array2.min(), array2.max()) #设定座标轴不显示标尺也不显示数字 ax.set_xticks(()) ax.set_yticks(())  #我们有三个座标系，但我们只需要在第一个座标系上有标题，因此设定if ds_index==0这个条件 if ds_index == 0: ax.set_title("Decision Tree")  #写在右下角的数字  ax.text(array1.max() - .3, array2.min() + .3, ('{:.1f}%'.format(score*100)), size=15, horizontalalignment='right')  #让i继续加一 i += 1​plt.tight_layout()plt.show()</pre><p>运行的结果如下所示：</p><div class=pgc-img><img alt=SKLearn分类树在合成数集上的表现 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a30bbaa14dc44146bbcde0a85690399f><p class=pgc-img-caption></p></div><p>从图上来看，每一条线都是决策树在二维平面上画出的一条决策边界，每当决策树分枝一次，就有一条线出现。当数据的维度更高的时候，这条决策边界就会由线变成面，甚至变成我们想象不出的多维图形。</p><p>同时，很容易看得出，分类树天生不擅长环形数据。每个模型都有自己的决策上限，所以一个怎样调整都无法提升表现的可能性也是有的。当一个模型怎么调整都不行的时候，我们可以选择换其他的模型使用，不要在一棵树上吊死。顺便一说，最擅长月亮型数据的是最近邻算法，RBF支持向量机和高斯过程；最擅长环形数据的是最近邻算法和高斯过程；最擅长对半分的数据的是朴素贝叶斯，神经网络和随机森林。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'SKLearn','分类','数集'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>