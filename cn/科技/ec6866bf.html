<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>自然语言处理之中文分词详解 | 极客快訊</title><meta property="og:title" content="自然语言处理之中文分词详解 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/b3a302029c5648c291e9f6a8bdecb125"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/ec6866bf.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/ec6866bf.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/ec6866bf.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/ec6866bf.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/ec6866bf.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/ec6866bf.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/ec6866bf.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/ec6866bf.html><meta property="article:published_time" content="2020-11-14T21:00:19+08:00"><meta property="article:modified_time" content="2020-11-14T21:00:19+08:00"><meta name=Keywords content><meta name=description content="自然语言处理之中文分词详解"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/ec6866bf.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>自然语言处理之中文分词详解</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p><strong>中文分词是中文文本处理的一个基础步骤，也是中文人机自然语言交互的基础模块，在进行中文自然语言处理时，通常需要先进行分词</strong>。“结巴”分词是一个Python 中文分词组件，可以对中文文本进行<strong>分词、词性标注、关键词抽取</strong>等功能，并且支持自定义词典。</p><p><br></p><h1 class=pgc-h-arrow-right>特点</h1><ul><li><strong>jieba分词支持四种分词模式：</strong></li></ul><ol start=1><li>精确模式，试图将句子最精确地切开，适合文本分析；</li><li>全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；</li><li>搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。</li><li>paddle模式，利用PaddlePaddle深度学习框架，训练序列标注（双向GRU）网络模型实现分词。同时支持词性标注。paddle模式使用需安装paddlepaddle-tiny，pip install paddlepaddle-tiny==1.6.1。目前paddle模式支持jieba v0.40及以上版本。jieba v0.40以下版本，请升级jieba，pip install jieba --upgrade 。</li></ol><ul><li><strong>支持繁体分词</strong></li></ul><ul><li><strong>支持自定义词典</strong></li></ul><ul><li><strong>MIT 授权协议</strong></li></ul><h1 class=pgc-h-arrow-right>安装说明</h1><p>在python2.x和python3.x均兼容，有以下三种：</p><p><strong>1. 全自动安装</strong>：easy_install jieba 或者 pip install jieba / pip3 install jieba</p><p><strong>2. 半自动安装:</strong> 先下载，网址为: http://pypi.python.org/pypi/jieba， 解压后运行: python setup.py install</p><p><strong>3. 手动安装: </strong>将jieba目录放置于当前目录或者site-packages目录，</p><p><strong>4.</strong>如果需要使用paddle模式下的分词和词性标注功能，请先安装paddlepaddle-tiny，pip install paddlepaddle-tiny==1.6.1</p><p><br></p><h1 class=pgc-h-arrow-right>主要功能</h1><ul><li>jieba分词主要功能</li></ul><p>先介绍主要的使用功能，再展示代码输出。jieba分词的主要功能有如下几种：</p><p><strong>1.</strong> <strong>jieba.cut：</strong>该方法接受三个输入参数：需要分词的字符串; cut_all 参数用来控制是否采用全模式；HMM参数用来控制是否适用HMM模型</p><p><strong>2. jieba.cut_for_search</strong>：该方法接受两个参数：需要分词的字符串；是否使用HMM模型，该方法适用于搜索引擎构建倒排索引的分词，粒度比较细。</p><p>3. 待分词的字符串可以是unicode或者UTF－8字符串，GBK字符串。注意不建议直接输入GBK字符串，可能无法预料的误解码成UTF－8，</p><p>4. jieba.cut 以及jieba.cut_for_search返回的结构都是可以得到的generator(生成器), 可以使用for循环来获取分词后得到的每一个词语或者使用</p><p><strong>5. jieb.lcut</strong> 以及 jieba.lcut_for_search 直接返回list</p><p><strong>6. jieba.Tokenizer(dictionary=DEFUALT_DICT) </strong>新建自定义分词器，可用于同时使用不同字典，jieba.dt为默认分词器，所有全局分词相关函数都是该分词器的映射。</p><p>代码演示：</p><pre><code># encoding=utf-8import jiebajieba.enable_paddle()# 启动paddle模式。 0.40版之后开始支持，早期版本不支持strs=["我来到北京清华大学","乒乓球拍卖完了","中国科学技术大学"]for str in strs:    seg_list = jieba.cut(str,use_paddle=True) # 使用paddle模式    print("Paddle Mode: " + '/'.join(list(seg_list)))seg_list = jieba.cut("我来到北京清华大学", cut_all=True)print("Full Mode: " + "/ ".join(seg_list))  # 全模式seg_list = jieba.cut("我来到北京清华大学", cut_all=False)print("Default Mode: " + "/ ".join(seg_list))  # 精确模式seg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式print(", ".join(seg_list))seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式print(", ".join(seg_list))</code></pre><p>其中下面的是输出结果。</p><pre><code>【全模式】: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学【精确模式】: 我/ 来到/ 北京/ 清华大学【新词识别】：他, 来到, 了, 网易, 杭研, 大厦    (此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了)【搜索引擎模式】： 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造</code></pre><ul><li>添加自定义词典</li></ul><p>jieba分词器还有一个方便的地方是<strong>开发者可以指定自己的自定义词典</strong>，以便包含词库中没有的词，虽然jieba分词有新词识别能力，但是自行添加新词可以保证更高的正确率。</p><p>使用命令：</p><p><strong>jieba.load_userdict(filename) # filename为自定义词典的路径</strong></p><p>在使用的时候，<strong>词典的格式和jieba分词器本身的分词器中的词典格式必须保持一致</strong>，一个词占一行，每一行分成三部分<strong>，一部分为词语，一部分为词频，最后为词性</strong>（可以省略），用空格隔开。下面其中userdict.txt中的内容为小修添加的词典，而第二部分为小修没有添加字典之后对text文档进行分词得到的结果，第三部分为小修添加字典之后分词的效果。</p><p>示例：</p><p>userdict.txt自定义文件</p><pre><code>云计算 5李小福 2 nr创新办 3 ieasy_install 3 eng好用 300韩玉赏鉴 3 nz八一双鹿 3 nz台中凯特琳 nzEdu Trust认证 2000</code></pre><p>test.py</p><pre><code>import jiebajieba.load_userdict("userdict.txt")import jieba.posseg as psegjieba.add_word('石墨烯')jieba.add_word('凯特琳')jieba.del_word('自定义词')test_sent = ("李小福是创新办主任也是云计算方面的专家; 什么是八一双鹿\n""例如我输入一个带“韩玉赏鉴”的标题，在自定义词库中也增加了此词为N类\n""「台中」正确应该不会被切开。mac上可分出「石墨烯」；此时又可以分出来凯特琳了。")words = jieba.cut(test_sent)print('/'.join(words))print("="*40)result = pseg.cut(test_sent)for w in result:    print(w.word, "/", w.flag, ", ", end=' ')print("\n" + "="*40)terms = jieba.cut('easy_install is great')print('/'.join(terms))terms = jieba.cut('python 的正则表达式是好用的')print('/'.join(terms))print("="*40)# test frequency tunetestlist = [('今天天气不错', ('今天', '天气')),('如果放到post中将出错。', ('中', '将')),('我们中出了一个叛徒', ('中', '出')),]for sent, seg in testlist:    print('/'.join(jieba.cut(sent, HMM=False)))    word = ''.join(seg)    print('%s Before: %s, After: %s' % (word, jieba.get_FREQ(word), jieba.suggest_freq(seg, True)))    print('/'.join(jieba.cut(sent, HMM=False)))    print("-"*40)</code></pre><p>输出结果：</p><p>之前： 李小福 / 是 / 创新 / 办 / 主任 / 也 / 是 / 云 / 计算 / 方面 / 的 / 专家 /</p><p>加载自定义词库后：　李小福 / 是 / 创新办 / 主任 / 也 / 是 / 云计算 / 方面 / 的 / 专家 /</p><p><br></p><ul><li>关键词抽取</li></ul><p>jieba中主要基于TF－IDF算法的关键词抽取, 只有<strong>关键词抽取并且进行词向量化</strong>之后，才好进行下一步的文本分析，可以说这一步<strong>是自然语言处理技术中文本处理最基础的一步</strong>。</p><p>jieba分词中含有analyse模块，在进行关键词提取时可以使用下列代码</p><div class=pgc-img><img alt=自然语言处理之中文分词详解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b3a302029c5648c291e9f6a8bdecb125><p class=pgc-img-caption></p></div><p>当然也可以使用基于TextRank算法的关键词抽取:</p><div class=pgc-img><img alt=自然语言处理之中文分词详解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1030172c209e42299284af915e46447f><p class=pgc-img-caption></p></div><p>这里举一个例子，分别使用两种方法对同一文本进行关键词抽取，并且显示相应的权重值。</p><div class=pgc-img><img alt=自然语言处理之中文分词详解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/430e41c9e95c4bdbb26711be6b8af434><p class=pgc-img-caption></p></div><ul><li><strong>词性标注</strong></li></ul><p><strong>jieba分词还可以进行词性标注</strong>，标注句子分词后每个词的词性，采用和ictclas兼容的标记法，这里只是简单的举一个栗子。</p><p>jieba.posseg.POSTokenizer(tokenizer=None) 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。除了jieba默认分词模式，提供paddle模式下的词性标注功能。paddle模式采用延迟加载方式，通过enable_paddle()安装paddlepaddle-tiny，并且import相关代码。</p><pre><code>import jiebaimport jieba.posseg as psegwords = pseg.cut("我爱北京天安门") #jieba默认模式jieba.enable_paddle() #启动paddle模式。 0.40版之后开始支持，早期版本不支持words = pseg.cut("我爱北京天安门",use_paddle=True) #paddle模式for word, flag in words:    print('%s %s' % (word, flag))</code></pre><p>输出：</p><pre><code>我 r爱 v北京 ns天安门 ns</code></pre><p>paddle模式词性标注对应表如下：</p><p>paddle模式词性和专名类别标签集合如下表，其中词性标签 24 个（小写字母），专名类别标签 4 个（大写字母）。</p><div class=pgc-img><img alt=自然语言处理之中文分词详解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b8302546e4954686ba0f5ac9b565be32><p class=pgc-img-caption></p></div><ul><li><strong>并行分词</strong></li></ul><p>原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升，基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows</p><p>用法：jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数jieba.disable_parallel() # 关闭并行分词模式</p><pre><code>import timeimport jiebajieba.enable_parallel()url = sys.argv[1]content = open(url,"rb").read()t1 = time.time()words = "/ ".join(jieba.cut(content))t2 = time.time()tm_cost = t2-t1log_f = open("1.log","wb")log_f.write(words.encode('utf-8'))print('speed %s bytes/second' % (len(content)/tm_cost))</code></pre><p>实验结果：在 4 核 3.4GHz Linux 机器上，对金庸全集进行精确分词，获得了 1MB/s 的速度，是单进程版的 3.3 倍。</p><p><strong>注意</strong>：并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。</p><ul><li>Tokenize：返回词语在原文的起止位置</li></ul><p>输入参数只接受 unicode</p><p><strong>默认模式</strong></p><pre><code>result = jieba.tokenize(u'永和服装饰品有限公司')for tk in result:    print("word %s\t\t start: %d \t\t end:%d" % (tk[0],tk[1],tk[2]))</code></pre><pre><code>word 永和                start: 0                end:2word 服装                start: 2                end:4word 饰品                start: 4                end:6word 有限公司            start: 6                end:10</code></pre><p><strong>搜索模式</strong></p><pre><code>result = jieba.tokenize(u'永和服装饰品有限公司', mode='search')for tk in result:    print("word %s\t\t start: %d \t\t end:%d" % (tk[0],tk[1],tk[2]))</code></pre><pre><code>word 永和                start: 0                end:2word 服装                start: 2                end:4word 饰品                start: 4                end:6word 有限                start: 6                end:8word 公司                start: 8                end:10word 有限公司            start: 6                end:10</code></pre><h1 class=pgc-h-arrow-right>其他词典</h1><p>有一些开源的自定义词典文件，可以直接使用</p><li>占用内存较小的词典文件 https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.small</li><li>支持繁体分词更好的词典件 https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big</li><p>下载你所需要的词典，然后覆盖 jieba/dict.txt 即可；或者用 jieba.set_dictionary('data/dict.txt.big')</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'语言','处理','分词'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>