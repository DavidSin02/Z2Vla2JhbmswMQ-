<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>理解梯度下降 | 极客快訊</title><meta property="og:title" content="理解梯度下降 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/27c4a533219b46df901c0c34ad0937bd"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f547bc70.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f547bc70.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f547bc70.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f547bc70.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f547bc70.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f547bc70.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f547bc70.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f547bc70.html><meta property="article:published_time" content="2020-11-14T21:07:33+08:00"><meta property="article:modified_time" content="2020-11-14T21:07:33+08:00"><meta name=Keywords content><meta name=description content="理解梯度下降"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/f547bc70.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>理解梯度下降</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p><br></p><h1 class=pgc-h-arrow-right>介绍</h1><p>在这篇文章中，我们将了解什么是真正的梯度下降，为什么它变得流行，为什么AI和ML中的大多数算法都遵循这种技术。</p><p>在开始之前，梯度下降实际上意味着什么？听起来很奇怪对吧！</p><p>柯西是1847年第一个提出梯度下降的人</p><p>嗯，梯度这个词的意思是一个性质的增加和减少！而下降意味着向下移动的动作。所以，总的来说，在下降到某个地方然后观察并且继续下降的行为被称为梯度下降</p><div class=pgc-img><img alt=理解梯度下降 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/27c4a533219b46df901c0c34ad0937bd><p class=pgc-img-caption></p></div><p>所以，在正常情况下，如图所示，山顶的坡度很高，通过不断的移动，当你到达山脚时的坡度最小，或者接近或等于零。同样的情况在数学上也适用。</p><p>让我们看看怎么做</p><h1 class=pgc-h-arrow-right>梯度下降数学</h1><div class=pgc-img><img alt=理解梯度下降 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/98fe065853a647b886ca6404fa89403b><p class=pgc-img-caption></p></div><p>所以，如果你看到这里的形状和这里的山是一样的。我们假设这是一条形式为y=f（x）的曲线。</p><p>这里我们知道，任何一点上的斜率都是y对x的导数，如果你用曲线来检查，你会发现，当向下移动时，斜率在尖端或最小位置减小并等于零，当我们再次向上移动时，斜率会增加</p><p>记住这一点，我们将研究在最小点处x和y的值会发生什么，</p><p>观察下图，我们有不同位置的五个点！</p><div class=pgc-img><img alt=理解梯度下降 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5a4e215ce37f449c8f4923acd594e6fb><p class=pgc-img-caption></p></div><p>当我们向下移动时，我们会发现y值会减小，所以在这里的所有点中，我们在图的底部得到了相对最小的值。因此，我们的结论是我们总是在图的底部找到最小值（x，y）。现在让我们看看如何在ML和DL中传递这个，以及如何在不遍历整个图的情况下达到最小点？</p><p>在任何一种算法中，我们的主要目的是最小化损失，这表明我们的模型表现良好。为了分析这一点，我们将使用线性回归</p><div class=pgc-img><img alt=理解梯度下降 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8b3d2540467b4328a1d8173cc64b7e7a><p class=pgc-img-caption></p></div><p>因为线性回归使用直线来预测连续输出-</p><p><strong>设直线y=w*x+c</strong></p><p>这里我们需要找到w和c，这样我们就得到了使误差最小化的最佳拟合线。所以我们的目标是找到最佳的w和c值</p><p>我们从一些随机值开始w和c，我们根据损失更新这些值，也就是说，我们更新这些权重，直到斜率等于或接近于零。</p><p>我们将取y轴上的损失函数，x轴上有w和c。查看下图-</p><div class=pgc-img><img alt=理解梯度下降 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c2843a2f3638438287b148d7558bd5b2><p class=pgc-img-caption></p></div><p>为了在第一个图中达到最小的w值，请遵循以下步骤-</p><ol start=1><li>用w和c开始计算给定的一组x _values的损失。</li><li>绘制点，现在将权重更新为- <strong>w_new =w_old – learning_rate * slope at (w_old,loss)</strong></li></ol><p>重复这些步骤，直到达到最小值！</p><ul><li>我们在这里减去梯度，因为我们想移到山脚下，或者朝着最陡的下降方向移动</li><li>当我们减去后，我们会得到一个比前一个小的斜率，这就是我们想要移动到斜率等于或接近于零的点</li><li>我们稍后再讨论学习率</li></ul><p>这同样适用于图2，即损失和c的函数</p><div class=pgc-img><img alt=理解梯度下降 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/5f96431696664d07ae8e3b361e44e0bd><p class=pgc-img-caption></p></div><p>现在的问题是为什么要把学习率放在等式中？因为我们不能在起点和最小值之间遍历所有的点</p><p>我们需要跳过一些点</p><ul><li>我们可以在最初阶段采取大步行动。</li></ul><ul><li>但是，当我们接近最小值时，我们需要小步走，因为我们可能会越过最小值，移动到一个斜坡的地方增加。为了控制图的步长和移动，引入了学习速率。即使没有学习速率，我们也会得到最小值，但我们关心的是我们的算法要更快!!</li></ul><div class=pgc-img><img alt=理解梯度下降 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0b605a5b0d424d118284817f6361f09c><p class=pgc-img-caption></p></div><p>下面是一个使用梯度下降的线性回归的示例算法。这里我们用均方误差作为损失函数-</p><p>1.用零初始化模型参数</p><p>m=0，c=0</p><p>2.使用（0,1）范围内的任何值初始化学习速率</p><p>lr=0.01</p><p>误差方程-</p><div class=pgc-img><img alt=理解梯度下降 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5183ac20fe3548f9afb4f92c7bad1b6a><p class=pgc-img-caption></p></div><p>现在用（w*x+c）代替Ypred并计算偏导</p><div class=pgc-img><img alt=理解梯度下降 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/094959c8dc4b49b9af18ffc9d6dbefb7><p class=pgc-img-caption></p></div><p>3.c也一样可以计算得出</p><div class=pgc-img><img alt=理解梯度下降 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bb2990ecb53e43e79e893e6d5b6cb663><p class=pgc-img-caption></p></div><p>4.将此应用于所有epoch的数据集</p><pre><code>for i in range(epochs):          y_pred = w * x +c          D_M = (-2/n) * sum(x * (y_original - y_pred))          D_C = (-2/n) * sum(y_original - y_pred)</code></pre><p>这里求和函数一次性将所有点的梯度相加！</p><p>更新所有迭代的参数</p><p>W = W – lr * D_M</p><p>C = C – lr * D_C</p><p>梯度下降法用于神经网络的深度学习…</p><div class=pgc-img><img alt=理解梯度下降 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a3950e2ecf5d492d897195dc1999c37d><p class=pgc-img-caption></p></div><p>在这里，我们更新每个神经元的权值，以便在最小误差的情况下得到最佳分类。我们使用梯度下降法来更新每一层的所有权值…</p><p>Wi = Wi – learning_rate * derivative (Loss function w.r.t Wi)</p><h1 class=pgc-h-arrow-right>为什么它受欢迎？</h1><p>梯度下降是目前机器学习和深度学习中最常用的优化策略。</p><p>它用于训练数据模型，可以与各种算法相结合，易于理解和实现</p><p>许多统计技术和方法使用GD来最小化和优化它们的过程。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'梯度','理解','下降'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>