<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>1ç¾å…ƒè®­ç»ƒBERTï¼Œæ•™ä½ å¦‚ä½•è–…è°·æ­ŒTPUç¾Šæ¯›ï½œé™„Colabä»£ç  | æå®¢å¿«è¨Š</title><meta property="og:title" content="1ç¾å…ƒè®­ç»ƒBERTï¼Œæ•™ä½ å¦‚ä½•è–…è°·æ­ŒTPUç¾Šæ¯›ï½œé™„Colabä»£ç  - æå®¢å¿«è¨Š"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/7e7fa9a848e043528bbbdc2b5acd77ca"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5602a09.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5602a09.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f5602a09.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5602a09.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5602a09.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f5602a09.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f5602a09.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5602a09.html><meta property="article:published_time" content="2020-10-29T21:12:41+08:00"><meta property="article:modified_time" content="2020-10-29T21:12:41+08:00"><meta name=Keywords content><meta name=description content="1ç¾å…ƒè®­ç»ƒBERTï¼Œæ•™ä½ å¦‚ä½•è–…è°·æ­ŒTPUç¾Šæ¯›ï½œé™„Colabä»£ç "><meta name=author content="æå®¢å¿«è¨Š"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/f5602a09.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>ğŸ¤“ æå®¢å¿«è®¯ Geek Bank</a></h1><p class=description>ä¸ºä½ å¸¦æ¥æœ€å…¨çš„ç§‘æŠ€çŸ¥è¯† ğŸ§¡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>çŒœä½ å–œæ­¡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=ç§‘æŠ€>ç§‘æŠ€</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=éŠæˆ²>éŠæˆ²</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=ç§‘å­¸>ç§‘å­¸</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>1ç¾å…ƒè®­ç»ƒBERTï¼Œæ•™ä½ å¦‚ä½•è–…è°·æ­ŒTPUç¾Šæ¯›ï½œé™„Colabä»£ç </h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>ç§‘æŠ€</a></span></div><div class=post-content><div><blockquote><p>æ™“æŸ¥ å‘è‡ª å‡¹éå¯º</p><p>é‡å­ä½ å‡ºå“ | å…¬ä¼—å· QbitAI</p></blockquote><p class=ql-align-center><br></p><div class=pgc-img><img alt=1ç¾å…ƒè®­ç»ƒBERTï¼Œæ•™ä½ å¦‚ä½•è–…è°·æ­ŒTPUç¾Šæ¯›ï½œé™„Colabä»£ç  onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7e7fa9a848e043528bbbdc2b5acd77ca><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>BERTæ˜¯è°·æ­Œå»å¹´æ¨å‡ºçš„NLPæ¨¡å‹ï¼Œä¸€ç»æ¨å‡ºå°±åœ¨å„é¡¹æµ‹è¯•ä¸­ç¢¾å‹ç«äº‰å¯¹æ‰‹ï¼Œè€Œä¸”BERTæ˜¯å¼€æºçš„ã€‚åªå¯æƒœè®­ç»ƒBERTçš„ä»·æ ¼å®åœ¨å¤ªé«˜ï¼Œè®©äººæœ›è€Œå´æ­¥ã€‚</p><p>ä¹‹å‰éœ€è¦ç”¨64ä¸ªTPUè®­ç»ƒ4å¤©æ‰èƒ½å®Œæˆï¼Œåæ¥è°·æ­Œç”¨å¹¶è¡Œè®¡ç®—ä¼˜åŒ–äº†åˆ°åªéœ€ä¸€ä¸ªå¤šå°æ—¶ï¼Œä½†æ˜¯éœ€è¦çš„TPUæ•°é‡é™¡å¢ï¼Œè¾¾åˆ°äº†æƒŠäººçš„1024ä¸ªã€‚</p><p>é‚£ä¹ˆæ€»å…±è¦å¤šå°‘é’±å‘¢ï¼Ÿè°·æ­Œäº‘TPUçš„ä½¿ç”¨ä»·æ ¼æ˜¯æ¯ä¸ªæ¯å°æ—¶6.5ç¾å…ƒï¼Œè®­ç»ƒå®Œæˆè®­ç»ƒå®Œæ•´ä¸ªæ¨¡å‹éœ€è¦è¿‘4ä¸‡ç¾å…ƒï¼Œç®€ç›´å°±æ˜¯å¤©ä»·ã€‚</p><p>ç°åœ¨ï¼Œæœ‰ä¸ªç¾Šæ¯›å‘Šè¯‰ä½ ï¼Œåœ¨åŸ¹å…»åŸºä¸Šæœ‰äººæ‰¾åˆ°äº†è–…è°·æ­Œç¾Šæ¯›çš„åŠæ³•ï¼Œåªéœ€1ç¾å…ƒå°±èƒ½è®­ç»ƒBERTï¼Œæ¨¡å‹è¿˜èƒ½ç•™å­˜åœ¨ä½ çš„è°·æ­Œäº‘ç›˜ä¸­ï¼Œç•™ä½œä»¥åä½¿ç”¨ã€‚</p><p><strong>å‡†å¤‡å·¥ä½œ</strong></p><p>ä¸ºäº†è–…è°·æ­Œçš„ç¾Šæ¯›ï¼Œæ‚¨éœ€è¦ä¸€ä¸ªGoogleäº‘å­˜å‚¨ï¼ˆGoogle Cloud Storageï¼‰ç©ºé—´ã€‚æŒ‰ç…§Googleäº‘TPUå¿«é€Ÿå…¥é—¨æŒ‡å—ï¼Œåˆ›å»ºGoogleäº‘å¹³å°ï¼ˆGoogle Cloud Platformï¼‰å¸æˆ·å’ŒGoogleäº‘å­˜å‚¨è´¦æˆ·ã€‚æ–°çš„è°·æ­Œäº‘å¹³å°ç”¨æˆ·å¯è·å¾—300ç¾å…ƒçš„å…è´¹èµ é€é‡‘é¢ã€‚</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=1ç¾å…ƒè®­ç»ƒBERTï¼Œæ•™ä½ å¦‚ä½•è–…è°·æ­ŒTPUç¾Šæ¯›ï½œé™„Colabä»£ç  onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ad9854f2e94c43b691b97771651cbe19><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>åœ¨TPUv2ä¸Šé¢„è®­ç»ƒBERT-Baseæ¨¡å‹å¤§çº¦éœ€è¦54å°æ—¶.Google Colabå¹¶éè®¾è®¡ç”¨äºæ‰§è¡Œé•¿æ—¶é—´è¿è¡Œçš„ä½œä¸šï¼Œå®ƒä¼šæ¯8å°æ—¶å·¦å³ä¸­æ–­ä¸€æ¬¡è®­ç»ƒè¿‡ç¨‹ã€‚å¯¹äºä¸é—´æ–­çš„è®­ç»ƒï¼Œè¯·è€ƒè™‘ä½¿ç”¨ä»˜è´¹çš„ä¸é—´æ–­ä½¿ç”¨TPUv2çš„æ–¹æ³•ã€‚</p><p>ä¹Ÿå°±æ˜¯è¯´ï¼Œä½¿ç”¨Colab TPUï¼Œä½ å¯ä»¥åœ¨ä»¥1ç¾å…ƒçš„ä»·æ ¼åœ¨è°·äº‘ç›˜ä¸Šå­˜å‚¨æ¨¡å‹å’Œæ•°æ®ï¼Œä»¥å‡ ä¹å¯å¿½ç•¥æˆæœ¬ä»å¤´å¼€å§‹é¢„è®­ç»ƒBERTæ¨¡å‹ã€‚</p><p>ä»¥ä¸‹æ˜¯æ•´ä¸ªè¿‡ç¨‹çš„ä»£ç ä¸‹é¢çš„ä»£ç ï¼Œå¯ä»¥åœ¨Colab Jupyterç¯å¢ƒä¸­è¿è¡Œã€‚</p><h1><strong>è®¾ç½®è®­ç»ƒç¯å¢ƒ</strong></h1><p>é¦–å…ˆï¼Œå®‰è£…è®­ç»ƒæ¨¡å‹æ‰€éœ€çš„åŒ….Jupyterå…è®¸ä½¿ç”¨ç›´æ¥ä»ç¬”è®°æœ¬æ‰§è¡Œçš„bashå‘½ä»¤ 'ï¼'ï¼š</p><pre>!pip install sentencepiece!git clone https://github.com/google-research/bert</pre><p>å¯¼å…¥åŒ…å¹¶åœ¨è°·æ­Œäº‘ä¸­æˆæƒï¼š</p><pre>import osimport sysimport jsonimport nltkimport randomimport loggingimport tensorflow as tfimport sentencepiece as spmfrom glob import globfrom google.colab import auth, drivefrom tensorflow.keras.utils import Progbarsys.path.append("bert")from bert import modeling, optimization, tokenizationfrom bert.run_pretraining import input_fn_builder, model_fn_builderauth.authenticate_user()# configure logginglog = logging.getLogger('tensorflow')log.setLevel(logging.INFO)# create formatter and add it to the handlersformatter = logging.Formatter('%(asctime)s : %(message)s')sh = logging.StreamHandler()sh.setLevel(logging.INFO)sh.setFormatter(formatter)log.handlers = [sh]if 'COLAB_TPU_ADDR' in os.environ: log.info("Using TPU runtime") USE_TPU = True TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR'] with tf.Session(TPU_ADDRESS) as session: log.info('TPU address is ' + TPU_ADDRESS) # Upload credentials to TPU. with open('/content/adc.json', 'r') as f: auth_info = json.load(f) tf.contrib.cloud.configure_gcs(session, credentials=auth_info)else: log.warning('Not connected to TPU runtime') USE_TPU = False</pre><h1><strong>ä¸‹è½½åŸå§‹æ–‡æœ¬æ•°æ®</strong></h1><p>æ¥ä¸‹æ¥ä»ç½‘ç»œä¸Šè·å–æ–‡æœ¬æ•°æ®è¯­æ–™åº“ã€‚åœ¨æœ¬æ¬¡å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨OpenSubtitlesæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…æ‹¬65ç§è¯­è¨€ã€‚</p><p>ä¸æ›´å¸¸ç”¨çš„æ–‡æœ¬æ•°æ®é›†ï¼ˆå¦‚ç»´åŸºç™¾ç§‘ï¼‰ä¸åŒï¼Œå®ƒä¸éœ€è¦ä»»ä½•å¤æ‚çš„é¢„å¤„ç†ï¼Œæä¾›é¢„æ ¼å¼åŒ–ï¼Œä¸€è¡Œä¸€ä¸ªå¥å­ã€‚</p><pre>AVAILABLE = {'af','ar','bg','bn','br','bs','ca','cs', 'da','de','el','en','eo','es','et','eu', 'fa','fi','fr','gl','he','hi','hr','hu', 'hy','id','is','it','ja','ka','kk','ko', 'lt','lv','mk','ml','ms','nl','no','pl', 'pt','pt_br','ro','ru','si','sk','sl','sq', 'sr','sv','ta','te','th','tl','tr','uk', 'ur','vi','ze_en','ze_zh','zh','zh_cn', 'zh_en','zh_tw','zh_zh'}LANG_CODE = "en" #@param {type:"string"}assert LANG_CODE in AVAILABLE, "Invalid language code selected"!wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/OpenSubtitles.raw.'$LANG_CODE'.gz -O dataset.txt.gz!gzip -d dataset.txt.gz!tail dataset.txt</pre><p>ä½ å¯ä»¥é€šè¿‡è®¾ç½®ä»£ç éšæ„é€‰æ‹©ä½ éœ€è¦çš„è¯­è¨€ã€‚å‡ºäºæ¼”ç¤ºç›®çš„ï¼Œä»£ç åªé»˜è®¤ä½¿ç”¨æ•´ä¸ªè¯­æ–™åº“çš„ä¸€å°éƒ¨åˆ†ã€‚åœ¨å®é™…è®­ç»ƒæ¨¡å‹æ—¶ï¼Œè¯·åŠ¡å¿…å–æ¶ˆé€‰ä¸­DEMO_MODEå¤é€‰æ¡†ï¼Œä½¿ç”¨å¤§100å€çš„æ•°æ®é›†ã€‚</p><p>å½“ç„¶ï¼Œ100Mæ•°æ®è¶³ä»¥è®­ç»ƒå‡ºç›¸å½“ä¸é”™çš„BERTåŸºç¡€æ¨¡å‹ã€‚</p><pre>DEMO_MODE = True #@param {type:"boolean"}if DEMO_MODE: CORPUS_SIZE = 1000000else: CORPUS_SIZE = 100000000 #@param {type: "integer"}!(head -n $CORPUS_SIZE dataset.txt) &gt; subdataset.txt!mv subdataset.txt dataset.txt</pre><h1><strong>é¢„å¤„ç†æ–‡æœ¬æ•°æ®</strong></h1><p>æˆ‘ä»¬ä¸‹è½½çš„åŸå§‹æ–‡æœ¬æ•°æ®åŒ…å«æ ‡ç‚¹ç¬¦å·ï¼Œå¤§å†™å­—æ¯å’ŒéUTFç¬¦å·ï¼Œæˆ‘ä»¬å°†åœ¨ç»§ç»­ä¸‹ä¸€æ­¥ä¹‹å‰å°†å…¶åˆ é™¤ã€‚åœ¨æ¨ç†æœŸé—´ï¼Œæˆ‘ä»¬å°†å¯¹æ–°æ•°æ®åº”ç”¨ç›¸åŒçš„è¿‡ç¨‹ã€‚</p><p>å¦‚æœä½ éœ€è¦ä¸åŒçš„é¢„å¤„ç†æ–¹å¼ï¼ˆä¾‹å¦‚åœ¨æ¨ç†æœŸé—´é¢„æœŸä¼šå‡ºç°å¤§å†™å­—æ¯æˆ–æ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œè¯·ä¿®æ”¹ä»¥ä¸‹ä»£ç ä»¥æ»¡è¶³ä½ çš„éœ€æ±‚ã€‚</p><pre>regex_tokenizer = nltk.RegexpTokenizer("\w+")def normalize_text(text): # lowercase text text = str(text).lower() # remove non-UTF text = text.encode("utf-8", "ignore").decode() # remove punktuation symbols text = " ".join(regex_tokenizer.tokenize(text)) return textdef count_lines(filename): count = 0 with open(filename) as fi: for line in fi: count += 1 return count</pre><p>ç°åœ¨è®©æˆ‘ä»¬é¢„å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼š</p><pre>RAW_DATA_FPATH = "dataset.txt" #@param {type: "string"}PRC_DATA_FPATH = "proc_dataset.txt" #@param {type: "string"}# apply normalization to the dataset# this will take a minute or twototal_lines = count_lines(RAW_DATA_FPATH)bar = Progbar(total_lines)with open(RAW_DATA_FPATH,encoding="utf-8") as fi: with open(PRC_DATA_FPATH, "w",encoding="utf-8") as fo: for l in fi: fo.write(normalize_text(l)+"\n") bar.add(1)</pre><h1><strong>æ„å»ºè¯æ±‡è¡¨</strong></h1><p>ä¸‹ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†è®­ç»ƒæ¨¡å‹å­¦ä¹ ä¸€ä¸ªæ–°çš„è¯æ±‡è¡¨ï¼Œç”¨äºè¡¨ç¤ºæˆ‘ä»¬çš„æ•°æ®é›†ã€‚</p><p>BERTæ–‡ä»¶ä½¿ç”¨WordPieceåˆ†è¯å™¨ï¼Œåœ¨å¼€æºä¸­ä¸å¯ç”¨ã€‚æˆ‘ä»¬å°†åœ¨å•å­—æ¨¡å¼ä¸‹ä½¿ç”¨SentencePieceåˆ†è¯å™¨ã€‚è™½ç„¶å®ƒä¸BERTä¸ç›´æ¥å…¼å®¹ï¼Œä½†æ˜¯é€šè¿‡ä¸€ä¸ªå°çš„å¤„ç†æ–¹æ³•ï¼Œå¯ä»¥ä½¿å®ƒå·¥ä½œã€‚</p><p>SentencePieceéœ€è¦ç›¸å½“å¤šçš„è¿è¡Œå†…å­˜ï¼Œå› æ­¤åœ¨Colabä¸­çš„è¿è¡Œå®Œæ•´æ•°æ®é›†ä¼šå¯¼è‡´å†…æ ¸å´©æºƒã€‚</p><p>ä¸ºé¿å…è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å°†éšæœºå¯¹æ•°æ®é›†çš„ä¸€å°éƒ¨åˆ†è¿›è¡Œå­é‡‡æ ·ï¼Œæ„å»ºè¯æ±‡è¡¨ã€‚å¦ä¸€ä¸ªé€‰æ‹©æ˜¯ä½¿ç”¨æ›´å¤§å†…å­˜çš„æœºå™¨æ¥æ‰§è¡Œæ­¤æ­¥éª¤ã€‚</p><p>æ­¤å¤–ï¼ŒSentencePieceé»˜è®¤æƒ…å†µä¸‹å°†BOSå’ŒEOSæ§åˆ¶ç¬¦å·æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚æˆ‘ä»¬é€šè¿‡å°†å…¶ç´¢å¼•è®¾ç½®ä¸º-1æ¥ç¦ç”¨å®ƒä»¬ã€‚</p><p>VOC_SIZEçš„å…¸å‹å€¼ä»‹äº32000å’Œ128000ä¹‹é—´ã€‚å¦‚æœæƒ³è¦æ›´æ–°è¯æ±‡è¡¨ï¼Œå¹¶åœ¨é¢„è®­ç»ƒé˜¶æ®µç»“æŸåå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬ä¼šä¿ç•™NUM_PLACEHOLDERSä¸ªä»¤ç‰Œã€‚</p><pre>MODEL_PREFIX = "tokenizer" #@param {type: "string"}VOC_SIZE = 32000 #@param {type:"integer"}SUBSAMPLE_SIZE = 12800000 #@param {type:"integer"}NUM_PLACEHOLDERS = 256 #@param {type:"integer"}SPM_COMMAND = ('--input={} --model_prefix={} ' '--vocab_size={} --input_sentence_size={} ' '--shuffle_input_sentence=true '  '--bos_id=-1 --eos_id=-1').format( PRC_DATA_FPATH, MODEL_PREFIX,  VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)spm.SentencePieceTrainer.Train(SPM_COMMAND)</pre><p>ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•è®©SentencePieceåœ¨BERTæ¨¡å‹ä¸Šå·¥ä½œã€‚</p><p>ä¸‹é¢æ˜¯ä½¿ç”¨æ¥è‡ªå®˜æ–¹çš„é¢„è®­ç»ƒè‹±è¯­BERTåŸºç¡€æ¨¡å‹çš„WordPieceè¯æ±‡è¡¨æ ‡è®°çš„è¯­å¥ã€‚</p><pre>&gt;&gt;&gt; wordpiece.tokenize("Colorless geothermal substations are generating furiously")['color', '##less', 'geo', '##thermal', 'sub', '##station', '##s', 'are', 'generating', 'furiously']</pre><p>WordPieceæ ‡è®°å™¨åœ¨â€œ##â€çš„å•è¯ä¸­é—´é¢„ç½®äº†å‡ºç°çš„å­å­—ã€‚åœ¨å•è¯å¼€å¤´å‡ºç°çš„å­è¯ä¸å˜ã€‚å¦‚æœå­è¯å‡ºç°åœ¨å•è¯çš„å¼€å¤´å’Œä¸­é—´ï¼Œåˆ™ä¸¤ä¸ªç‰ˆæœ¬ï¼ˆå¸¦å’Œä¸å¸¦â€ ##'ï¼‰éƒ½ä¼šæ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚</p><p>SentencePieceåˆ›å»ºäº†ä¸¤ä¸ªæ–‡ä»¶ï¼štokenizer.modelå’Œtokenizer.vocabè®©æˆ‘ä»¬æ¥çœ‹çœ‹å®ƒå­¦åˆ°çš„è¯æ±‡ï¼š</p><pre>def read_sentencepiece_vocab(filepath): voc = [] with open(filepath, encoding='utf-8') as fi: for line in fi: voc.append(line.split("\t")[0]) # skip the first &lt;unk&gt; token voc = voc[1:] return vocsnt_vocab = read_sentencepiece_vocab("{}.vocab".format(MODEL_PREFIX))print("Learnt vocab size: {}".format(len(snt_vocab)))print("Sample tokens: {}".format(random.sample(snt_vocab, 10)))</pre><p>è¿è¡Œç»“æœï¼š</p><pre>Learnt vocab size: 31743 Sample tokens: ['â–cafe', 'â–slippery', 'xious', 'â–resonate', 'â–terrier', 'â–feat', 'â–frequencies', 'ainty', 'â–punning', 'modern']</pre><p>SentencePieceä¸WordPieceçš„è¿è¡Œç»“æœå®Œå…¨ç›¸åä»æ–‡æ¡£ä¸­å¯ä»¥çœ‹å‡ºï¼šSentencePieceé¦–å…ˆä½¿ç”¨å…ƒç¬¦å·â€œ_â€å°†ç©ºæ ¼è½¬ä¹‰ä¸ºç©ºæ ¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š</p><ul><li><br></li></ul><pre>Hello_Worldã€‚</pre><p>ç„¶åæ–‡æœ¬è¢«åˆ†æ®µä¸ºå°å—ï¼š</p><ul><li><br></li></ul><pre>[Hello] [_Wor] [ld] [.]</pre><p>åœ¨ç©ºæ ¼ä¹‹åå‡ºç°çš„å­è¯ï¼ˆä¹Ÿæ˜¯å¤§å¤šæ•°è¯å¼€å¤´çš„å­è¯ï¼‰å‰é¢åŠ ä¸Šâ€œ_â€ï¼Œè€Œå…¶ä»–å­è¯ä¸å˜ã€‚è¿™æ’é™¤äº†ä»…å‡ºç°åœ¨å¥å­å¼€å¤´è€Œä¸æ˜¯å…¶ä»–åœ°æ–¹çš„å­è¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ¡ˆä»¶åº”è¯¥éå¸¸ç½•è§ã€‚</p><p>å› æ­¤ï¼Œä¸ºäº†è·å¾—ç±»ä¼¼äºWordPieceçš„è¯æ±‡è¡¨ï¼Œæˆ‘ä»¬éœ€è¦æ‰§è¡Œä¸€ä¸ªç®€å•çš„è½¬æ¢ï¼Œä»åŒ…å«å®ƒçš„æ ‡è®°ä¸­åˆ é™¤â€œ_â€ï¼Œå¹¶å°†â€œ##â€æ·»åŠ åˆ°ä¸åŒ…å«å®ƒçš„æ ‡è®°ä¸­ã€‚</p><p>æˆ‘ä»¬è¿˜æ·»åŠ äº†ä¸€äº›BERTæ¶æ„æ‰€éœ€çš„ç‰¹æ®Šæ§åˆ¶ç¬¦å·ã€‚æŒ‰ç…§æƒ¯ä¾‹ï¼Œæˆ‘ä»¬æŠŠå®ƒä»¬æ”¾åœ¨è¯æ±‡çš„å¼€å¤´ã€‚</p><p>å¦å¤–ï¼Œæˆ‘ä»¬åœ¨è¯æ±‡è¡¨ä¸­æ·»åŠ äº†ä¸€äº›å ä½ç¬¦æ ‡è®°ã€‚</p><p>å¦‚æœä½ å¸Œæœ›ä½¿ç”¨æ–°çš„ç”¨äºç‰¹å®šä»»åŠ¡çš„ä»¤ç‰Œæ¥æ›´æ–°é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹ï¼Œé‚£ä¹ˆè¿™äº›æ–¹æ³•æ˜¯å¾ˆæœ‰ç”¨çš„ã€‚</p><p>åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå ä½ç¬¦æ ‡è®°è¢«æ›¿æ¢ä¸ºæ–°çš„ä»¤ç‰Œï¼Œé‡æ–°ç”Ÿæˆé¢„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”å¯¹æ–°æ•°æ®è¿›è¡Œå¾®è°ƒã€‚</p><pre>def parse_sentencepiece_token(token): if token.startswith("â–"): return token[1:] else: return "##" + tokenbert_vocab = list(map(parse_sentencepiece_token, snt_vocab))ctrl_symbols = ["[PAD]","[UNK]","[CLS]","[SEP]","[MASK]"]bert_vocab = ctrl_symbols + bert_vocabbert_vocab += ["[UNUSED_{}]".format(i) for i in range(VOC_SIZE - len(bert_vocab))]print(len(bert_vocab))</pre><p>æœ€åï¼Œæˆ‘ä»¬å°†è·å¾—çš„è¯æ±‡è¡¨å†™å…¥æ–‡ä»¶ã€‚</p><pre>VOC_FNAME = "vocab.txt" #@param {type:"string"}with open(VOC_FNAME, "w") as fo: for token in bert_vocab: fo.write(token+"\n")</pre><p>ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ–°è¯æ±‡åœ¨å®è·µä¸­æ˜¯å¦‚ä½•è¿ä½œçš„ï¼š</p><pre>&gt;&gt;&gt; testcase = "Colorless geothermal substations are generating furiously"&gt;&gt;&gt; bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)&gt;&gt;&gt; bert_tokenizer.tokenize(testcase)['color',  '##less',  'geo',  '##ther',  '##mal',  'sub',  '##station',  '##s',  'are',  'generat',  '##ing',  'furious',  '##ly']</pre><h1><strong>åˆ›å»ºåˆ†ç‰‡é¢„è®­ç»ƒæ•°æ®ï¼ˆç”Ÿæˆé¢„è®­ç»ƒæ•°æ®ï¼‰</strong></h1><p>é€šè¿‡æ‰‹å¤´çš„è¯æ±‡è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºBERTæ¨¡å‹ç”Ÿæˆé¢„è®­ç»ƒæ•°æ®ã€‚</p><p>ç”±äºæˆ‘ä»¬çš„æ•°æ®é›†å¯èƒ½éå¸¸å¤§ï¼Œæˆ‘ä»¬å°†å…¶æ‹†åˆ†ä¸ºç¢ç‰‡ï¼š</p><pre>mkdir ./shardssplit -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_</pre><p>ç°åœ¨ï¼Œå¯¹äºæ¯ä¸ªéƒ¨åˆ†ï¼Œæˆ‘ä»¬éœ€è¦ä»BERTä»“åº“è°ƒç”¨create_pretraining_data.pyè„šæœ¬ï¼Œéœ€è¦ä½¿ç”¨xargsçš„å‘½ä»¤ã€‚</p><p>åœ¨å¼€å§‹ç”Ÿæˆä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®ä¸€äº›å‚æ•°ä¼ é€’ç»™è„šæœ¬ã€‚ä½ å¯ä»¥ä»è‡ªè¿°æ–‡ä»¶ä¸­æ‰¾åˆ°æœ‰å…³å®ƒä»¬å«ä¹‰çš„æ›´å¤šä¿¡æ¯ã€‚</p><pre>MAX_SEQ_LENGTH = 128 #@param {type:"integer"}MASKED_LM_PROB = 0.15 #@paramMAX_PREDICTIONS = 20 #@param {type:"integer"}DO_LOWER_CASE = True #@param {type:"boolean"}PRETRAINING_DIR = "pretraining_data" #@param {type:"string"}# controls how many parallel processes xargs can createPROCESSES = 2 #@param {type:"integer"}</pre><p>è¿è¡Œæ­¤æ“ä½œå¯èƒ½éœ€è¦ç›¸å½“é•¿çš„æ—¶é—´ï¼Œå…·ä½“å–å†³äºæ•°æ®é›†çš„å¤§å°ã€‚</p><pre>XARGS_CMD = ("ls ./shards/ | " "xargs -n 1 -P {} -I{} " "python3 bert/create_pretraining_data.py " "--input_file=./shards/{} " "--output_file={}/{}.tfrecord " "--vocab_file={} " "--do_lower_case={} " "--max_predictions_per_seq={} " "--max_seq_length={} " "--masked_lm_prob={} " "--random_seed=34 " "--dupe_factor=5")XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}',  VOC_FNAME, DO_LOWER_CASE,  MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)tf.gfile.MkDir(PRETRAINING_DIR)!$XARGS_CMD</pre><h1><strong>ä¸ºæ•°æ®å’Œæ¨¡å‹è®¾ç½®GCSå­˜å‚¨ï¼Œå°†æ•°æ®å’Œæ¨¡å‹å­˜å‚¨åˆ°äº‘ç«¯</strong></h1><p>ä¸ºäº†ä¿ç•™æ¥ä¹‹ä¸æ˜“çš„è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬ä¼šå°†å…¶ä¿ç•™åœ¨è°·æ­Œäº‘å­˜å‚¨ä¸­ã€‚</p><p>åœ¨è°·æ­Œäº‘å­˜å‚¨ä¸­åˆ›å»ºä¸¤ä¸ªç›®å½•ï¼Œä¸€ä¸ªç”¨äºæ•°æ®ï¼Œä¸€ä¸ªç”¨äºæ¨¡å‹ã€‚åœ¨æ¨¡å‹ç›®å½•ä¸­ï¼Œæˆ‘ä»¬å°†æ”¾ç½®æ¨¡å‹è¯æ±‡è¡¨å’Œé…ç½®æ–‡ä»¶ã€‚</p><p>åœ¨ç»§ç»­æ“ä½œä¹‹å‰ï¼Œè¯·é…ç½®BUCKET_NAMEå˜é‡ï¼Œå¦åˆ™å°†æ— æ³•è®­ç»ƒæ¨¡å‹ã€‚</p><pre>BUCKET_NAME = "bert_resourses" #@param {type:"string"}MODEL_DIR = "bert_model" #@param {type:"string"}tf.gfile.MkDir(MODEL_DIR)if not BUCKET_NAME: log.warning("WARNING: BUCKET_NAME is not set. " "You will not be able to train the model.")</pre><p>ä¸‹é¢æ˜¯BERTåŸºçš„è¶…å‚æ•°é…ç½®ç¤ºä¾‹ï¼š</p><pre># use this for BERT-basebert_base_config = { "attention_probs_dropout_prob": 0.1,  "directionality": "bidi",  "hidden_act": "gelu",  "hidden_dropout_prob": 0.1,  "hidden_size": 768,  "initializer_range": 0.02,  "intermediate_size": 3072,  "max_position_embeddings": 512,  "num_attention_heads": 12,  "num_hidden_layers": 12,  "pooler_fc_size": 768,  "pooler_num_attention_heads": 12,  "pooler_num_fc_layers": 3,  "pooler_size_per_head": 128,  "pooler_type": "first_token_transform",  "type_vocab_size": 2,  "vocab_size": VOC_SIZE}with open("{}/bert_config.json".format(MODEL_DIR), "w") as fo: json.dump(bert_base_config, fo, indent=2)with open("{}/{}".format(MODEL_DIR, VOC_FNAME), "w") as fo: for token in bert_vocab: fo.write(token+"\n")</pre><p>ç°åœ¨ï¼Œæˆ‘ä»¬å·²å‡†å¤‡å¥½å°†æ¨¡å‹å’Œæ•°æ®å­˜å‚¨åˆ°è°·æ­Œäº‘å½“ä¸­ï¼š</p><pre>if BUCKET_NAME: !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME</pre><h1><strong>åœ¨äº‘TPUä¸Šè®­ç»ƒæ¨¡å‹</strong></h1><p>æ³¨æ„ï¼Œä¹‹å‰æ­¥éª¤ä¸­çš„æŸäº›å‚æ•°åœ¨æ­¤å¤„ä¸ç”¨æ”¹å˜ã€‚è¯·ç¡®ä¿åœ¨æ•´ä¸ªå®éªŒä¸­è®¾ç½®çš„å‚æ•°å®Œå…¨ç›¸åŒã€‚</p><pre>BUCKET_NAME = "bert_resourses" #@param {type:"string"}MODEL_DIR = "bert_model" #@param {type:"string"}PRETRAINING_DIR = "pretraining_data" #@param {type:"string"}VOC_FNAME = "vocab.txt" #@param {type:"string"}# Input data pipeline configTRAIN_BATCH_SIZE = 128 #@param {type:"integer"}MAX_PREDICTIONS = 20 #@param {type:"integer"}MAX_SEQ_LENGTH = 128 #@param {type:"integer"}MASKED_LM_PROB = 0.15 #@param# Training procedure configEVAL_BATCH_SIZE = 64LEARNING_RATE = 2e-5TRAIN_STEPS = 1000000 #@param {type:"integer"}SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:"integer"}NUM_TPU_CORES = 8if BUCKET_NAME: BUCKET_PATH = "gs://{}".format(BUCKET_NAME)else: BUCKET_PATH = "."BERT_GCS_DIR = "{}/{}".format(BUCKET_PATH, MODEL_DIR)DATA_GCS_DIR = "{}/{}".format(BUCKET_PATH, PRETRAINING_DIR)VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)CONFIG_FILE = os.path.join(BERT_GCS_DIR, "bert_config.json")INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))log.info("Using checkpoint: {}".format(INIT_CHECKPOINT))log.info("Using {} data shards".format(len(input_files)))</pre><p>å‡†å¤‡è®­ç»ƒè¿è¡Œé…ç½®ï¼Œå»ºç«‹è¯„ä¼°å™¨å’Œè¾“å…¥å‡½æ•°ï¼Œå¯åŠ¨BERTï¼</p><pre>model_fn = model_fn_builder( bert_config=bert_config, init_checkpoint=INIT_CHECKPOINT, learning_rate=LEARNING_RATE, num_train_steps=TRAIN_STEPS, num_warmup_steps=10, use_tpu=USE_TPU, use_one_hot_embeddings=True)tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)run_config = tf.contrib.tpu.RunConfig( cluster=tpu_cluster_resolver, model_dir=BERT_GCS_DIR, save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS, tpu_config=tf.contrib.tpu.TPUConfig( iterations_per_loop=SAVE_CHECKPOINTS_STEPS, num_shards=NUM_TPU_CORES, per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))estimator = tf.contrib.tpu.TPUEstimator( use_tpu=USE_TPU, model_fn=model_fn, config=run_config, train_batch_size=TRAIN_BATCH_SIZE, eval_batch_size=EVAL_BATCH_SIZE)train_input_fn = input_fn_builder( input_files=input_files, max_seq_length=MAX_SEQ_LENGTH, max_predictions_per_seq=MAX_PREDICTIONS, is_training=True)</pre><p>æ‰§è¡Œï¼</p><pre>estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)</pre><p>æœ€åï¼Œä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒæ¨¡å‹éœ€è¦100ä¸‡æ­¥ï¼Œçº¦54å°æ—¶çš„è¿è¡Œæ—¶é—´ã€‚å¦‚æœå†…æ ¸ç”±äºæŸç§åŸå› é‡æ–°å¯åŠ¨ï¼Œå¯ä»¥ä»æ–­ç‚¹å¤„ç»§ç»­è®­ç»ƒã€‚</p><p>ä»¥ä¸Šå°±æ˜¯æ˜¯åœ¨äº‘TPUä¸Šä»å¤´å¼€å§‹é¢„è®­ç»ƒBERTçš„æŒ‡å—ã€‚</p><h1><strong>ä¸‹ä¸€æ­¥</strong></h1><p>å¥½çš„ï¼Œæˆ‘ä»¬å·²ç»è®­ç»ƒå¥½äº†æ¨¡å‹ï¼Œæ¥ä¸‹æ¥å¯ä»¥åšä»€ä¹ˆï¼Ÿ</p><p>å¦‚å›¾1æ‰€ç¤ºï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ¨¡å‹ä½œä¸ºé€šç”¨çš„è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å—;</p><p>2ï¼Œé’ˆå¯¹æŸäº›ç‰¹å®šçš„åˆ†ç±»ä»»åŠ¡å¾®è°ƒæ¨¡å‹;</p><p>3ï¼Œä½¿ç”¨BERTä½œä¸ºæ„å»ºå—ï¼Œå»åˆ›å»ºå¦ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</p><h1><strong>ä¼ é€é—¨</strong></h1><p>åŸæ–‡åœ°å€ï¼š</p><p>https ï¼š//towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379</p><p>Colabä»£ç ï¼š</p><p>https ï¼š//colab.research.google.com/drive/1nVn6AFpQSzXBt8_ywfx6XR8ZfQXlKGAz</p><p>â€” å®Œ â€”</p><p>è¯šæŒšæ‹›è˜</p><p>é‡å­ä½æ­£åœ¨æ‹›å‹Ÿç¼–è¾‘/è®°è€…ï¼Œå·¥ä½œåœ°ç‚¹åœ¨åŒ—äº¬ä¸­å…³æ‘ã€‚æœŸå¾…æœ‰æ‰æ°”ã€æœ‰çƒ­æƒ…çš„åŒå­¦åŠ å…¥æˆ‘ä»¬ï¼ç›¸å…³ç»†èŠ‚ï¼Œè¯·åœ¨é‡å­ä½å…¬ä¼—å·(QbitAI)å¯¹è¯ç•Œé¢ï¼Œå›å¤â€œæ‹›è˜â€ä¸¤ä¸ªå­—ã€‚</p><p>é‡å­ä½ QbitAI Â· å¤´æ¡å·ç­¾çº¦ä½œè€…</p><p>Õ¾'á´—' Õ« è¿½è¸ªAIæŠ€æœ¯å’Œäº§å“æ–°åŠ¨æ€</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'è®­ç»ƒ','BERT','TPU'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=æœç´¢>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>ğŸ”</button></form></section><section class=widget><h3 class=widget-title>æœ€æ–°æ–‡ç«  âš¡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>å…¶ä»–</h3><ul class=widget-list><li><a href=TOS.html>ä½¿ç”¨æ¢æ¬¾</a></li><li><a href=CommentPolicy.html>ç•™è¨€æ”¿ç­–</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>è¯çµ¡æˆ‘å€‘</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>æå®¢å¿«è¨Š</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>