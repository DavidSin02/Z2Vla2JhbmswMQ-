<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>对 ResNet 本质的一些思考 | 极客快訊</title><meta property="og:title" content="对 ResNet 本质的一些思考 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/ba6acae0844a4e54a0d27835501368cb"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f7fbf79.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f7fbf79.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f7fbf79.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f7fbf79.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f7fbf79.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f7fbf79.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f7fbf79.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f7fbf79.html><meta property="article:published_time" content="2020-10-29T20:59:02+08:00"><meta property="article:modified_time" content="2020-10-29T20:59:02+08:00"><meta name=Keywords content><meta name=description content="对 ResNet 本质的一些思考"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/f7fbf79.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>对 ResNet 本质的一些思考</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p class=ql-align-center><br></p><div class=pgc-img><img alt="对 ResNet 本质的一些思考" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ba6acae0844a4e54a0d27835501368cb><p class=pgc-img-caption></p></div><p><strong>【新智元导读】</strong>本文作者重新审视了ResNet之所以工作的原因，得出结论: ResNet 本质上就干了一件事：降低数据中信息的冗余度。</p><p class=ql-align-justify><br></p><p class=ql-align-justify>最近在总结完成语义分割任务的轻量级神经网络时，看到了 MobileNet V2 中对于 ReLU 层的思考，于是我也回过头重新审视 ResNet 之所以 work 的本质原因。以下是一些个人的见解，如有错误，还望及时指正。</p><p class=ql-align-justify>在谈及 ResNet 之前，我们先聊聊故事的背景。</p><p class=ql-align-justify>我们知道，在神经网络中，非线性激活层可以为模型引入了非线性，让模型具有更强的拟合能力。如果只是单纯的线性操作层的叠加，则完全可以等价为一个线性层，这就浪费了深度神经网络的一身好本领。</p><p class=ql-align-justify>所谓针无两头尖，那么非线性激活层会带来一些什么问题呢？我们以 <strong>ReLU</strong> 为例来进行说明，其他非线性激活层亦同理。</p><p class=ql-align-justify>首先，最直观的，从实验中我们可以注意到一个事实：<strong>ReLU 会造成的低维数据的坍塌（collapse）</strong>。顾名思义，即是说，低维度的 feature 在通过 ReLU 的时候，这个 feature 会像塌方了一样，有一部分被毁掉了，或者说失去了。能恢复吗？能，但是基本无法百分百还原了。</p><p class=ql-align-justify>具体表现出来就是：若是我们对一个 feature，先通过一个给定的变换规则 T，将它映射到它的 embedding space 中，再在该 embedding space 中，利用一个 ReLU 去处理该 feature，最后再把这个 feature 以同样的变换规则（逆方向）给映射回原始空间，我们会发现，这时，这个 feature 已经变得连亲妈都不认得了。如图↓</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="对 ResNet 本质的一些思考" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3579245a8f3b429d8802c78c1a3be431><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>图片来自《MobileNetV2: Inverted Residuals and Linear Bottlenecks》</p><p class=ql-align-justify>ReLU 这个东西，其实就是一个滤波器，只不过这个滤波器的作用域不是信号处理中的频域，而是特征域。那么滤波器又有什么作用呢？维度压缩，俗话说就是降维啦：如果我们有 m 个 feature 被送入 ReLU 层，过滤剩下 n 个（n&lt;m），这不就是相当于对 feature 的维度进行了压缩，使其从 m 维变为 n 维嘛。</p><p class=ql-align-justify>那么，为什么低维数据流经非线性激活层会发生坍塌（信息丢失），而高维数据就不会呢？</p><p class=ql-align-justify>打个简单但不严谨的比方：大家都有过年抢高铁票的经验吧？几个人（维度低）帮你抢一张票，肯定没有一群人（维度高）帮你抢一张票，成功的概率高啊。几个人里面，大概率全军覆没，没一个能帮上你忙的。而一群人里面，大概率总有那么一个手速惊人的单身青年，帮你抢到你心心念念的回家票。</p><p class=ql-align-justify>在数据上也是一个道理，维度低的 feature，分布到 ReLU 的激活带上的概率小，因此经过后信息丢失严重，甚至可能完全丢失。而维度高的 feature，分布到 ReLU 的激活带上的概率大，虽然可能也会有信息的部分丢失，但是无伤大雅，大部分的信息仍然得以保留。所谓留得青山在，不愁没柴烧嘛。更何况被 ReLU 截杀的信息，可能只是一些无用游民（冗余信息）。</p><p class=ql-align-justify><strong>那么数据的坍塌，是个很严重的事吗？</strong></p><p class=ql-align-justify>那事儿可大了。如果把神经网络比作一个人的话，你这就是给它的某个部位的血管里，丢了个血栓。</p><p class=ql-align-justify>当信息无法流过 ReLU 时，该神经元的输出就会变为 0。而在反向传播的过程中，ReLU 对 0 值的梯度为 0，即发生了梯度消失，这将导致神经元的权重无法再通过梯度下降法进行更新，这种现象被称为特征退化。所以这个神经元相当于死掉了，丧失了学习能力。我们说，一旦神经元的输出陷入 0 值，就无法恢复了。</p><p class=ql-align-justify></p><p class=ql-align-center><br></p><div class=pgc-img><img alt="对 ResNet 本质的一些思考" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/405e3c218dc742a3baec5de8fb40e8d7><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p class=ql-align-justify>那么，我们应该<strong>怎么去规避数据的坍塌呢</strong>？非线性激活层到底是个什么样的东西？</p><p class=ql-align-justify>其实，对于一个数据，利用非线性激活层对其进行激活，其实是从该数据的信息中提取出其潜在的稀疏性，但是这种提取的结果是否正确，就要分情况讨论了。</p><p class=ql-align-justify>对于一个 M 维的数据，我们可以将其看成是在 M 维空间中的一个 M 维流形（manifold）。而其中的有用信息，就是在该 M 维空间中的一个子空间（子空间的维度记为 N 维，N&lt;=M）中的一个 N 维流形。非线性激活层相当于压缩了这个 M 维空间的维度（还记得前面提过的维度压缩吗？）。若是该 M 维空间中的 M 维流形本来就不含有冗余信息（M=N），那么再对其进行维度压缩，必然导致信息的丢失。</p><p class=ql-align-justify>而维度低的数据其实就是这么一种情况：其信息的冗余度高的可能性本来就低，如果强行对其进行非线性激活（维度压缩），则很有可能丢失掉有用信息，甚至丢失掉全部信息（输出为全 0）。</p><p class=ql-align-justify>与非线性激活层不同的是，线性激活层并不压缩特征空间的维度。于是，我们得到了一条<strong>使用激活层的原则</strong>：</p><ol><li class=ql-align-justify>对含有冗余信息的数据使用非线性激活（如 ReLU），对不含冗余信息的数据使用线性激活（如一些线性变换）。</li><li class=ql-align-justify>两种类型的激活交替灵活使用，以同时兼顾非线性和信息的完整性。</li><li class=ql-align-justify>由于冗余信息和非冗余信息所携带的有用信息是一样多的，因此在设计网络时，对内存消耗大的结构最好是用在非冗余信息上。</li></ol><p class=ql-align-justify><br></p><p class=ql-align-justify>根据以上的原则设计出来的结构，聪明的你想到了什么？ResNet。不得不赞叹 Kaiming He 的天才，ResNet 这东西，描述起来固然简单，但是对它的理解每深一层，就会愈发发现它的精妙及优雅，从数学上解释起来非常简洁，非常令人信服，而且直切传统痛点。</p><p class=ql-align-justify>ResNet 本质上就干了一件事：<strong>降低数据中信息的冗余度</strong>。</p><p class=ql-align-justify>具体说来，就是对非冗余信息采用了线性激活（通过 skip connection 获得无冗余的 identity 部分），然后对冗余信息采用了非线性激活（通过 ReLU 对 identity 之外的其余部分进行信息提取 / 过滤，提取出的有用信息即是残差）。</p><p class=ql-align-justify>其中，<strong>提取 identity 这一步，就是 ResNet 思想的核心</strong>。</p><p class=ql-align-justify>从本文的观点来看，因为从数据中拿掉了非冗余信息的 identity 部分，会导致余下部分的信息冗余度变高。这就像从接近饱和的溶液中移走了一部分溶质，会使得剩下的溶液的饱和度降低，一个道理。</p><p class=ql-align-justify>在这里也引用一下其他的一些观点，方便大家可以从一个更全面的角度去看这个问题：</p><p class=ql-align-justify>从特征复用的观点来看，提取 identity 部分，可以让网络不用再去学习一个 identity mapping（虽然是一样的东西，但是毕竟又要从头学起，讲真，换你来试试，这其实真的不容易学到），而是直接学习 residual。这就轻松愉快多了：站在巨人的肩膀上，做一点微小的工作什么的...</p><p class=ql-align-justify>既然说了 ResNet 解决的痛点，也顺便多说几句它带来的好处：</p><ol><li class=ql-align-justify>由于 identity 之外的其余部分的信息冗余度较高，因此在对其使用 ReLU 进行非线性激活时，丢失的有用信息也会较少，ReLU 层输出为 0 的可能性也会较低。这就降低了在反向传播时 ReLU 的梯度消失的概率，从而便于网络的加深，以大大地发挥深度网络的潜能。</li><li class=ql-align-justify>特征复用能加快模型的学习速度，因为参数的优化收敛得快（从 identity 的基础上直接学习残差，总比从头学习全部来得快）。</li></ol><p class=ql-align-justify><br></p><p class=ql-align-justify>最后是两个小 tips：</p><ol><li class=ql-align-justify>如果一个信息可以完整地流过一个非线性激活层，则这个非线性激活层对于这个信息而言，相当于仅仅作了一个线性激活。</li><li class=ql-align-justify>解决由非线性激活导致的反向传播梯度消失的窍门，就是要提高进行非线性激活的信息的冗余度。</li></ol><p class=ql-align-justify><br></p><p class=ql-align-justify>如果您觉得本文对您有所帮助，请高抬贵手点个赞～</p><p class=ql-align-justify>接下来会填之前语义软分割的坑和图神经网络的坑，还有一些杂七杂八的：如姿态估计网络啦、deepSLAM 啦、视觉跟踪网络啦、VQA 啦... 最近光忙着看 paper 和写笔记了，有空再整理后发上来。</p><p class=ql-align-justify>以上。</p><p class=ql-align-justify>本文经授权转载自知乎，作者黄二二</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'ResNet','本质','思考'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>