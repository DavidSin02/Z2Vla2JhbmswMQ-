<!doctype html><html lang=cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>图同构下等变,计算高效,韦灵思团队提出'自然图网络'消息传递方法 | 极客快訊</title><meta property="og:title" content="图同构下等变,计算高效,韦灵思团队提出'自然图网络'消息传递方法 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="cn"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/46068927b2b544928c767f7cb00ed2ed"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fd0df95.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fd0df95.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fd0df95.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fd0df95.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fd0df95.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fd0df95.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fd0df95.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fd0df95.html><meta property="article:published_time" content="2020-10-29T21:00:44+08:00"><meta property="article:modified_time" content="2020-10-29T21:00:44+08:00"><meta name=Keywords content><meta name=description content="图同构下等变,计算高效,韦灵思团队提出'自然图网络'消息传递方法"><meta name=author content="极客快訊"><meta property="og:url" content="/cn/%E7%A7%91%E6%8A%80/fd0df95.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快讯 Geek Bank</a></h1><p class=description>为你带来最全的科技知识 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>图同构下等变,计算高效,韦灵思团队提出'自然图网络'消息传递方法</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=cn/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>选自arXiv</p><p style=text-align:justify><strong>作者：Pim de Haan、Taco Cohen、Max Welling</strong></p><p style=text-align:justify><strong>机器之心编译</strong></p><p style=text-align:justify><strong>编辑：小舟、杜伟</strong></p><blockquote class=pgc-blockquote-abstract><p style=text-align:justify>近日，韦灵思团队的一项研究通过研究图的局部对称性，提出了一种新的算法。该算法在不同的边上使用不同的核，从而使网络在局部与全局的图同构体上是等变的，也更易于表达。</p></blockquote><p style=text-align:justify>通常来说，常规神经消息传递算法在消息排列下是不变的，因此会忘记信息流如何在网络中传递。</p><p style=text-align:justify>近日，阿姆斯特丹大学 ML 教授、高通技术副总裁韦灵思（Max Welling）团队<strong>通过研究图的局部对称性，提出了一种通用性更强的算法</strong>。该算法在不同的边上使用不同的核，从而使得网络在局部图和全局图同构上呈现等变化，也因而更易于表达。</p><div class=pgc-img><img alt='图同构下等变,计算高效,韦灵思团队提出"自然图网络"消息传递方法' onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/46068927b2b544928c767f7cb00ed2ed><p class=pgc-img-caption></p></div><p style=text-align:justify>论文地址：https://arxiv.org/abs/2007.08349v1</p><p style=text-align:justify>具体而言，<strong>研究者使用了初级范畴论，将许多显式等变神经网络形式化为自然图网络（Natural Graph Network, NGN），并表明它们的核正是两个函子（functor）之间的自然转换</strong>。</p><p style=text-align:justify>他们还提供了一个自然网络的图实例，该网络使用等变消息网络参数化，在多个基准上实现了良好的性能。</p><p style=text-align:justify>接下来我们来看这篇论文的具体内容。</p><p style=text-align:justify><strong>自然图网络</strong></p><p style=text-align:justify>在图上构建神经网络有一种完全不同的策略，即使用图卷积神经网络或消息传递网络（Kipf 和 Welling, 2016；Gilmer 等人, 2017）。研究者将这类方法称为局部图网络（local graph network, LIGN）。</p><p style=text-align:justify>以最简单的形式，这些每个节点上具有特征 v_p 的转换图信号 v，使用单个共享线性变换 W 在图的边上传递消息，如下公式 2 所示：</p><div class=pgc-img><img alt='图同构下等变,计算高效,韦灵思团队提出"自然图网络"消息传递方法' onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/75ffc1032a4d4f5f9b4761afd5d2b5b1><p class=pgc-img-caption></p></div><p style=text-align:justify>其中 E 是图的边集。这类卷积架构通常比全局方法具有更高的计算效率，这是因为计算线性变换的计算成本与边的数量呈线性比例关系。</p><p style=text-align:justify>为了克服现有消息传递网络的局限性，同时又保持更高的计算效率，研究者提出了<strong>一种新型的消息传递网络，其中的权重是由图结构决定的</strong>。</p><p style=text-align:justify>也就是说，研究者对公式 2 做了改进，得到以下公式 3：</p><div class=pgc-img><img alt='图同构下等变,计算高效,韦灵思团队提出"自然图网络"消息传递方法' onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/69d3c063866c438da63730378c2c35da><p class=pgc-img-caption></p></div><p style=text-align:justify>其中线性核在每个图每条边上都是不同的。显然，并非所有此类核都会导致等变网络。接下来研究者详细介绍了如何定义核空间。</p><p style=text-align:justify><strong>全局和局部图对称性</strong></p><p style=text-align:justify>研究者用整数数组表示图 G 中的节点 N_G，即</p><div class=pgc-img><img alt='图同构下等变,计算高效,韦灵思团队提出"自然图网络"消息传递方法' onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/95eccb625c3b4623b49c0cc38f13a81f><p class=pgc-img-caption></p></div><p style=text-align:justify>，然后图中的边用整数对表示，边的集合是ε(G)，则边(p,q)∈ε(G)。</p><p style=text-align:justify>如果图是带有 p→q 这样箭头符号的有向图，那么图 G 和 G’是相似或同构的。换句话说，图同构将节点映射到节点，边映射到边。一种特殊的同构是自同构，只是节点的排列顺序有所变化，边集保持不变。根据定义，一个组中的自同构，称为自同构组。</p><p style=text-align:justify><strong>特征</strong></p><p style=text-align:justify>为了使等变神经网络具有可表达的核，必须将节点 p 处的特征向量 v_p 进行变换，因为节点 p 通过某种全局对称性映射到 p’，而不是像固定消息传递网络中那样保持不变。研究者重新定义了特征向量在局部节点对称性下的变换规则。</p><p style=text-align:justify><strong>局部等</strong><strong>变性</strong></p><p style=text-align:justify>边 (p,q) 上的核是 p 点的向量空间 V_p 到 q 点的向量空间 V_q 的映射。核的局部等变性意味着，如果有局部同构的边的空集</p><div class=pgc-img><img alt='图同构下等变,计算高效,韦灵思团队提出"自然图网络"消息传递方法' onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/6775c681ab2d4feeb6a1ab8aaa5b7a12><p class=pgc-img-caption></p></div><p style=text-align:justify>，则这样做和以下两种情况的结果一样：</p><p style=text-align:justify>将信号从 p 传递到 p’，然后再申请内核 K^(G’)_p’q’；</p><p style=text-align:justify>先申请内核 K^G_pq，然后将 q 转换成 q’。</p><p style=text-align:justify>具体如下图 6 所示：</p><div class=pgc-img><img alt='图同构下等变,计算高效,韦灵思团队提出"自然图网络"消息传递方法' onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ddadf36e408b4eb59126326dac2ecf0f><p class=pgc-img-caption></p></div><p style=text-align:justify>因此需要以下公式 4：</p><div class=pgc-img><img alt='图同构下等变,计算高效,韦灵思团队提出"自然图网络"消息传递方法' onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/78ae5420be4c48b2b88d98ba272b6f82><p class=pgc-img-caption></p></div><p style=text-align:justify><strong>图神经网络的消息参数化</strong></p><p style=text-align:justify>等变性只需要在具有同构邻域的边之间共享权重，因此在定理中，我们可以将分类参数用于每个同构类的边邻域，以参数化等变核的空间。</p><p style=text-align:justify>实际上，像社交图（social graph）这类图的异构性很强，很少有边是同构的，并且很少需要共享权重，因而学习和泛化也是很困难的。</p><p style=text-align:justify>这一点可以通过以下方式解决：将 p 到 q 的消息</p><div class=pgc-img><img alt='图同构下等变,计算高效,韦灵思团队提出"自然图网络"消息传递方法' onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/766f6011464a4547b5f4ac24b7b66ee8><p class=pgc-img-caption></p></div><p style=text-align:justify>重新解释为函数</p><div class=pgc-img><img alt='图同构下等变,计算高效,韦灵思团队提出"自然图网络"消息传递方法' onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2d5eee874a9245af8fca07d14a9fc137><p class=pgc-img-caption></p></div><p style=text-align:justify>，其中 G_pq 是边邻域，v_p 是 p 点的特征值，在 v_p 中可能被泛化为非线性的，K 是基于消息网络的神经网络。</p><p style=text-align:justify>下图 7 所示为作为图卷积的消息传递过程：</p><div class=pgc-img><img alt='图同构下等变,计算高效,韦灵思团队提出"自然图网络"消息传递方法' onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ba558af5a04b4c9da5d3b918b8479c4a><p class=pgc-img-caption></p></div><p style=text-align:justify><strong>范畴论</strong></p><p style=text-align:justify>全局对称性的等变约束，比如机器学习中广泛使用的公式 1 最近已经被扩展到局部对称性或规范对称性中。</p><div class=pgc-img><img alt='图同构下等变,计算高效,韦灵思团队提出"自然图网络"消息传递方法' onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c9cd103361f246afa6f67f54c506bc56><p class=pgc-img-caption></p></div><p style=text-align:justify>但是，这些形式不包括图的动态局部对称性，并且需要一种通用性更强的语言。</p><p style=text-align:justify>基于此，研究者使用了范畴论，该理论最初是从代数拓扑发展而来的，近来也被用作更多问题的建模工具。范畴论的结构为建立等变消息传递网络（称为自然网络）提供了一个良好的框架，研究者称为「自然网络（Natural Network）」。</p><p style=text-align:justify><strong>实验</strong></p><p style=text-align:justify><strong>二十面体（Icosahedral）的 MNIST</strong></p><p style=text-align:justify>为了在实验中验证该方法与全局对称的等变性，并增强在不变消息传递网络（GCN）上的可表达性，研究者对投影到二十面体的 MNIST 进行了分类。</p><p style=text-align:justify>下表 1 第一列显示了在一个固定（fixed）投影上进行训练和测试的准确率。在第二列中，研究者在通过随机二十面体对称性变换的投影上测试了相同的模型。</p><p style=text-align:justify>结果表明，NGN 的性能优于 GCN，并且准确率相等表明该模型是完全等变的。</p><div class=pgc-img><img alt='图同构下等变,计算高效,韦灵思团队提出"自然图网络"消息传递方法' onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/aeb1907083d74c48825471f810c99c9e><p class=pgc-img-caption></p></div><p style=text-align:justify><strong>图分类</strong></p><p style=text-align:justify>在 Yanardag 和 Vishwanathan 于 2015 年提出的 8 个标准图分类基准集上（包括 5 个生物学数据集和 3 个社交图），研究者使用 GCN 消息参数化评估了该模型。</p><p style=text-align:justify>具体而言，研究者使用了十倍交叉验证（10-fold cross validation）方法，并给出了十倍情况下的最佳平均准确率，如下表 2 所示：</p><div class=pgc-img><img alt='图同构下等变,计算高效,韦灵思团队提出"自然图网络"消息传递方法' onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/bfc0f728633146938fc06d624cf41535><p class=pgc-img-caption></p></div><p style=text-align:justify>实验结果表明，在大多数数据集上，该研究提出的局部等变方法性能不逊于全局等变方法。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>'图同构','计算','韦灵'</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-146415161-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>