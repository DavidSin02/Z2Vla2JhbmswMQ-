<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器愛學習13——支持向量機SVM之預測函數、代價函數 | 极客快訊</title><meta property="og:title" content="機器愛學習13——支持向量機SVM之預測函數、代價函數 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/154043506714200d4a1b95e"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/01081bc.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/01081bc.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/01081bc.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/01081bc.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/01081bc.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/01081bc.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/01081bc.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/01081bc.html><meta property="article:published_time" content="2020-10-29T20:56:37+08:00"><meta property="article:modified_time" content="2020-10-29T20:56:37+08:00"><meta name=Keywords content><meta name=description content="機器愛學習13——支持向量機SVM之預測函數、代價函數"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/01081bc.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器愛學習13——支持向量機SVM之預測函數、代價函數</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><div><p>本章開始，將為大家介紹一個比較複雜的機器學習算法：<strong>支持向量機</strong>Support Vector Machine，以下簡稱SVM。</p><p>網絡上有很多關於SVM的講解，講的都很詳細，內容很完整，本篇及之後關於SVM的文章，權當筆者的學習筆記好了，各位看官若能有所收穫，那就再好不過了～</p><h1>什麼是SVM</h1><p>SVM的全稱為Support Vector Machine，即支持向量機。SVM屬於有監督學習算法的一種（前面文章好像沒有講到有監督學習、無監督學習，大家可以自行百度下），主要用來解決數據分類問題，簡單的來說，就是解決<strong>二分類問題</strong>。</p><p>等等？二分類問題？二分類問題不是可以通過邏輯迴歸來解決嗎？為什麼還要學習SVM？</p><p>相對於邏輯迴歸，SVM當然有她自己的優勢。在大家完全理解SVM之前，我們暫時不介紹兩者的異同，待SVM講解完成之後，我們再做總結。</p><h1>SVM的預測函數</h1><p>當SVM用來解決分類問題時，其hypothesis function和邏輯迴歸的hypothesis function基本類似。</p><p>我們先回顧下邏輯迴歸的hypothesis function：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/154043506714200d4a1b95e><p class=pgc-img-caption>邏輯迴歸的hypothesis function</p></div><p>SVM的hypothesis function中，對f(x)做了點轉換，具體如下：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15404351967509e46b4af01><p class=pgc-img-caption>SVM的hypothesis function</p></div><p>其實，SVM和邏輯迴歸的兩個f(x)，說的是一個東西，只不過表達方式不同。</p><p>現在，我們知道了SVM的hypothesis function，接下來，只要求出"最佳"的向量W和實數b就可以了，要求出"最佳"的變量值，就需要藉助於cost function，接下來，我們就看看SVM的cost function長什麼樣。</p><p>注意：</p><ul><li>不管是邏輯迴歸還是SVM，都將正類記為"+1"，但是邏輯迴歸將負類記為"0"，而SVM將負類記為"-1"，其實這些都無關緊要，只是一個標記而已，大家記住就好了。</li><li>上面說的"最佳"變量，指的是可以使代價函數最小的變量</li></ul><h1>SVM代價函數的幾何意義</h1><p>在前面講線性迴歸時，我們的cost functiuon是：樣本誤差（y-h(x))的平方和，我們的要求是讓樣本誤差的平方和”儘量小“。那麼SVM的代價函數是什麼？假設我們的要求也是讓”變量Z”最小，那”變量Z“到底是什麼”</p><p>我們現在就來看看“變量Z”到底是什麼！</p><p>講解之前，先讓大家做兩道題：</p><p><strong>問題1：下圖的二分類問題中，哪條直線（H1、H2、H3）可以作為target function？</strong></p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/15410605392584ce3323453><p class=pgc-img-caption>哪條直線可以作為target function？</p></div><p>我們來一一進行分析：</p><ul><li>對於H1，存在預測錯誤的情況，肯定是不能作為target function的；</li><li>對於H2，不存在預測錯誤的情況，可以作為target function；</li><li>對於H3，不存在預測錯誤的情況，可以作為target function；</li></ul><p>所以，H1不能作為target function；H2、H3都可以作為target function。</p><p>依據training set中有沒有出現預測錯誤的情況，我們很容易得到上面的結論</p><ul><li>有預測錯誤：不能作為target function</li><li>沒有預測錯誤：可以作為target function</li></ul><p><strong>問題2：上面的兩條直線：H2、H3，哪一條更“完美”？</strong></p><p>哪一條更“完美”？？？</p><p>小編你先來告訴我啥叫“完美”？</p><p>OK，換個問法：</p><p><strong>上面兩條直線H2、H3，哪一個作為target function時，在test set中更不容易出現預測錯誤？</strong></p><p>從圖中，我們很容易看出來：</p><ul><li>離H2最近的3個點（黑點1、黑點2、白點3），到H2的距離都太小，如果test set中的點正好在這3個點附近，那H2很有可能出現預測錯誤</li><li>離H3最近的3個點（黑點2、白點3、白點4），到H3的距離都比較大，即使test set中的點在這三個點附近，H3也不太容易出現預測錯誤</li></ul><p>所以，H2作為target function時，更容易預測錯誤，而H3更不容易預測錯誤，因此，H3更“完美”。</p><p>我們再思考下：得到上面結論的依據是什麼？</p><p>我們假設離target function最近的點，到target function的距離為d，則：</p><ul><li>當d很小時，target function更容易預測錯誤</li><li>當d很大時，target function更不容易預測錯誤</li></ul><p>target function在二維平面中是一條線，在多維空間中就是一個多維平面，SVM稱之為<strong>超平面hyperplane</strong>。</p><p>在SVM中，這些離超平面最近的點（向量）有另外一個名字：<strong>支持向量support vertor</strong>。</p><p>SVM要解決的問題，就是去找一個超平面，使支持向量到超平面的距離儘可能的大。</p><p>根據上面的問題2，我們有理由相信：SVM找到的超平面更不容易預測錯誤。</p><p>然而，講了這麼多，跟代價函數又有什麼關係？</p><p>如果我們將支持向量到超平面的距離作為代價函數，通過求代價函數的最大值，找到對應的W和b，不就找到target function了嘛～</p><p>完全正確！<strong>SVM代價函數就是支持向量到超平面的距離，通過最大化這個距離來進行求解</strong>。</p><h1>SVM代價函數的約束：“不偏不倚”</h1><p>前面我們講到：通過最大化距離d，可以保證target function不容易出現預測錯誤。在二分類問題中，我們該如何理解這句話？</p><p>二分類問題中，</p><ul><li>正類為了不出現預測錯誤，當然想讓支持向量中正類的點（向量），到超平面的距離越遠越好</li><li>同樣，負類為了不出現預測錯誤，當然想讓支持向量中負類的點（向量），到超平面的距離越遠越好</li></ul><p>兩者都想讓超平面離自己越遠越好，可是超平面離支持向量中正類的點越遠，就離負類點越近；離負類點越遠，就離正類點越近。</p><p>超平面很無奈，為了保證“不偏不倚”，最後只能呆在中間位置。</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a338343a9f2a4d34902c01b88fa7fea5><p class=pgc-img-caption>圖示超平面、支持向量</p></div><p>上圖中，有三個超平面H、H1、H2（H、H1、H2互相平行）：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4f111909a4614d40818da989f2be97b5><p class=pgc-img-caption></p></div><p>圖中可以看到，H到H1、H2的距離相等，這樣H作為target function時，可以保證正類、負類均不容易出現預測錯誤。</p><p>對於上圖，大家可能會有如下疑惑，我們一一解答（其實也是小編自己的疑惑，答案如有紕漏，還望大家不惜賜教）</p><p><strong>問題1：為什麼超平面H的表達式中，等號右側為0，而不是等於其他值？</strong></p><p>假設等號右側為A(A !=0)，那麼我們完全可以在等號兩側同時減去A，得到一個新超平面（此時表達式等號右側為0），這個新超平面其實和轉換前的超平面是同一個超平面。</p><p>所以，我們可以令等號右側為0，而不會對超平面有任何影響（只不過計算出來的b會有所不同）。</p><p><strong>問題2：為什麼超平面H1的表達式中，等號右側為1，而不是其他值？</strong></p><p>由於H和H1平行，假設H移動到H1後，H1真實表達式如下：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/36839965cd3542a99c851ec04eb3d029><p class=pgc-img-caption></p></div><p>我們完全可以在等式兩邊同時乘以1/M（成比例的改變W、b），得到一個新超平面（此時表達式等號右側為1），這個新超平面其實和轉換前的超平面是同一個超平面。</p><p>所以，我們可以令等號右側為1，而不會對超平面有任何影響（只不過計算出來的W、b會有所不同）。</p><p>下面在計算支持向量到超平面距離時，還會遇到成比例縮放W、b的情況，這樣做只是讓表達式更簡單，並不會影響計算結果。</p><p><strong>問題3：為什麼H2的表達式中，等號右側為-1，而不是其他值？</strong></p><p>與上面的問題類似，不再贅述～</p><p><strong>問題4：H到H1、H2的距離1/||w||是如何算出來的？</strong></p><p>關於距離，下文會專門去計算，這裡大家可以暫不關注</p><p>下面，我們再介紹兩個結論：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1bd97c8a58194789a0827b5161dd8628><p class=pgc-img-caption></p></div><p>為什麼有這些結論？這些結論是什麼意思？</p><p>對於正類，我們可以認為支持向量到超平面H的距離（函數距離，什麼是函數距離，大家不必太關注）為1，其他向量點到超平面H的距離會更遠，即大於1。因此所有正類點滿足上面的條件。負類也是同樣的道理～</p><p>現在大家基本明白上面的兩個結論，我們很容易將兩個結論合併為如下不等式（正類時，y=+1；負類時，y=-1，大家現在明白為什麼負類時y取-1了吧）：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b214ec5a44294e6493b9e5a9b44f2a36><p class=pgc-img-caption></p></div><p>這個不等式有特殊的含義：如果所有的點都滿足上面不等式，則SVM就做到了"不偏不倚"（通過上面的講解，這個應該不難理解）。</p><p>所以，SVM在對代價函數求極值時，需要考慮上面的不等式約束，即在"不偏不倚"的情況下，找到距離最大值對應的參數W、b。</p><p>OK，接下來，我們就來看下如何推導出代價函數，並求解包含不等式約束的極值問題。</p><p>本章剩餘內容主要做以下兩個工作：</p><ul><li>將代價函數具體化，方便用數學方法求解</li><li>將代價函數變形，方便用簡單的數學方法求解</li></ul><h1>代價函數的數學表示</h1><p>前面我們提到：代價函數可以是支持向量到超平面的距離。既然是點到平面的距離，那我們先看看數學上：點到平面的距離公式。</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/154112782540404bf31c143><p class=pgc-img-caption>點到平面的距離公式推導</p></div><p>上圖直接截了距離公式的推導過程，大家不必關注過程，只需要關注下面的距離公式即可（其中||W||稱為L2範式）：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1541128707397e60735dba9><p class=pgc-img-caption>點到平面距離公式</p></div><p>上面是一個樣本點到超平面的距離，我們假設所有樣本點中最小距離為γ，則：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/13ecf71f81bc4d37ab26d6aaef6f9440><p class=pgc-img-caption>最小几何間隔</p></div><p>這個最小距離，我們稱之為最小几何間隔。我們的代價函數就是這個最小几何間隔，通過<strong>求最小几何間隔的最大值（注意是求某一組數據最小值的最大值），</strong>找到一組W和b，從而得到我們的預測函數。</p><p>據此，我們第一版代價函數格式如下：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/aa4c325540704178981b3e07535bda3f><p class=pgc-img-caption>SVM代價函數版本--1</p></div><h1>代價函數變形－－縮放函數間隔</h1><p>為了方便計算，我們對代價函數做一點縮放（回想一下前面提到的縮放），從而使</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/710ab580a6fc4b8094f56a2f997a59d7><p class=pgc-img-caption>最小函數間隔</p></div><p>縮放後，新的代價函數如下：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bf78c7c494454f8d8fbf820d54e39d6d><p class=pgc-img-caption>SVM代價函數版本--2</p></div><p>縮放後會不會影響W、b？</p><p>縮放相當於在等式兩邊同時乘以一個正數，使分子變成1，此時等式仍然成立，而且W、b也沒有被改變。</p><p>其實上面的數值是點到平面的<strong>函數間隔</strong>，我們通過縮放座標，使函數間隔變為1，但是幾何間隔並不會改變，也不會影響W、b。</p><h1>包含約束條件的代價函數</h1><p>還記得我們前面推導的約束條件："不偏不倚"嗎？考慮到約束條件，新的代價函數如下：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0240010efcc84845bcdcb6618c1c4453><p class=pgc-img-caption>SVM代價函數版本--3</p></div><p>縮寫s.t.表示"Subject to"，是"服從某某條件"的意思。</p><p>我們習慣求代價函數的最小值，因此，這裡再對代價函數做一點改動（求1/||W||的最大值，改為求||W||的最小值，為了方便計算，再改為求||W||平方的最小值）。</p><p>改動後，新的代價函數為：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4fcc655aa5f24aa5bea7fd95d5bb3527><p class=pgc-img-caption>SVM代價函數版本--4</p></div><h1>SVM代價函數變形--拉格朗日函數</h1><p>上面的代價函數，是一個包含約束條件的極值問題，這種問題可以用現成的QP (Quadratic Programming) 優化包進行求解。不過我們有更好的求解方法--拉格朗日乘數法。</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7e381d0d18314f8aa6df75e3928a6d4b><p class=pgc-img-caption>拉格朗日乘數法-百科</p></div><p>總結一下，<strong>拉格朗日乘數法可以將有約束的極值問題，轉化為無約束的極值問題</strong>。</p><p>針對上面帶約束的代價函數，得到對應的拉格朗日函數如下：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/10ac6868a6ff4b109390233eac9f5e63><p class=pgc-img-caption>拉格朗日函數</p></div><p>其中αi稱為拉格朗日乘子，並且αi>=0。</p><p>對於上面的拉格朗日函數，有如下結論：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/576fb815fbbf46c88cddc3c295dc81bf><p class=pgc-img-caption></p></div><p>上面的結論是如何得到的呢？</p><p>考慮代價函數中"不偏不倚"的約束條件，則有</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c9e959b763fb441faf6e7353a4e07fe5><p class=pgc-img-caption></p></div><p>故：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/02eec73f40c24436a544fff7c6827c33><p class=pgc-img-caption></p></div><p>簡單的來說，就是在滿足"不偏不倚"條件、αi>=0時，我們的代價函數就等於拉格朗日函數的最大值。</p><p>藉助於拉格朗日乘數法，"不偏不倚"的約束條件就被消除掉了。</p><p>至此，我們的SVM代價函數如下：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fb0d0456f7d1449aa84cf8233aa78bb1><p class=pgc-img-caption>SVM代價函數版本--5</p></div><p>這裡的代價函數是min(max)的形式，數學上稱為拉格朗日函數的極小極大問題。</p><h1>SVM代價函數變形--拉格朗日對偶問題</h1><p>對於SVM代價函數版本5，我們可以去求解得到W、b。但實際中，我們並不會直接對上面的代價函數進行求解，而是求其對偶問題：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/55aa13602a324b708533bca405c0abda><p class=pgc-img-caption>拉格朗日對偶問題</p></div><p>與原始問題相比，對偶問題只是調整了極大、極小的求解順序。對偶問題又稱為拉格朗日極大極小問題。</p><p>接下來先看兩個問題：</p><p><strong>1：我們為什麼不直接求原始問題，反而去求其對偶問題？</strong></p><p>原因如下：</p><ul><li>對偶問題中，可以使用SMO算法高效的進行求解</li><li>對偶問題中可以很容易的引入核函數，而通過核函數，可以將線性不可分的數據集轉換為線性可分的數據集</li></ul><p>關於SMO算法、核函數，後面會進行講解。</p><p><strong>2：求解對偶問題得到的W、b，與原始問題的W、b相等嗎？</strong></p><p>我們假設原始問題的最優解為p*，對偶問題的最優解為d*，則總是存在如下的關係：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a8d69d9958f940e2960a3d713bcc7093><p class=pgc-img-caption></p></div><p>並且，當<strong>滿足強對偶性時，d* == p*。</strong>如果我們的問題滿足強對偶性，那通過求對偶問題的最優解，就相當於得到了原始問題的最優解。</p><p>那什麼是強對偶性？我們的問題滿足強對偶性嗎？</p><p>Slater定理告訴我們，當滿足以下兩個條件時，強對偶性成立：</p><ul><li>Slater條件成立（即數據線性可分）</li><li>原始問題是凸優化問題</li></ul><p>前面我們默認數據集是線性可分的，即存在一個超平面可以將所有數據正確分類，因此Slater條件成立（實際情況中數據可能是線性不可分的，為了滿足強對偶性，可以引入核函數，通過核函數的轉換，將數據集轉換為線性可分）</p><p>另外原始問題中，目標函數1/2||W||**2為凸函數，對應的優化問題即為凸優化問題。</p><p>因此，我們的原始問題是滿足強對偶性的，所以對偶問題的最優解即為原始問題的最優解。</p><p>OK，我們來梳理一下思路：</p><ol><li>通過拉格朗日乘數法，我們將帶約束的極值問題轉化為無約束的原始問題</li><li>我們又證明了原始問題的最優解，等於其對偶問題的最優解（滿足強對偶性）</li></ol><p>現在我們的cost function轉化成了對偶問題，格式如下：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1abe26e331474e01b2372ac0849be538><p class=pgc-img-caption>SVM代價函數最終版</p></div><p>那麼該如何求對偶問題的最優解呢？</p><p>我們先看一個結論（大家感興趣，可以翻翻高數書，這裡直接寫出結論）</p><p><strong>如果凸優化問題具有可微的目標函數和約束條件，且滿足Slater條件，那麼KKT條件是最優解的充要條件。</strong></p><p>我們來分析一下上面的結論：</p><p>首先，我們的原目標函數和約束條件是可微的（具體大家可以查閱可微的充要條件，推導一下），而且滿足Slater條件，因此，我們的最優解滿足KKT條件。</p><p>既然最優解滿足KKT條件，那麼從KKT條件中，我們就能得到一些最優解的線索。</p><p>那麼什麼是KKT條件？KKT條件具體包含如下約束：</p><ol><li>最優解必須滿足不等式約束：1-y*(WT*x +b) &lt;=0</li><li>L(W,b,a)對各個變量偏導數為零</li><li>ai>=0</li></ol><p>從KKT條件1、3，我們好像得不到什麼線索，我們不妨看看KKT條件2能帶給我們什麼。</p><p>KKT條件（2），即L(W,b,a)對各個變量偏導數為零，我們來求L（W，b，a）對W、b的偏導數：</p><div class=pgc-img><img alt=機器愛學習13——支持向量機SVM之預測函數、代價函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fdb41d5f8e6a464a839b41252ab48236><p class=pgc-img-caption></p></div><p>上面就是我們通過KKT條件得到的最優解的一些線索，我們可以將這些線索代入對偶問題來做簡化，如何代入？又該如何求解對偶問題最優解，我們留在下一章進行講解。</p><p>本章內容就講到這裡，講了這麼多，有用的東西有三個：</p><ul><li>SVM預測函數</li><li>SVM代價函數（拉格朗日對偶問題）</li><li>KKT條件</li></ul></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>函數</a></li><li><a>機器</a></li><li><a>愛學習</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/cfb83fa2.html alt=機器視覺｜晏磊：航空遙感平臺通用物理模型及可變基高比系統精度評價 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1530929302432561dc6cda6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cfb83fa2.html title=機器視覺｜晏磊：航空遙感平臺通用物理模型及可變基高比系統精度評價>機器視覺｜晏磊：航空遙感平臺通用物理模型及可變基高比系統精度評價</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/098d4a35.html alt=「火爐煉AI」機器學習048-Harris檢測圖像角點 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d756b20a1dbc4ab4b4f22d6b61be2043 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/098d4a35.html title=「火爐煉AI」機器學習048-Harris檢測圖像角點>「火爐煉AI」機器學習048-Harris檢測圖像角點</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e087ca41.html alt=偏導數和函數的梯度 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/9d20a1e4cbff42a094d57df057fe9597 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e087ca41.html title=偏導數和函數的梯度>偏導數和函數的梯度</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5fc113b1.html alt=梯度原理：梯度在每一點上都指向函數增長最快的方向 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/cdb8db41d5024f38a2e490e66baebdb4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5fc113b1.html title=梯度原理：梯度在每一點上都指向函數增長最快的方向>梯度原理：梯度在每一點上都指向函數增長最快的方向</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/bc93016e.html alt=人工智能時代，機器人真的能在對話中識別人的意圖嘛？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7804be8632614272aab014f77d8f40a9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bc93016e.html title=人工智能時代，機器人真的能在對話中識別人的意圖嘛？>人工智能時代，機器人真的能在對話中識別人的意圖嘛？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/51723e35.html alt=AI也有偏見：你在機器“眼裡”是好人還是壞蛋？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RfDgA10IBHJqK9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/51723e35.html title=AI也有偏見：你在機器“眼裡”是好人還是壞蛋？>AI也有偏見：你在機器“眼裡”是好人還是壞蛋？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ee69f5ac.html alt=基於機器視覺技術快速準確地確定收穫後幹大豆種子的品質 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/519b968bf69146fda9bf55f89779d373 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ee69f5ac.html title=基於機器視覺技術快速準確地確定收穫後幹大豆種子的品質>基於機器視覺技術快速準確地確定收穫後幹大豆種子的品質</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dbe0144c.html alt=如何減少焊接機器人出現焊接件變形的情況？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/3a62de9fde6c4a09ac0950d5f16dea0a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dbe0144c.html title=如何減少焊接機器人出現焊接件變形的情況？>如何減少焊接機器人出現焊接件變形的情況？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/408d3387.html alt=EXCEL:VLOOKUP函數綜合運用，實現供應商每月數據自動查詢 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/03e79d133c994f9f8a386b20b04b3da1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/408d3387.html title=EXCEL:VLOOKUP函數綜合運用，實現供應商每月數據自動查詢>EXCEL:VLOOKUP函數綜合運用，實現供應商每月數據自動查詢</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/526a98f3.html alt=Excel條件求和函數那麼多，高手一直都在用這一個，而你卻沒聽過 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/b2fd76f54f9b46c8bd79500ba4dac2fa style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/526a98f3.html title=Excel條件求和函數那麼多，高手一直都在用這一個，而你卻沒聽過>Excel條件求和函數那麼多，高手一直都在用這一個，而你卻沒聽過</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2664a94c.html alt=excel中的DSUM函數——條件求和原來如此簡單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/0dc7aeaa9b61467e86dbaeae6dfeaaa4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2664a94c.html title=excel中的DSUM函數——條件求和原來如此簡單>excel中的DSUM函數——條件求和原來如此簡單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/48f18e97.html alt=excel常用函數用法解析第四篇——COLUMN、ROW函數 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4c3daf45857e4f169def37fc08d652fd style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/48f18e97.html title=excel常用函數用法解析第四篇——COLUMN、ROW函數>excel常用函數用法解析第四篇——COLUMN、ROW函數</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>