<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>「數學基礎」簡單易懂的張量求導和計算圖講解 | 极客快訊</title><meta property="og:title" content="「數學基礎」簡單易懂的張量求導和計算圖講解 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/e7e49571c34c48f585b79c3126db7467"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0a4e37a.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0a4e37a.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/0a4e37a.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0a4e37a.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0a4e37a.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/0a4e37a.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/0a4e37a.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0a4e37a.html><meta property="article:published_time" content="2020-10-29T21:01:39+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:39+08:00"><meta name=Keywords content><meta name=description content="「數學基礎」簡單易懂的張量求導和計算圖講解"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/0a4e37a.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>「數學基礎」簡單易懂的張量求導和計算圖講解</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><p>以下文章來源於王的機器 ，作者王聖元</p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e7e49571c34c48f585b79c3126db7467><p class=pgc-img-caption></p></div><p><strong>王的機器</strong></p><p>Machine Learning -- 機器學習 Financial Engineering -- 金融工程 Quantitative Investing -- 量化投資</p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b28469730c1f495ab830dfae34368429><p class=pgc-img-caption></p></div><p><br></p><p>引言</p><p><strong>情景 1</strong></p><p><br></p><p style=text-align:justify>斯蒂文買了一股京東，當京東變動 1 美元，斯蒂文的組合變動 1 美元。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f6d2fd63fadf41e28dae8ec820530871><p class=pgc-img-caption></p></div><p><br></p><p>這是“標量對標量”求導數。</p><p><br></p><p><strong>情景 2</strong></p><p><br></p><p>斯蒂文買了一股京東，兩股百度，三股臉書</p><p><br></p><ul><li>當京東變動 1 美元，斯蒂文的組合變動 1 美元</li><li>當百度變動 1 美元，斯蒂文的組合變動 2 美元</li><li>當臉書變動 1 美元，斯蒂文的組合變動 3 美元</li></ul><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/8f4ce5a081a845ffbe0cd7638c8898d2><p class=pgc-img-caption></p></div><p><br></p><p>如果令將三隻股票整合成列向量，<strong>股票</strong> = [京東 百度 臉書]，那麼</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/385458c7372e4d2ba336aa7a76965daf><p class=pgc-img-caption></p></div><p><br></p><p>如果令將三隻股票整合成行向量，<strong>股票</strong> = [京東 百度 臉書]T，那麼</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6fd2d7434fcf4484bd35ee8af19fbf9f><p class=pgc-img-caption></p></div><p><br></p><p>這是“標量對向量”求導數，行向量或列向量都不重要，向量只是一組標量的表現形式，重要的是導數“d組合/d<strong>股票</strong>”的“<strong>股票</strong>”的向量類型一致 (要不就是行向量，要不就是列向量)。</p><p><br></p><p><strong>情景 3</strong></p><p><br></p><p>斯蒂文買了一股京東，雪莉買了四股京東。當京東變動 1 美元，斯蒂文的組合 A 變動 1 美元，雪莉的組合 B 變動 4 美元。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e60ff1cd19d3414480d352d5179d73ed><p class=pgc-img-caption></p></div><p><br></p><p>如果令將兩個組合整合成列向量，<strong>組合</strong> = [組合A 組合B]，那麼</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ec9b0c33c12043f1bc538b220ef1bc40><p class=pgc-img-caption></p></div><p><br></p><p>如果令將兩個組合整合成行向量，<strong>組合</strong> = [組合A 組合B]T，那麼</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1d54b05122884c03ba761b8c9b979a59><p class=pgc-img-caption></p></div><p><br></p><p>這是“向量對標量”求導數，行向量或列向量都不重要，向量只是一組標量的表現形式，重要的是導數“d<strong>組合</strong>/d京東”的“<strong>組合</strong>”的向量類型一致 (要不就是行向量，要不就是列向量)。</p><p><br></p><p><strong>情景 4</strong></p><p><br></p><p style=text-align:justify>斯蒂文買了一股京東，兩股百度，三股臉書；雪莉買了四股京東，五股百度，六股臉書，則</p><p style=text-align:justify><br></p><ul><li>當京東變動 1 美元，斯蒂文的組合 A 變動 1 美元，雪莉的組合 B 變動 4 美元</li><li>當百度變動 1 美元，斯蒂文的組合 A 變動 2 美元，雪莉的組合 B 變動 5 美元</li><li>當臉書變動 1 美元，斯蒂文的組合 A 變動 3 美元，雪莉的組合 B 變動 6 美元</li></ul><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cdca1ba738854ebcb1c63a124283a574><p class=pgc-img-caption></p></div><p><br></p><p>如果令將兩個組合和三隻股票整合成向量 (行或列)，<strong>組合</strong> = [組合A 組合B]，那麼</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/07011e29e8e148c18f8f0455978e97c2><p class=pgc-img-caption></p></div><p style=text-align:justify><br></p><p style=text-align:justify>這是“向量對向量”求導數，上面四種情況乍一看眼花繚亂，實際上就是先將“d<strong>組合</strong>/d<strong>股票</strong>”寫成和“<strong>組合</strong>”一樣大小的向量，再根據“<strong>股票</strong>”的大小，把<strong>組合</strong>向量裡每個元素展開。這樣</p><p style=text-align:justify><br></p><ul><li>情況 1 - 當<strong>組合</strong>和<strong>股票</strong>都是行向量，“d<strong>組合</strong>/d<strong>股票</strong>”是一個更長的行向量</li><li>情況 2 - 當<strong>組合</strong>是行向量，<strong>股票</strong>是列向量，“d<strong>組合</strong>/d<strong>股票</strong>”是一個矩陣</li><li>情況 3 - 當<strong>組合</strong>是列向量，<strong>股票</strong>是行向量，“d<strong>組合</strong>/d<strong>股票</strong>”是一個矩陣</li><li>情況 4 - 當<strong>組合</strong>和<strong>股票</strong>都是列向量，“d<strong>組合</strong>/d<strong>股票</strong>”是一個更高的列向量</li></ul><p style=text-align:justify><br></p><p style=text-align:justify>通常使用情況 3 的形式來表示導數。</p><p style=text-align:justify><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eaed8e46f24241d5a03ffae71d66050c><p class=pgc-img-caption></p></div><p style=text-align:justify><br></p><p style=text-align:justify>但是這只是一種慣例表示，具體表示要看具體問題，沒有最好的表示，只有最方便的表示。</p><p style=text-align:justify><br></p><p><strong>情景 5</strong></p><p><br></p><p style=text-align:justify>現在有四家基金，分別在組合 A 和 B 上投資 20% 和 80%，50% 和 50%，70% 和 30%，90% 和 10%，寫成矩陣形式為<strong>F</strong> = <strong>U</strong>·<strong>P</strong> = <strong>U</strong>·(<strong>W</strong>·<strong>S</strong>)</p><p style=text-align:justify><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d0cae23fa0594a8b884be9df21571741><p class=pgc-img-caption></p></div><p style=text-align:justify><br></p><p style=text-align:justify>現在來看看所有基金對所有股票的敏感度，即推導“d<strong>基金</strong>/d<strong>股票</strong>”，根據上面的慣例得知應該是個 4×2 的矩陣。</p><p style=text-align:justify><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9a81f2aed681471b829a05106dc35dbe><p class=pgc-img-caption></p></div><p style=text-align:justify><br></p><p style=text-align:justify>注意力只放在紅色的 3.4 上，它的計算過程是</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9dfe1f9572584a5ca0208992475966cd><p class=pgc-img-caption></p></div><p><br></p><p style=text-align:justify>矩陣其他每個結果都可以用同樣方法算出。上面矩陣可進一步表示成</p><p style=text-align:justify><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d0e0d823f2fe43f6b0ed450592a57dd9><p class=pgc-img-caption></p></div><p style=text-align:justify><br></p><p style=text-align:justify>這個就是向量求導的鏈式法則。</p><p style=text-align:justify><br></p><hr><p style=text-align:justify>在深度學習中求解中，兩個問題最重要</p><p style=text-align:justify><br></p><ol start=1><li>怎樣有效的推導出損失函數對所有函數的偏導數？</li><li>怎樣有效的計算它們？</li></ol><p style=text-align:justify><br></p><p style=text-align:justify>解決問題 1 需要了解張量求導 (第一節)，解決問題 2 需要了解計算圖(第二節)。要理解張量請參考《<strong>張量 101</strong>》。</p><p><br></p><p>本帖目錄如下：</p><p><strong>目錄</strong></p><p><strong>第一章 - 張量求導</strong></p><p><br></p><p>1.1 類型一 ∂y/∂x</p><p>1.2 類型二 ∂y/∂x</p><p>1.3 類型三 ∂y/∂x</p><p>1.4 神經網絡應用</p><p><br></p><p><strong>第二章 - 計算圖</strong></p><p><br></p><p>2.1 數學符號</p><p>2.2 神經網絡 I</p><p>2.3 神經網絡 II</p><p>2.4 神經網絡 III</p><p><br></p><p><strong>總結</strong></p><p><strong>參考資料</strong></p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/07c8597ec1bc4125ab2eb3fcf9ff3662><p class=pgc-img-caption></p></div><p><br></p><p>1</p><p>張量求導</p><p>從高層次來看，張量求導 ∂y/∂x 可分為三大類型：</p><li><strong>類型一</strong>：y 或 x 中有一個是標量</li><ul><li><strong>類型二</strong>：y 和 x 中都不是標量</li><li><strong>類型三</strong>：y 作用在元素層面作用上</li></ul><p><br></p><p>1.1</p><p><strong>類型一 ∂y/∂x</strong></p><p><br></p><p>求類型一的偏導數，只需記住下面的形狀規則。</p><p><br></p><p><strong>規則 0 (形狀規則)</strong>：只要 y 或 x 中有一個是標量，那麼導數 ∂y/∂x<strong> </strong>的形狀和非標量的形狀一致。</p><p><br></p><p>讀起來有點繞口，看下面的數學表達式更為直觀。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/68c5b7a1744b42e7bdd6412b3aad45ea><p class=pgc-img-caption></p></div><p><br></p><p>上面形狀 (D1, D2, …, Dn) 代表的是 n 個維度，第一個維度有 D1 個元素，第二個維度有 D2 個元素，…，第 n 個維度有 Dn 個元素。比如</p><li>標量的形狀為 ()</li><ul><li>8 個元素的向量的形狀為 (8,)</li><li>3×5 的矩陣的形狀為 (3, 5)</li><li>4×2×5×3 的高維張量的形狀為 (4, 2, 5, 3)</li></ul><p><br></p><p>類型一再往下細分有 5 類</p><p><br></p><ol start=1><li>∂標量/∂標量</li><li>∂標量/∂向量</li><li>∂標量/∂矩陣</li><li>∂向量/∂標量</li><li>∂矩陣/∂標量</li></ol><p><br></p><p>每一類只需用<strong>形狀規則</strong>就可以寫出其偏導數，下面來看看這五個具體實例。</p><p><br></p><p>∂標量/∂標量</p><p><br></p><p>當 y 和 x 都是標量。該求導類型在單變量微積分裡面已學過，通俗的講，就是求“y 的變化和 x 的變化”的比率，用符號 ∂y/∂x 來表示。嚴格來說，單變量導數應寫成 dy/dx，但為了和後面偏導數符號一致，就用偏導 ∂ 符號。</p><p><br></p><p>注：“∂標量/∂標量”是張量求導基礎，所有困難的求導都可以先從“∂標量/∂標量”開始，摸清規律後再推廣到“∂張量/∂張量”。</p><p><br></p><p>∂標量/∂向量</p><p><br></p><p>當 y 是標量，<strong>x </strong>是含有 n 個元素的向量。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/99ed193c93b0426baf2fbfea23ce505e><p class=pgc-img-caption></p></div><p><br></p><p>該導數是 y 對 <strong>x </strong>中的每個元素 (一共 n 個元素) 求導，然後按 <strong>x </strong>的形狀排列出來 (<strong>形狀規則</strong>)，即，<strong>x </strong>是行 (列) 向量，∂y/∂<strong>x</strong> 就是行 (列) 向量。</p><p><br></p><p>注：神經網絡的誤差函數是 l 一個標量，在求參數最優解時，我們需要計算 l 對向量偏置 <strong>b</strong> 的偏導數 ∂l/∂<strong>b</strong> (∂標量/∂向量)。</p><p><br></p><p>∂標量/∂矩陣</p><p><br></p><p>當 y 是標量，<strong>x </strong>是大小為 m×n 的矩陣。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/46a79e888d164afdb3bca22925feb630><p class=pgc-img-caption></p></div><p><br></p><p>該導數是 y 對 <strong>x </strong>中的每個元素 (一共 mn 個元素) 求導，然後按 <strong>x </strong>的形狀排列出來 (<strong>形狀規則</strong>)。</p><p>注：神經網絡的誤差函數是 l 一個標量，在求參數最優解時，我們需要計算 l 對矩陣權重 <strong>W</strong> 的偏導數 ∂l/∂<strong>W</strong> (∂標量/∂矩陣)。</p><p><br></p><p>∂向量/∂標量</p><p><br></p><p>當 <strong>y </strong>是含有 m 個元素的向量，x 是標量。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/580952a0860846ef8d0a689cefeffc69><p class=pgc-img-caption></p></div><p><br></p><p>該導數是 <strong>y</strong> 中的每個元素 (一共 m 個元素) 對 x 求導，然後按 <strong>y </strong>的形狀排列出來 (<strong>形狀規則</strong>)，即，<strong>y </strong>是行 (列) 向量，∂<strong>y</strong>/∂x 就是行 (列) 向量。</p><p><br></p><p>注：此類偏導數比較少見，通常我們研究的是單變量輸出對多變量輸入，而不是反過來的。</p><p><br></p><p>∂矩陣/∂標量</p><p><br></p><p>當 <strong>y </strong>是大小為 m×n 的矩陣，x 是標量。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/05e8912d13b2412d8f255c546600b7ee><p class=pgc-img-caption></p></div><p><br></p><p>該導數是 <strong>y </strong>中的每個元素 (一共 mn 個元素) 對 x 求導，然後按 <strong>y </strong>的形狀排列出來 (<strong>形狀規則</strong>)。</p><p><br></p><p>注：此類偏導數比較少見，通常我們研究的是單變量輸出對多變量輸入，而不是反過來的。</p><p><br></p><p>1.2</p><p><strong>類型二 ∂y/∂x</strong></p><p><br></p><p>∂向量/∂向量</p><p><br></p><p>當 <strong>y </strong>是含有 m 個元素的向量，<strong>x </strong>是含有 n 個元素的向量。無論 <strong>y </strong>和 <strong>x </strong>是列向量還是行向量，通常偏導數 ∂<strong>y</strong>/∂<strong>x </strong>寫成矩陣形式，而且喜歡把 <strong>y </strong>放在矩陣的行上，而 <strong>x </strong>放在矩陣的列上 [<strong>1</strong>]。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ff9948ab37f043d1803d746214b6232a><p class=pgc-img-caption></p></div><p><br></p><p>該矩陣的大小是 m×n，稱為雅克比 (Jacobian) 矩陣。看個簡單的具體例子：</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/77ff1ca5b4e8495a8196f5e4194f4255><p class=pgc-img-caption></p></div><p><br></p><p>在神經網絡中，<strong>y </strong>和 <strong>x </strong>有兩種線性關係用的最多，如下：</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a062698f77f84614b819b0fd0ed36868><p class=pgc-img-caption></p></div><p><br></p><p>根據具體問題，<strong>y </strong>和 <strong>x </strong>會寫成列向量或行向量。套用上面介紹的雅克比矩陣可以給出通解 [<strong>2</strong>]。</p><p><br></p><p><strong>情況一</strong>：列向量 <strong>y</strong> 對 <strong>x</strong> 求導，其中 <strong>y</strong> = <strong>Wx</strong></p><p><br></p><p>我們知道 ∂<strong>y</strong>/∂<strong>x </strong>的結果是個矩陣，但是一次性寫出它比較困難，不如來看看它第 i 行第 j 列的元素長成什麼樣，即求 ∂yi/∂xj<strong></strong></p><p></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ffd0a9ac7cc9482a9f4b3a01543bdf47><p class=pgc-img-caption></p></div><p><br></p><p>看到 ∂yi/∂xj = Wij 應該可以秒推出 ∂<strong>y</strong>/∂<strong>x = W </strong>了吧，不信驗證下。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c376a3fdcbe04fba93dd1244c7f24591><p class=pgc-img-caption></p></div><p><br></p><p><strong>情況二</strong>：行向量 <strong>y</strong> 對 <strong>x</strong> 求導，其中 <strong>y </strong>=<strong> xW</strong></p><p><br></p><p>我們知道 ∂<strong>y</strong>/∂<strong>x </strong>的結果是個矩陣，但是一次性寫出它比較困難，不如來看看它第 i 行第 j 列的元素長成什麼樣，即求 ∂yi/∂xj<strong></strong></p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/664688863df246019cf4ff5baeee22e6><p class=pgc-img-caption></p></div><p><br></p><p>看到 ∂yi/∂xj = Wji 應該可以秒推出 ∂<strong>y</strong>/∂<strong>x = W</strong>T 了吧，不信驗證下。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/604b961730114676b261e987b9dc9be8><p class=pgc-img-caption></p></div><p><br></p><p><strong>規則 1：</strong>當 <strong>y</strong>,<strong> x </strong>都是列向量且 <strong>y</strong> = <strong>Wx</strong>，有 ∂<strong>y</strong>/∂<strong>x </strong>=<strong> W</strong>。</p><p><br></p><p><strong>規則 2：</strong>當 <strong>y, x </strong>都是行向量且 <strong>y</strong> = <strong>xW</strong>，有 ∂<strong>y</strong>/∂<strong>x </strong>=<strong> W</strong>T。</p><p><br></p><p>∂向量/∂矩陣</p><p><br></p><p>規則 1 和 2 是向量對向量求導，現在關注向量對矩陣求導 (當然困難一些)。接著上面 <strong>y </strong>和 <strong>x </strong>的關係，</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/70c7621b940b4d2896085b7045727ce8><p class=pgc-img-caption></p></div><p><br></p><p><strong>情況一</strong>：列向量 <strong>y</strong> 對矩陣 <strong>W</strong> 求導，其中 <strong>y</strong> = <strong>Wx</strong></p><p><br></p><p>根據向量 <strong>y </strong>(n×1) 和矩陣 <strong>W </strong>(n×m) 的大小，∂<strong>y</strong>/∂<strong>W</strong> 是個三維張量，大小為 n×(n×m)。類比於“向量對向量”的雅克比矩陣，∂<strong>y</strong>/∂<strong>W </strong>是個雅克比張量，形式如下：</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d5bb47f901dd4a2bb193c5305a1c36c1><p class=pgc-img-caption></p></div><p><br></p><p>顯然一次性寫出上面這個三維張量很困難，我們按照 ∂yi/∂Wij<strong> </strong>⇨ ∂yi/∂<strong>W </strong>⇨ ∂<strong>y</strong>/∂<strong>W</strong>層層推導</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/203d1ef5a8af4c2ea0900bf516abf5ec><p class=pgc-img-caption></p></div><p><br></p><p><strong>情況二</strong>：行向量 <strong>y</strong> 對矩陣 <strong>W</strong> 求導，其中 <strong>y</strong> = <strong>xW</strong></p><p><br></p><p>根據向量 <strong>y </strong>(1×n) 和矩陣 <strong>W </strong>(m×n) 的大小，∂<strong>y</strong>/∂<strong>W</strong> 是個三維張量，大小為 n×(m×n)。類比於“向量對向量”的雅克比矩陣，∂<strong>y</strong>/∂<strong>W </strong>是個雅克比張量，形式如下：</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/70dbfbf0037543b68bca28400aa50467><p class=pgc-img-caption></p></div><p><br></p><p>顯然一次性寫出上面這個三維張量很困難，我們按照 ∂yj/∂Wij<strong> </strong>⇨ ∂yj/∂<strong>W </strong>⇨ ∂<strong>y</strong>/∂<strong>W</strong>層層推導</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8ca8486aefd24406bc422e4a7385a1a1><p class=pgc-img-caption></p></div><p><br></p><p>注：實踐中一般不會顯性的把“向量對矩陣”的偏導數寫出來，維度太高 (因為向量是一維張量，矩陣是二維張量，因此向量對矩陣的偏導是個三維張量)，空間太費。我們只是把它當做中間產出來用。</p><p><br></p><p>對於誤差函數 l ，它是 <strong>y </strong>的標量函數。比起求 ∂<strong>y</strong>/∂<strong>W</strong>，我們更有興趣求 ∂l/∂<strong>W </strong>(比如想知道變動權重 <strong>W</strong> 對誤差函數l的影響有多大)。∂l/∂<strong>W </strong>是和 <strong>W </strong>一樣大小的矩陣 (<strong>形狀規則</strong>)。</p><p><br></p><p>接著上面討論的兩種情況，</p><p><br></p><p><strong>情況一</strong>：<strong>x </strong>和 <strong>y </strong>是列向量，按照 ∂l/∂Wij<strong> </strong>⇨ ∂l/∂<strong>W </strong>層層推導</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a73f257b6cf143a38c88b383613daa9e><p class=pgc-img-caption></p></div><p><br></p><p><strong>情況二</strong>：<strong>y </strong>和 <strong>x </strong>是列向量，按照 ∂l/∂Wij<strong> </strong>⇨ ∂l/∂<strong>W </strong>層層推導</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1b7c7725cd5843a3846e055cab2170da><p class=pgc-img-caption></p></div><p><br></p><p><strong>規則 3：</strong>當 <strong>y</strong>, <strong>x </strong>都是列向量且 <strong>y</strong> = <strong>Wx</strong>，l 是 <strong>y </strong>的標量函數，有 ∂l/∂<strong>W </strong>= ∂l/∂<strong>y </strong>·<strong> x</strong>T。</p><p><br></p><p><strong>規則 4</strong>：當<strong> y</strong>, <strong>x</strong> 都是行向量且 <strong>y</strong> = <strong>xW</strong>，l 是 <strong>y</strong> 的標量函數，有 ∂l/∂<strong>W </strong>=<strong> x</strong>T · ∂l/∂<strong>y</strong>。</p><p><br></p><p>∂矩陣/∂向量</p><p><br></p><p>本節的“矩陣對向量”和上節的“向量對矩陣”都是下節的“矩陣對矩陣”的特殊形式，因此研究最通用的“矩陣對矩陣”就足夠了。</p><p><br></p><p>∂矩陣/∂矩陣</p><p><br></p><p>假設 <strong>X </strong>是 d×m 的矩陣，<strong>W </strong>是 n×d 的矩陣，而 <strong>Y </strong>=<strong> WX </strong>是 n×m 的矩陣。在下面特殊例子裡，d= 3, m = 2, n = 2 (以下推出的結果對一般的 d, m, n 也適用)。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f334717b85374016816a1514038ae3a2><p class=pgc-img-caption></p></div><p><br></p><p>和“向量對矩陣”的道理一樣，實踐中不會像下面顯性的把“矩陣對矩陣”的偏導數 (四維張量，矩陣裡面套矩陣) 寫出來。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/fca12dd89b01413b8ce27cc7857f1e53><p class=pgc-img-caption></p></div><p><br></p><p>我們只是把它們當做中間產出來用。比如誤差函數 l 是 <strong>Y </strong>的標量函數，比起求 ∂<strong>Y</strong>/∂<strong>W</strong>和 ∂<strong>Y</strong>/∂<strong>X</strong>，我們更有興趣求 ∂l/∂<strong>W </strong>和 ∂l/∂<strong>X</strong>。根據矩陣鏈式法則，我們有</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/197020bd47c24d0cb2662060e67c3837><p class=pgc-img-caption></p></div><p><br></p><p>注：上面矩陣鏈式法則的表達式這樣寫可能不嚴謹，因為我們並不知道矩陣和四維張量之間的乘法是如何定義的。比如根據<strong>形狀規則</strong>可推出 ∂l/∂<strong>Y</strong>, ∂l/∂<strong>X </strong>和 ∂l/∂<strong>W </strong>的大小，如下表所示：</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9dcb55d12ef048918af8de8f0d08c0a5><p class=pgc-img-caption></p></div><p><br></p><p>但是我們不知道</p><p><br></p><ul><li>2×2 的 ∂l/∂<strong>Y</strong> 和 2×2×3×2 的 ∂<strong>Y</strong>/∂<strong>X </strong>相乘如何等於 3×2 的 ∂l/∂<strong>X</strong></li><li>2×2 的 ∂l/∂<strong>Y</strong> 和 2×2×2×3 的 ∂<strong>Y</strong>/∂<strong>W </strong>相乘如何等於2×3 的 ∂l/∂<strong>W</strong></li></ul><p><br></p><p>因此要推導出矩陣鏈式法則還需回到基本的標量鏈式法則。</p><p><br></p><p>首先關注 ∂l/∂<strong>X</strong>，一個個元素來看</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/52698ec6288740e7a31447b3a2117808><p class=pgc-img-caption></p></div><p><br></p><p>注：點乘“<strong>A</strong>⊙<strong>B</strong>”代表將 <strong>A</strong> 和 <strong>B</strong> 裡所有元素相乘再加總成一個標量。</p><p><br></p><p>將 ∂yij/∂x11寫成矩陣形式，後面是點乘！類比上式可寫出</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9c683de23f874206b542e742f0e6ade5><p class=pgc-img-caption></p></div><p><br></p><p>將這六項帶入矩陣 ∂l/∂<strong>X </strong>整理得到</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/100cb0e7b77340a08bc96153e772ded9><p class=pgc-img-caption></p></div><p><br></p><p>同理得到 ∂l/∂<strong>W</strong></p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/38f524b2a55340f3aa3a66e146222d33><p class=pgc-img-caption></p></div><p><br></p><p><strong>規則 5：Y</strong>, <strong>X </strong>是矩陣且 <strong>Y</strong> = <strong>WX</strong>，l 是 <strong>Y </strong>的標量函數，有 ∂l/∂<strong>W </strong>= ∂l/∂<strong>Y </strong>·<strong> X</strong>T。</p><p><strong>規則 6：Y</strong>, <strong>X</strong> 是矩陣且 <strong>Y</strong> = <strong>WX</strong>，l 是 <strong>Y</strong> 的標量函數，有 ∂l/∂<strong>X </strong>=<strong> W</strong>T · ∂l/∂<strong>Y</strong>。</p><p>1.3 <strong>類型三 ∂y/∂x</strong></p><p style=text-align:justify>這一類型的 y = f(x) 都是作用在元素層面上，比如一些基本函數 y = exp(x) 和 y = sin(x)，還有神經網絡用的函數 y = sigmoid(x) 和 y = relu(x)，它們都是</p><li>標量進標量出</li><ul><li>向量進向量出 (常見)</li><li>矩陣進矩陣出 (常見)</li><li>張量進張量出</li></ul><p>拿 <strong>y</strong> = sin(<strong>x</strong>) 舉例，整個推導還是可以用“∂向量/∂向量”那一套，但由於 <strong>y</strong> 和 <strong>x</strong> 一一對應，因此</p><li><strong>y </strong>和 <strong>x </strong>是一樣的形狀</li><ul><li>y1 只和 x1 有關，因為 ∂y1/∂xi = 0 當 i ≠ 1</li></ul><p>首先寫出雅可比矩陣</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/72adb55199f24af69518b9903e8e3d58><p class=pgc-img-caption></p></div><p><br></p><p>這種元素層面的函數求得的偏導數都是對角矩陣 (diagonal matrix)。</p><p><br></p><p>再次把誤差函數 l 請出來 (l 是 <strong>y </strong>的標量函數)，通常更感興趣的是求 ∂l/∂<strong>x</strong>。對某個 xi，根據鏈式法則得到</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/386863246c294ef38fe7e460ba3c39c7><p class=pgc-img-caption></p></div><p><br></p><p>將上面 n 項整理成向量得到 (發現這種情況下“向量版鏈式法則”成立)</p><p><br></p><p>但在實操上，我們更喜歡用“元素層面操作”來描述上式，通常用 ◯ 加符號來表示，比如</p><p><br></p><ul><li>⊗：元素層面相乘</li><li>⊘：元素層面相除</li></ul><p><br></p><p>用此慣例，上式可寫成</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/57d8e091ca184c58b5676d99a31075e8><p class=pgc-img-caption></p></div><p><br></p><p><strong>規則 7</strong>：當函數 y = f(<strong>x</strong>) 是在元素層面操作，l 是 <strong>y</strong> 的標量函數，有 ∂l/∂<strong>x </strong>=<strong> </strong>∂l/∂<strong>y </strong>⊗ f'(<strong>x</strong>)。</p><p><br></p><p>1.4 <strong>神經網絡應用</strong></p><p><br></p><p>神經網絡裡有兩大類函數需要特別留意：</p><p><br></p><ul><li>層與層之間的轉換函數 (類型三求導)</li><li>輸出結果和標籤之間的誤差函數 (類型一求導)</li></ul><p><br></p><p>轉換函數</p><p><br></p><p>轉換函數是將神經網絡上一層輸出轉換成下一層輸入的函數，常用的轉換函數見下表。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/adf718e20e56445082417dc2a44d522d><p class=pgc-img-caption></p></div><p><br></p><p>其中 identity 函數是線性的，可用於迴歸問題的輸出層，其他三個是非線性的，sigmoid 可用於二分類的輸出層，而 tanh 和 relu 比較多用在隱藏層。此外多分類問題的輸出層用到的函數是 softmax，其函數和導數如下：</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b7ef113821a24d6083317fb5860b7d1a><p class=pgc-img-caption></p></div><p><br></p><p>關於 softmax 的偏導數用“向量對向量”的方法可以輕鬆推出來。</p><p><br></p><p>誤差函數</p><p><br></p><p>迴歸問題的均方誤差 (mean square error, MSE) 函數，和二分類、多分類問題的交叉熵 (cross entropy, CE) 函數。對 m 個數據點，假設真實值為 <strong>Y</strong>，預測值為 <strong>A</strong>，這三種誤差函數的具體形式如下：</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b0bee27a050c4f0e8384e3fd307e1c53><p class=pgc-img-caption></p></div><p><br></p><p>其中上標 (i) 代表第 i 個數據。</p><p><br></p><p>注意多分類問題 CE 函數中 Y 和 A 有下標 k，代表著是第 k 類輸出。以數字分類為例，最後數字“零到九”一共有 10 類，因此 K = 10，而通常用獨熱編碼 (one-hot encoding) 來表示“零到九”。具體來說，用 10×1 的向量代表數字 i，而該向量第 i 個元素是 1，其它元素為 0。如下圖所示：</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/32259b39a74a4f06adae64cbf5d8ec43><p class=pgc-img-caption></p></div><p><br></p><p>再回到求導這來，在神經網絡裡，誤差函數的預測值 <strong>A </strong>就是最後輸出層的產出，通過某個轉換函數而得到。具體來說，</p><p><br></p><ul><li>迴歸問題的 <strong>A </strong>由 identity 轉換函數 f(<strong>x</strong>) 算出</li><li>二分類問題的 <strong>A </strong>由 sigmoid 轉換函數 f(<strong>x</strong>) 算出</li><li>多分類問題的 <strong>A </strong>由 softmax 轉換函數 f(<strong>x</strong>) 算出</li></ul><p><br></p><p>那麼標量 l 對 <strong>x </strong>的偏導數可先由下面“不怎麼嚴謹”的鏈式法則表示出來</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a9c16b93c3484a6f9f42b9e881c1a1f6><p class=pgc-img-caption></p></div><p><br></p><ul><li>對第一項 ∂l/∂<strong>A</strong>，因為 l 是標量，<strong>A </strong>是向量，由<strong>規則 0</strong> (<strong>形狀規則</strong>) 可知 ∂l/∂<strong>A </strong>和 <strong>A </strong>的形狀一樣。</li><li>對第二項 ∂f(<strong>x</strong>)/∂<strong>x</strong>，由<strong>規則 7</strong> 可知，∂f(<strong>x</strong>)/∂<strong>x </strong>和 ∂l/∂<strong>A </strong>的形狀一樣。</li></ul><p></p><p style=text-align:justify>上面兩項形狀相同，因此 ∂l/∂<strong>x</strong> 就是將它們在元素層面相乘得到的結果，因此上面“不怎麼嚴謹”的鏈式法則可嚴謹寫成</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/fde3563ab3024543ac8f0997cf549fce><p class=pgc-img-caption></p></div><p><br></p><p>但上面這種形式只對迴歸問題和二分類問題適用，多分類問題需要更細緻的處理。下面一一來分析。</p><p><br></p><hr><p style=text-align:center><strong>迴歸問題</strong></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1384c87a8d124d29912a3f8654edaa79><p class=pgc-img-caption></p></div><p><br></p><hr><p style=text-align:center><strong>二分類問題</strong></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a2e5d8d4bb6a44acaae803ba7f1c392f><p class=pgc-img-caption></p></div><p><br></p><hr><p style=text-align:center><strong>多分類問題</strong></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1beca5f081214d1c9f0c036eca34f61f><p class=pgc-img-caption></p></div><p><br></p><p>咋一看這兩項形狀不一樣，不能像迴歸問題和二分類問題那樣在元素層面操作了，而只能張量點乘了。</p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3c5890b408d24de8815864ae4a62fc3b><p class=pgc-img-caption></p></div><p><br></p><p>上面公式太複雜，我們把注意力先只放在第 i 個數據上。但細想一下 Y(i) 的元素，它是個 K×1 的向量，只有一個元素為 1 (假設第 j 個) 其餘為 0，因此</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/591d217359e64d96862eec6ebdb6842b><p class=pgc-img-caption></p></div><p><br></p><p>其中下標 :,j 表示矩陣 <strong>S</strong>(i)<strong> </strong>的第 j 列。合併起來得</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8e1aabe1fe574ef0baf6904e12f3a7e1><p class=pgc-img-caption></p></div><p><br></p><p>其中 n1, n2, …, nm 是標籤 Y(1), Y(2), …, Y(m) 中元素為 1 的索引。比如 Y(1) 第 3 個元素為 1，Y(2) 第 6 個元素為 1，Y(m) 第 10 個元素為 1，那麼 [n1, n2, …, nm] = [3 6 10]。</p><p><br></p><p>2計算圖</p><p>2.1 <strong>數學符號</strong></p><p>以下數學符號務必認真看！慣例是用小括號 (i) 上標表示第 i 個數據，用中括號 [L] 上標表示神經網絡的第 L 層。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/84d9e182a91f4c8c96487528d6f0da48><p class=pgc-img-caption></p></div><p><br></p><p>本節只用兩層神經網絡來說明一些核心問題，比如正向傳播、反向傳播、計算圖等等。我們由易到難的討論以下三種情況：</p><p><br></p><ol start=1><li>一個數據點，單特徵輸入 x 和輸出 y 都是標量 (以迴歸問題舉例)</li><li>一個數據點，多特徵輸入 <strong>x </strong>是向量，輸出 y<strong> </strong>是表量 (以二分類問題舉例)</li><li>m 個數據點，輸入 <strong>X </strong>和輸出 <strong>Y </strong>都是矩陣 (以多分類問題舉例)</li></ol><p><br></p><p>關於神經網絡結構裡面基本元素的介紹，請參考《<strong>人工神經網絡</strong>》和《<strong>正向傳播和反向傳播</strong>》。</p><p><br></p><p><strong>注：情況一和情況二隻有一個數據點，現實中幾乎不會出現這樣的神經網絡。這樣設計的情況就是為了能由易到難層層深入來解釋正向傳播和反向傳播。</strong></p><p><br></p><p>2.2<strong>神經網絡 I</strong></p><p>該神經網絡只有單數據點(x, y)，每個輸入 x 只有單特徵，只有單個輸出 y。在下圖中，所有 x, w[1], b[1], z[1], a[1], w[2], b[2], z[2], a[2], y 都是標量。下圖用正向傳播一步步計算出 a[2]。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5254d2a629f1498493c9c407cdaf6f39><p class=pgc-img-caption></p></div><p><br></p><p>以迴歸問題為例，單個數據點的誤差為</p><p style=text-align:center><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/799762345c574b73ae8d287bc2beae8e><p class=pgc-img-caption></p></div><p><br></p><p>神經網絡裡面 w[1], b[1], w[2] 和 b[2] 都是標量型參數，通過鏈式法則以反向傳播的方式解出它們</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/28fb57a7786e476eb06007e364730bfa><p class=pgc-img-caption></p></div><p><br></p><p>在反向傳播過程中</p><p><br></p><ul><li>所有彩色的偏導數，∂l/∂a[2], ∂l/∂z[2], ∂l/∂a[1], ∂l/∂z[1]，都可以重複使用，稱為中間梯度 (intermediate gradient)，格式為“∂損失/∂中間變量”</li></ul><p><br></p><ul><li>所有黑色的偏導數，∂a[2]/∂z[2], ∂z[2]/∂w[2], ∂z[2]/∂b[2], ∂a[1]/∂z[1], ∂z[1]/∂w[1], ∂z[1]/∂b[1]，稱為局部梯度 (local gradient)，格式為“∂中間變量/∂中間變量”或“∂中間變量/∂參數”</li></ul><p><br></p><ul><li>所有帶方框的偏導數，∂l/∂w[2], ∂l/∂b[2], ∂l/∂w[1], ∂l/∂b[1]，是最終需要的，稱為最終梯度 (final gradient)，格式為“∂損失/∂參數”</li></ul><p><br></p><p>整個反向傳播的精髓就是使用局部梯度來計算中間梯度，再盡多不重複的用中間梯度計算最終梯度。</p><p><br></p><p>最終梯度 = 中間梯度 × 局部梯度</p><p><br></p><p>為了能更精確地描述反向傳播算法，使用更精確的計算圖 (computational graph) 是很有幫助的。</p><p><br></p><p>計算圖就是將計算形式化圖形的方法，由<strong>輸入結點</strong>、<strong>輸出結點</strong>、<strong>函數 </strong>(從輸入到輸出的)三部分組成。</p><p><br></p><ul><li>每個一節點來表示一個變量，可以是標量、向量、矩陣或張量。</li><li>每個函數表示一個操作，可以作用在單個變量上，也可以作用在多個變量上。</li></ul><p><br></p><p>計算圖的實例如下：</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/12e85cfb1a46435792bbbcc4e391cd51><p class=pgc-img-caption></p></div><p><br></p><p>上圖中</p><p><br></p><ul><li>x, w[1], b[1], w[2], b[2], y 是輸入節點，l 是輸出節點，它們都是標量。</li><li>綠色圓框裡面的函數 +, ×, f1, f2, J 都可看做操作，比如 +, ×, J 作用在兩個變量上，而 f1, f2 作用在單變量上。</li></ul><p><br></p><p>反向傳播顧名思義就是從最後輸出節點的梯度“∂l /∂l = 1”開始，沿著反方向計算梯度。核心基礎就是</p><p><br></p><ol start=1><li>利用“上一個已算好的中間梯度”</li><li>根據綠色圓框的函數得到“局部梯度”</li><li>再用鏈式法則得到“下一個中間梯度”</li><li>把這個過程反向傳播下去</li></ol><p></p><p>四個步驟如下圖：</p><p style=text-align:center><strong>步驟 1</strong></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d47263b3ab464aa58a8999fca8ae020a><p class=pgc-img-caption></p></div><p style=text-align:center><strong>步驟 2</strong></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a55efbffa5974e4ca93ec2f1e683ced6><p class=pgc-img-caption></p></div><p style=text-align:center><strong>步驟 3</strong></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/50a1a75c30e2446391d95a0c1105dc8a><p class=pgc-img-caption></p></div><p style=text-align:center><strong>步驟 4</strong></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/69b0040a2d1648948bc1ea215ed36be1><p class=pgc-img-caption></p></div><p><br></p><p>上面四個步驟圖看懂之後，結合本節的神經網絡實例，再理解下面六張圖的解釋就容易了 (點擊看大圖)。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/99a8dd3686434fe0ade8ebe50dff41b0><p class=pgc-img-caption></p></div><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3b76ceab2e604b69b75036b1c7e72b15><p class=pgc-img-caption></p></div><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/45b77558d3fb47689a210c4d281c4017><p class=pgc-img-caption></p></div><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cd0c1cd5558647fab08f890e53cee107><p class=pgc-img-caption></p></div><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8fd10708bf7b445489b4420261a92aa8><p class=pgc-img-caption></p></div><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/722e243e988c4ac2a8576744a2161eab><p class=pgc-img-caption></p></div><p><br></p><p>2.3<strong>神經網絡 II</strong></p><p>該神經網絡只有單數據點 (<strong>x</strong>, y)，每個輸入 <strong>x </strong>有多特徵，還是單個輸出 y。在下圖中，<strong>W</strong>[1], <strong>W</strong>[2] 是矩陣，<strong>x</strong>, <strong>z</strong>[1], <strong>a</strong>[1], <strong>b</strong>[1] 是向量，b[2], z[2], a[2], y 是標量。下圖用正向傳播一步步計算出 a[2]。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4a175b6830a047f585117d06635775ca><p class=pgc-img-caption></p></div><p><br></p><p>整個正向傳播計算的變量之間的形狀是匹配的，驗證如下：</p><p style=text-align:center><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/74494af311c54a6a87a0129650893772><p class=pgc-img-caption></p></div><p><br></p><p>在上式中</p><li>輸入層的神經元的個數 = 特徵個數，n[0] = nx，這樣看 <strong>W</strong>[1]<strong>x </strong>的形狀完全匹配</li><ul><li>輸出層的神經元的個數 = 單輸出，n[2] = ny = 1，這樣看 a[2] 是個標量</li></ul><p><br></p><p>以二分類問題為例，單個數據點的誤差為 (y = 0 或 1)</p><p style=text-align:center><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/16898a5f432a41228ea0544ecf3cf429><p class=pgc-img-caption></p></div><p style=text-align:center><br></p><p>神經網絡裡面 <strong>W</strong>[1] 和 <strong>W</strong>[2] 是矩陣型參數，<strong>b</strong>[1] 是向量型參數，b[2] 是標量型參數，通過鏈式法則以反向傳播的方式解出它們</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a876ed70d994416198a35c36e8168a66><p class=pgc-img-caption></p></div><p><br></p><p>根據第一章的張量求導規則，計算上表的偏導數不能更簡單：</p><p><br></p><ul><li>計算 ∂l/∂a[2]，規則 0</li><li>計算 ∂l/∂z[2]，標量鏈式法則</li><li>計算 ∂l/∂<strong>W</strong>[2]，規則 5</li><li>計算 ∂l/∂b[2]，標量鏈式法則</li><li>計算 ∂l/∂<strong>a</strong>[1]，規則 6</li><li>計算 ∂l/∂<strong>z</strong>[1]，規則 7</li><li>計算 ∂l/∂<strong>W</strong>[1]，規則 5</li><li>計算 ∂l/∂b[1]，規則 6</li></ul><p><br></p><p>計算完上面偏導數之後可用<strong>規則 0</strong> (<strong>形狀規則</strong>) 來檢查左右兩邊的形狀是否吻合。</p><p><br></p><p>計算圖對於非標量也適用，大致結構和上節的非常類似。需要注意的是有時“向量或矩陣版鏈式法則”不能自然以連乘的方式寫出來，因為我們其顯示錶達式寫出來了。如下圖 (點擊看大圖)，</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/153c34e5abde49d296d46faf0ecbfd58><p class=pgc-img-caption></p></div><p><br></p><p>2.4 <strong>神經網絡 III</strong></p><p>該神經網絡考慮 m 個數據點 (<strong>X</strong>, <strong>Y</strong>)，每個輸入 <strong>X </strong>有多特徵，每個輸出 <strong>Y </strong>有多類值 (可以想成是 MNIST 手寫數字分類問題，其中有 50000 個訓練數據，<strong>X </strong>有 784 個像素特徵，<strong>Y </strong>是數字零到九的 10 類)。在下圖中，<strong>X</strong>, <strong>W</strong>[1], <strong>Z</strong>[1], <strong>A</strong>[1], <strong>W</strong>[2], <strong>Z</strong>[2], <strong>A</strong>[2], <strong>Y </strong>都是矩陣，<strong>b</strong>[1] 和 <strong>b</strong>[2] 是向量。下圖用正向傳播一步步計算出 <strong>A</strong>[2]。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a783579b2ae14464af4b0a110c738e03><p class=pgc-img-caption></p></div><p><br></p><p>整個正向傳播計算的變量之間的形狀是匹配的，驗證如下：</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a798ad06f7a9471fa5779a1fdee45097><p class=pgc-img-caption></p></div><p><br></p><p>在上式中</p><p><br></p><ul><li>輸入層的神經元的個數 = 特徵個數，n[0] = nx，這樣看 <strong>W</strong>[1]<strong>X </strong>的形狀完全匹配</li><li>輸出層的神經元的個數 = 10 類輸出，n[2] = ny = 10</li><li>為了能達到廣播機制一樣的效果，在相應的 <strong>b </strong>前面加入全部元素都為 1 的向量 <strong>1</strong>m</li></ul><p><br></p><p>多分類問題下的誤差函數為</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/39a596296ad945fb84237063ce995a51><p class=pgc-img-caption></p></div><p><br></p><p>其中 Y 是真實值，A 是神經網絡的預測值 (具體說明見小節 1.4)。</p><p><br></p><p>神經網絡裡面 <strong>W</strong>[1] 和 <strong>W</strong>[2] 是矩陣型參數，<strong>b</strong>[1] 和 <strong>b</strong>[2] 是向量型參數，通過鏈式法則以反向傳播的方式解出它們。</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/eb8051cb24774b4182beb8b57ff7f594><p class=pgc-img-caption></p></div><p><br></p><p>根據第一章的張量求導規則，計算上表的偏導數不能更簡單：</p><p><br></p><ul><li>計算 ∂l/∂<strong>A</strong>[2]，規則 0</li><li>計算 ∂l/∂<strong>Z</strong>[2]，見小節 1.4</li><li>計算 ∂l/∂<strong>W</strong>[2]，規則 5</li><li>計算 ∂l/∂<strong>b</strong>[2]，規則 5</li><li>計算 ∂l/∂<strong>A</strong>[1]，規則 6</li><li>計算 ∂l/∂<strong>Z</strong>[1]，規則 7</li><li>計算 ∂l/∂<strong>W</strong>[1]，規則 5</li><li>計算 ∂l/∂<strong>b</strong>[1]，規則 5</li></ul><p><br></p><p>計算完上面偏導數之後可用<strong>規則 0</strong> (<strong>形狀規則</strong>) 來檢查左右兩邊的形狀是否吻合。</p><p><br></p><p>計算圖對於非標量也適用，大致結構和上節的非常類似。需要注意的是有時“矩陣版鏈式法則”不能自然以連乘的方式寫出來，因為我們其顯示錶達式寫出來了。如下圖 (點擊看大圖)，</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/2cf95f1e1a4448de995fdf8e4f092055><p class=pgc-img-caption></p></div><p><br></p><p>3 總結</p><p>本帖除了文中參考的資料 [<strong>1</strong>] 和[<strong>2</strong>]，還參考了 [<strong>3</strong>], [<strong>4</strong>], [<strong>5</strong>]。</p><p><br></p><p>深度學習可以不嚴謹的認為是各類架構神經網絡，神經網絡的正向傳播就是張量計算，既一連串操作在張量上。做優化第一步是要求出損失函數對所有參數的導數 (張量形式)。我知道</p><p><br></p><ul><li>寫出張量導數表達式難，就精簡出 8 條規則</li><li>串聯張量導數在一起難，就用計算圖來理解</li></ul><p><br></p><p>這兩點都會了，你會發現反向傳播就是張量版鏈式法則。</p><p><br></p><p>8 條規則</p><p><br></p><p>注：l 是<strong> y </strong>或<strong> Y </strong>的標量函數</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f531835869fe46d1badbf749f726d5c9><p class=pgc-img-caption></p></div><p><br></p><p>在這 8 條規則中</p><p><br></p><ul><li>規則 0 (形狀規則) 最重要，很多時候你只要檢查各個張量的形狀就知道公式推導的是否正確。</li><li>規則 1 和 2 是基礎，做法就是每次對標量求導，再按照雅克比矩陣一個個填滿就得到了答案。</li><li>有些人喜歡把 <strong>x </strong>寫成列向量，有些人喜歡把 <strong>x </strong>寫成行向量，規則 4 和 5 根據這些喜好給出相應的偏導數。</li><li>現實中 <strong>X</strong> 通常是個二維矩陣，一個維度是特徵數，一個維度是數據數，因此規則 5 和 6 最普適，是規則 3 和 4 的推廣。</li></ul><p><br></p><p>計算圖核心</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/30cf4840dee94f24aed86eca9300a691><p class=pgc-img-caption></p></div><p><br></p><p>注意我把“中間梯度×局部梯度”該成 dot(中間梯度, 局部梯度)，這個函數實際上是 numpy 裡面張量點乘的操作。</p><p><br></p><p>本帖內容吃透，終於可以放手來研究深度學習中的各種網絡結構了，最重要的是，再也不用悚各種反向傳播推導了。你可以自信的說，不就是鏈式法則麼，帶張量的？Stay Tuned！</p><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/116010296ef1461baeadf2a46d36d2bc><p class=pgc-img-caption></p></div><p><strong>參考資料</strong></p><p style=text-align:center><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d2d9f68d31504e0dbaa16e388dbd0f85><p class=pgc-img-caption></p></div><p><br></p><div class=pgc-img><img alt=「數學基礎」簡單易懂的張量求導和計算圖講解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1b79e86427114fa695611bc9b9ca999c><p class=pgc-img-caption></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>數學</a></li><li><a>基礎</a></li><li><a>簡單</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/09bdf3b9.html alt=高考數學基礎知識鞏固專題：同角三角函數的基本關係和誘導公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/4ad700017a27db3c428d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/09bdf3b9.html title=高考數學基礎知識鞏固專題：同角三角函數的基本關係和誘導公式>高考數學基礎知識鞏固專題：同角三角函數的基本關係和誘導公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f0326a27.html alt=VLAN基礎及簡單的VLAN劃分方法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/03a138f8448e46cb9cf3086783ddcfb8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f0326a27.html title=VLAN基礎及簡單的VLAN劃分方法>VLAN基礎及簡單的VLAN劃分方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/dcf9a906.html alt=數學基礎弱？這個寒假您要這麼規劃 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d3ac7cf90a6b44c4bec28ef0cfa70ef9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/dcf9a906.html title=數學基礎弱？這個寒假您要這麼規劃>數學基礎弱？這個寒假您要這麼規劃</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/587ec87f.html alt=最全初中數學基礎知識結構圖，把重難點知識講得一清二楚！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/71b23f2b0e66460499c804197d41a880 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/587ec87f.html title=最全初中數學基礎知識結構圖，把重難點知識講得一清二楚！>最全初中數學基礎知識結構圖，把重難點知識講得一清二楚！</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/7ea57021.html alt=高中數學基礎知識結構圖，把知識點講得一清二楚，高考生必看 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/c991c07b96e440acbe50a33e4d65af2b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/7ea57021.html title=高中數學基礎知識結構圖，把知識點講得一清二楚，高考生必看>高中數學基礎知識結構圖，把知識點講得一清二楚，高考生必看</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/111e252f.html alt=數學幾何基礎知識“線和角”總結——記得收藏起來！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ea35546831474771b4fe3535ae127b9d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/111e252f.html title=數學幾何基礎知識“線和角”總結——記得收藏起來！>數學幾何基礎知識“線和角”總結——記得收藏起來！</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/60676763.html alt=數學推理的基礎—三個基本原則 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/b0e757cd032b468ea1ebb21f6388581e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/60676763.html title=數學推理的基礎—三個基本原則>數學推理的基礎—三個基本原則</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/9c8725bd.html alt=1900頁數學基礎：面向CS的線性代數、拓撲、微積分和最優化 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/4f172119842c4f22a7e2015d71707b9d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/9c8725bd.html title=1900頁數學基礎：面向CS的線性代數、拓撲、微積分和最優化>1900頁數學基礎：面向CS的線性代數、拓撲、微積分和最優化</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/d22b344f.html alt=人工智能數學基礎----矩陣 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/26a0df4aa6054555855eb60291e265c5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/d22b344f.html title=人工智能數學基礎----矩陣>人工智能數學基礎----矩陣</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/eb4d7908.html alt=小學數學基礎概念歸類彙總「比和比例」 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/7b804d5b-8a15-4452-b121-290e21c5b8fc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/eb4d7908.html title=小學數學基礎概念歸類彙總「比和比例」>小學數學基礎概念歸類彙總「比和比例」</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9cbf8d8d.html alt=高中數學筆記梳理之向量基礎知識 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9cef5f2789e146a1b1e03ebb0580abc0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9cbf8d8d.html title=高中數學筆記梳理之向量基礎知識>高中數學筆記梳理之向量基礎知識</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce788776.html alt=高考數學基礎知識鞏固：平面向量的概念及其線性運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/37ef0004ace363bc6f23 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce788776.html title=高考數學基礎知識鞏固：平面向量的概念及其線性運算>高考數學基礎知識鞏固：平面向量的概念及其線性運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/37ab99c.html alt=生物很簡單，基礎最關鍵！高一到高三生物基礎知識點速記必背彙總 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/4178978ea00444518c067f25ad0a5d0a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/37ab99c.html title=生物很簡單，基礎最關鍵！高一到高三生物基礎知識點速記必背彙總>生物很簡單，基礎最關鍵！高一到高三生物基礎知識點速記必背彙總</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/221d378.html alt=生物很簡單，基礎最關鍵，高一到高三生物400頁知識點彙總大全 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/f16358580e01435f8c3cd2b36c6e060c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/221d378.html title=生物很簡單，基礎最關鍵，高一到高三生物400頁知識點彙總大全>生物很簡單，基礎最關鍵，高一到高三生物400頁知識點彙總大全</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0ccadce.html alt=一年級數學基礎加減法規律和法則，給孩子收藏了，口算能力提升快 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/e2b61eaa-5cd9-448a-ac7d-d9bc04ef59c0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0ccadce.html title=一年級數學基礎加減法規律和法則，給孩子收藏了，口算能力提升快>一年級數學基礎加減法規律和法則，給孩子收藏了，口算能力提升快</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>