<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ | 极客快訊</title><meta property="og:title" content="打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RqSgmP9HojtMNK"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0eccae6c.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0eccae6c.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/0eccae6c.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0eccae6c.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0eccae6c.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/0eccae6c.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/0eccae6c.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0eccae6c.html><meta property="article:published_time" content="2020-11-14T20:53:56+08:00"><meta property="article:modified_time" content="2020-11-14T20:53:56+08:00"><meta name=Keywords content><meta name=description content="打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/0eccae6c.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RqSgmP9HojtMNK><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCAZJFgANtBGAu><p>作者 | Chaitanya K. Joshi</p><p>譯者 | 蘇本如，責編 | 夕顏</p><p>來源 | CSDN（ID:CSDNnews）</p><p>我的工程師朋友經常問我：圖深度學習聽起來很棒，但是有沒有實際應用呢？</p><p>雖然圖神經網絡被用於Pinterest、阿里巴巴和推特的推薦系統，但一個更巧妙的成功案例是Transformer架構，它在NLP（Natural Language Processing ，自然語言處理）世界掀起了一場風暴。</p><p>在這篇文章中，我嘗試在圖神經網絡（GNNs）和Transformers之間建立一種聯繫。我將討論NLP和GNN社區對於模型架構背後的直覺，用方程和圖形建立兩者之間的聯繫，並討論兩者如何合作來共同進步。</p><p>讓我們從模型架構的目的——表示學習（representation learning）開始。</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqLxPCXik3SR><p><strong class=highlight-text toutiao-origin=span>NLP的表示學習</strong></p><p>在較高的層次上，所有的神經網絡結構都將輸入數據的“表示”構建為嵌入向量，並對有關數據的有用統計和語義信息進行編碼。這些潛在的或隱藏的“表示”可以用於執行一些有用的操作，例如對圖像進行分類或翻譯句子。神經網絡通過接收反饋(通常是通過誤差/損失函數)來學習構建更好的“表示”。</p><p>對於自然語言處理（NLP），通常遞歸神經網絡（RNNs）以順序的方式構建句子中每個單詞的“表示”，即一次一個單詞。直觀地說，我們可以把一個RNN層想象成一個傳送帶，上面的文字從左到右進行遞歸處理。最後，我們得到了句子中每個單詞的一個隱藏的特徵，我們將其傳遞給下一個RNN層或用於我們選擇的NLP任務。</p><p>如果你想回顧一下RNNs和NLP的表示學習，我強烈推薦Chris Olah的博客。</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSJd0HGONhej><p>Transformers 最初是為機器翻譯而引入的，現在已經逐漸取代了主流NLP中的RNNs。該架構採用了一種全新的表示學習方法：完全不需要使用遞歸，Transformers使用一種注意力機制（attention mechanism）來構建每個單詞的特徵，以確定句子中所有其他單詞對前述單詞的重要性。瞭解了這一點，單詞的更新特徵就是所有單詞特徵的線性變換的和，並根據其重要性進行加權。</p><p>在2017年的時候，這種想法聽起來非常激進，因為NLP社區已經習慣了使用RNN處理文本的順序式方式（即一次一個單詞）。它的名字可能也起到了推波助瀾的作用。</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqLy93x95UEx><p><strong class=highlight-text toutiao-origin=span>詳解Transformer</strong></p><p>讓我們通過將前一段翻譯成數學符號和向量的語言來發展關於架構的直覺。我們將句子S中第i個單詞的隱藏特徵h從從第 ℓ 層更新到第 ℓ+1層，如下所示：</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSJdgELo3Vwt><p>例如：</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCWSJdtqjAMfM><p>這裡的：</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSJeCDjx3eFr><p>其中j∈S表示句子中的詞集，<img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCWSJzd8SmeveY>、<img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCWSK00GoqI646>、<img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSK0NB40786i>是可學習的線性權重（分別表示注意力計算的Query、Key和Value）。</p><p>對於句子中的每個單詞，注意力機制是並行執行的，以一個單詞一個單詞地方式獲得更新後的特徵，這是RNNs上的Transformer的另一個優點：它逐詞逐詞地更新特徵。</p><p>我們可以通過以下管道（pipeline）來更好地理解注意力機制：</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCWSK0g39RYxn4><p>考慮到單詞的特徵 和其他詞集的特徵，通過向量點積計算每對（i，j）的注意力權重，然後對所有j的注意力權重執行softmax運算。最後。我們得到單詞i的最新特徵。句子中的每個單詞都會並行地通過相同的管道來更新其特徵。</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RqMqMYY7LcGio6><p>多頭注意力機制</p><p>讓這個簡單的向量點積注意力機制發揮作用是很棘手的。因為錯誤的可學習權重的隨機初始化會使訓練過程變得不穩定。</p><p>我們可以通過並行執行多個注意力“頭”並將結果串聯起來（讓每個“頭”現在都有獨立的可學習權重）來解決這個問題：</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSK1F1hUcz2N><p>式中，<img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCWSKI31tcUQtg>是第k個注意力頭的可學習的權重，<img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSKIY4ItXUNV>是降維投影，以匹配跨層的<img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCWSKIw8qKFUMP>和<img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCWSKJIFn9Sztj>的維度。</p><p>多個頭部允許注意力機制從本質上“對衝賭注”，可以從上一層觀察前一層隱藏特徵的不同轉換或不同的方面。我們稍後會詳細討論。</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqMZM85xvsbJ><p><strong class=highlight-text toutiao-origin=span>規模大小問題</strong></p><p>Transformer架構的一個關鍵問題是，經過注意力機制之後的單詞特徵可能具有不同的規模和大小。這可能是由於在對一些單詞的其他單詞特徵進行求和的時候，這些單詞具有非常尖銳或非常分散的注意力權重<img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSKJc2KKm98c>。此外，在單個特徵向量條目層面上，將多個注意力頭拼接在一起，每個注意力頭的輸出值可以在不同的尺度上，從而導致最終向量<img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSKUaAYXgB43>的值具有很寬的動態值範圍。</p><p>按照傳統的機器學習（ML）的經驗，這時候向pipeline中添加一個歸一化層似乎是合理的。</p><p>Transformer通過使用LayerNorm克服了第二個問題，LayerNorm在特徵級別進行規一化並學習仿射變換。此外，通過特徵維度的平方根來調整向量點積注意力有助於解決第一個問題。</p><p>最後，Transformer的作者提出了另一個控制尺度問題的“技巧”：一個具有特殊結構的基於位置排列的2層MLP。在多頭注意力之後，他們通過一個可學習的權重將投射到一個（荒謬的）更高的維度，在那裡它經歷了ReLU非線性後，再被投射回其原始維度，然後再進行另一次歸一化：</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCWSKV04zircTD><p>老實說，我不確定這個過於參數化的前饋子層背後的確切直覺是什麼。我想LayerNorm和scaled dot products並沒有完全解決前面提到的問題，所以大的MLP可以說是一種獨立地重新縮放特徵向量的hack方法。根據Jannes Muenchmeyer的說法，前饋子層確保了Transformer是一個萬能逼近器。因此，投影到一個非常高的維度空間，經歷一次ReLU非線性，然後重新投射到原始維度，使模型能夠比在隱藏層中保持相同維度時可以“表示”更多的功能。</p><p>Transformer層的最終結構圖看起來是這樣的：</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCWSKVN1kWuUNz><p>Transformer架構也非常適合深度學習網絡，這使得NLP社區在模型參數和擴展數據方面都能夠進行擴展。</p><p>每個多頭注意力子層和前饋子層的輸入和輸出之間的<strong>殘差連接</strong>是堆疊Transformer層的關鍵(但為了清晰起見，在圖中省略了)。</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqNMhMM9NBY><p><strong class=highlight-text toutiao-origin=span>使用GNNs構建圖的表示</strong></p><p>現在，我們暫時先不討論NLP。</p><p>圖神經網絡（GNNs）或圖卷積神經網絡（GCNs）構建圖數據中節點和邊的表示。它們通過鄰域聚合（或消息傳遞）來實現這一點，每個節點從其鄰域收集特徵，以更新其對周圍的局部圖結構的表示。堆疊幾個GNN層使得模型能夠在整個圖中傳播每個節點的特徵--從它的鄰居傳播到鄰居的鄰居，依此類推。</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSKVlD6Ts6Dn><p>以這個表情符號社交網絡為例: GNN產生的節點特徵可以用於預測任務，例如：識別最有影響力的成員或提出潛在的聯繫。</p><p>在其最基本的形式中，GNNs通過對第ℓ層節點（比如說<img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSKW7Ev5pAsg>）自身特徵的非線性變換，在每個相鄰節點j∈N（i）的特徵的集合中加入節點自身特徵的非線性變換，從而更新第ℓ層節點i的隱藏特徵h：</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSL8R6mvfn0U><p>在這裡，，是GNN層的可學習權重矩陣，σ是一個類似於ReLU的非線性變換函數。在本示例中，<img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCWSL9VErwjK95>= {<img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCWSL9v3K7IfYT>}。</p><p>鄰域節點j∈N（i）上的求和可以用其他輸入大小不變的聚合函數來代替，例如簡單的mean/max或更強大的函數，比如基於注意力機制的加權求和函數。</p><p>這聽起來耳熟嗎？</p><p>也許一個pipeline（管道）將有助於實現連接：</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCWSLASHpgc2W9><p>如果我們將多個並行的鄰域頭進行聚合，並用注意力機制（即加權和）代替鄰域j上的求和，加上歸一化和前饋MLP，看，我們就得到了一個圖<strong>Transformer</strong>！</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RqMqdp52DyYCQP><p><strong class=highlight-text toutiao-origin=span>句子是全連通的詞圖</strong></p><p>為了使連接更加明確，可以將一個句子看作一個完全連通的圖，其中每個單詞都與其他每個單詞相連。現在，我們可以使用GNN為圖(句子)中的每個節點(單詞)構建特性，然後我們可以使用它執行NLP任務。</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSLAu2PtaIAT><p>廣義地說，這就是Transformers正在做的: 它們是具有以多頭注意力作為鄰域聚合函數的GNNs。而標準的GNNs從其局部鄰域節點j∈N（i）聚合特徵，NLP的Transformer將整個句子S視為局部鄰域，在每一層聚合來自每個單詞j∈S的特徵。</p><p>重要的是，各種特定於問題的技巧，-- 例如位置編碼、因果/屏蔽聚合、學習速率調度器和廣泛的預訓練 -- 對Transformers 的成功至關重要，但在GNN社區中很少出現。同時，從GNN的角度來看，Transformers可以啟發我們擺脫架構中的許多華而不實的東西。</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rqqo5387tkSUcT><p><strong class=highlight-text toutiao-origin=span>我們學到了什麼？</strong></p><p><strong>句子都是全連通圖嗎？</strong></p><p>既然我們已經在Transformer和GNNs之間建立了聯繫，讓我來談談一些想法。</p><p>首先，全連通圖是NLP的最佳輸入格式嗎？</p><p>在統計NLP和ML（機器學習）流行之前，像Noam Chomsky這樣的語言學家專注於發展語言結構的形式化理論，例如語法樹/圖。樹形長短期記憶網絡（Tree LSTMs）模型已經被嘗試過了，<strong>但是否有可能Transformers/GNNs是可以將語言理論和統計NLP這兩個世界結合在一起的更好的架構？</strong>例如，MILA(蒙特利爾學習算法研究所和斯坦福大學最近的一項研究探索了使用語法樹增強預訓練的Transformer，如Sachan等人在2020年提出的基於Transformer的雙向編碼器表示（ BERT）。</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCWSLYc2KPeI0d><p class=pgc-img-caption>圖片來源: 維基百科</p><p><strong>長期依賴性</strong></p><p>全連通圖的另一個問題是，它們使得學習單詞之間的長期依賴關係變得困難。原因很簡單，這是因為圖的邊數量和節點的數量成平方量級關係，即在一個有著n個單詞的句子中，Transformer/GNN將在<img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSLZ9G4kd8me>對單詞的量級上進行計算。對於非常大的n來說，這個計算規模大到無法控制。</p><p>NLP社區對長序列和依賴關係問題的看法很有趣:使注意力機制在輸入大小方面變得稀疏或自適應，在每一層中添加遞歸或壓縮，以及使用局部敏感哈希來獲得有效的注意力，這些都是可能使得Transformers變得更好的新想法。</p><p>看到來自GNN社區的想法加入其中是一件很有趣的事，例如用於句子圖稀疏化的二分法（BP- Binary Partitioning）似乎是另一種令人興奮的方法。BP-Transformers遞歸地將句子分為兩部分，直到它們能夠從句子標記中構造出一個分層二叉樹。這種結構化的歸納偏置有助於模型以內存級效率的方式處理較長的文本序列。</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCWSLZW5JFR6bd><p class=pgc-img-caption>資料來源：Ye等人，2019年</p><p><strong>Transformers正在學習“神經語法嗎” ？</strong></p><p>在一些有關Transformers學習的文章中，基本假設是Transformers對句子中的所有詞對進行注意力計算，以確定哪些詞對是最有趣的，也就是能讓“Transformer”學習一些類似於特定任務語法的東西。在多頭注意力中，不同的頭也可以“觀察”不同的句法屬性。</p><p>用圖的術語來說，通過在全圖上使用GNN，我們能從GNN在每一層執行鄰域聚合的方式恢復最重要的邊以及它們可能包含的內容嗎？我還不太相信這個觀點。</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSLZq3l9h30a><p class=pgc-img-caption>資料來源：Clark等人， 2019</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Rqqo547DNIp69u><p><strong class=highlight-text toutiao-origin=span>為什麼是多頭注意力？為什麼是注意力？</strong></p><p>我更贊同多頭機制的優化觀點，即擁有多個注意力頭可以改進學習，並克服糟糕的隨機初始化。例如，這些論文表明，Transformer頭可以在訓練後被“修剪”或移除，而不會對性能產生顯著影響。</p><p>多頭鄰域聚合機制在GNNs中也被證明是有效的，例如，GAT使用相同的多頭注意力，MoNet使用多個高斯核來聚合特徵。雖然這些是為了穩定注意力機制而發明的，但這些多頭技巧會成為擠出額外模型性能的標準嗎？</p><p>相反，具有簡單聚合函數（如sum或max）的GNN不需要多個聚合頭來進行穩定的訓練。如果我們不必計算句子中每個詞對之間的配對兼容性，那對Transformers來說不是很好嗎？</p><p>Transformers能從完全擺脫注意力中獲益嗎？Yann Dauphin和合作者的最近工作提出了一種替代的ConvNet的架構。Transformers也可能最終會做一些類似的事情。</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SCWSLaICdTmi3U><p class=pgc-img-caption>資料來源：Wu等人，2019年</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RrVjZyC2xbYdsX><p><strong class=highlight-text toutiao-origin=span>為什麼Transformers的訓練這麼難？</strong></p><p>閱讀最新的Transformer論文讓我覺得，訓練這些模型需要一些類似於黑魔法的東西來確定最佳學習速率調度器、熱身策略和衰減設置。這可能只是因為模型太過龐大，而NLP的研究任務又太具有挑戰性了。</p><p>但是最近的結果表明，這也可能是因為歸一化的具體排列和架構內的殘差連接所導致的。</p><blockquote><div><div><p>我很喜歡閱讀最新的@DeepMind Transformer論文，但是訓練這些模型為什麼需要它樣的黑魔法呢？”對於基於單詞的語言模型（LM），我們使用了16,000個warmup-step和500,000個decay-step，並犧牲了9000個goat。”</p><p>https://t.co/dP49GTa4zepic.twitter.com/1K3Fx4s3M8</p><p>- Chaitanya K.Joshi（@chaitjo）於2020年2月17日</p></div></div></blockquote><p>我知道自己過分激動了，但這讓我提出疑問：我們真的需要多頭的昂貴的配對的注意力，過分參數化的MLP子層，和複雜的學習速度調度器嗎？</p><p>我們真的需要如此之大的模型嗎？對於手頭的任務來說，具有良好的歸納偏差的體系結構不應該更容易訓練嗎？</p><p>原文鏈接：</p><p>https://thegradient.pub/transformers-are-graph-neural-networks/</p><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/R69FpRH4d90a7d><img alt=打通語言理論和統計NLP，Transformers/GNNs架構能做到嗎？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SCAZLuFCeGLmCt></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>語言理論</a></li><li><a>統計</a></li><li><a>NLP</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/24249b2f.html alt=2020年雅安市各區縣高速路長度統計，有兩個縣還沒有高速公路 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7627cb26b45c4b73b7ddc64e52586ccc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/24249b2f.html title=2020年雅安市各區縣高速路長度統計，有兩個縣還沒有高速公路>2020年雅安市各區縣高速路長度統計，有兩個縣還沒有高速公路</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/419a0323.html alt=樂山市各區縣高速路長度統計，這五個地區還沒開通高速公路 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/fac6df0b-5dff-46a6-8659-6046a0793dd8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/419a0323.html title=樂山市各區縣高速路長度統計，這五個地區還沒開通高速公路>樂山市各區縣高速路長度統計，這五個地區還沒開通高速公路</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/30e1a9cb.html alt=Excel中，如何統計列數呢？一個設置搞定 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1521683037053abb4c6de06 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/30e1a9cb.html title=Excel中，如何統計列數呢？一個設置搞定>Excel中，如何統計列數呢？一個設置搞定</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/971127b3.html alt=Excel中，N個名字組合的提取統計 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/4331000413bce8c04397 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/971127b3.html title=Excel中，N個名字組合的提取統計>Excel中，N個名字組合的提取統計</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b22dfac7.html alt=「Excel技巧」統計同一個單元格內的人員數量，一個公式就搞定 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/f45549dd2cae4e14aa958ce201c1ea72 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b22dfac7.html title=「Excel技巧」統計同一個單元格內的人員數量，一個公式就搞定>「Excel技巧」統計同一個單元格內的人員數量，一個公式就搞定</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a7ad4761.html alt=大數據統計：80%癌症術後3年復發轉移，療法替癌患擋過“生死劫” class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/401e795037ab41d5a2481e58bccf0d23 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a7ad4761.html title=大數據統計：80%癌症術後3年復發轉移，療法替癌患擋過“生死劫”>大數據統計：80%癌症術後3年復發轉移，療法替癌患擋過“生死劫”</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8539bc64.html alt=統計學學習之路——參數檢驗和方差分析學習分享 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/538609d9d5a94c7db9bbad687fc6dcfc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8539bc64.html title=統計學學習之路——參數檢驗和方差分析學習分享>統計學學習之路——參數檢驗和方差分析學習分享</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/33d04d4d.html alt=2020年各大頂會NLP、ML優質論文分類整理分享 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/fb47112700b049aa88994c8949ec9403 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/33d04d4d.html title=2020年各大頂會NLP、ML優質論文分類整理分享>2020年各大頂會NLP、ML優質論文分類整理分享</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5753325c.html alt=如何實現一個分佈式統計和過濾的鏈路追蹤 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3476ff40e4614445b3755a142c79cba8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5753325c.html title=如何實現一個分佈式統計和過濾的鏈路追蹤>如何實現一個分佈式統計和過濾的鏈路追蹤</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d15ba16e.html alt=淺談IVD統計系列--基本概念 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/1a5b5302b22a4fc4980c652fe03d27f8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d15ba16e.html title=淺談IVD統計系列--基本概念>淺談IVD統計系列--基本概念</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/289a0294.html alt=VBA｜選區字符統計並顯示到狀態欄 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/4ae20001d9162686bb27 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/289a0294.html title=VBA｜選區字符統計並顯示到狀態欄>VBA｜選區字符統計並顯示到狀態欄</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7ced8c7c.html alt=概率論和數理統計：概率和頻率之間的關係是什麼？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/275c597550f145c796e0f7c8d6f9eba9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7ced8c7c.html title=概率論和數理統計：概率和頻率之間的關係是什麼？>概率論和數理統計：概率和頻率之間的關係是什麼？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/156d63ed.html alt=人物：吉布斯——熱力學大師與統計物理奠基人 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/24dd555559f34fc99e9699713b5d2bbb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/156d63ed.html title=人物：吉布斯——熱力學大師與統計物理奠基人>人物：吉布斯——熱力學大師與統計物理奠基人</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c423b956.html alt="Word 字數統計的方法" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/2ac80960973f4b06ae8ebe40d3fccf45 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c423b956.html title="Word 字數統計的方法">Word 字數統計的方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/992fc5cb.html alt=python怎麼統計txt文件的字數 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/c7073b5396334cfb8fbdadd7fc1fbb5c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/992fc5cb.html title=python怎麼統計txt文件的字數>python怎麼統計txt文件的字數</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>