<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習與深度學習核心知識點總結(一) | 极客快訊</title><meta property="og:title" content="機器學習與深度學習核心知識點總結(一) - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/3f3c0a9fa03f4897bcc171d785ec0c37"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0fe19d8.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0fe19d8.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/0fe19d8.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0fe19d8.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0fe19d8.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/0fe19d8.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/0fe19d8.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/0fe19d8.html><meta property="article:published_time" content="2020-10-29T20:56:37+08:00"><meta property="article:modified_time" content="2020-10-29T20:56:37+08:00"><meta name=Keywords content><meta name=description content="機器學習與深度學習核心知識點總結(一)"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/0fe19d8.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習與深度學習核心知識點總結(一)</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><div><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3f3c0a9fa03f4897bcc171d785ec0c37><p class=pgc-img-caption></p></div><blockquote><p>作者 | 小小挖掘機</p><p>來源 | SIGAI</p></blockquote><h1><strong>數學</strong></h1><p><strong>1.列舉常用的最優化方法</strong></p><p>梯度下降法</p><p>牛頓法，</p><p>擬牛頓法</p><p>座標下降法</p><p>梯度下降法的改進型如AdaDelta，AdaGrad，Adam，NAG等。</p><p><strong>2.梯度下降法的關鍵點</strong></p><p>梯度下降法沿著梯度的反方向進行搜索，利用了函數的一階導數信息。梯度下降法的迭代公式為：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3272272ab80f4cadb25ffccd8e9e67fe><p class=pgc-img-caption></p></div><p>根據函數的一階泰勒展開，在負梯度方向，函數值是下降的。只要學習率設置的足夠小，並且沒有到達梯度為0的點處，每次迭代時函數值一定會下降。需要設置學習率為一個非常小的正數的原因是要保證迭代之後的x<em>k</em>+1位於迭代之前的值x<em>k</em>的鄰域內，從而可以忽略泰勒展開中的高次項，保證迭代時函數值下降。</p><p>梯度下降法只能保證找到梯度為0的點，不能保證找到極小值點。迭代終止的判定依據是梯度值充分接近於0，或者達到最大指定迭代次數。</p><p>梯度下降法在機器學習中應用廣泛，尤其是在深度學習中。AdaDelta，AdaGrad，Adam，NAG等改進的梯度下降法都是用梯度構造更新項，區別在於更新項的構造方式不同。</p><p><strong>3.牛頓法的關鍵點</strong></p><p>牛頓法利用了函數的一階和二階導數信息，直接尋找梯度為0的點。牛頓法的迭代公式為：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/93d3fcfccd1d4a0192adc037a178a106><p class=pgc-img-caption></p></div><p>其中H為Hessian矩陣，g為梯度向量。牛頓法不能保證每次迭代時函數值下降，也不能保證收斂到極小值點。在實現時，也需要設置學習率，原因和梯度下降法相同，是為了能夠忽略泰勒展開中的高階項。學習率的設置通常採用直線搜索（line search）技術。</p><p>在實現時，一般不直接求Hessian矩陣的逆矩陣，而是求解下面的線性方程組：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e1152b2dda71436f8a5335713a6cb89a><p class=pgc-img-caption></p></div><p>其解d稱為牛頓方向。迭代終止的判定依據是梯度值充分接近於0，或者達到最大指定迭代次數。</p><p>牛頓法比梯度下降法有更快的收斂速度，但每次迭代時需要計算Hessian矩陣，並求解一個線性方程組，運算量大。另外，如果Hessian矩陣不可逆，則這種方法失效。</p><p><strong>4.拉格朗日乘數法</strong></p><p>拉格朗日乘數法是一個理論結果，用於求解帶有等式約束的函數極值。對於如下問題：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4f58fac7d9104372bd4cc94a0dce5613><p class=pgc-img-caption></p></div><p>構造拉格朗日乘子函數：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/597fa8ab1ec64130996b55bfea3dd10f><p class=pgc-img-caption></p></div><p>在最優點處對x和乘子變量的導數都必須為0：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cbef93a467d44403a9342d8017760c37><p class=pgc-img-caption></p></div><p>解這個方程即可得到最優解。對拉格朗日乘數法更詳細的講解可以閱讀任何一本高等數學教材。機器學習中用到拉格朗日乘數法的地方有：</p><p>主成分分析</p><p>線性判別分析</p><p>流形學習中的拉普拉斯特徵映射</p><p>隱馬爾科夫模型</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3a746e09a32c4d5e9fe6f7f3f967102f><p class=pgc-img-caption></p></div><p><strong>5.凸優化</strong></p><p>數值優化算法面臨兩個方面的問題：局部極值，鞍點。前者是梯度為0的點，也是極值點，但不是全局極小值；後者連局部極值都不是，在鞍點處Hessian矩陣不定，即既非正定，也非負定。</p><p>凸優化通過對目標函數，優化變量的可行域進行限定，可以保證不會遇到上面兩個問題。凸優化是一類特殊的優化問題，它要求：</p><p>優化變量的可行域是一個凸集</p><p>目標函數是一個凸函數</p><p>凸優化最好的一個性質是：所有局部最優解一定是全局最優解。機器學習中典型的凸優化問題有：</p><p>線性迴歸</p><p>嶺迴歸</p><p>LASSO迴歸</p><p>Logistic迴歸</p><p>支持向量機</p><p>Softamx迴歸</p><p><strong>6.拉格朗日對偶</strong></p><p>對偶是最優化方法裡的一種方法，它將一個最優化問題轉換成另外一個問題，二者是等價的。拉格朗日對偶是其中的典型例子。對於如下帶等式約束和不等式約束的優化問題：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6e38c16f2d6540a698eebbe658836e0b><p class=pgc-img-caption></p></div><p>與拉格朗日乘數法類似，構造廣義拉格朗日函數：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8e9ea5ed984b45caa5a13bd456da8a20><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4d34bbd300aa47e4ac1ddef81f2ff941><p class=pgc-img-caption></p></div><p>必須滿足</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e192917b81a841feb66e55ee150e1205><p class=pgc-img-caption></p></div><p>的約束。原問題為：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/209ef425c1a04d9ea9e29db9fc707b86><p class=pgc-img-caption></p></div><p>即先固定住x，調整拉格朗日乘子變量，讓函數L取極大值；然後控制變量x，讓目標函數取極小值。原問題與我們要優化的原始問題是等價的。</p><p>對偶問題為：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/57a759b02d26441da485624b82355054><p class=pgc-img-caption></p></div><p>和原問題相反，這裡是先控制變量x，讓函數L取極小值；然後控制拉格朗日乘子變量，讓函數取極大值。</p><p>一般情況下，原問題的最優解大於等於對偶問題的最優解，這稱為弱對偶。在某些情況下，原問題的最優解和對偶問題的最優解相等，這稱為強對偶。</p><p>強對偶成立的一種條件是Slater條件：一個凸優化問題如果存在一個候選x使得所有不等式約束都是嚴格滿足的，即對於所有的i都有<em>gi</em> (x)&lt;0，不等式不取等號，則強對偶成立，原問題與對偶問題等價。注意，Slater條件是強對偶成立的充分條件而非必要條件。</p><p>拉格朗日對偶在機器學習中的典型應用是支持向量機。</p><p><strong>7.KKT條件</strong></p><p>KKT條件是拉格朗日乘數法的推廣，用於求解既帶有等式約束，又帶有不等式約束的函數極值。對於如下優化問題：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/331b8c36e18c4e50a578d15247e3f77b><p class=pgc-img-caption></p></div><p>和拉格朗日對偶的做法類似，KKT條件構如下乘子函數：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/aaf2555af7524fbab907aa9743867e00><p class=pgc-img-caption></p></div><p>λ和μ稱為KKT乘子。在最優解處</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a8fe1f1e2b8e42f99cbaee1dbba8d97d><p class=pgc-img-caption></p></div><p>應該滿足如下條件：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2ca102ed01b24c99adcb07f1d3321630><p class=pgc-img-caption></p></div><p>等式約束</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4da28daaa0b943fdaa24e22cfe4d6c3a><p class=pgc-img-caption></p></div><p>和不等式約束</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4b5e08bb67af490ca3a1673addd75cc7><p class=pgc-img-caption></p></div><p>是本身應該滿足的約束，</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f56efeb4386e4b90a328e7837088e85c><p class=pgc-img-caption></p></div><p>和之前的拉格朗日乘數法一樣。唯一多了關於<em>gi</em> (x)的條件：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1309fb6d1c36485991c0a4696aa550c0><p class=pgc-img-caption></p></div><p>KKT條件只是取得極值的必要條件而不是充分條件。</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e8a416d17c4d446083ea5aee20eb16df><p class=pgc-img-caption></p></div><p><strong>8.特徵值與特徵向量</strong></p><p>對於一個n階矩陣A，如果存在一個數λ和一個非0向量X，滿足：</p><p>則稱λ為矩陣A的特徵值，X為該特徵值對應的特徵向量。根據上面的定義有下面線性方程組成立：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/660477f9a0d74e6f9b9408352534b0d6><p class=pgc-img-caption></p></div><p>根據線性方程組的理論，要讓齊次方程有非0解，係數矩陣的行列式必須為0，即：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5a737402a51c4475aeeee356556aa8b5><p class=pgc-img-caption></p></div><p>上式左邊的多項式稱為矩陣的特徵多項式。矩陣的跡定義為主對角線元素之和：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/111c13ac2b684c8caa87c4bbb40d678b><p class=pgc-img-caption></p></div><p>根據韋達定理，矩陣所有特徵值的和為矩陣的跡：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/70eaf67b01fc496a86bea0c2827c440f><p class=pgc-img-caption></p></div><p>同樣可以證明，矩陣所有特徵值的積為矩陣的行列式：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1f2cc4f8e7ea4273bb94c3c61d6addb7><p class=pgc-img-caption></p></div><p>利用特徵值和特徵向量，可以將矩陣對角化，即用正交變換將矩陣化為對角陣。實對稱矩陣一定可以對角化，半正定矩陣的特徵值都大於等於0，在機器學習中，很多矩陣都滿足這些條件。特徵值和特徵向量在機器學習中的應用包括：正態貝葉斯分類器、主成分分析，流形學習，線性判別分析，譜聚類等。</p><p><strong>9.奇異值分解</strong></p><p>矩陣對角化只適用於方陣，如果不是方陣也可以進行類似的分解，這就是奇異值分解，簡稱SVD。假設A是一個<em>m x n</em>的矩陣，則存在如下分解：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/43800133d0e5476ab1aedb934f9ff849><p class=pgc-img-caption></p></div><p>其中U為<em>m x m</em>的正交矩陣，其列稱為矩陣A的左奇異向量；</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/22d57325ab5f468fbfc9abb31791c2f7><p class=pgc-img-caption></p></div><p>為<em>m x n</em>的對角矩陣，除了主對角線</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9370702a25b943339102db392e010bd9><p class=pgc-img-caption></p></div><p>以外，其他元素都是0；V為<em>n x n</em>的正交矩陣，其行稱為矩陣A的右奇異向量。U的列為AAT的特徵向量，V的列為AT A的特徵向量。</p><p><strong>10.最大似然估計</strong></p><p>有些應用中已知樣本服從的概率分佈，但是要估計分佈函數的參數</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/c734554fc4cb4abd88ef5ef9697b64f6><p class=pgc-img-caption></p></div><p>，確定這些參數常用的一種方法是最大似然估計。</p><p>最大似然估計構造一個似然函數，通過讓似然函數最大化，求解出θ。最大似然估計的直觀解釋是，尋求一組參數，使得給定的樣本集出現的概率最大。</p><p>假設樣本服從的概率密度函數為</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/ed6a2111288a4167a5ade7779e21b77e><p class=pgc-img-caption></p></div><p>，其中X為隨機變量，θ為要估計的參數。給定一組樣本x<em>i</em>,<em>i</em> =1,...,<em>l</em>，它們都服從這種分佈，並且相互獨立。最大似然估計構造如下似然函數：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e9f2bef94aa548ccbb93eea7321a8702><p class=pgc-img-caption></p></div><p>其中x<em>i</em>是已知量，這是一個關於θ的函數，我們要讓該函數的值最大化，這樣做的依據是這組樣本發生了，因此應該最大化它們發生的概率，即似然函數。這就是求解如下最優化問題：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a17a30e008194a4798956c75e3eab9ce><p class=pgc-img-caption></p></div><p>乘積求導不易處理，因此我們對該函數取對數，得到對數似然函數：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/f72e85c152594c9aa9313895918eb1a0><p class=pgc-img-caption></p></div><p>最後要求解的問題為：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/eef6a6dec27e41ec8ad0207f17381dc4><p class=pgc-img-caption></p></div><p>最大似然估計在機器學習中的典型應用包括logistic迴歸，貝葉斯分類器，隱馬爾科夫模型等。</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/63749fc640a747e59a2eef943469357e><p class=pgc-img-caption></p></div><h1><strong>基本概念</strong></h1><p><strong>1.有監督學習與無監督學習</strong></p><p>根據樣本數據是否帶有標籤值，可以將機器學習算法分成有監督學習和無監督學習兩類。有監督學習的樣本數據帶有標籤值，它從訓練樣本中學習得到一個模型，然後用這個模型對新的樣本進行預測推斷。有監督學習的典型代表是分類問題和迴歸問題。</p><p>無監督學習對沒有標籤的樣本進行分析，發現樣本集的結構或者分佈規律。無監督學習的典型代表是聚類，表示學習，和數據降維，它們處理的樣本都不帶有標籤值。</p><p><strong>2.分類問題與迴歸問題</strong></p><p>在有監督學習中，如果樣本的標籤是整數，則預測函數是一個向量到整數的映射，這稱為分類問題。如果標籤值是連續實數，則稱為迴歸問題，此時預測函數是向量到實數的映射。</p><p><strong>3.生成模型與判別模型</strong></p><p>分類算法可以分成判別模型和生成模型。給定特徵向量x與標籤值y，生成模型對聯合概率<em>p</em>(x,<em>y</em>)建模，判別模型對條件概率p(y|x)進行建模。另外，不使用概率模型的分類器也被歸類為判別模型，它直接得到預測函數而不關心樣本的概率分佈：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f868d9889eb442d1885ce00c755cc648><p class=pgc-img-caption></p></div><p>判別模型直接得到預測函數f(x)，或者直接計算概率值p(y|x)，比如SVM和logistic迴歸，softmax迴歸，判別模型只關心決策面，而不管樣本的概率分佈的密度。</p><p>生成模型計算p(x, y)或者p(x|y) ，通俗來說，生成模型假設每個類的樣本服從某種概率分佈，對這個概率分佈進行建模。</p><p>機器學習中常見的生成模型有貝葉斯分類器，高斯混合模型，隱馬爾可夫模型，受限玻爾茲曼機，生成對抗網絡等。典型的判別模型有決策樹，kNN算法，人工神經網絡，支持向量機，logistic迴歸，AdaBoost算法等。</p><p><strong>4.交叉驗證</strong></p><p>交叉驗證（cross validation）是一種統計準確率的技術。<em>k</em>折交叉驗證將樣本隨機、均勻的分成<em>k</em>份，輪流用其中的<em>k-1</em>份訓練模型，1份用於測試模型的準確率，用<em>k</em>個準確率的均值作為最終的準確率。</p><p><strong>5.過擬合與欠擬合</strong></p><p>欠擬合也稱為欠學習，直觀表現是訓練得到的模型在訓練集上表現差，沒有學到數據的規律。引起欠擬合的原因有模型本身過於簡單，例如數據本身是非線性的但使用了線性模型；特徵數太少無法正確的建立映射關係。</p><p>過擬合也稱為過學習，直觀表現是在訓練集上表現好，但在測試集上表現不好，推廣泛化性能差。過擬合產生的根本原因是訓練數據包含抽樣誤差，在訓練時模型將抽樣誤差也進行了擬合。所謂抽樣誤差，是指抽樣得到的樣本集和整體數據集之間的偏差。引起過擬合的可能原因有：</p><p>模型本身過於複雜，擬合了訓練樣本集中的噪聲。此時需要選用更簡單的模型，或者對模型進行裁剪。訓練樣本太少或者缺乏代表性。此時需要增加樣本數，或者增加樣本的多樣性。訓練樣本噪聲的干擾，導致模型擬合了這些噪聲，這時需要剔除噪聲數據或者改用對噪聲不敏感的模型。</p><p><strong>6.偏差與方差分解</strong></p><p>模型的泛化誤差可以分解成偏差和方差。偏差是模型本身導致的誤差，即錯誤的模型假設所導致的誤差，它是模型的預測值的數學期望和真實值之間的差距。</p><p>方差是由於對訓練樣本集的小波動敏感而導致的誤差。它可以理解為模型預測值的變化範圍，即模型預測值的波動程度。</p><p>模型的總體誤差可以分解為偏差的平方與方差之和：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ca50f096203f47d09129dfa90a5e2a9d><p class=pgc-img-caption></p></div><p>如果模型過於簡單，一般會有大的偏差和小的方差；反之如果模型複雜則會有大的方差但偏差很小。</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9c9598af7b794a35af24cb91fcddc152><p class=pgc-img-caption></p></div><p><strong>7.正則化</strong></p><p>為了防止過擬合，可以為損失函數加上一個懲罰項，對複雜的模型進行懲罰，強制讓模型的參數值儘可能小以使得模型更簡單，加入懲罰項之後損失函數為：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d788a2ec0a934628a2a83d374b420f49><p class=pgc-img-caption></p></div><p>正則化被廣泛應用於各種機器學習算法，如嶺迴歸，LASSO迴歸，logistic迴歸，神經網絡等。除了直接加上正則化項之外，還有其他強制讓模型變簡單的方法，如決策樹的剪枝算法，神經網絡訓練中的dropout技術，提前終止技術等。</p><p><strong>8.維數災難</strong></p><p>為了提高算法的精度，會使用越來越多的特徵。當特徵向量維數不高時，增加特徵確實可以帶來精度上的提升；但是當特徵向量的維數增加到一定值之後，繼續增加特徵反而會導致精度的下降，這一問題稱為維數災難。</p><h1><strong>貝葉斯分類器</strong></h1><p>貝葉斯分類器將樣本判定為後驗概率最大的類，它直接用貝葉斯公式解決分類問題。假設樣本的特徵向量為x，類別標籤為y，根據貝葉斯公式，樣本屬於每個類的條件概率（後驗概率）為：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/27c073c43d0a41e2b387693627f5569d><p class=pgc-img-caption></p></div><p>分母p(x)對所有類都是相同的，分類的規則是將樣本歸到後驗概率最大的那個類，不需要計算準確的概率值，只需要知道屬於哪個類的概率最大即可，這樣可以忽略掉分母。分類器的判別函數為：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c3a82308ffd34578adbf037f5b1f1ae3><p class=pgc-img-caption></p></div><p>在實現貝葉斯分類器時，需要知道每個類的條件概率分佈p(x|y)即先驗概率。一般假設樣本服從正態分佈。訓練時確定先驗概率分佈的參數，一般用最大似然估計，即最大化對數似然函數。</p><p>如果假設特徵向量的各個分量之間相互獨立，則稱為樸素貝葉斯分類器，此時的分類判別函數為：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2aa0d45188cf457eb4eac88b03f24bcb><p class=pgc-img-caption></p></div><p>實現時可以分為特徵分量是離散變量和連續變量兩種情況。貝葉斯分分類器是一種生成模型，可以處理多分類問題，是一種非線性模型。</p><h1><strong>決策樹</strong></h1><p>決策樹是一種基於規則的方法，它用一組嵌套的規則進行預測。在樹的每個決策節點處，根據判斷結果進入一個分支，反覆執行這種操作直到到達葉子節點，得到預測結果。這些規則通過訓練得到，而不是人工制定的。</p><p>決策樹既可以用於分類問題，也可以用於迴歸問題。分類樹的映射函數是多維空間的分段線性劃分，用平行於各座標軸的超平面對空間進行切分；迴歸樹的映射函數是分段常數函數。決策樹是分段線性函數而不是線性函數。只要劃分的足夠細，分段常數函數可以逼近閉區間上任意函數到任意指定精度，因此決策樹在理論上可以對任意複雜度的數據進行擬合。對於分類問題，如果決策樹深度夠大，它可以將訓練樣本集的所有樣本正確分類。</p><p>決策樹的訓練算法是一個遞歸的過程，首先創建根節點，然後遞歸的建立左子樹和右子樹。如果練樣本集為D，訓練算法的流程為：</p><p>1.用樣本集D建立根節點，找到一個判定規則，將樣本集分裂成D1和D2兩部分，同時為根節點設置判定規則。</p><p>2.用樣本集D1遞歸建立左子樹。</p><p>3.用樣本集D2遞歸建立右子樹。</p><p>4.如果不能再進行分裂，則把節點標記為葉子節點，同時為它賦值。</p><p>對於分類樹，如果採用Gini係數作為度量準則，決策樹在訓練時尋找最佳分裂的依據為讓Gini不純度最小化，這等價於讓下面的值最大化：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4ff0e4932d824ac79bca5c92c63cc8bb><p class=pgc-img-caption></p></div><p>尋找最佳分裂時需要計算用每個閾值對樣本集進行分裂後的純度值，尋找該值最大時對應的分裂，它就是最佳分裂。如果是數值型特徵，對於每個特徵將l個訓練樣本按照該特徵的值從小到大排序，假設排序後的值為：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c065b583309643a0a414b36541db2e27><p class=pgc-img-caption></p></div><p>接下來從<em>x</em>1開始，依次用每個<em>xi</em>作為閾值，將樣本分成左右兩部分，計算上面的純度值，該值最大的那個分裂閾值就是此特徵的最佳分裂閾值。在計算出每個特徵的最佳分裂閾值和上面的純度值後，比較所有這些分裂的純度值大小，該值最大的分裂為所有特徵的最佳分裂。</p><p>決策樹可以處理屬性缺失問題，採用的方法是使用替代分裂規則。為了防止過擬合，可以對樹進行剪枝，讓模型變得更簡單。</p><p>決策樹是一種判別模型，既支持分類問題，也支持迴歸問題，是一種非線性模型，它支持多分類問題。</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ef3f167c774e45749c2dc842c4d81ead><p class=pgc-img-caption></p></div><h1><strong>隨機森林</strong></h1><p>隨機森林是一種集成學習算法，是Bagging算法的具體實現。集成學習是機器學習中的一種思想，而不是某一具體算法，它通過多個模型的組合形成一個精度更高的模型，參與組合的模型稱為弱學習器。在預測時使用這些弱學習器模型聯合進行預測，訓練時需要依次訓練出這些弱學習器。</p><p>隨機森林用有放回抽樣（Bootstrap抽樣）構成出的樣本集訓練多棵決策樹，訓練決策樹的每個節點時只使用了隨機抽樣的部分特徵。預測時，對於分類問題，一個測試樣本會送到每一棵決策樹中進行預測，然後投票，得票最多的類為最終分類結果。對於迴歸問題，隨機森林的預測輸出是所有決策樹輸出的均值。</p><p>假設有n個訓練樣本。訓練每一棵樹時，從樣本集中有放回的抽取n個樣本，每個樣本可能會被抽中多次，也可能一次都沒抽中。如果樣本量很大，在整個抽樣過程中每個樣本有0.368的概率不被抽中。由於樣本集中各個樣本是相互獨立的，在整個抽樣中所有樣本大約有36.8%沒有被抽中。這部分樣本稱為包外（Out Of Bag，簡稱OOB）數據。</p><p>用這個抽樣的樣本集訓練一棵決策樹，訓練時，每次尋找最佳分裂時，還要對特徵向量的分量採樣，即只考慮部分特徵分量。由於使用了隨機抽樣，隨機森林泛化性能一般比較好，可以有效的降低模型的方差。</p><p>如果想更詳細的瞭解隨機森林的原理，請閱讀SIGAI之前的公眾號文章“隨機森林概述”。隨機森林是一種判別模型，既支持分類問題，也支持迴歸問題，並且支持多分類問題，這是一種非線性模型。</p><h1><strong>AdaBoost算法</strong></h1><p>AdaBoost算法也是一種集成學習算法，用於二分類問題，是Boosting算法的一種實現。它用多個弱分類器的線性組合來預測，訓練時重點關注錯分的樣本，準確率高的弱分類器權重大。AdaBoost算法的全稱是自適應，它用弱分類器的線性組合來構造強分類器。弱分類器的性能不用太好，僅比隨機猜測強，依靠它們可以構造出一個非常準確的強分類器。強分類器的計算公式為：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ee841159e0d749cea93ae9dd58f73c09><p class=pgc-img-caption></p></div><p>其中x是輸入向量，F(x)是強分類器，ft(x)是弱分類器，at是弱分類器的權重，T為弱分類器的數量，弱分類器、的輸出值為+1或-1，分別對應正樣本和負樣本。分類時的判定規則為：</p><p>、、、、、、、、、、、、、、、、、、、、、、、、、、、、、、、、、、、、、</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1cfe5d02146b4f7997d23484bf23d3cd><p class=pgc-img-caption></p></div><p>強分類器的輸出值也為+1或-1，同樣對應於正樣本和負樣本。</p><p>訓練時，依次訓練每一個若分類器，並得到它們的權重值。訓練樣本帶有權重值，初始時所有樣本的權重相等，在訓練過程中，被前面的弱分類器錯分的樣本會加大權重，反之會減小權重，這樣接下來的弱分類器會更加關注這些難分的樣本。弱分類器的權重值根據它的準確率構造，精度越高的弱分類器權重越大。</p><p>給定l個訓練樣本(x<em>i</em>,<em>yi</em> )，其中x<em>i</em>是特徵向量，<em>yi</em>為類別標籤，其值為+1或-1。訓練算法的流程為：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d413301cb6574219a203d351b3e800d7><p class=pgc-img-caption></p></div><p>根據計算公式，錯誤率低的弱分類器權重大，它是準確率的增函數。AdaBoost算法在訓練樣本集上的錯誤率會隨著弱分類器數量的增加而指數級降低。它能有效的降低模型的偏差。</p><p>AdaBoost算法從廣義加法模型導出，訓練時求解的是指數損失函數的極小值：</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d79847ded2ec441785a2e91a2ef6d90c><p class=pgc-img-caption></p></div><p>求解時採用了分階段優化，先得到弱分類器，然後確定弱分類器的權重值，這就是弱分類器，弱分類器權重的來歷。除了離散型AdaBoost之外，從廣義加法模型還可以導出其他幾種AdaBoost算法，分別是實數型AdaBoost，Gentle型AdaBoost，Logit型AdaBoost，它們使用了不同的損失函數和最優化算法。</p><p>標準的AdaBoost算法是一種判別模型，只能支持二分類問題。它的改進型可以處理多分類問題。</p><div class=pgc-img><img alt=機器學習與深度學習核心知識點總結(一) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9be58fdb574f44b09a251a2c6cbda089><p class=pgc-img-caption></p></div><p><strong>搜索進入我們的小程序，解鎖更多新鮮資訊和優質內容，還有很多免費試聽課程，不要錯過喲！</strong></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>學習</a></li><li><a>機器</a></li><li><a>知識點</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3873d795.html alt=金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/694a9289cde541dca807f9a30d291d0d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3873d795.html title=金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例>金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>