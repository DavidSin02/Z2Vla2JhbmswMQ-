<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>聚類算法的評估指標 | 极客快訊</title><meta property="og:title" content="聚類算法的評估指標 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/974fc865eb49412c93803c49542098c5"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/151340c.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/151340c.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/151340c.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/151340c.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/151340c.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/151340c.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/151340c.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/151340c.html><meta property="article:published_time" content="2020-10-29T20:55:19+08:00"><meta property="article:modified_time" content="2020-10-29T20:55:19+08:00"><meta name=Keywords content><meta name=description content="聚類算法的評估指標"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/151340c.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>聚類算法的評估指標</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><p>在學習聚類算法得時候並沒有涉及到評估指標，主要原因是聚類算法屬於非監督學習，並不像分類算法那樣可以使用訓練集或測試集中得數據計算準確率、召回率等。那麼如何評估聚類算法得好壞呢？好的聚類算法,一般要求類簇具有：</p><ul><li>高的類內 (intra-cluster) 相似度</li><li>低的類間 (inter-cluster) 相似度</li></ul><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/974fc865eb49412c93803c49542098c5><p class=pgc-img-caption></p></div><p>對於聚類算法大致可分為 2類度量標準：</p><ul><li>內部評估的方法：通過一個單一的量化得分來評估算法好壞；該類型的方法</li><li>外部評估的方法：通過將聚類結果與已經有“ground truth”分類進行對比。要麼通過人類進行手動評估，要麼通過一些指標在特定的應用場景中進行聚類用法的評估。不過該方法是有問題的，如果真的有了label，那麼還需要聚類幹嘛，而且實際應用中，往往都沒label；另一方面，這些label只反映了數據集的一個可能的劃分方法，它並不能告訴你存在一個不同的更好的聚類算法。</li></ul><h2 class=pgc-h-arrow-right>內部評價指標</h2><p>當一個聚類結果是基於數據聚類自身進行評估的，這一類叫做內部評估方法。如果某個聚類算法聚類的結果是類間相似性低，類內相似性高，那麼內部評估方法會給予較高的分數評價。不過內部評價方法的缺點是：</p><ul><li>那些高分的算法不一定可以適用於高效的信息檢索應用場景；</li><li>這些評估方法對某些算法有傾向性，如k-means聚類都是基於點之間的距離進行優化的，而那些基於距離的內部評估方法就會過度的讚譽這些生成的聚類結果。</li></ul><p>這些內部評估方法可以基於特定場景判定一個算法要優於另一個，不過這並不表示前一個算法得到的結果比後一個結果更有意義。這裡的意義是假設這種結構事實上存在於數據集中的，如果一個數據集包含了完全不同的數據結構，或者採用的評價方法完全和算法不搭，比如k-means只能用於凸集數據集上，許多評估指標也是預先假設凸集數據集。在一個非凸數據集上不論是使用k-means還是使用假設凸集的評價方法，都是徒勞的。</p><h3 class=pgc-h-arrow-right>SSE(和方差)</h3><p>該統計參數計算的是擬合數據和原始數據對應點的誤差的平方和，計算公式如下：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/695e23b41e3646708be802e7be18edb1><p class=pgc-img-caption></p></div><p>SSE越接近於0，說明模型選擇和擬合更好，數據預測也越成功。</p><pre><code>#斷崖碎石圖選取最優K值import pandas as pd  from sklearn.cluster import KMeans  import matplotlib.pyplot as plt  '利用SSE選擇k'  SSE = []  # 存放每次結果的誤差平方和  for k in range(1,9):      estimator = KMeans(n_clusters=k)  # 構造聚類器      estimator.fit(df[['calories','sodium','alcohol','cost']])      SSE.append(estimator.inertia_)  N = range(1,9)  plt.xlabel('k')  plt.ylabel('SSE')  plt.plot(N,SSE,'o-')  plt.show()</code></pre><h3 class=pgc-h-arrow-right>輪廓係數 Silhouette Coefficient</h3><p>輪廓係數適用於實際類別信息未知的情況。對於單個樣本，設a是與它同類別中其他樣本的平均距離，b是與它距離最近不同類別中樣本的平均距離，其輪廓係數為：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7a80e57f2aaa47ec875944301e1b3078><p class=pgc-img-caption></p></div><p>對於一個樣本集合，它的輪廓係數是所有樣本輪廓係數的平均值。輪廓係數的取值範圍是[-1,1]，同類別樣本距離越相近不同類別樣本距離越遠，分數越高。缺點：不適合基高密度的聚類算法DBSCAN。</p><pre><code>from sklearn import metricsfrom sklearn.metrics import pairwise_distancesfrom sklearn import datasetsdataset = datasets.load_iris()X = dataset.datay = dataset.target import numpy as npfrom sklearn.cluster import KMeanskmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)labels = kmeans_model.labels_metrics.silhouette_score(X, labels, metric='euclidean')</code></pre><h3 class=pgc-h-arrow-right>Calinski-Harabaz Index</h3><p>在真實的分群label不知道的情況下，Calinski-Harabasz可以作為評估模型的一個指標。Calinski-Harabasz指標通過計算類中各點與類中心的距離平方和來度量類內的緊密度，通過計算各類中心點與數據集中心點距離平方和來度量數據集的分離度，CH指標由分離度與緊密度的比值得到。從而，CH越大代表著類自身越緊密，類與類之間越分散，即更優的聚類結果。</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5649351531e64a8d9134c12e5859a785><p class=pgc-img-caption></p></div><p>其中m為訓練樣本數，k是類別個數，是類別之間協方差矩陣，是類別內部數據協方差矩陣，為矩陣的跡。也就是說，類別內部數據的協方差越小越好，類別之間的協方差越大越好，這樣的Calinski-Harabasz分數會高。同時，數值越小可以理解為：組間協方差很小，組與組之間界限不明顯。</p><p>優點</p><ul><li>當 cluster （簇）密集且分離較好時，分數更高，這與一個標準的 cluster（簇）有關。</li><li>得分計算很快與輪廓係數的對比，最大的優勢：快！相差幾百倍！毫秒級。</li></ul><p>缺點</p><ul><li>凸的簇的 Calinski-Harabaz index（Calinski-Harabaz 指數）通常高於其他類型的 cluster（簇），例如通過 DBSCAN 獲得的基於密度的 cluster（簇）。所以不適合基於密度的聚類算法，DBSCAN。</li></ul><pre><code>import numpy as npfrom sklearn.cluster import KMeanskmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)labels = kmeans_model.labels_print(metrics.calinski_harabaz_score(X, labels))</code></pre><h3 class=pgc-h-arrow-right>Compactness(緊密性)(CP)</h3><p>CP計算每一個類各點到聚類中心的平均距離CP越低意味著類內聚類距離越近。著名的 K-Means 聚類算法就是基於此思想提出的。缺點：沒有考慮類間效果。</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3f311c147c6f4e86b8f095f0d3df484c><p class=pgc-img-caption></p></div><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/76897601a2a74a62a31aeaf345517d09><p class=pgc-img-caption></p></div><h3 class=pgc-h-arrow-right>Separation(間隔性)(SP)</h3><p>SP計算各聚類中心兩兩之間平均距離，SP越高意味類間聚類距離越遠。缺點：沒有考慮類內效果。</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/4b89f2933f474ec3920ad257ef06e0c9><p class=pgc-img-caption></p></div><h3 class=pgc-h-arrow-right>Davies-Bouldin Index(戴維森堡丁指數)(分類適確性指標)(DB)(DBI)</h3><p>DB計算任意兩類別的類內距離平均距離(CP)之和除以兩聚類中心距離求最大值。DB越小意味著類內距離越小同時類間距離越大。該指標的計算公式：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f2e0ac18afd546f0ae9b8caae1a7efa6><p class=pgc-img-caption></p></div><p>其中n是類別個數，是第i個類別的中心，是類別i中所有的點到中心的平均距離；</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/29516586eb3b45219fc2f817792ff977><p class=pgc-img-caption></p></div><p>中心點和之間的距離。算法生成的聚類結果越是朝著類內距離最小（類內相似性最大）和類間距離最大（類間相似性最小）變化，那麼Davies-Bouldin指數就會越小。缺點：因使用歐式距離所以對於環狀分佈聚類評測很差。</p><pre><code>from sklearn import datasets from sklearn.cluster import KMeans from sklearn.metrics import davies_bouldin_score from sklearn.datasets.samples_generator import make_blobs   # loading the dataset X, y_true = make_blobs(n_samples=300, centers=4,                         cluster_std=0.50, random_state=0)   # K-Means kmeans = KMeans(n_clusters=4, random_state=1).fit(X)   # we store the cluster labels labels = kmeans.labels_   print(davies_bouldin_score(X, labels))</code></pre><h3 class=pgc-h-arrow-right>Dunn Validity Index (鄧恩指數)(DVI)</h3><p>DVI計算任意兩個簇元素的最短距離(類間)除以任意簇中的最大距離(類內)。DVI越大意味著類間距離越大同時類內距離越小。</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/e162ac534ab8457caf3fe440e4020b35><p class=pgc-img-caption></p></div><p>其中</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/aca308de3896466f87ddeedd8a1224f9><p class=pgc-img-caption></p></div><p>表示類別,之間的距離；</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dae00df915084d0b90da00fccf1f5896><p class=pgc-img-caption></p></div><p>表示類別內部的類內距離：</p><ul><li>類間距離 可以是任意的距離測度，例如兩個類別的中心點的距離；</li><li>類內距離 可以以不同的方法去測量，例如類別kk中任意兩點之間距離的最大值。</li></ul><p>因為內部評估方法是搜尋類內相似最大，類間相似最小，所以算法生成的聚類結果的Dunn指數越高，那麼該算法就越好。缺點：對離散點的聚類測評很高、對環狀分佈測評效果差。</p><pre><code>import pandas as pd from sklearn import datasets from jqmcvi import base  # loading the dataset X = datasets.load_iris() df = pd.DataFrame(X.data)  # K-Means from sklearn import cluster k_means = cluster.KMeans(n_clusters=3) k_means.fit(df) #K-means training y_pred = k_means.predict(df)  # We store the K-means results in a dataframe pred = pd.DataFrame(y_pred) pred.columns = ['Type']  # we merge this dataframe with df prediction = pd.concat([df, pred], axis = 1)  # We store the clusters clus0 = prediction.loc[prediction.Species == 0] clus1 = prediction.loc[prediction.Species == 1] clus2 = prediction.loc[prediction.Species == 2] cluster_list = [clus0.values, clus1.values, clus2.values]  print(base.dunn(cluster_list))</code></pre><h2 class=pgc-h-arrow-right>外部評價指標</h2><p>在外部評估方法中，聚類結果是通過使用沒被用來做訓練集的數據進行評估。例如已知樣本點的類別信息和一些外部的基準。這些基準包含了一些預先分類好的數據，比如由人基於某些場景先生成一些帶label的數據，因此這些基準可以看成是金標準。這些評估方法是為了測量聚類結果與提供的基準數據之間的相似性。然而這種方法也被質疑不適用真實數據。</p><h3 class=pgc-h-arrow-right>純度（Purity）</h3><p>純度（Purity）是一種簡單而透明的評估手段，為了計算純度（Purity），我們把每個簇中最多的類作為這個簇所代表的類，然後計算正確分配的類的數量，然後除以N。形式化表達如下：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/aac77419b5c146fa80014a56b059f7a9><p class=pgc-img-caption></p></div><p>其中：</p><ul><li>是聚類的集合，表示第k個聚類的集合。</li><li>是文檔集合，表示第J個文檔。</li><li>表示文檔總數。</li></ul><p>上述過程即給每個聚類簇分配一個類別,且這個類別的樣本在該簇中出現的次數最多，然後計算所有 K 個聚類簇的這個次數之和再歸一化即為最終值。Purity值在0～1之間 ,越接近1表示聚類結果越好。</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/987802f704084001923b067d465dd110><p class=pgc-img-caption></p></div><p>如圖認為x代表一類文檔，o代表一類文檔，方框代表一類文檔。如上圖的purity = ( 3+ 4 + 5) / 17 = 0.71，其中第一類正確的有5個，第二個4個，第三個3個，總文檔數17。</p><p>當簇的數量很多的時候，容易達到較高的純度——特別是，如果每個文檔都被分到獨立的一個簇中，那麼計算得到的純度就會是1。因此，不能簡單用純度來衡量聚類質量與聚類數量之間的關係。另外Purity無法用於權衡聚類質量與簇個數之間的關係。</p><pre><code>def purity(result, label):    # 計算純度     total_num = len(label)    cluster_counter = collections.Counter(result)    original_counter = collections.Counter(label)     t = []    for k in cluster_counter:        p_k = []        for j in original_counter:            count = 0            for i in range(len(result)):                if result[i] == k and label[i] == j: # 求交集                    count += 1            p_k.append(count)        temp_t = max(p_k)        t.append(temp_t)        return sum(t)/total_num</code></pre><h3 class=pgc-h-arrow-right>標準化互信息（NMI）</h3><p>互信息（Normalized Mutual Information）是用來衡量兩個數據分佈的吻合程度。也是一有用的信息度量，它是指兩個事件集合之間的相關性。互信息越大，詞條和類別的相關程度也越大。NMI (Normalized Mutual Information) 即歸一化互信息：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/8e23f38df6e64d4ebe79dac805a7dbff><p class=pgc-img-caption></p></div><p>其中,表示互信息(Mutual Information),為熵，當 log 取 2 為底時，單位為 bit，取 e 為底時單位為 nat。</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/85915b1408da44468872826d22ee64d2><p class=pgc-img-caption></p></div><p>其中,</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ac97965cbf904783913f091dd97fcbcc><p class=pgc-img-caption></p></div><p>可以分別看作樣本 (document) 屬於聚類簇, 屬於類別, 同時屬於的概率。第二個等價式子則是由概率的極大似然估計推導而來。</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/000ad65d90614002834e617910532598><p class=pgc-img-caption></p></div><p>互信息</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/392f18bc892d4665ac917563d53d4f2b><p class=pgc-img-caption></p></div><p>表示給定類簇信息的前提條件下,類別信息的增加量，或者說其不確定度的減少量。直觀地，互信息還可以寫出如下形式：</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9591eedf93c04456a929703ea9a33a46><p class=pgc-img-caption></p></div><p>互信息的最小值為 0, 當類簇相對於類別只是隨機的, 也就是說兩者獨立的情況下,對於未帶來任何有用的信息.如果得到的與關係越密切, 那麼</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/392f18bc892d4665ac917563d53d4f2b><p class=pgc-img-caption></p></div><p>值越大。如果完整重現了, 此時互信息最大：</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0897c1c9a5c74d298fa7ce8eab92c2b8><p class=pgc-img-caption></p></div><p>當K=N時,即類簇數和樣本個數相等，MI 也能達到最大值。所以 MI 也存在和純度類似的問題,即它並不對簇數目較大的聚類結果進行懲罰,因此也不能在其他條件一樣的情況下,對簇數目越小越好的這種期望進行形式化。NMI 則可以解決上述問題,因為熵會隨著簇的數目的增長而增大。當K=N時,</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4e1f9aa5e03649bf89e2db9a033b237f><p class=pgc-img-caption></p></div><p>會達到其最大值</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b71cf5f5031f495397c42a83b5c244d4><p class=pgc-img-caption></p></div><p>, 此時就能保證 NMI 的值較低。之所以採用</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5ca039ced56642ecb20e3023dc16035a><p class=pgc-img-caption></p></div><p>作為分母是因為它是</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a6370f13fb7b412b85951247a65da5bd><p class=pgc-img-caption></p></div><p>的緊上界, 因此可以保證</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3a1af69075914fdab8272e7c03ef82cf><p class=pgc-img-caption></p></div><p>。</p><p>示例：</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2f8299166da74bd89fee18f7410e975b><p class=pgc-img-caption></p></div><p>gnd 是 ground truth 的意思，grps 表示聚類後的 groups. 問題：計算序列 gnd 和 grps 的 NMI.</p><p>先計算聯合概率分佈</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c565d4d56f5f49b38ac37f0fb50eb587><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fe39cbb1acfe44a0bc21f1ef938027aa><p class=pgc-img-caption></p></div><h4 class=pgc-h-arrow-right>計算邊際分佈</h4><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/eebdc776756d45dba9365086a6fb2aa1><p class=pgc-img-caption></p></div><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/da3182c2fbca408d830a56c2e3b75673><p class=pgc-img-caption></p></div><h4 class=pgc-h-arrow-right>計算熵和互信息</h4><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1f22ac42f16542089d67a5ed510742dd><p class=pgc-img-caption></p></div><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a7cd7ead58ba43059a14de9de3af41af><p class=pgc-img-caption></p></div><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/02d7d8a0744b45b2bb36b6729a3f9b34><p class=pgc-img-caption></p></div><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/bf820c60ae67452995f93c9666b91952><p class=pgc-img-caption></p></div><h4 class=pgc-h-arrow-right>計算 NMI</h4><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3b6eedbe17b345d7aa3ef27b6196e946><p class=pgc-img-caption></p></div><p>代碼實現：</p><pre><code>def NMI(result, label):    # 標準化互信息     total_num = len(label)    cluster_counter = collections.Counter(result)    original_counter = collections.Counter(label)        # 計算互信息量    MI = 0    eps = 1.4e-45 # 取一個很小的值來避免log 0        for k in cluster_counter:        for j in original_counter:            count = 0            for i in range(len(result)):                if result[i] == k and label[i] == j:                    count += 1            p_k = 1.0*cluster_counter[k] / total_num            p_j = 1.0*original_counter[j] / total_num            p_kj = 1.0*count / total_num            MI += p_kj * math.log(p_kj /(p_k * p_j) + eps, 2)        # 標準化互信息量    H_k = 0    for k in cluster_counter:        H_k -= (1.0*cluster_counter[k] / total_num) * math.log(1.0*cluster_counter[k] / total_num+eps, 2)    H_j = 0    for j in original_counter:        H_j -= (1.0*original_counter[j] / total_num) * math.log(1.0*original_counter[j] / total_num+eps, 2)            return 2.0 * MI / (H_k + H_j)</code></pre><p>sklearn中自帶的方法：</p><pre><code>from sklearn.metrics.cluster import normalized_mutual_info_scoreprint(normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1]))</code></pre><h3 class=pgc-h-arrow-right>調整互信息AMI（ Adjusted mutual information）</h3><p>已知聚類標籤與真實標籤，互信息（mutual information）能夠測度兩種標籤排列之間的相關性，同時忽略標籤中的排列。有兩種不同版本的互信息以供選擇，一種是Normalized Mutual Information（NMI）,一種是Adjusted Mutual Information（AMI）。</p><p>假設U與V是對N個樣本標籤的分配情況，則兩種分佈的熵（熵表示的是不確定程度）分別為：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9e6318f8f849458cad909ca99b8b76a1><p class=pgc-img-caption></p></div><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/dc0a7adb051342bdb37f62785c8c0890><p class=pgc-img-caption></p></div><p>其中：</p><ul><li>是從U中隨機選取的對象到類的概率</li><li>從V中隨機選取的對象到類的概率</li></ul><p>U與V之間的互信息（MI）定義為：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a97ec086ea8d49c0809043462e1061b7><p class=pgc-img-caption></p></div><p>其中</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8a08d4705b6a46ef8c857804bf39bf66><p class=pgc-img-caption></p></div><p>是隨機選擇的對象落入兩個類的概率和。</p><p>調整互信息（Adjusted mutual information）定義為：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b582a68d940c483a93c74c8acd0b6d32><p class=pgc-img-caption></p></div><p>MI的期望可以用以下公式來計算。在這個方程式中，</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e1c7219bc212419cac98106dd9c34fbc><p class=pgc-img-caption></p></div><p>為元素的數量，</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fc84b7cb13b045e6b314550fe5749e65><p class=pgc-img-caption></p></div><p>為元素的數量：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/22ef359b090f4aff8faf58e1b16f9b76><p class=pgc-img-caption></p></div><p>利用基於互信息的方法來衡量聚類效果需要實際類別信息，MI與NMI取值範圍為[0,1]，AMI取值範圍為[-1,1]，它們都是值越大意味著聚類結果與真實情況越吻合。</p><p>優點</p><ul><li>隨機（統一）標籤分配的AMI評分接近0）</li><li>有界範圍 [0, 1]: 接近 0 的值表示兩個主要獨立的標籤分配，而接近 1 的值表示重要的一致性。此外，正好 0 的值表示 purely（純粹） 獨立標籤分配，正好為 1 的 AMI 表示兩個標籤分配相等（有或者沒有 permutation）。</li><li>對簇的結構沒有作出任何假設: 可以用於比較聚類算法</li></ul><p>缺點：</p><ul><li>與 inertia 相反， MI-based measures 需要了解 ground truth classes，而在實踐中幾乎不可用，或者需要人工標註或手動分配（如在監督學習環境中）。然而，基於 MI-based measures （基於 MI 的測量方式）也可用於純無人監控的設置，作為可用於聚類模型選擇的 Consensus Index （共識索引）的構建塊。</li><li>NMI 和 MI 沒有調整機會。</li></ul><pre><code>from sklearn import metricslabels_true = [0, 0, 0, 1, 1, 1]labels_pred = [0, 0, 1, 1, 2, 2] print(metrics.adjusted_mutual_info_score(labels_true, labels_pred))</code></pre><h3 class=pgc-h-arrow-right>Rand index蘭德指數</h3><p>蘭德指數 (Rand index, RI), 將聚類看成是一系列的決策過程,即對文檔集上所有N(N-1)/2個文檔 (documents) 對進行決策。當且僅當兩篇文檔相似時,我們將它們歸入同一簇中。</p><p>Positive:</p><ul><li>TP 將兩篇相似文檔歸入一個簇 (同 – 同)</li><li>TN 將兩篇不相似的文檔歸入不同的簇 (不同 – 不同)</li></ul><p>Negative:</p><ul><li>FP 將兩篇不相似的文檔歸入同一簇 (不同 – 同)</li><li>FN 將兩篇相似的文檔歸入不同簇 (同- 不同) (worse)</li></ul><p>RI 則是計算「正確決策」的比率(精確率, accuracy)：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/34dbc26b0e184315af2b20e9085cf47b><p class=pgc-img-caption></p></div><p>RI取值範圍為[0,1]，值越大意味著聚類結果與真實情況越吻合。</p><pre><code>def contingency_table(result, label):        total_num = len(label)        TP = TN = FP = FN = 0    for i in range(total_num):        for j in range(i + 1, total_num):            if label[i] == label[j] and result[i] == result[j]:                TP += 1            elif label[i] != label[j] and result[i] != result[j]:                TN += 1            elif label[i] != label[j] and result[i] == result[j]:                FP += 1            elif label[i] == label[j] and result[i] != result[j]:                FN += 1    return (TP, TN, FP, FN) def rand_index(result, label):    TP, TN, FP, FN = contingency_table(result, label)    return 1.0*(TP + TN)/(TP + FP + FN + TN)</code></pre><h3 class=pgc-h-arrow-right>調整蘭德係數 （Adjusted Rand index）</h3><p>對於隨機結果，RI並不能保證分數接近零。為了實現“在聚類結果隨機產生的情況下，指標應該接近零”，調整蘭德係數（Adjusted rand index）被提出，它具有更高的區分度：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/80a7a7e394744693af625b0c47dec4a8><p class=pgc-img-caption></p></div><p>ARI取值範圍為[-1,1]，值越大意味著聚類結果與真實情況越吻合。從廣義的角度來講，ARI衡量的是兩個數據分佈的吻合程度。</p><p>優點：</p><ul><li>對任意數量的聚類中心和樣本數，隨機聚類的ARI都非常接近於0</li><li>取值在［－1，1］之間，負數代表結果不好，越接近於1越好</li><li>對簇的結構不需作出任何假設：可以用於比較聚類算法。</li></ul><p>缺點：</p><ul><li>與 inertia 相反，ARI 需要 ground truth classes 的相關知識，ARI需要真實標籤，而在實踐中幾乎不可用，或者需要人工標註者手動分配（如在監督學習環境中）。然而，ARI 還可以在 purely unsupervised setting （純粹無監督的設置中）作為可用於 聚類模型選擇（TODO）的共識索引的構建塊。</li></ul><pre><code>from sklearn import metricslabels_true = [0, 0, 0, 1, 1, 1]labels_pred = [0, 0, 1, 1, 2, 2] print(metrics.adjusted_rand_score(labels_true, labels_pred)) </code></pre><h3 class=pgc-h-arrow-right>F值方法</h3><p>這是基於上述RI方法衍生出的一個方法，我們可以 FN 罰更多,通過取</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2c636f91eecb4e318bc8e9169ae25583><p class=pgc-img-caption></p></div><p>中的大於 1, 此時實際上也相當於賦予召回率更大的權重：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/c942314f8ee4417998b76c83704a12eb><p class=pgc-img-caption></p></div><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/85880e86df6b4076b400d35d59771536><p class=pgc-img-caption></p></div><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/5d0a382231b34ac79ba79132c780e8df><p class=pgc-img-caption></p></div><p>RI方法有個特點就是把準確率和召回率看得同等重要，事實上有時候我們可能需要某一特性更多一點，這時候就適合F值方法。</p><pre><code>def precision(result, label):    TP, TN, FP, FN = contingency_table(result, label)    return 1.0*TP/(TP + FP) def recall(result, label):    TP, TN, FP, FN = contingency_table(result, label)    return 1.0*TP/(TP + FN) def F_measure(result, label, beta=1):    prec = precision(result, label)    r = recall(result, label)    return (beta*beta + 1) * prec * r/(beta*beta * prec + r)</code></pre><h3 class=pgc-h-arrow-right>Fowlkes-Mallows scores</h3><p>Fowlkes-Mallows Scores（FMI） FMI是成對的precision（精度）和recall（召回）的幾何平均數。取值範圍為 [0,1]，越接近1越好。定義為：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/019fd12cc2d140df8b44897144e2e640><p class=pgc-img-caption></p></div><p>代碼實現：</p><pre><code>from sklearn import metricslabels_true = [0, 0, 0, 1, 1, 1]labels_pred = [0, 0, 1, 1, 2, 2] print(metrics.fowlkes_mallows_score(labels_true, labels_pred))</code></pre><h3 class=pgc-h-arrow-right>調和平均V-measure</h3><p>說V-measure之前要先介紹兩個指標：</p><ul><li>同質性（homogeneity）：每個群集只包含單個類的成員。</li><li>完整性（completeness）：給定類的所有成員都分配給同一個群集。</li></ul><p>同質性和完整性分數基於以下公式得出：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9a7201a4ae7345d1b0854cb0258b2385><p class=pgc-img-caption></p></div><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0176cfcebbd4435aa589446937bb129e><p class=pgc-img-caption></p></div><p>其中</p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e3c8736d2de841b08603ce1251400e6b><p class=pgc-img-caption></p></div><p>是給定給定簇賦值的類的條件熵，由以下公式求得：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8d1f4eb5a0e24d71bfb84a8c70d66af4><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f2c5d8b636a2403c9fa1c07072812ef2><p class=pgc-img-caption></p></div><p>是類熵，公式為：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/30f6bd01a19a4a22ae31f49426159d99><p class=pgc-img-caption></p></div><p>其中，n是樣本總數，和分別屬於類c和類k的樣本數，而是從類c劃分到類k的樣本數量。條件熵H(K|C)和類熵H(K)，根據以上公式對稱求得。</p><p>V-measure是同質性homogeneity和完整性completeness的調和平均數，V-measure取值範圍為 [0,1]，越大越好，但當樣本量較小或聚類數據較多的情況，推薦使用AMI和ARI。公式：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5badc6805b2a4931809bdfe9e76595b3><p class=pgc-img-caption></p></div><p>代碼實現：</p><pre><code>from sklearn import metricslabels_true = [0, 0, 0, 1, 1, 1]labels_pred = [0, 0, 1, 1, 2, 2] print(metrics.homogeneity_score(labels_true, labels_pred))print(metrics.completeness_score(labels_true, labels_pred))print(metrics.v_measure_score(labels_true, labels_pred))</code></pre><p>優點：</p><ul><li>分數明確：從0到1反應出最差到最優的表現；</li><li>解釋直觀：差的調和平均數可以在同質性和完整性方面做定性的分析；</li><li>對簇結構不作假設：可以比較兩種聚類算法如k均值算法和譜聚類算法的結果。</li></ul><p>缺點：</p><ul><li>以前引入的度量在隨機標記方面沒有規範化，這意味著，根據樣本數，集群和先驗知識，完全隨機標籤並不總是產生相同的完整性和均勻性的值，所得調和平均值V-measure也不相同。特別是，隨機標記不會產生零分，特別是當簇的數量很大時。</li><li>當樣本數大於一千，聚類數小於10時，可以安全地忽略該問題。對於較小的樣本量或更大數量的集群，使用經過調整的指數（如調整蘭德指數）更為安全。</li><li>這些指標要求的先驗知識，在實踐中幾乎不可用或需要手動分配的人作註解者（如在監督學習環境中）。</li></ul><h3 class=pgc-h-arrow-right>Jaccard 指數</h3><p>該指數用於量化兩個數據集之間的相似性，該值得範圍為0-1.其中越大表明兩個數據集越相似：</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/30f82c197b1346b19e465a2dd96ea614><p class=pgc-img-caption></p></div><p>該指數和近年來的IOU計算方法一致</p><h3 class=pgc-h-arrow-right>Dice 指數</h3><p>該指數是基於jaccard指數上將TP的權重置為2倍。</p><p></p><div class=pgc-img><img alt=聚類算法的評估指標 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/02a0e2527a624098afca475f3aa04592><p class=pgc-img-caption></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>聚類</a></li><li><a>評估</a></li><li><a>指標</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E5%AD%B8/19ab3a8.html alt=聚類評估指標總結 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/5701491d73914e70b0bfda6987923d9a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/19ab3a8.html title=聚類評估指標總結>聚類評估指標總結</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/3009ee7.html alt="如何評估聚類模型？蘭德指數、輪廓係數、Calinski Harabaz指數" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/516332e51ebc4f52aab50f66dd11ef1b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/3009ee7.html title="如何評估聚類模型？蘭德指數、輪廓係數、Calinski Harabaz指數">如何評估聚類模型？蘭德指數、輪廓係數、Calinski Harabaz指數</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/d2dd4ef.html alt=聚類（二）：聚類性能評估、肘部法則、輪廓係數 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/05a2c3fcf29f4a5783bd2579b953a7e3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/d2dd4ef.html title=聚類（二）：聚類性能評估、肘部法則、輪廓係數>聚類（二）：聚類性能評估、肘部法則、輪廓係數</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/48532629.html alt=循環⽔養殖指標-溶解氧 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/56d1472f673a48fcb736dca88bf7b39c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/48532629.html title=循環⽔養殖指標-溶解氧>循環⽔養殖指標-溶解氧</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/114d7b14.html alt=功放的功率指標代表的意義是什麼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/5e77000075d4d8e13ca0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/114d7b14.html title=功放的功率指標代表的意義是什麼>功放的功率指標代表的意義是什麼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9ef66d5e.html alt=建築用石、砂質量指標 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/15300888056032990dddeee style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9ef66d5e.html title=建築用石、砂質量指標>建築用石、砂質量指標</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2e63d071.html alt=砂石質量檢測中的重要指標 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/55b469d2-23d7-421e-80b5-6a2a3cb6a429 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2e63d071.html title=砂石質量檢測中的重要指標>砂石質量檢測中的重要指標</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3ce14f5f.html alt=地鐵信號系統指標評估（詳細講解）——建議專業人士收藏 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ee118a6385c445fb8e775a28c64c50b8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3ce14f5f.html title=地鐵信號系統指標評估（詳細講解）——建議專業人士收藏>地鐵信號系統指標評估（詳細講解）——建議專業人士收藏</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5dcd6724.html alt=幾種基於可靠性指標的容量支持機制分析 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RZYfwddDNSSyQW style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5dcd6724.html title=幾種基於可靠性指標的容量支持機制分析>幾種基於可靠性指標的容量支持機制分析</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a98801f6.html alt=岸電電源的性能指標與應用實例 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/baba28d683b44b03be62af9ac7dc6d8a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a98801f6.html title=岸電電源的性能指標與應用實例>岸電電源的性能指標與應用實例</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ab2e795e.html alt="主要指標持續向好 交通運輸業加速回歸常態" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ab2e795e.html title="主要指標持續向好 交通運輸業加速回歸常態">主要指標持續向好 交通運輸業加速回歸常態</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0f340ebb.html alt="交通運輸經濟主要指標不斷反彈 貨運領域全力“跑”起來" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0f340ebb.html title="交通運輸經濟主要指標不斷反彈 貨運領域全力“跑”起來">交通運輸經濟主要指標不斷反彈 貨運領域全力“跑”起來</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c8decdbb.html alt=評估視覺相關學習障礙需要做哪些檢測（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/612b9d8df3c84d7aa047c3ed886e1341 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c8decdbb.html title=評估視覺相關學習障礙需要做哪些檢測（一）>評估視覺相關學習障礙需要做哪些檢測（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a2ff7ea1.html alt=想避免眩光？除評價指標，你還需瞭解這些場館照明的眩光測量點 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/55e82ee9dae44d18aa1380793c4eddad style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a2ff7ea1.html title=想避免眩光？除評價指標，你還需瞭解這些場館照明的眩光測量點>想避免眩光？除評價指標，你還需瞭解這些場館照明的眩光測量點</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/12a8383c.html alt=實時分析頁面指標，把握轉化「黃金一頁」 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/c968363cd3bd4c3fbe82449dbc96de3c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/12a8383c.html title=實時分析頁面指標，把握轉化「黃金一頁」>實時分析頁面指標，把握轉化「黃金一頁」</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>