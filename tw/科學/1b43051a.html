<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>在深度學習中對正則化的直觀認識 | 极客快訊</title><meta property="og:title" content="在深度學習中對正則化的直觀認識 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/1823dce03cf84870a3a026b913ecaa1d"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/1b43051a.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/1b43051a.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/1b43051a.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/1b43051a.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/1b43051a.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/1b43051a.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/1b43051a.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/1b43051a.html><meta property="article:published_time" content="2020-11-14T20:52:24+08:00"><meta property="article:modified_time" content="2020-11-14T20:52:24+08:00"><meta name=Keywords content><meta name=description content="在深度學習中對正則化的直觀認識"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/1b43051a.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>在深度學習中對正則化的直觀認識</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1823dce03cf84870a3a026b913ecaa1d><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>獲得對正則化的直觀認識</h1><p>在機器學習中，正則化是一種用來對抗高方差的方法——換句話說，就是模型學習再現數據的問題，而不是關於問題的潛在語義。與人類學習類似，我們的想法是構建家庭作業問題來測試和構建知識，而不是簡單的死記硬背：例如，學習乘法表，而不是學習如何乘。</p><p>這種現象在神經網絡學習中尤為普遍——學習能力越強，記憶的可能性就越大，這取決於我們這些實踐者如何引導深度學習模型來吸收我們的問題，而不是我們的數據。你們中的許多人在過去都曾遇到過這些方法，並且可能已經對不同的正則化方法如何影響結果形成了自己的直觀認識。為你們中那些不知道的人（甚至為那些知道的人！）本文為正則化神經網絡參數的形成提供了直觀的指導。將這些方面可視化是很重要的，因為人們很容易將許多概念視為理所當然；本文中的圖形和它們的解釋將幫助你直觀地瞭解，當你增加正則化時，模型參數的實際情況。</p><p>在本文中，我將把 L2 和 dropouts 作為正則化的標準形式。我不會討論其他方法（例如收集更多數據）如何改變模型的工作方式。</p><p>所有的圖形和模型都是用標準的科學Python堆棧製作的：numpy、matplotlib、scipy、sklearn，而神經網絡模型則是用PyTorch構建的。</p><h1 class=pgc-h-arrow-right>開發複雜函數</h1><p>深度學習的核心原則之一是深度神經網絡作為通用函數逼近的能力。無論你感興趣的是什麼，疾病傳播，自動駕駛汽車，天文學等，都可以通過一個自學習模型來壓縮和表達，這種想法絕對是令人驚奇的！儘管你感興趣的問題實際上是是否可以用解析函數f來表示這些問題，但當你通過訓練來調整機器學習模型時，該模型採用的參數θ允許模型近似地學習 f*。</p><p>出於演示的目的，我們將查看一些相對簡單的數據:理想情況下，一維中的某些數據足夠複雜，足以使老式曲線擬合變得痛苦，但還不足以使抽象和理解變得困難。我要創建一個複雜的函數來模擬週期信號，但是要加入一些有趣的東西。下面的函數實現如下方程:</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/457535dd702e46e3ae8f88cd69436f10><p class=pgc-img-caption></p></div><p>其中A，B，C是從不同高斯分佈中採樣的隨機數。這些值的作用是在非常相似的函數之間加上滯後，使得它們隨機地加在一起產生非常不同的f值。我們還將在數據中添加白色（高斯）噪聲，以模擬所收集數據的效果。</p><p>讓我們將隨機生成的數據樣本可視化：在本文的其餘部分中，我們將使用一個小的神經網絡來重現這條曲線。</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9f1cb508e1ca4f57b9b9fe78ecade91b><p class=pgc-img-caption></p></div><p>為了進行我們的模型訓練，我們將把它分成訓練/驗證集。為此，我將在sklearn.model_selection中使用極其方便的train_test_split功能。讓我們設計訓練和驗證集：</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0db81d21bd2244539b936b21d06d5ea6><p class=pgc-img-caption></p></div><p>正如我們在圖中看到的，這兩個集合在表示整個曲線方面都做得相當好：如果我們刪除其中一個，我們可以或多或少地收集到數據表示的相同圖片。這是交叉驗證的一個非常重要的方面！</p><h1 class=pgc-h-arrow-right>開發我們的模型</h1><p>現在我們有了一個數據集，我們需要一個相對簡單的模型來嘗試複製它。為了達到這個目的，我們將要處理一個四層的神經網絡，它包含三個隱藏層的單個輸入和輸出值，每個隱藏層64個神經元。</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/47466496046f46c794c44cab9ae0b6c9><p class=pgc-img-caption></p></div><p>為了方便起見，每個隱藏層都有一個LeakyReLU激活，輸出上有ReLU激活。原則上，這些應該不那麼重要，但是在測試過程中，模型有時無法學習一些“複雜”的功能，特別是當使用像tanh和sigmoid這樣容易飽和的激活函數時。在本文中，這個模型的細節並不重要：重要的是它是一個完全連接的神經網絡，它有能力學習逼近某些函數。</p><p>為了證明模型的有效性，我使用均方誤差（MSE）損失和ADAM優化器執行了通常的訓練/驗證週期，沒有任何形式的正則化，最後得到了以下結果：</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/88c238e4ecc942a7b7131fbb73a968bb><p class=pgc-img-caption></p></div><p>當我們使用此模型來預測：</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1823dce03cf84870a3a026b913ecaa1d><p class=pgc-img-caption></p></div><p>除了曲率變化很快的區域（接近x=11）之外，這個模型很好地再現了我們的“複雜”函數！</p><p>現在，我可以聽到你在問：如果模型運行良好，我為什麼要做任何正則化？在本演示中，我們的模型是否過擬合併不重要：我想要理解的是正則化如何影響一個模型;在我們的例子中，它甚至會對一個完美的工作模型產生不利影響。在某種意義上，你可以把這理解為一個警告:當你遇到過度擬合時要處理它，但在此之前不要處理。用Donald Knuth的話說，“不成熟的優化是萬惡之源”。</p><h1 class=pgc-h-arrow-right>正則化如何影響參數</h1><p>現在我們已經完成了所有的樣板文件，我們可以進入文章的核心了!我們的重點是建立對正則化的直觀認識，即不同的正則化方法如何從三個角度影響我們的簡單模型:</p><ol start=1><li>訓練/驗證的損失會怎樣?</li><li>我們的模型性能會發生什麼變化?</li><li>實際的參數會怎樣呢?</li></ol><p>雖然前兩點很簡單，但是很多人可能不熟悉如何量化第三點。在這個演示中，我將使用核密度評估來測量參數值的變化:對於那些熟悉Tensorboard的人來說，你將看到這些圖;對於那些不知道的人，可以把這些圖看作是複雜的直方圖。目標是可視化我們的模型參數如何隨正則化而變化，下圖顯示了訓練前後θ分佈的差異：</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fbc3b22f8de34ca4a18664ec0673bf8e><p class=pgc-img-caption></p></div><p>藍色曲線被標記為“均勻的”，因為它代表了我們用均勻分佈初始化的模型參數:你可以看到這基本上是一個頂帽函數，在中心具有相等的概率。這與訓練後的模型參數形成了鮮明的對比：經過訓練，模型需要不均勻的θ值才能表達我們的功能。</p><h1 class=pgc-h-arrow-right>L2正則化</h1><p>正則化最直接的方法之一是所謂的L2正則化:L2指的是使用參數矩陣的L2範數。由線性代數可知，矩陣的範數為:</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c7878407e4e740dd94181f7218b90a36><p class=pgc-img-caption></p></div><p>在前神經網絡機器學習中，參數通常用向量而不是矩陣/張量來表示，這就是歐幾里得範數。在深度學習中，我們通常處理的是矩陣/高維張量，而歐幾里德範數並不能很好地擴展(超越歐幾里德幾何)。L2範數實際上是上述方程的一個特例，其中p=q=2被稱為Frobenius或Hilbert-schmidt範數，它可以推廣到無限維度(即Hilbert空間)。</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/95166f60739140b8a2dfa3fcc4bc9ad8><p class=pgc-img-caption></p></div><p>在深度學習應用中，應用這種L2正則化的一般形式是在代價函數J的末尾附加一個“懲罰”項:</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9771f0b6e1984c1ea2b5937d790fb282><p class=pgc-img-caption></p></div><p>很簡單，這個方程定義了代價函數J為MSE損失，以及L2範數。L2範數的影響代價乘以這個前因子λ；這在許多實現中被稱為“權值衰減”超參數，通常在0到1之間。因為它控制了正則化的數量，所以我們需要了解這對我們的模型有什麼影響!</p><p>在一系列的實驗中，我們將重複與之前相同的訓練/驗證/可視化週期，但是這是在一系列的λ值上。首先，它是如何影響我們的訓練的?</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bf6b8090475a4b309888beca13645d3e><p class=pgc-img-caption></p></div><p>讓我們來分析一下。更深的紅色對應於更大的λ值(儘管這不是一個線性映射!),將訓練損失的痕跡顯示為MSE損失的日誌。記住，在我們的非正則化模型中，這些曲線是單調遞減的。在這裡,當我們增加λ的值，最終訓練誤差大大增加,並且早期損失的減少也沒有那麼顯著。當我們試圖使用這些模型來預測我們的功能時，會發生什麼?</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/df23d8ad22a04885a24f5d94b1f592f5><p class=pgc-img-caption></p></div><p>我們可以看到，當λ值很小時，函數仍然可以很好地表達。轉折點似乎在λ=0.01附近，在這裡，曲線的定性形狀被再現，但不是實際的數據點。從λ>0.01，模型只是預測整個數據集的平均值。如果我們把這些解釋為我們在訓練上的損失，那麼損失就會停止，這也就不足為奇了。</p><p>那麼參數的分佈呢?</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e1cf508157574d6c820ec76c17c13a53><p class=pgc-img-caption></p></div><p>我們看到,參數值的傳播大大受阻,正如我們的 λ 從低到高。與均勻分佈相比，參數值的擴展越來越接近於零，λ=1.0時，θ的分佈看起來就像一個在0處的狄拉克δ函數。由此，我們可以消除L2正則化作用於約束參數空間——強制θ非常稀疏並且接近零。</p><h1 class=pgc-h-arrow-right>dropouts呢?</h1><p>另一種流行且成本高效的正則化方法是在模型中包含dropouts。這個想法是，每次模型通過時，一些神經元通過根據概率p將它們的權值設置為0來失活。換句話說，我們對參數應用一個布爾掩碼，每次數據通過不同的單元時都被激活。這背後的基本原理是將模型學習分佈在整個網絡中，而不是特定的一層或兩層/神經元。</p><p>在我們的實驗中，我們將在每個隱藏層之間加入dropout層，並將dropout概率p從0調整為1。在前一種情況下，我們應該有一個非正則化的模型，而在後一種情況下，我們各自的學習能力應該有所下降，因為每一個隱藏層都被停用了。</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4f5f7a5da60d49a097caa430f64f803a><p class=pgc-img-caption></p></div><p>我們看到了與L2正則化非常相似的效果:總體而言，模型的學習能力下降，並且隨著dropout概率值的增大，最終損失的比例也增大。</p><p>當我們試圖使用這些模型來預測我們的功能時:</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/fa72082aeb154f3bb497d9e656fe020d><p class=pgc-img-caption></p></div><p>如圖，我們逐步增加了dropout概率。從p=0.1開始，我們可以看到我們的模型對於它的預測開始變得相當不可靠:最有趣的是，它似乎近似地跟蹤了我們的數據，包括噪音!</p><p>在p=0.2和0.3時，這一點在x=11時更加明顯——回想一下，我們的非正則化模型很難得到正確的函數區域。我們看到，帶dropout的預測實際上使這一區域難以置信的模糊，這幾乎就像模型告訴我們，它是不確定的!(後面會詳細介紹)。</p><p>從p=0.4開始，模型的能力似乎受到了極大的限制，除了第一部分之外，它幾乎無法再現曲線的其他部分。在p=0.6時，預測結果似乎接近數據集的平均值，這似乎也發生在L2正則化的大值上。</p><p>我們的模型參數呢?</p><div class=pgc-img><img alt=在深度學習中對正則化的直觀認識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6a48c4a4aae042c58c76c8f05a0cfda8><p class=pgc-img-caption></p></div><p>將此結果與我們的L2範數結果進行比較:對於dropout，我們的參數分佈更廣，這增加了我們的模型表達的能力。除p=1.0外，dropout概率的實際值對參數的分佈影響不大，如果有影響的話。在p=1.0時，我們的模型沒有學到任何東西，只是類似於均勻分佈。在p值降低時，儘管速度降低了，模型仍然能夠學習。</p><h1 class=pgc-h-arrow-right>最後</h1><p>從我們簡單的實驗中，我希望你已經從我們探索的三個角度，對這兩種正則化方法如何影響神經網絡模型形成了一些直觀認識。</p><p>L2正則化非常簡單，只需要調整一個超參數。當我們增加L2懲罰的權重時，因為參數空間的變化，對於大的值(0.01-1)，模型容量下降得非常快。對於較小的值,你甚至可能不會看到模型預測有什麼變化。</p><p>Dropouts是一種更復雜的正則化方法，因為現在必須處理另一層超參數複雜性(p可以為不同的層提供不同的值)。儘管如此，這實際上可以提供模型表達的另一個維度:模型不確定性的形式。</p><p>在這兩種方法中，我們看到正則化增加了最終的訓練損失。這些人工形式的正則化(與獲取更多的訓練數據相反)的代價是它們會降低模型的容量: 除非你確定你的模型需要正則化，否則在這樣的結果下，你不會希望正則化。但是，通過本指南，你現在應該知道這兩種形式如何影響你的模型!</p><p>如果你感興趣，可以在Binder上（https://mybinder.org/v2/gh/laserkelvin/understanding-ml/master） 運行一些代碼。我不需要運行torch模型(這會耗盡它們的資源)，但是你可以使用它在notebook中查看代碼。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>學習</a></li><li><a>正則化</a></li><li><a>直觀</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/eb1b4d26.html alt=對“偏導數”和“梯度”最形象直觀的解釋 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8222f17b50274c9c87ba3a6c21c5bc65 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/eb1b4d26.html title=對“偏導數”和“梯度”最形象直觀的解釋>對“偏導數”和“梯度”最形象直觀的解釋</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html alt=直流鍋爐給水控制學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/eba10edcc8d14d9f8cde6fd5b212d90e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html title=直流鍋爐給水控制學習>直流鍋爐給水控制學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html alt=HTMLCSS學習筆記（六）——元素類型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb5988349894ce9bf568c6418f85b7d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html title=HTMLCSS學習筆記（六）——元素類型>HTMLCSS學習筆記（六）——元素類型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html alt="web前端（從零開始），每天更新學習筆記 HTML5元素分類" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/46d70004fcd55e1ddad3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html title="web前端（從零開始），每天更新學習筆記 HTML5元素分類">web前端（從零開始），每天更新學習筆記 HTML5元素分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html alt="MySQL 學習筆記" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html title="MySQL 學習筆記">MySQL 學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html alt=深入學習MySQL事務：ACID特性的實現原理「轉」 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/cdc702d66d6943499997d11e931425eb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html title=深入學習MySQL事務：ACID特性的實現原理「轉」>深入學習MySQL事務：ACID特性的實現原理「轉」</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html alt=如何學習模擬IC設計？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html title=如何學習模擬IC設計？>如何學習模擬IC設計？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html alt=小猿圈python學習-三大特性之多態 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ad0e8e3777854337abeb7c779ad79a04 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html title=小猿圈python學習-三大特性之多態>小猿圈python學習-三大特性之多態</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/03a295fc.html alt=地理學習5——地球的運動（地球的公轉及其地理意義） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7b2b74c871eb40beb8ee143627d29611 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/03a295fc.html title=地理學習5——地球的運動（地球的公轉及其地理意義）>地理學習5——地球的運動（地球的公轉及其地理意義）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ebad378f.html alt=繼續學習打卡，還真心學不會了，努力，堅持 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/f36d6d47a06840aaaf78138853b9d9d1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ebad378f.html title=繼續學習打卡，還真心學不會了，努力，堅持>繼續學習打卡，還真心學不會了，努力，堅持</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>