<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>Tomaso Poggio深度學習理論：深度網絡「過擬合缺失」的本質 | 极客快訊</title><meta property="og:title" content="Tomaso Poggio深度學習理論：深度網絡「過擬合缺失」的本質 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/1531814636219d8413e89af"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/2581c4e.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/2581c4e.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/2581c4e.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/2581c4e.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/2581c4e.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/2581c4e.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/2581c4e.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/2581c4e.html><meta property="article:published_time" content="2020-10-29T21:03:14+08:00"><meta property="article:modified_time" content="2020-10-29T21:03:14+08:00"><meta name=Keywords content><meta name=description content="Tomaso Poggio深度學習理論：深度網絡「過擬合缺失」的本質"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/2581c4e.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>Tomaso Poggio深度學習理論：深度網絡「過擬合缺失」的本質</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><div><p><strong>選自arXiv，作者：Tomaso Poggio等，機器之心編譯，參與：路、李亞洲、劉曉坤。</strong></p><blockquote><p>本文是 DeepMind 創始人 Demis Hassabis 和 Mobileye 創始人 Amnon Shashua 的導師、MIT 教授 Tomaso Poggio 的深度學習理論系列的第三部分，分析深度神經網絡的泛化能力。該系列前兩部分討論了深度神經網絡的表徵和優化問題，機器之心之前對整個理論系列進行了簡要總結。在深度網絡的實際應用中，通常會添加顯性（如權重衰減）或隱性（如早停）正則化來避免過擬合，但這並非必要，尤其是在分類任務中。在本文中，Poggio 討論了深度神經網絡的過擬合缺失問題，即在參數數量遠遠超過訓練樣本數的情況下模型也具備良好的泛化能力。其中特別強調了經驗損失和分類誤差之間的差別，證明深度網絡每一層的權重矩陣可收斂至極小范數解，並得出深度網絡的泛化能力取決於多種因素的互相影響，包括損失函數定義、任務類型、數據集類型等。</p></blockquote><div class=pgc-img><img alt="Tomaso Poggio深度學習理論：深度網絡「過擬合缺失」的本質" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1531814636219d8413e89af><p class=pgc-img-caption></p></div><p><strong>1 引言</strong></p><p>過去幾年來，深度學習在許多機器學習應用領域都取得了極大的成功。然而，我們對深度學習的理論理解以及開發原理的改進能力上都有所落後。如今對深度學習令人滿意的理論描述正在形成。這涵蓋以下問題：1）深度網絡的表徵能力；2）經驗風險的優化；3）泛化——當網絡過參數化（overparametrized）時，即使缺失顯性的正則化，為什麼期望誤差沒有增加？</p><p>本論文解決了第三個問題，也就是非過擬合難題，這在最近的多篇論文中都有提到。論文 [1] 和 [7] 展示了線性網絡的泛化特性可被擴展到 DNN 中從而解決該難題，這兩篇論文中的泛化即用梯度下降訓練的帶有特定指數損失的線性網絡收斂到最大間隔解，提供隱性的正則化。本論文還展示了同樣的理論可以預測經驗風險的不同零最小值（zero minimizer）的泛化。</p><p><strong>2 過擬合難題</strong></p><p>經典的學習理論將學習系統的泛化行為描述為訓練樣本數 n 的函數。從這個角度看，DNN 的行為和期望一致：更多訓練數據帶來更小的測試誤差，如圖 1a 所示。該學習曲線的其他方面似乎不夠直觀，但也很容易解釋。例如即使在訓練誤差為零時，測試誤差也會隨著 n 的增加而減小（正如 [1] 中所指出的那樣，因為被報告的是分類誤差，而不是訓練過程中被最小化的風險，如交叉熵）。看起來 DNN 展示出了泛化能力，從技術角度上可定義為：隨著 n → ∞，訓練誤差收斂至期望誤差。圖 1 表明對於正常和隨機標籤，模型隨 n 的增加的泛化能力變化。這與之前研究的結果一致（如 [8]），與 [9] 的穩定性結果尤其一致。注意泛化的這一特性並不尋常：很多算法（如 K 最近鄰算法）並不具備該保證。</p><div class=pgc-img><img alt="Tomaso Poggio深度學習理論：深度網絡「過擬合缺失」的本質" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1531814611398fe4cd52a80><p class=pgc-img-caption></p></div><p>圖 1：不同數量訓練樣本下的泛化。（a）在 CIFAR 數據集上的泛化誤差。（b）在隨機標籤的 CIFAR 數據集上的泛化誤差。該深度神經網絡是通過最小化交叉熵損失訓練的，並且是一個 5 層卷積網絡（即沒有池化），每個隱藏層有 16 個通道。ReLU 被用做層之間的非線性函數。最終的架構有大約 1 萬個參數。圖中每一個點使用批大小為 100 的 SGD 並訓練 70 個 epoch 而得出，訓練過程沒有使用數據增強和正則化。</p><p>泛化的這一特性雖然重要，但在這裡也只是學術上很重要。現在深度網絡典型的過參數化真正難題（即本論文的重點）是在缺乏正則化的情況下出現明顯缺乏過擬合的現象。從隨機標註數據中獲得零訓練誤差的同樣網絡（圖 1b）很顯然展示出了大容量，但並未展示出在不改變多層架構的情況下，每一層神經元數量增加時期望誤差會有所增加（圖 2a）。具體來說，當參數數量增加並超過訓練集大小時，未經正則化的分類誤差在測試集上的結果並未變差。</p><div class=pgc-img><img alt="Tomaso Poggio深度學習理論：深度網絡「過擬合缺失」的本質" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153181461141355346e0dba><p class=pgc-img-caption></p></div><p>圖 2：在 CIFAR-10 中的期望誤差，橫軸為神經元數量。該 DNN 與圖 1 中的 DNN 一樣。（a）期望誤差與參數數量增加之間的相關性。（b）交叉熵風險與參數數量增加之間的相關性。期望風險中出現部分「過擬合」，儘管該指數損失函數的特點略微有些誇大。該過擬合很小，因為 SGD 收斂至每一層具備最小弗羅貝尼烏斯範數（Frobenius norm）的網絡。因此，當參數數量增加時，這裡的期望分類誤差不會增加，因為分類誤差比損失具備更強的魯棒性（見附錄 9）。</p><p>我們應該明確參數數量只是過參數化的粗略表徵。實驗設置詳見第 6 章。</p><p><strong>5 深度網絡的非線性動態</strong></p><p><strong>5.3 主要問題</strong></p><p>把所有引理合在一起，就得到了</p><p><strong>定理 3</strong>：給定一個指數損失函數和非線性分割的訓練數據，即對於訓練集中的所有 x_n，∃f(W; x_n) 服從 y_n*f(W; x_n) > 0，獲得零分類誤差。以下特性展示了漸近平衡（asymptotic equilibrium）：</p><ol><li>GD 引入的梯度流從拓撲學角度來看等於線性化流；</li><li>解是每一層權重矩陣的局部極小弗羅貝尼烏斯範數解。</li></ol><p>在平方損失的情況下分析結果相同，但是由於線性化動態只在零初始條件下收斂至極小范數，因此該定理的最終表述「解是局部極小範數解」僅適用於線性網絡，如核機器，而不適用於深度網絡。因此在非線性的情況下，平方損失和指數損失之間的差別變得非常顯著。對其原因的直觀理解見圖 3。對於全局零最小值附近的深度網絡，平方損失的「地形圖」通常有很多零特徵值，且在很多方向上是平坦的。但是，對於交叉熵和其他指數損失而言，經驗誤差山谷有一個很小的向下的坡度，在||w||無限大時趨近於零（詳見圖 3）。</p><p>在補充材料中，研究者展示了通過懲罰項 λ 寫出 W_k = ρ_k*V_k，並使 ||V_k||^2 = 1，來考慮相關動態，從而展示初始條件的獨立性以及早停和正則化的等效性。</p><div class=pgc-img><img alt="Tomaso Poggio深度學習理論：深度網絡「過擬合缺失」的本質" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/15318146113123d02bdf252><p class=pgc-img-caption></p></div><p>圖 3：具備參數 w_1 和 w_2 的平方損失函數（左）。極小值具備一個退化 Hessian（特徵值為零）。如文中所述，它表示在零最小值的小近鄰區域的「一般」情況，零最小值具備很多零特徵值，和針對非線性多層網絡的 Hessian 的一些正特徵值。收斂處的全局最小值附近的交叉熵風險圖示如右圖所示。隨著||w|| → ∞，山谷坡度稍微向下。在多層網絡中，損失函數可能表面是分形的，具備很多退化全局最小值，每個都類似於這裡展示的兩個最小值的多維度版本。</p><p><strong>5.4 為什麼分類比較不容易過擬合</strong></p><p>由於這個解是線性化系統的極小範數解，因此我們期望，對於低噪聲數據集，與交叉熵最小化相關的分類誤差中幾乎很少或沒有過擬合。注意：交叉熵作為損失函數的情況中，梯度下降在線性分離數據上可收斂至局部極大間隔解（local max-margin solution），起點可以是任意點（原因是非零斜率，如圖 3 所示）。因此，對於期望分類誤差，過擬合可能根本就不會發生，如圖 2 所示。通常相關損失中的過擬合很小，至少在幾乎無噪聲的數據情況下是這樣，因為該解是局部極大間隔解，即圍繞極小值的線性化系統的偽逆。近期結果（Corollary 2.1 in [10]）證明具備 RELU 激活函數的深度網絡的 hinge-loss 的梯度最小值具備大的間隔，前提是數據是可分離的。這個結果與研究者將 [1] 中針對指數損失的結果擴展至非線性網絡的結果一致。注意：目前本論文研究者沒有對期望誤差的性質做出任何聲明。不同的零最小值可能具備不同的期望誤差，儘管通常這在 SGD 的類似初始化場景中很少出現。本文研究者在另一篇論文中討論了本文提出的方法或許可以預測與每個經驗最小值相關的期望誤差。</p><p>總之，本研究結果表明多層深度網絡的行為在分類中類似於線性模型。更準確來說，在分類任務中，通過最小化指數損失，可確保全局最小值具備局部極大間隔。因此動態系統理論為非過擬合的核心問題提供了合理的解釋，如圖 2 所示。主要結果是：接近經驗損失的零極小值，非線性流的解繼承線性化流的極小範數特性，因為這些流拓撲共軛。損失中的過擬合可以通過正則化來顯性（如通過權重衰減）或隱性（通過早停）地控制。分類誤差中的過擬合可以被避免，這要取決於數據集類型，其中漸近解是與特定極小值相關的極大間隔解（對於交叉熵損失來說）。</p><p><strong>6 實驗</strong></p><div class=pgc-img><img alt="Tomaso Poggio深度學習理論：深度網絡「過擬合缺失」的本質" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1531814611346e78475508c><p class=pgc-img-caption></p></div><p>圖 4：使用平方損失在特徵空間中對線性網絡進行訓練和測試（即 y = WΦ(X)），退化 Hessian 如圖 3 所示。目標函數是一個 sine 函數 f(x) = sin(2πfx)，在區間 [−1, 1] 上 frequency f = 4。訓練數據點有 9 個，而測試數據點的數量是 100。第一對圖中，特徵矩陣 φ(X) 是多項式的，degree 為 39。第一對圖中的數據點根據 Chebyshev 節點機制進行採樣，以加速訓練，使訓練誤差達到零。訓練使用完整梯度下降進行，步長 0.2，進行了 10, 000, 000 次迭代。每 120, 000 次迭代後權重受到一定的擾動，每一次擾動後梯度下降被允許收斂至零訓練誤差（機器準確率的最高點）。通過使用均值 0 和標準差 0.45 增加高斯噪聲，進而擾動權重。在第 5, 000, 000 次迭代時擾動停止。第二張圖展示了權重的 L_2 範數。注意訓練重複了 29 次，圖中報告了平均訓練和測試誤差，以及權重的平均範數。第二對圖中，特徵矩陣 φ(X) 是多項式的，degree 為 30。訓練使用完整梯度下降進行，步長 0.2，進行了 250, 000 次迭代。第四張圖展示了權重的 L_2 範數。注意：訓練重複了 30 次，圖中報告了平均訓練和測試誤差，以及權重的平均範數。該實驗中權重沒有遭到擾動。</p><p><strong>7 解決過擬合難題</strong></p><p>本研究的分析結果顯示深度網絡與線性模型類似，儘管它們可能過擬合期望風險，但不經常過擬合低噪聲數據集的分類誤差。這遵循線性網絡梯度下降的特性，即風險的隱性正則化和對應的分類間隔最大化。在深度網絡的實際應用中，通常會添加顯性正則化（如權重衰減）和其他正則化技術（如虛擬算例），而且這通常是有益的，雖然並非必要，尤其是在分類任務中。</p><p>如前所述，平方損失與指數損失不同。在平方損失情況中，具備任意小的 λ 的正則化（沒有噪聲的情況下）保留梯度系統的雙曲率，以收斂至解。但是，解的範數依賴於軌跡，且無法確保一定會是線性化引入的參數中的局部極小範數解（在非線性網絡中）。在沒有正則化的情況下，可確保線性網絡（而不是深度非線性網絡）收斂至極小范數解。在指數損失線性網絡和非線性網絡的情況下，可獲得雙曲梯度流。因此可確保該解是不依賴初始條件的極大間隔解。對於線性網絡（包括核機器），存在一個極大間隔解。在深度非線性網絡中，存在多個極大間隔解，每個對應一個全局最小值。在某種程度上來說，本研究的分析結果顯示了正則化主要提供了動態系統的雙曲率。在條件良好的線性系統中，即使 λ → 0，這種結果也是對的，因此內插核機器的通用情況是在無噪聲數據情況下無需正則化（即條件數依賴於 x 數據的分割，因此 y 標籤與噪聲無關，詳見 [19]）。在深度網絡中，也會出現這種情況，不過只適用於指數損失，而非平方損失。</p><p>結論就是深度學習沒什麼神奇，在泛化方面深度學習需要的理論與經典線性網絡沒什麼不同，泛化本身指收斂至期望誤差，尤其是在過參數化時出現了過擬合缺失的情況。本研究分析通過將線性網絡的特性（如 [1] 強調的那些）應用到深度網絡，解釋了深度網絡泛化方面的難題，即不會過擬合期望分類誤差。</p><p><strong>8 討論</strong></p><p>當然，構建對深度網絡性能有用的量化邊界仍然是一個開放性問題，因為它是非常常見的情形，即使是對於簡單的僅包含一個隱藏層的網絡，如 SVM。本論文研究者主要的成果是圖 2 所展示的令人費解的行為可以通過經典理論得到定性解釋。</p><p>該領域存在很多開放性問題。儘管本文解釋了過擬合的缺失，即期望誤差對參數數量增加的容錯，但是本文並未解釋為什麼深度網絡泛化得這麼好。也就是說，本論文解釋了為什麼在參數數量增加並超過訓練數據數量時，圖 2 中的測試分類誤差沒有變差，但沒有解釋為什麼測試誤差這麼低。</p><p>基於 [20]、[18]、[16]、[10]，研究者猜測該問題的答案包含在以下深度學習理論框架內：</p><ul><li>不同於淺層網絡，深度網絡能逼近層級局部函數類，且不招致維數災難（[21, 20]）。</li><li>經由 SGD 選擇，過參數化的深度網絡有很大概率會產生很多全局退化，或者大部分退化，以及「平滑」極小值（[16]）。</li><li>過參數化，可能會產生預期風險的過擬合。因為梯度下降方法獲得的間隔最大化，過參數化也能避免過擬合低噪聲數據集的分類誤差。</li></ul><p>根據這一框架，淺層網絡與深度網絡之間的主要區別在於，基於特定任務的組織結構，兩種網絡從數據中學習較好表徵的能力，或者說是逼近能力。不同於淺層網絡，深度局部網絡特別是卷積網絡，能夠避免逼近層級局部合成函數類時的維度災難（curse of dimensionality）。這意味著對於這類函數，深度局部網絡可以表徵一種適當的假設類，其允許可實現的設置，即以最小容量實現零逼近誤差。</p><p><strong>論文：Theory IIIb: Generalization in Deep Networks</strong></p><div class=pgc-img><img alt="Tomaso Poggio深度學習理論：深度網絡「過擬合缺失」的本質" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153181461129558aa8f277f><p class=pgc-img-caption></p></div><p>論文鏈接：https://arxiv.org/abs/1806.11379</p><p><strong>摘要：</strong>深度神經網絡（DNN）的主要問題圍繞著「過擬合」的明顯缺失，本論文將其定義如下：當神經元數量或梯度下降迭代次數增加時期望誤差卻沒有變差。鑑於 DNN 擬合隨機標註數據的大容量和顯性正則化的缺失，這實在令人驚訝。近期 Srebro 等人的研究結果為二分類線性網絡中的該問題提供瞭解決方案。他們證明損失函數（如 logistic、交叉熵和指數損失）最小化可在線性分離數據集上漸進、「緩慢」地收斂到最大間隔解，而不管初始條件如何。本論文中我們證明了對於非線性多層 DNN 在經驗損失最小值接近零的情況下也有類似的結果。指數損失的結果也是如此，不過不適用於平方損失。具體來說，我們證明深度網絡每一層的權重矩陣可收斂至極小范數解，達到比例因子（在獨立案例下）。我們對動態系統的分析對應多層網絡的梯度下降，這展示了一種對經驗損失的不同零最小值泛化性能的簡單排序標準。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>深度</a></li><li><a>Tomaso</a></li><li><a>Poggio</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/3b9ab266.html alt=深度分析：陳雨菲和戴資穎的差距在哪？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9fc30f4d17404df887ad596f1d1109d3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3b9ab266.html title=深度分析：陳雨菲和戴資穎的差距在哪？>深度分析：陳雨菲和戴資穎的差距在哪？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f9156380.html alt=深度分析｜人與人的差距是怎樣拉開的？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/153466915270902d99ccf45 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f9156380.html title=深度分析｜人與人的差距是怎樣拉開的？>深度分析｜人與人的差距是怎樣拉開的？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/86bc70f6.html alt=做人，如茶！（深度） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/472ec1d2a06d41df94be24f073fea09e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/86bc70f6.html title=做人，如茶！（深度）>做人，如茶！（深度）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5a7c0dad.html alt=深度學習中的線性代數 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/dfic-imagehandler/677561e3-e0ec-4693-bd88-0cd182b21a17 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5a7c0dad.html title=深度學習中的線性代數>深度學習中的線性代數</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/26ac9044.html alt=努力成為了更好的自己，世界才是你的（深度好文） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5e40df14eb5c4b0d85aa32b6aecfbf42 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/26ac9044.html title=努力成為了更好的自己，世界才是你的（深度好文）>努力成為了更好的自己，世界才是你的（深度好文）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/22096117.html alt=【行業深度】汽車半導體深度-功率器件：SiC時代，產業公司梳理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/5057861a-2c6d-4bbf-80ba-06bebff5ea95 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/22096117.html title=【行業深度】汽車半導體深度-功率器件：SiC時代，產業公司梳理>【行業深度】汽車半導體深度-功率器件：SiC時代，產業公司梳理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/186ea096.html alt=位置（深度好文） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/Ryh1J263rrjMJ4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/186ea096.html title=位置（深度好文）>位置（深度好文）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/963d0e3f.html alt=你的，位置！（深度好文） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/270d26ab6660436184ebcf647e3e6c98 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/963d0e3f.html title=你的，位置！（深度好文）>你的，位置！（深度好文）</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/c647112d.html alt=深度丨“327系”擼過的上市公司 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1534862687233dc3f34c12e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/c647112d.html title=深度丨“327系”擼過的上市公司>深度丨“327系”擼過的上市公司</a></li><hr><li><a href=../../tw/%E9%81%8A%E6%88%B2/36673545.html alt=秦始皇陵地宮深度揭祕 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/ec21092d-d278-442e-8d00-715645fc1ab9 style=border-radius:25px></a>
<a href=../../tw/%E9%81%8A%E6%88%B2/36673545.html title=秦始皇陵地宮深度揭祕>秦始皇陵地宮深度揭祕</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0dad8233.html alt=「深度」中美人工智能科技政策比較 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/43a3a32fa91b4fa985d2ffcfe2c5d71e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0dad8233.html title=「深度」中美人工智能科技政策比較>「深度」中美人工智能科技政策比較</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/94849101.html alt=PSU提出深度k-最近鄰算法，解決深度學習應用的安全漏洞 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/1521454533837a1e512919e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/94849101.html title=PSU提出深度k-最近鄰算法，解決深度學習應用的安全漏洞>PSU提出深度k-最近鄰算法，解決深度學習應用的安全漏洞</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/623272eb.html alt=深度｜需求分析的高階指南 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/REvGrnRIvoFsnK style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/623272eb.html title=深度｜需求分析的高階指南>深度｜需求分析的高階指南</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/31ece80.html alt="深度 | 劉群：基於深度學習的自然語言處理，邊界在哪裡？" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/RaimUrx1Qpfnvn style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/31ece80.html title="深度 | 劉群：基於深度學習的自然語言處理，邊界在哪裡？">深度 | 劉群：基於深度學習的自然語言處理，邊界在哪裡？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/aab1258.html alt=失去了，再也回不來（深度好文） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9337e2b036df467da6897cb4a8ab757d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/aab1258.html title=失去了，再也回不來（深度好文）>失去了，再也回不來（深度好文）</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>