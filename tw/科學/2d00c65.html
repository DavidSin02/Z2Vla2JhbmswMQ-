<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>理解神經網絡 | 极客快訊</title><meta property="og:title" content="理解神經網絡 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/15409758920775d7570f483"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/2d00c65.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/2d00c65.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/2d00c65.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/2d00c65.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/2d00c65.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/2d00c65.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/2d00c65.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/2d00c65.html><meta property="article:published_time" content="2020-10-29T21:03:13+08:00"><meta property="article:modified_time" content="2020-10-29T21:03:13+08:00"><meta name=Keywords content><meta name=description content="理解神經網絡"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/2d00c65.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>理解神經網絡</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><div><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15409758920775d7570f483><p class=pgc-img-caption></p></div><p>神經網絡是機器學習領域中最強大、應用最廣泛的算法之一。乍一看，神經網絡似乎是個黑盒子;輸入層將數據輸入到“隱藏層”中，經過一個魔術之後，我們可以看到輸出層提供的信息。然而，理解隱藏層在做什麼是神經網絡實現和優化的關鍵步驟。</p><p>在我們理解神經網絡的道路上，我們將回答三個問題:</p><ul><li>什麼是神經網絡</li><li>神經網絡是如何工作的</li><li>為什麼神經網絡能夠學習</li></ul><h1>什麼是神經網絡？</h1><p>我們將要考慮的神經網絡被嚴格地稱為人工神經網絡，顧名思義，它是基於科學對人腦結構和功能的瞭解。</p><p>簡單地說，神經網絡被定義為一種計算系統，它由許多簡單但高度互聯的元素或節點組成，稱為“神經元”，這些元素或節點被組織成層，利用外部輸入的動態狀態響應處理信息。在這種結構的上下文中，輸入層將模式引入到神經網絡中，輸入層為輸入數據中出現的每個組件都有一個神經元，並與網絡中出現的一個或多個隱藏層進行通信;之所以稱為“隱藏”，是因為它們不構成輸入或輸出層。在隱藏層中，所有的處理實際上都是通過一個以權重和偏差(通常稱為W和b)為特徵的連接系統進行的:接收輸入,神經元計算加權和添加偏差並根據結果和一個預設激活函數(最常見的一個是sigmoid,σ),它決定是否應該“fired”或激活。之後，神經元將信息傳遞到下游的其他連接的神經元，這個過程被稱為“forward pass”。在這個過程的最後，最後一個隱藏層被連接到輸出層，輸出層對於每個可能需要的輸出都有一個神經元。</p><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1540975914144b6e652ea73><p class=pgc-img-caption>2層神經網絡的基本結構</p></div><p>Wi：相應連接的權重。注意：計算網絡中存在的層數時，不包括輸入層。</p><h1>神經網絡如何工作？</h1><p>現在我們已經瞭解了神經網絡基本結構的外觀，我們將繼續解釋它是如何工作的。為了做到這一點，我們需要解釋我們可以包含在網絡中的不同類型的神經元。</p><p>我們要解釋的第一種神經元是Perceptron。即使它的使用已經在今天衰退，瞭解它們如何工作將為我們提供關於更多現代神經元如何運作的良好線索。</p><p>感知器使用函數通過將二進制變量的矢量映射到單個二進制輸出來學習二元分類器，並且它還可以用於監督學習。在這種情況下，感知器遵循以下步驟：</p><ol><li>將所有輸入乘以其權重w， 表示相應輸入對輸出的重要程度的實數，</li><li>將它們加在一起稱為加權和：Σwjxj，</li><li>應用激活函數，換句話說，確定加權和是否大於閾值，其中-threshold等於bias，並指定1 或更小並將0指定為輸出。</li></ol><p>我們也可以把感知器函數寫成這樣:</p><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15409759456278bf099358d><p class=pgc-img-caption></p></div><p>注意：b是偏差，相當於-threshold，wx是w的點積，是矢量，其中分量是權重，x是由輸入組成的矢量。</p><p>該算法最突出的一點是，我們可以改變權重和偏差，以獲得不同的決策模型。我們可以給這些輸入賦予更多的權重，這樣如果它們是正的，就會有利於我們想要的輸出。另外，因為偏差可以理解為輸出1的難易程度的度量，如果我們想讓期望的輸出或多或少發生，我們可以降低或提高它的值。如果我們注意這個公式，我們可以觀察到一個大的正向偏差會使輸出1變得非常容易;然而，負的偏見將使輸出1的任務變得非常不可能。</p><p>因此，感知器可以分析不同的數據，並根據設定的偏好做出決定。事實上，創建更復雜的網絡是有可能的，包括更多的感知器層，每一層都取前一層的輸出並加權，做出越來越複雜的決定。</p><p>如果感知器能很好地做出複雜的決定，為什麼我們還需要其他類型的神經元呢?包含感知器的網絡的一個缺點是，甚至在只有一個感知器中，權重或偏壓的小變化，也會嚴重地改變從0到1的輸出，反之亦然。我們真正想要的是通過引入權重或偏差的小修改來逐漸改變我們網絡的行為。這就是一種更現代的神經元派上用場的地方:Sigmoid神經元。Sigmoid neurons和感知器的主要區別在於輸入和輸出可以是0到1之間的任意連續值。在考慮權重w和偏差b的情況下，將sigmoid函數應用到輸入中，得到輸出結果。為了更直觀的理解，我們可以這樣寫:</p><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15409759667803ce993ddd1><p class=pgc-img-caption></p></div><p>所以，輸出的公式是：</p><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15409759817345f415a8607><p class=pgc-img-caption></p></div><p>如果我們對這個函數進行數學分析，我們可以得到我們的函數σ 的圖形，如下所示，並得出結論：當z很大且正數時，函數達到其最大漸近值1; 但是，如果z很大且為負，則函數達到其最小漸近值0.這裡是sigmoid函數變得非常有趣的地方，因為它具有中等的z值，函數採用平滑且接近線性的形狀。在此間隔中，權重（Δwj）或偏差（Δbj）的微小變化將在輸出中產生微小變化; 我們所期待的行為是感知器的改進。</p><pre>z = np.arange(-10, 10, 0.3)sigm = 1 / (1 + np.exp(-z))plt.plot(z, sigm, color = 'mediumvioletred', linewidth= 1.5)plt.xlabel('Z', size = 14, alpha = 0.8)plt.ylabel('σ(z)', size = 14, alpha = 0.8)a = plt.title('Sigmoid Function', size = 14)a = a.set_position([.5, 1.05])</pre><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/154097603011774e0850818><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15409760534190b1254b29c><p class=pgc-img-caption></p></div><p>我們知道一個函數的導數是y值相對於變量x的變化速率的度量，在這種情況下，變量y是我們的輸出變量x是權重和偏差的函數。我們可以利用這一點，用導數來計算輸出的變化，特別是偏導數(關於w和關於b的)。在sigmoid函數中，導數將被縮減為計算:f(z)*(1-f(z))。</p><p>這裡有一個簡單的Python代碼，可以用來建模一個sigmoid函數:</p><pre>'''Build a sigmoid function to map any value to a value between zero and one\n",Refers to case of logistic function defined by: s(z) = 1/(1+e^-z)which derivative is bell shape. derivative is equal to f(z)*(1-f(z))'''def sigmoid(x, deriv = False): if deriv == True: return x*(1-x) return 1/(1+np.exp(-x))</pre><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1540976082001a98e380d6e><p class=pgc-img-caption></p></div><p>我們剛剛解釋了我們網絡中每個神經元的功能，現在，我們可以檢查其餘神經元是如何工作的。將來自一層的輸出用作下一層的輸入的神經網絡稱為前饋，特別是因為不涉及循環並且信息僅pass forward而從不返回。</p><p>假設我們有一個訓練集，我們想要使用一個3層神經網絡，我們也使用上面看到的sigmoid神經​​元來預測某個特徵。根據我們對神經網絡結構的解釋，需要首先將權重和偏差分配給一層​​中的神經元與下一層中的神經元之間的連接。通常，偏差和權重都在突觸矩陣中隨機初始化。如果我們在python中編碼神經網絡，我們可以使用Numpy函數np.random.random 生成高斯分佈（其中均值等於0，標準差為1）。</p><pre>#Create Synapsis matrixsyn0 = 2+np.random.random((3,4)) -1syn1 = 2+np.random.random((4,1)) -1</pre><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15409761100367b7d5e6c32><p class=pgc-img-caption></p></div><p>之後，我們將從前饋步驟開始構建神經網絡，以計算預測輸出; 換句話說，我們只需要構建網絡中涉及的不同層：</p><ul><li>layer0是輸入層; 我們的訓練集讀作矩陣（我們可以稱之為X）</li><li>layer1通過應用激活函數a'=σ（w.X + b）獲得，在我們的例子中，執行輸入layer0和突觸矩陣syn0之間的點乘</li><li>layer2是通過layer1它和它的突觸之間的點乘法獲得的輸出層syn1</li></ul><p>我們還需要迭代訓練集以讓網絡學習。為此，我們將添加for 循環。</p><pre>#For loop iterate over the training setfor i in range(60000):  #First layer is the input layer0 = X  #Second layer can be obtained with the multiplication of each layer  #and its synapsis and then running sigmoid function  layer1 = sigmoid(np.dot(layer0, syn0))  #Do the same with l1 and its synapsis  layer2 = sigmoid(np.dot(layer1,syn1))</pre><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15409761667283b315f9571><p class=pgc-img-caption></p></div><p>到目前為止，我們已經創建了神經網絡的基本結構：不同的層，神經元之間連接的權重和偏差，以及sigmoid 函數。但這些都沒有解釋神經網絡如何在預測數據集中的模式方面做得如此出色。這就是我們最後一個問題。</p><h1>為什麼神經網絡能夠學習？</h1><p>機器學習算法的主要優勢在於它們每次預測輸出時都能學習和改進。但他們能學到什麼意味著什麼呢？在神經網絡的背景下，它意味著定義神經元之間連接的權重和偏差變得更加精確; 最後，選擇權重和偏差，例如來自網絡的輸出近似於所有訓練輸入的實際值y（x）。</p><p>那麼，為了讓我們知道是否需要繼續尋找更精確的參數，我們如何量化我們的預測與實際值的距離？為了這個目標，我們需要計算一個誤差，或者換句話說，定義一個成本函數（成本函數不是預測網絡正確輸出的誤差;換句話說，這就是預期和預期輸出的差值）。在神經網絡中，最常用的是二次成本函數，也稱為均方誤差，公式定義為：</p><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/15409761814706fc7a9652b><p class=pgc-img-caption></p></div><p>w和b分別表示網絡中的所有權重和偏差。n是訓練輸入的總數。a是輸出，而x是輸入。Σ是所有訓練輸入的總和。</p><p>該函數優於線性誤差，因為在神經網絡中，權重和偏差的微小變化不會使正確輸出的數量發生任何變化; 因此，使用二次函數，其中較大的差值對成本函數的影響比小的差異更有助於確定如何修改這些參數。</p><p>另一方面，我們可以看到，對於所有訓練輸入，我們的成本函數隨著輸出更接近實際值y而變小。我們算法的主要目標是通過找到一組權重和偏差來使這個成本函數最小化，以使其儘可能小。實現這一目標的主要工具是一種名為Gradient Descent的算法。</p><p>那麼，我們應該回答的下一個問題是如何最大限度地降低成本函數。從微積分中，我們知道函數可以具有全局最大值和/或最小值，即函數實現其可以具有的最大值或最小值。我們也知道獲得這一點的一種方法是計算導數。但是，當我們有一個帶有兩個變量的函數時，很容易計算，但在神經網絡的情況下，它們包含許多變量，這使得這個計算很難完成。</p><p>讓我們看一下隨機函數，如下圖：</p><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15409762088904673871bcf><p class=pgc-img-caption></p></div><p>我們可以看到這個函數具有全局最小值。正如我們之前所說，我們可以計算導數以計算最小值的位置，或者我們可以採用另一種方法。我們可以從一個隨機點開始嘗試沿箭頭方向做一個小的移動，我們在數學上說，在x方向上移動Δx，在y方向上移動Δy，並計算我們的函數ΔC的變化。因為方向的變化率是函數的導數，我們可以將函數的變化表示為：</p><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15409762230487ef45279ee><p class=pgc-img-caption></p></div><p>在這裡，我們將從函數梯度的微積分中定義：</p><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/15409762352203e60534dda><p class=pgc-img-caption></p></div><p>函數的梯度：具有偏導數的向量</p><p>現在，我們可以將函數中的更改重寫為：</p><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15409762484895184eb509f><p class=pgc-img-caption></p></div><p>C的梯度將函數C的變化與（x，y）的變化聯繫起來</p><p>現在，我們可以看到當我們選擇參數的某個變化時，成本函數會發生什麼。我們選擇向任何方向移動的數量稱為學習率，它定義了我們向全局最小化移動的速度。如果我們選擇一個非常小的數字，我們需要做出太多的動作來達到這一點; 但是，如果我們選擇一個非常大的數字，我們就有可能超越這一點而永遠無法達到它。所以挑戰在於選擇足夠小的學習率。選擇學習率後，我們可以更新我們的權重和偏見，並採取另一種行動; 我們在每次迭代中重複的過程。</p><p>因此，簡而言之，梯度下降通過重複計算梯度∇C，然後更新權重和偏差，並試圖找到最小化值來實現。這就是神經網絡學習的方式。</p><p>有時候，計算梯度可能非常複雜。然而，有一種方法可以加速這種計算，叫做隨機梯度下降法。這是通過估算梯度∇C通過計算梯度而不是隨機選擇的小樣本訓練的輸入。然後，將這些小樣本平均起來，就能很好地估計出真實的梯度，加速梯度下降，從而學習得更快。</p><p>但是我們如何計算成本函數的梯度呢？這是另一個算法的地方：反向傳播。該算法的目標是計算關於任何權重w和任何偏差b的成本函數的偏導數;實際上，這意味著計算從最終層開始的誤差矢量，然後將其傳播回以更新權重和偏差。我們需要回去的原因是成本是網絡輸出的函數。我們可以觀察公式。</p><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1540976265316aeeb8cca50><p class=pgc-img-caption></p></div><p>反向傳播算法給出的四個基本公式，可用於實現神經網絡</p><p>反向傳播算法僅針對一個訓練示例計算成本函數的梯度。因此，我們需要將反向傳播與學習算法相結合，例如隨機梯度下降，以便計算所有訓練集的梯度。</p><p>現在，我們如何將它應用於python中的神經網絡？在這裡，我們可以逐步看到計算結果：</p><pre>#Compute the error by checking how far the prediction  #is from the real value. l2_error = y - l2  #multiply error rate by result of sigmoide on l2 to get derivative #from output #Delta will be use to reduce error rate of prediction when update syn   #FORMULA 1 l2_delta = l2_error*sigmoid(l2, deriv=True)  #How much l1 contributed to error in l2. Multiply #layer2_delta with syn1 transpose.  l1_error = l2_delta.dot(syn1.T)  #get delta for l1  l1_delta = l1_error * sigmoid(l1, deriv=True)  #Update our synapse rates to reduce the error rate every iteration #Multiply each layer by a delta  #*BACKPROPAGATION* syn1 += l1.T.dot(l2_delta) syn0 += l0.T.dot(l1_delta)</pre><div class=pgc-img><img alt=理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1540976296429c7738d4703><p class=pgc-img-caption></p></div><h1>最後</h1><p>現在我們可以將我們在算法方面看到的所有這些公式和概念放在一起，看看我們如何實現的：</p><ul><li>INPUT：我們輸入一組訓練樣例，並設置對應於輸入層的激活a 。</li><li>FEEDFORWARD：對於每一層，我們計算函數z = w.a + b，a =σ（z）</li><li>輸出誤差：我們使用上面引用的公式＃1計算輸出誤差。</li><li>反向傳播：現在我們反向傳播誤差; 對於每一層，我們計算上面引用的公式＃2。</li><li>輸出：我們使用公式＃3和＃4計算相對於任何權重和偏差的梯度下降。</li></ul><p>當然，還有更多的概念、實現和改進，可以對神經網絡進行改進，這些神經網絡已經在過去的幾年裡越來越廣泛地被使用。但我希望本文能告訴你什麼是神經網絡，它是如何工作的，以及它是如何利用梯度下降和反向傳播學習的。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>神經</a></li><li><a>網絡</a></li><li><a>理解</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html alt=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/65c4000bda98898dcdbb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html title=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼>谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2bc1496a.html alt=為了更好的深度神經網絡視覺，只需添加反饋（循環） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/17fccfd7096d44eeb3921bbd0dc29a13 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2bc1496a.html title=為了更好的深度神經網絡視覺，只需添加反饋（循環）>為了更好的深度神經網絡視覺，只需添加反饋（循環）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fd4c22a3.html alt=你還不知道神經網絡是啥？十分鐘教你跟上人工智能熱潮 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/470f0001d893b2ad09e2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fd4c22a3.html title=你還不知道神經網絡是啥？十分鐘教你跟上人工智能熱潮>你還不知道神經網絡是啥？十分鐘教你跟上人工智能熱潮</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5d2a6211.html alt=神經網絡與圖靈機的複雜度博弈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/4af200040ff1f5233c1c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5d2a6211.html title=神經網絡與圖靈機的複雜度博弈>神經網絡與圖靈機的複雜度博弈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cc9d1be9.html alt=基於二維材料、用於人工神經網絡的高密度憶阻陣列的晶圓級集成 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/3c0b503678da4b15be05f6f56c0d213f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cc9d1be9.html title=基於二維材料、用於人工神經網絡的高密度憶阻陣列的晶圓級集成>基於二維材料、用於人工神經網絡的高密度憶阻陣列的晶圓級集成</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/6062a4c0.html alt=BP神經網絡的線性本質的理解和剖析-卷積小白的隨機世界 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/6d474536ff3d4b1fba0cbfc85968ff6f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/6062a4c0.html title=BP神經網絡的線性本質的理解和剖析-卷積小白的隨機世界>BP神經網絡的線性本質的理解和剖析-卷積小白的隨機世界</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/f149efd9.html alt=用於調整深度神經網絡的簡單參考指南 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/15379529924702cde52ac04 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/f149efd9.html title=用於調整深度神經網絡的簡單參考指南>用於調整深度神經網絡的簡單參考指南</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0396dba3.html alt=貝葉斯神經網絡(系列)：第二篇 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RKYlnth9DPo8ac style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0396dba3.html title=貝葉斯神經網絡(系列)：第二篇>貝葉斯神經網絡(系列)：第二篇</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a4bbdd29.html alt=針對深度神經網絡的簡單黑盒對抗攻擊 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/b9ec712cd33442338496141ebfcecb45 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a4bbdd29.html title=針對深度神經網絡的簡單黑盒對抗攻擊>針對深度神經網絡的簡單黑盒對抗攻擊</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cafcc06.html alt=模式識別與神經網絡的發展 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1523254283784d3d276a90f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cafcc06.html title=模式識別與神經網絡的發展>模式識別與神經網絡的發展</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fcf9e89.html alt=BP神經網絡學習筆記 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/fc5cec456c184c48b1ee22a233b9ee0b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fcf9e89.html title=BP神經網絡學習筆記>BP神經網絡學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d7196c1.html alt=手工打造神經網絡：透視分析 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6ee200033390f3f6b2ca style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d7196c1.html title=手工打造神經網絡：透視分析>手工打造神經網絡：透視分析</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9f3924a.html alt=機器學習：神經網絡學習之多層前饋神經網絡（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a0a4cd0f7d9244a6a12da3c0af6893a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9f3924a.html title=機器學習：神經網絡學習之多層前饋神經網絡（一）>機器學習：神經網絡學習之多層前饋神經網絡（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/556321d.html alt=機器學習：神經網絡學習之多層前饋神經網絡（二） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/2d53a815-ab09-4da3-94a2-5b6843366e3a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/556321d.html title=機器學習：神經網絡學習之多層前饋神經網絡（二）>機器學習：神經網絡學習之多層前饋神經網絡（二）</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/f3732f4.html alt=一文幫你梳理清楚深度神經網絡的基礎知識！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2f16bcb220e14085a04994454ea4998a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/f3732f4.html title=一文幫你梳理清楚深度神經網絡的基礎知識！>一文幫你梳理清楚深度神經網絡的基礎知識！</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>