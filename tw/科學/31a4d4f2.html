<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>吳恩達深度學習筆記(38)-優化算法(Optimization algorithms) | 极客快訊</title><meta property="og:title" content="吳恩達深度學習筆記(38)-優化算法(Optimization algorithms) - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p9.pstatp.com/large/pgc-image/c34f86938ddb484298ca8e0a20494ff2"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/31a4d4f2.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/31a4d4f2.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/31a4d4f2.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/31a4d4f2.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/31a4d4f2.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/31a4d4f2.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/31a4d4f2.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/31a4d4f2.html><meta property="article:published_time" content="2020-11-14T20:52:19+08:00"><meta property="article:modified_time" content="2020-11-14T20:52:19+08:00"><meta name=Keywords content><meta name=description content="吳恩達深度學習筆記(38)-優化算法(Optimization algorithms)"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/31a4d4f2.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>吳恩達深度學習筆記(38)-優化算法(Optimization algorithms)</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><div><h1>Mini-batch 梯度下降（Mini-batch gradient descent）</h1><p>本週將學習優化算法，這能讓你的神經網絡運行得更快。</p><p>機器學習的應用是一個高度依賴經驗的過程，伴隨著大量迭代的過程，你需要訓練諸多模型，才能找到合適的那一個，所以，優化算法能夠幫助你快速訓練模型。</p><p>其中一個難點在於，深度學習沒有在大數據領域發揮最大的效果，我們可以利用一個巨大的數據集來訓練神經網絡，而在巨大的數據集基礎上進行訓練速度很慢。</p><p>因此，你會發現，使用快速的優化算法，使用好用的優化算法能夠大大提高你和團隊的效率，那麼，我們首先來談談mini-batch梯度下降法。</p><div class=pgc-img><img alt="吳恩達深度學習筆記(38)-優化算法(Optimization algorithms)" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/c34f86938ddb484298ca8e0a20494ff2><p class=pgc-img-caption></p></div><p>你之前學過，<strong>向量化能夠讓你有效地對所有m個樣本進行計算</strong>，允許你處理整個訓練集，而無需某個明確的公式。</p><p>所以我們要把訓練樣本放大巨大的矩陣X當中去，X=[x^((1) ) x^((2) ) x^((3) )……x^((m) ) ]。</p><p>Y也是如此，Y=[y^((1) ) y^((2) ) y^((3) )……y^((m) ) ]。</p><p><strong>所以X的維數是(n_x,m)，Y的維數是(1,m)，向量化能夠讓你相對較快地處理所有m個樣本。</strong>如果m很大的話，處理速度仍然緩慢。比如說，如果m是500萬或5000萬或者更大的一個數，在對整個訓練集執行梯度下降法時，你要做的是，你必須處理整個訓練集，然後才能進行一步梯度下降法，然後你需要再重新處理500萬個訓練樣本，才能進行下一步梯度下降法。所以如果你在處理完整個500萬個樣本的訓練集之前，先讓梯度下降法處理一部分，你的算法速度會更快，準確地說，這是你可以做的一些事情。</p><p><strong>你可以把訓練集分割為小一點的子集訓練，這些子集被取名為mini-batch，假設每一個子集中只有1000個樣本，那麼把其中的x^((1))到x^((1000))取出來，將其稱為第一個子訓練集，也叫做mini-batch，然後你再取出接下來的1000個樣本，從x^((1001))到x^((2000))，然後再取1000個樣本，以此類推。</strong></p><p>接下來我要說一個新的符號，把x^((1))到x^((1000))稱為X^({1})，x^((1001))到x^((2000))稱為X^({2})，如果你的訓練樣本一共有500萬個，每個mini-batch都有1000個樣本，也就是說，你有5000個mini-batch，因為5000乘以1000就是5000萬。</p><div class=pgc-img><img alt="吳恩達深度學習筆記(38)-優化算法(Optimization algorithms)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a799826844904c5e89d95cc20ee17605><p class=pgc-img-caption></p></div><p>你共有5000個mini-batch，所以最後得到是X^{5000}</p><div class=pgc-img><img alt="吳恩達深度學習筆記(38)-優化算法(Optimization algorithms)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2e868eccce2a48ed81ff615c6d5ebf7b><p class=pgc-img-caption></p></div><p>對Y也要進行相同處理，你也要相應地拆分Y的訓練集，所以這是Y^({1})，然後從y^((1001))到y^((2000))，這個叫Y^({2})，一直到Y^({5000})。</p><div class=pgc-img><img alt="吳恩達深度學習筆記(38)-優化算法(Optimization algorithms)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a573a038f5994ebc85aaa772414ff4e5><p class=pgc-img-caption></p></div><p>mini-batch的數量t組成了X^({t})和Y^({t})，這就是1000個訓練樣本，包含相應的輸入輸出對。</p><div class=pgc-img><img alt="吳恩達深度學習筆記(38)-優化算法(Optimization algorithms)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/16b8fd34f2814701b1be953710692674><p class=pgc-img-caption></p></div><p>在繼續課程之前，先確定一下我的符號，之前我們使用了上角小括號(i)表示訓練集裡的值，所以x^((i))是第i個訓練樣本。我們用了上角中括號[l]來表示神經網絡的層數，z^([l])表示神經網絡中第l層的z值，我們現在引入了大括號t來代表不同的mini-batch，所以我們有X^({t})和Y^({t})，檢查一下自己是否理解無誤。</p><div class=pgc-img><img alt="吳恩達深度學習筆記(38)-優化算法(Optimization algorithms)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ec006c67640b41cf943ac224f94e4e6b><p class=pgc-img-caption></p></div><p>X^({t})和Y^({t})的維數：如果X^({1})是一個有1000個樣本的訓練集，或者說是1000個樣本的x值，所以維數應該是(n_x,1000)，X^({2})的維數應該是(n_x,1000)，以此類推。因此所有的子集維數都是(n_x,1000)，而這些（Y^({t})）的維數都是(1,1000)。</p><p><strong>解釋一下這個算法的名稱，batch梯度下降法指的是我們之前講過的梯度下降法算法，就是同時處理整個訓練集，這個名字就是來源於能夠同時看到整個batch訓練集的樣本被處理，這個名字不怎麼樣，但就是這樣叫它。</strong></p><p>相比之下，mini-batch梯度下降法，指的是我們在下一張幻燈片中會講到的算法，你每次同時處理的單個的mini-batch X^({t})和Y^({t})，而不是同時處理全部的X和Y訓練集。</p><h1>那麼究竟mini-batch梯度下降法的原理是什麼？</h1><p>在訓練集上運行mini-batch梯度下降法，你運行for t=1……5000，因為我們有5000個各有1000個樣本的組，在for循環裡你要做得基本就是對X^({t})和Y^({t})執行一步梯度下降法。假設你有一個擁有1000個樣本的訓練集，而且假設你已經很熟悉一次性處理完的方法，你要用向量化去幾乎同時處理1000個樣本。</p><div class=pgc-img><img alt="吳恩達深度學習筆記(38)-優化算法(Optimization algorithms)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8e61b834729444eba0f96a1e8e6ccdea><p class=pgc-img-caption></p></div><p>首先對輸入也就是X^({t})，執行前向傳播，然後執行z^([1])=W^([1]) X+b^([1])，之前我們這裡只有，但是現在你正在處理整個訓練集，你在處理第一個mini-batch，在處理mini-batch時它變成了X^({t})，即z^([1])=W^([1]) X^({t})+b^([1])，然後執行A^([1]k)=g^([1]) (Z^([1]))，之所以用大寫的Z是因為這是一個向量內涵，以此類推，直到A^([L])=g^[L] (Z^([L]))，這就是你的預測值。</p><p>注意這裡你需要用到一個向量化的執行命令，這個向量化的執行命令，一次性處理1000個而不是500萬個樣本。</p><p>接下來你要計算損失成本函數J，因為子集規模是1000，J=1/1000</p><div class=pgc-img><img alt="吳恩達深度學習筆記(38)-優化算法(Optimization algorithms)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7b2e1469a982444a900a817371bc1250><p class=pgc-img-caption></p></div><p>，說明一下，這（L(^y^((i)),y^((i)))）指的是來自於mini-batchX^({t})和Y^({t})中的樣本。</p><p>如果你用到了正則化，你也可以使用正則化的術語:</p><div class=pgc-img><img alt="吳恩達深度學習筆記(38)-優化算法(Optimization algorithms)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4eb64e0f406946ad8d93b935c3e318f8><p class=pgc-img-caption></p></div><p>，因為這是一個mini-batch的損失，所以我將J損失記為上角標t，放在大括號裡</p><div class=pgc-img><img alt="吳恩達深度學習筆記(38)-優化算法(Optimization algorithms)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4f57465599c947b3a91942e689e3eeff><p class=pgc-img-caption></p></div><p>你也會注意到，我們做的一切似曾相識，其實跟之前我們執行梯度下降法如出一轍，除了你現在的對象不是X，Y，而是X^({t})和Y^({t})。接下來，你執行反向傳播來計算J^({t})的梯度，你只是使用X^({t})和Y^({t})，然後你更新加權值，</p><p>W實際上是W^([l])，更新為W^([l]):=W^([l])-adW^([l])，</p><p>對b做相同處理，b^([l]):=b^([l])-adb^([l])。</p><div class=pgc-img><img alt="吳恩達深度學習筆記(38)-優化算法(Optimization algorithms)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d6b470cc3e7c440ca2fc539b84fe45b6><p class=pgc-img-caption></p></div><p>這是使用mini-batch梯度下降法訓練樣本的一步，我寫下的代碼也可被稱為進行“一代”（1 epoch）的訓練。一代這個詞意味著只是一次遍歷了訓練集。</p><div class=pgc-img><img alt="吳恩達深度學習筆記(38)-優化算法(Optimization algorithms)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/638c7aaf8c4e4ca4acda40d74cdf662c><p class=pgc-img-caption></p></div><p>使用batch梯度下降法，一次遍歷訓練集只能讓你做一個梯度下降，使用mini-batch梯度下降法，一次遍歷訓練集，能讓你做5000個梯度下降。</p><p>當然正常來說你想要多次遍歷訓練集，還需要為另一個while循環設置另一個for循環。所以你可以一直處理遍歷訓練集，直到最後你能收斂到一個合適的精度。</p><p>如果你有一個丟失的訓練集，<strong>mini-batch梯度下降法比batch梯度下降法運行地更快</strong>，</p><p>所以幾乎每個研習深度學習的人在訓練巨大的數據集時都會用到，下一個筆記中，我們將進一步深度討論mini-batch梯度下降法，你也會因此更好地理解它的作用和原理。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>吳恩達</a></li><li><a>學習</a></li><li><a>筆記</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/f564827a.html alt="吳恩達深度學習筆記(108)-序列模型介紹(Sequence Models)" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/dfa0587764f14afb8ccfd7a33796cd74 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f564827a.html title="吳恩達深度學習筆記(108)-序列模型介紹(Sequence Models)">吳恩達深度學習筆記(108)-序列模型介紹(Sequence Models)</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/efabb5bb.html alt=吳恩達深度學習筆記(106)-風格遷移網絡講解 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/01def15aa8654252990f709965d5b769 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/efabb5bb.html title=吳恩達深度學習筆記(106)-風格遷移網絡講解>吳恩達深度學習筆記(106)-風格遷移網絡講解</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3605c009.html alt="吳恩達深度學習筆記(67)-遷移學習（Transfer learning)" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/f00ccb946c7a451aa25adba4b28799e5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3605c009.html title="吳恩達深度學習筆記(67)-遷移學習（Transfer learning)">吳恩達深度學習筆記(67)-遷移學習（Transfer learning)</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9819f021.html alt="吳恩達深度學習筆記(89)-遷移學習（Transfer Learning）" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d934f6fbf66144dca95cbb6ce8223d62 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9819f021.html title="吳恩達深度學習筆記(89)-遷移學習（Transfer Learning）">吳恩達深度學習筆記(89)-遷移學習（Transfer Learning）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/72a9b3c.html alt=吳恩達深度學習筆記(28)-網絡訓練驗證測試數據集的組成介紹 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/b64cd54d8477428589d89e078a3e37b4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/72a9b3c.html title=吳恩達深度學習筆記(28)-網絡訓練驗證測試數據集的組成介紹>吳恩達深度學習筆記(28)-網絡訓練驗證測試數據集的組成介紹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2b4ff01.html alt="吳恩達深度學習筆記(132) | 序列模型 | 改進集束搜索" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/62cda93c07d14841930e00586835064e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2b4ff01.html title="吳恩達深度學習筆記(132) | 序列模型 | 改進集束搜索">吳恩達深度學習筆記(132) | 序列模型 | 改進集束搜索</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html alt=HTMLCSS學習筆記（六）——元素類型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb5988349894ce9bf568c6418f85b7d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html title=HTMLCSS學習筆記（六）——元素類型>HTMLCSS學習筆記（六）——元素類型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html alt="MySQL 學習筆記" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html title="MySQL 學習筆記">MySQL 學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/112d1b5f.html alt=一造學習筆記—管理篇（2）：工程造價管理的組織和內容 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/9e65b076-038f-4720-96ff-182898f42dee style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/112d1b5f.html title=一造學習筆記—管理篇（2）：工程造價管理的組織和內容>一造學習筆記—管理篇（2）：工程造價管理的組織和內容</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0a02618e.html alt=某教程學習筆記（一）：17、php漏洞 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/6d21bdb33b0a49e8b6eaa2c2a725a1d8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0a02618e.html title=某教程學習筆記（一）：17、php漏洞>某教程學習筆記（一）：17、php漏洞</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b37aac89.html alt=【學習筆記】Android開發之kotlin語言（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ee3f9c8348ae4de58a5f62922d2042e1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b37aac89.html title=【學習筆記】Android開發之kotlin語言（一）>【學習筆記】Android開發之kotlin語言（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b33d00d0.html alt=CV學習筆記（十二）：圖像金字塔 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/74c73f94d57a4f5d8f2e5183e78c8e67 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b33d00d0.html title=CV學習筆記（十二）：圖像金字塔>CV學習筆記（十二）：圖像金字塔</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7f69ae29.html alt="Python 3 學習筆記：Excel 基礎操作（二）" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a777a023a91a465fb317de8de7e4cd37 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7f69ae29.html title="Python 3 學習筆記：Excel 基礎操作（二）">Python 3 學習筆記：Excel 基礎操作（二）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b75ffe84.html alt=學習《建築材料第9節》的筆記 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b75ffe84.html title=學習《建築材料第9節》的筆記>學習《建築材料第9節》的筆記</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>