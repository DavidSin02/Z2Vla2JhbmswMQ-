<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>22道機器學習常見面試題目 | 极客快訊</title><meta property="og:title" content="22道機器學習常見面試題目 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/e44a08e54ad447c6a79b6fa817970b98"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/3f659b8.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/3f659b8.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/3f659b8.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/3f659b8.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/3f659b8.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/3f659b8.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/3f659b8.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/3f659b8.html><meta property="article:published_time" content="2020-10-29T20:56:37+08:00"><meta property="article:modified_time" content="2020-10-29T20:56:37+08:00"><meta name=Keywords content><meta name=description content="22道機器學習常見面試題目"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/3f659b8.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>22道機器學習常見面試題目</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e44a08e54ad447c6a79b6fa817970b98><p class=pgc-img-caption></p></div><p>來源：機器學習算法與自然語言處理</p><p>本文共<strong>6600字</strong>，建議閱讀<strong>13分鐘。</strong></p><p>本文為你帶來22道機器學習常見的面試問題和回答。</p><p><strong>1、無監督和有監督算法的區別？</strong></p><p>有監督學習：對具有概念標記（分類）的訓練樣本進行學習，以儘可能對訓練樣本集外的數據進行標記（分類）預測。這裡，所有的標記（分類）是已知的。因此，訓練樣本的岐義性低。</p><p>無監督學習：對沒有概念標記（分類）的訓練樣本進行學習，以發現訓練樣本集中的結構性知識。這裡，所有的標記（分類）是未知的。因此，訓練樣本的岐義性高。聚類就是典型的無監督學習。</p><p><strong>2、SVM 的推導，特性？多分類怎麼處理？</strong></p><p>SVM是最大間隔分類器，幾何間隔和樣本的誤分次數之間存在關係，</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0b73fa02de4b4c8698fdab35ad08e811><p class=pgc-img-caption></p></div><p>，其中</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/afaef4ce90a74b68a9548b82164bc3ee><p class=pgc-img-caption></p></div><p>從線性可分情況下，原問題，特徵轉換後的dual問題，引入kernel(線性kernel，多項式，高斯)，最後是soft margin。</p><p>線性：簡單，速度快，但是需要線性可分。</p><p>多項式：比線性核擬合程度更強，知道具體的維度，但是高次容易出現數值不穩定，參數選擇比較多。</p><p>高斯：擬合能力最強，但是要注意過擬合問題。不過只有一個參數需要調整。</p><p>多分類問題，一般將二分類推廣到多分類的方式有三種，一對一，一對多，多對多。</p><p>一對一：將N個類別兩兩配對，產生N(N-1)/2個二分類任務，測試階段新樣本同時交給所有的分類器，最終結果通過投票產生。</p><p>一對多：每一次將一個例作為正例，其他的作為反例，訓練N個分類器，測試時如果只有一個分類器預測為正類，則對應類別為最終結果，如果有多個，則一般選擇置信度最大的。從分類器角度一對一更多，但是每一次都只用了2個類別，因此當類別數很多的時候一對一開銷通常更小(只要訓練複雜度高於O(N)即可得到此結果)。</p><p>多對多：若干各類作為正類，若干個類作為反類。注意正反類必須特殊的設計。</p><p><strong>3、LR 的推導，特性？</strong></p><p>LR的優點在於實現簡單，並且計算量非常小，速度很快，存儲資源低，缺點就是因為模型簡單，對於複雜的情況下會出現欠擬合，並且只能處理2分類問題(可以通過一般的二元轉換為多元或者用softmax迴歸)。</p><p><strong>4、決策樹的特性？</strong></p><p>決策樹基於樹結構進行決策，與人類在面臨問題的時候處理機制十分類似。其特點在於需要選擇一個屬性進行分支，在分支的過程中選擇信息增益最大的屬性，定義如下　</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f28670153f114fc5aa8f29e60b041763><p class=pgc-img-caption></p></div><p><br>在劃分中我們希望決策樹的分支節點所包含的樣本屬於同一類別，即節點的純度越來越高。決策樹計算量簡單，可解釋性強，比較適合處理有缺失屬性值的樣本，能夠處理不相關的特徵，但是容易過擬合，需要使用剪枝或者隨機森林。信息增益是熵減去條件熵，代表信息不確定性較少的程度，信息增益越大，說明不確定性降低的越大，因此說明該特徵對分類來說很重要。由於信息增益準則會對數目較多的屬性有所偏好，因此一般用信息增益率(c4.5)</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ad064ecd98d34af68448290b607cb39e><p class=pgc-img-caption></p></div><p><br>其中分母可以看作為屬性自身的熵。取值可能性越多，屬性的熵越大。</p><p>Cart決策樹使用基尼指數來選擇劃分屬性，直觀的來說，Gini(D)反映了從數據集D中隨機抽取兩個樣本，其類別標記不一致的概率，因此基尼指數越小數據集D的純度越高，一般為了防止過擬合要進行剪枝，有預剪枝和後剪枝，一般用cross validation集進行剪枝。</p><p>連續值和缺失值的處理，對於連續屬性a，將a在D上出現的不同的取值進行排序，基於劃分點t將D分為兩個子集。一般對每一個連續的兩個取值的中點作為劃分點，然後根據信息增益選擇最大的。與離散屬性不同，若當前節點劃分屬性為連續屬性，該屬性還可以作為其後代的劃分屬性。</p><p><strong>5、SVM、LR、決策樹的對比？</strong></p><p>SVM既可以用於分類問題，也可以用於迴歸問題，並且可以通過核函數快速的計算，LR實現簡單，訓練速度非常快，但是模型較為簡單，決策樹容易過擬合，需要進行剪枝等。從優化函數上看，soft margin的SVM用的是hinge loss，而帶L2正則化的LR對應的是cross entropy loss，另外adaboost對應的是exponential loss。所以LR對遠點敏感，但是SVM對outlier不太敏感，因為只關心support vector，SVM可以將特徵映射到無窮維空間，但是LR不可以，一般小數據中SVM比LR更優一點，但是LR可以預測概率，而SVM不可以，SVM依賴於數據測度，需要先做歸一化，LR一般不需要，對於大量的數據LR使用更加廣泛，LR向多分類的擴展更加直接，對於類別不平衡SVM一般用權重解決，即目標函數中對正負樣本代價函數不同，LR可以用一般的方法，也可以直接對最後結果調整(通過閾值)，一般小數據下樣本維度比較高的時候SVM效果要更優一些。</p><p><strong>6、GBDT 和隨機森林的區別？</strong></p><p>隨機森林採用的是bagging的思想，bagging又稱為bootstrap aggreagation，通過在訓練樣本集中進行有放回的採樣得到多個採樣集，基於每個採樣集訓練出一個基學習器，再將基學習器結合。隨機森林在對決策樹進行bagging的基礎上，在決策樹的訓練過程中引入了隨機屬性選擇。傳統決策樹在選擇劃分屬性的時候是在當前節點屬性集合中選擇最優屬性，而隨機森林則是對結點先隨機選擇包含k個屬性的子集，再選擇最有屬性，k作為一個參數控制了隨機性的引入程度。</p><p>另外，GBDT訓練是基於Boosting思想，每一迭代中根據錯誤更新樣本權重，因此是串行生成的序列化方法，而隨機森林是bagging的思想，因此是並行化方法。</p><p><strong>7、如何判斷函數凸或非凸？什麼是凸優化<br></strong><br></p><p>首先定義凸集，如果x，y屬於某個集合C，並且所有的</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b8214ac688b84d94bfce61b709cc4081><p class=pgc-img-caption></p></div><p>也屬於c，那麼c為一個凸集，進一步，如果一個函數其定義域是凸集，並且<br></p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d78910b73a68414b980dccea7a226f9f><p class=pgc-img-caption></p></div><p><br>則該函數為凸函數。上述條件還能推出更一般的結果，</p><p><br></p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a6cd433c71784fd5ae406eda0793cc42><p class=pgc-img-caption></p></div><p><br></p><p>如果函數有二階導數，那麼如果函數二階導數為正，或者對於多元函數，Hessian矩陣半正定則為凸函數。</p><p>(也可能引到SVM，或者凸函數局部最優也是全局最優的證明，或者上述公式期望情況下的Jessen不等式)</p><p><strong>8、如何解決類別不平衡問題？<br></strong><br></p><p>有些情況下訓練集中的樣本分佈很不平衡，例如在腫瘤檢測等問題中，正樣本的個數往往非常的少。從線性分類器的角度，在用</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cafd63252a294d5d852af55c3d0e3bb8><p class=pgc-img-caption></p></div><p>對新樣本進行分類的時候，事實上在用預測出的y值和一個y值進行比較，例如常常在y>0.5的時候判為正例，否則判為反例。機率</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c56a8b0be27a4f6b9c9e399ad2fd9440><p class=pgc-img-caption></p></div><p>反映了正例可能性和反例可能性的比值，閾值0.5恰好表明分類器認為正反的可能性相同。在樣本不均衡的情況下，應該是分類器的預測機率高於觀測機率就判斷為正例，因此應該是</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/69b559f2dc2d487eaba4ca7d7bb30928><p class=pgc-img-caption></p></div><p>時預測為正例，這種策略稱為rebalancing。但是訓練集並不一定是真實樣本總體的無偏採樣，通常有三種做法，一種是對訓練集的負樣本進行欠採樣，第二種是對正例進行升採樣，第三種是直接基於原始訓練集進行學習，在預測的時候再改變閾值，稱為閾值移動。注意過採樣一般通過對訓練集的正例進行插值產生額外的正例，而欠採樣將反例劃分為不同的集合供不同的學習器使用。</p><p><strong>9、解釋對偶的概念。</strong></p><p>一個優化問題可以從兩個角度進行考察，一個是primal 問題，一個是dual 問題，就是對偶問題，一般情況下對偶問題給出主問題最優值的下界，在強對偶性成立的情況下由對偶問題可以得到主問題的最優下界，對偶問題是凸優化問題，可以進行較好的求解，SVM中就是將primal問題轉換為dual問題進行求解，從而進一步引入核函數的思想。</p><p><strong>10、如何進行特徵選擇？</strong></p><p>特徵選擇是一個重要的數據預處理過程，主要有兩個原因，首先在現實任務中我們會遇到維數災難的問題(樣本密度非常稀疏)，若能從中選擇一部分特徵，那麼這個問題能大大緩解，另外就是去除不相關特徵會降低學習任務的難度，增加模型的泛化能力。冗餘特徵指該特徵包含的信息可以從其他特徵中推演出來，但是這並不代表該冗餘特徵一定沒有作用，例如在欠擬合的情況下也可以用過加入冗餘特徵，增加簡單模型的複雜度。</p><p>在理論上如果沒有任何領域知識作為先驗假設那麼只能遍歷所有可能的子集。但是這顯然是不可能的，因為需要遍歷的數量是組合爆炸的。一般我們分為子集搜索和子集評價兩個過程，子集搜索一般採用貪心算法，每一輪從候選特徵中添加或者刪除，分別成為前向和後先搜索。或者兩者結合的雙向搜索。子集評價一般採用信息增益，對於連續數據往往排序之後選擇中點作為分割點。</p><p>常見的特徵選擇方式有過濾式，包裹式和嵌入式，filter，wrapper和embedding。Filter類型先對數據集進行特徵選擇，再訓練學習器。Wrapper直接把最終學習器的性能作為特徵子集的評價準則，一般通過不斷候選子集，然後利用cross-validation過程更新候選特徵，通常計算量比較大。嵌入式特徵選擇將特徵選擇過程和訓練過程融為了一體，在訓練過程中自動進行了特徵選擇，例如L1正則化更易於獲得稀疏解，而L2正則化更不容易過擬合。L1正則化可以通過PGD，近端梯度下降進行求解。</p><p><strong>11、為什麼會產生過擬合，有哪些方法可以預防或克服過擬合？</strong></p><p>一般在機器學習中，將學習器在訓練集上的誤差稱為訓練誤差或者經驗誤差，在新樣本上的誤差稱為泛化誤差。顯然我們希望得到泛化誤差小的學習器，但是我們事先並不知道新樣本，因此實際上往往努力使經驗誤差最小化。然而，當學習器將訓練樣本學的太好的時候，往往可能把訓練樣本自身的特點當做了潛在樣本具有的一般性質。這樣就會導致泛化性能下降，稱之為過擬合，相反，欠擬合一般指對訓練樣本的一般性質尚未學習好，在訓練集上仍然有較大的誤差。</p><p>欠擬合：一般來說欠擬合更容易解決一些，例如增加模型的複雜度，增加決策樹中的分支，增加神經網絡中的訓練次數等等。</p><p>過擬合：一般認為過擬合是無法徹底避免的，因為機器學習面臨的問題一般是np-hard，但是一個有效的解一定要在多項式內可以工作，所以會犧牲一些泛化能力。過擬合的解決方案一般有增加樣本數量，對樣本進行降維，降低模型複雜度，利用先驗知識(L1，L2正則化)，利用cross-validation，early stopping等等。</p><p><strong>12、什麼是偏差與方差？</strong></p><p>泛化誤差可以分解成偏差的平方加上方差加上噪聲。偏差度量了學習算法的期望預測和真實結果的偏離程度，刻畫了學習算法本身的擬合能力，方差度量了同樣大小的訓練集的變動所導致的學習性能的變化，刻畫了數據擾動所造成的影響，噪聲表達了當前任務上任何學習算法所能達到的期望泛化誤差下界，刻畫了問題本身的難度。偏差和方差一般稱為bias和variance，一般訓練程度越強，偏差越小，方差越大，泛化誤差一般在中間有一個最小值，如果偏差較大，方差較小，此時一般稱為欠擬合，而偏差較小，方差較大稱為過擬合。</p><p>偏差：</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2bf51e88160344b48f052265adaa31f8><p class=pgc-img-caption></p></div><p><br>方差：</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/48249686446442b5b6475b0e5ddb0ea7><p class=pgc-img-caption></p></div><p><br></p><p><strong>13、神經網絡的原理，如何進行訓練？</strong></p><p>神經網絡自發展以來已經是一個非常龐大的學科，一般而言認為神經網絡是由單個的神經元和不同神經元之間的連接構成，不夠的結構構成不同的神經網絡。最常見的神經網絡一般稱為多層前饋神經網絡，除了輸入和輸出層，中間隱藏層的個數被稱為神經網絡的層數。BP算法是訓練神經網絡中最著名的算法，其本質是梯度下降和鏈式法則。</p><p><strong>14、介紹卷積神經網絡，和 DBN 有什麼區別？</strong></p><p>卷積神經網絡的特點是卷積核，CNN中使用了權共享，通過不斷的上採用和卷積得到不同的特徵表示，採樣層又稱為pooling層，基於局部相關性原理進行亞採樣，在減少數據量的同時保持有用的信息。DBN是深度信念網絡，每一層是一個RBM，整個網絡可以視為RBM堆疊得到，通常使用無監督逐層訓練，從第一層開始，每一層利用上一層的輸入進行訓練，等各層訓練結束之後再利用BP算法對整個網絡進行訓練。</p><p><strong>15、採用 EM 算法求解的模型有哪些，為什麼不用牛頓法或梯度下降法？</strong></p><p>用EM算法求解的模型一般有GMM或者協同過濾，k-means其實也屬於EM。EM算法一定會收斂，但是可能收斂到局部最優。由於求和的項數將隨著隱變量的數目指數上升，會給梯度計算帶來麻煩。</p><p><strong>16、用 EM 算法推導解釋 Kmeans。</strong></p><p>k-means算法是高斯混合聚類在混合成分方差相等，且每個樣本僅指派一個混合成分時候的特例。注意k-means在運行之前需要進行歸一化處理，不然可能會因為樣本在某些維度上過大導致距離計算失效。k-means中每個樣本所屬的類就可以看成是一個隱變量，在E步中，我們固定每個類的中心，通過對每一個樣本選擇最近的類優化目標函數，在M步，重新更新每個類的中心點，該步驟可以通過對目標函數求導實現，最終可得新的類中心就是類中樣本的均值。</p><p><strong>17、用過哪些聚類算法，解釋密度聚類算法。</strong></p><p>k-means算法，聚類性能的度量一般分為兩類，一類是聚類結果與某個參考模型比較(外部指標)，另外是直接考察聚類結果(內部指標)。後者通常有DB指數和DI，DB指數是對每個類，找出類內平均距離/類間中心距離最大的類，然後計算上述值，並對所有的類求和，越小越好。類似k-means的算法僅在類中數據構成簇的情況下表現較好，密度聚類算法從樣本密度的角度考察樣本之間的可連接性，並基於可連接樣本不斷擴展聚類蔟得到最終結果。DBSCAN(density-based spatial clustering of applications with noise)是一種著名的密度聚類算法，基於一組鄰域參數</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a46a96a01d41440e96091b10879e1e43><p class=pgc-img-caption></p></div><p>進行刻畫，包括鄰域，核心對象(鄰域內至少包含</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fa8edc78ceb54322bcb46f7caf0c2c86><p class=pgc-img-caption></p></div><p>個對象)，密度直達(j由i密度直達，表示j在i的鄰域內，且i是一個核心對象)，密度可達(j由i密度可達，存在樣本序列使得每一對都密度直達)，密度相連(xi，xj存在k，i，j均有k可達)，先找出樣本中所有的核心對象，然後以任一核心對象作為出發點，找出由其密度可達的樣本生成聚類蔟，直到所有核心對象被訪問過為止。</p><p><strong>18、聚類算法中的距離度量有哪些？</strong></p><p>聚類算法中的距離度量一般用閩科夫斯基距離，在p取不同的值下對應不同的距離，例如p=1的時候對應曼哈頓距離，p=2的情況下對應歐式距離，p=inf的情況下變為切比雪夫距離，還有jaccard距離，冪距離(閩科夫斯基的更一般形式)，餘弦相似度，加權的距離，馬氏距離(類似加權)作為距離度量需要滿足非負性，同一性，對稱性和直遞性，閩科夫斯基在p>=1的時候滿足讀來那個性質，對於一些離散屬性例如{飛機，火車，輪船}則不能直接在屬性值上計算距離，這些稱為無序屬性，可以用VDM(Value Diffrence Metrix)，屬性u上兩個離散值a，b之間的VDM距離定義為</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7ed5915610274912a754df3ff6cde834><p class=pgc-img-caption></p></div><p><br></p><p>其中</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/20e42d62c36c4f278973f6d6e6766382><p class=pgc-img-caption></p></div><p>表示在第i個簇中屬性u上a的樣本數，樣本空間中不同屬性的重要性不同的時候可以採用加權距離，一般如果認為所有屬性重要性相同則要對特徵進行歸一化。一般來說距離需要的是相似性度量，距離越大，相似度越小，用於相似性度量的距離未必一定要滿足距離度量的所有性質，例如直遞性。比如人馬和人，人馬和馬的距離較近，然後人和馬的距離可能就很遠。</p><p><strong>19、解釋貝葉斯公式和樸素貝葉斯分類。</strong></p><p>貝葉斯公式：<br></p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f9efed805eb14c4b8bf3d9a91329e9a7><p class=pgc-img-caption></p></div><p><br>最小化分類錯誤的貝葉斯最優分類器等價於最大化後驗概率。</p><p>基於貝葉斯公式來估計後驗概率的主要困難在於，條件概率</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8a57acac4cf04c81a4d3b99f2e18478d><p class=pgc-img-caption></p></div><p>是所有屬性上的聯合概率，難以從有限的訓練樣本直接估計得到。樸素貝葉斯分類器採用了屬性條件獨立性假設，對於已知的類別，假設所有屬性相互獨立。這樣，樸素貝葉斯分類則定義為</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b5a484ace65447fab778f68352b4c215><p class=pgc-img-caption></p></div><p><br></p><p>如果有足夠多的獨立同分布樣本，那麼</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/adfec73e6b77490597d48adeae15b524><p class=pgc-img-caption></p></div><p>可以根據每個類中的樣本數量直接估計出來。在離散情況下先驗概率可以利用樣本數量估計或者離散情況下根據假設的概率密度函數進行最大似然估計。樸素貝葉斯可以用於同時包含連續變量和離散變量的情況。如果直接基於出現的次數進行估計，會出現一項為0而乘積為0的情況，所以一般會用一些平滑的方法，例如拉普拉斯修正，</p><p>　　　　　　　　　</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3b0c1a10f8c84b1aa54b01bb761d9c3f><p class=pgc-img-caption></p></div><p><br>這樣既可以保證概率的歸一化，同時還能避免上述出現的現象。</p><p><strong>20、解釋L1和L2正則化的作用。</strong></p><p>L1正則化是在代價函數後面加上</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1c82acc8af8e4969a5c0282f1037b7fb><p class=pgc-img-caption></p></div><p>，L2正則化是在代價函數後面增加了</p><div class=pgc-img><img alt=22道機器學習常見面試題目 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/365a7d611c25410384b242342a0e8459><p class=pgc-img-caption></p></div><p>，兩者都起到一定的過擬合作用，兩者都對應一定的先驗知識，L1對應拉普拉斯分佈，L2對應高斯分佈，L1偏向於參數稀疏性，L2偏向於參數分佈較為稠。</p><p><strong>21、TF-IDF是什麼？</strong></p><p>TF指Term frequecy，代表詞頻，IDF代表inverse document frequency，叫做逆文檔頻率，這個算法可以用來提取文檔的關鍵詞，首先一般認為在文章中出現次數較多的詞是關鍵詞，詞頻就代表了這一項，然而有些詞是停用詞，例如的，是，有這種大量出現的詞，首先需要進行過濾，比如過濾之後再統計詞頻出現了中國，蜜蜂，養殖且三個詞的詞頻幾乎一致，但是中國這個詞出現在其他文章的概率比其他兩個詞要高不少，因此我們應該認為後兩個詞更能表現文章的主題，IDF就代表了這樣的信息，計算該值需要一個語料庫，如果一個詞在語料庫中出現的概率越小，那麼該詞的IDF應該越大，一般來說TF計算公式為(某個詞在文章中出現次數/文章的總詞數)，這樣消除長文章中詞出現次數多的影響，IDF計算公式為log(語料庫文章總數/(包含該詞的文章數)+1)。將兩者乘乘起來就得到了詞的TF-IDF。傳統的TF-IDF對詞出現的位置沒有進行考慮，可以針對不同位置賦予不同的權重進行修正，注意這些修正之所以是有效的，正是因為人觀測過了大量的信息，因此建議了一個先驗估計，人將這個先驗估計融合到了算法裡面，所以使算法更加的有效。</p><p><strong>22、文本中的餘弦距離是什麼，有哪些作用？</strong></p><p>餘弦距離是兩個向量的距離的一種度量方式，其值在-1~1之間，如果為1表示兩個向量同相，0表示兩個向量正交，-1表示兩個向量反向。使用TF-IDF和餘弦距離可以尋找內容相似的文章，例如首先用TF-IDF找出兩篇文章的關鍵詞，然後每個文章分別取出k個關鍵詞(10-20個)，統計這些關鍵詞的詞頻，生成兩篇文章的詞頻向量，然後用餘弦距離計算其相似度。</p><p>原文鏈接：</p><p>https://www.cnblogs.com/hanxiaosheng/p/9934237.html</p><p><br></p><p>編輯：於騰凱</p><p>校對：王欣</p><p><strong>— 完 —</strong></p><p>關注清華-青島數據科學研究院官方微信公眾平臺“<strong>THU數據派</strong>”及姊妹號“<strong>數據派THU</strong>”獲取更多講座福利及優質內容。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>22</a></li><li><a>機器</a></li><li><a>學習</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E5%AD%B8/11e7ba2.html alt=22道機器學習常見面試題目彙總！(附詳細答案) class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1ae8da4165c2420186ab28efd33bd080 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/11e7ba2.html title=22道機器學習常見面試題目彙總！(附詳細答案)>22道機器學習常見面試題目彙總！(附詳細答案)</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>