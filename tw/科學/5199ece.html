<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 | 极客快訊</title><meta property="og:title" content="機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/76d31ab63a7249a5abaeec98d8891354"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/5199ece.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/5199ece.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/5199ece.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/5199ece.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/5199ece.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/5199ece.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/5199ece.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/5199ece.html><meta property="article:published_time" content="2020-10-29T20:55:38+08:00"><meta property="article:modified_time" content="2020-10-29T20:55:38+08:00"><meta name=Keywords content><meta name=description content="機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/5199ece.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><div><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/76d31ab63a7249a5abaeec98d8891354><p class=pgc-img-caption></p></div><h1>聚類</h1><p><strong>K最近鄰（KNN）</strong></p><p>KNN找到k個最近鄰，利用它們的標籤進行預測。例如，下面的黑點應該通過簡單多數投票被分類為藍色。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3a88db14084941e6946859ed6d4db59b><p class=pgc-img-caption></p></div><p>有不同的指標來衡量距離。最常見的是歐氏距離(L2)。其他指標還包括曼哈頓距離、餘弦距離、文本編輯距離、距離相關性等(更多的指標可以在scikit中找到)。通常，如果我們在進行預測時增加k的數量，則偏差會增加，而方差降低。</p><p><strong>K均值聚類</strong></p><p>K-均值聚類使用以下算法將數據點分組為K個聚類：</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5fd27ad8b8674e44b6c73be91c79795a><p class=pgc-img-caption></p></div><p>對質心進行重新估計，對數據點進行重新聚類。這個過程不斷地重複，直到收斂為止。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/57217d5fda0d42bfa5f731da21b13b03><p class=pgc-img-caption></p></div><p><strong>凸性</strong></p><p>K均值聚類的成本函數為</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/4cdded96b97d494abd4329b65455554e><p class=pgc-img-caption></p></div><p>其中cᵢ是數據點i的聚類分配，而μⱼ是聚類j的質心。K-均值聚類算法在步驟中改進了cᵢ和μⱼ。因此，成本單調下降。但是，成本函數是非凸的。因此，該算法保證僅達到局部最優。不同的隨機種子可能會導致不同的聚類。但是，通過對質心的初始猜測進行合理的隨機初始化，可以產生高質量的結果。</p><p>我們還可以使用不同的隨機種子重複該過程，並在選擇最佳模型時使用相應的質心的總歐幾里德距離。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/e5a49c20a04843058a726e708d597f10><p class=pgc-img-caption></p></div><p><strong>K值</strong></p><p>那麼K在K均值聚類中的合適值是多少呢?如果我們在樣本中有N個數據點，如果我們使用N個聚類，我們可以訓練模型為零成本。但這不是我們想要的。</p><p><em>K</em>的選擇可以由明確的目標確定。例如，我們要製造尺碼為XS，S，M，L和XL的T恤。因此<em>K</em>為5。我們可以研究成本下降的速度。我們可以在拐點處停下來，成本的進一步下降將有少得多的回報。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/49cd7582884245c487700e53fb8e566c><p class=pgc-img-caption></p></div><p>我們還可以監視訓練數據集和驗證數據集之間的誤差差距。如果驗證訓練數據集中的誤差趨於平坦或增大，則不應進一步增大<em>K。</em></p><p><strong>K-median聚類</strong></p><p>使用中位數，而不是均值聚類避免outliners。 我們還使用L1來測量距離和成本，以減少outliners的影響。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/35265ba1d4864a6d84dd76ec17f7598e><p class=pgc-img-caption></p></div><p><strong>K-均值++聚類</strong></p><p>在K-均值聚類中，我們不是一開始就隨機初始化K個質心，而是一次隨機選擇一個質心。對於下一個質心，我們仍然隨機地選擇它，但是我們更傾向於選擇離前一個平均值更遠的數據點。這是算法</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a5617fb4ea274d8ea3c72839efbfb270><p class=pgc-img-caption></p></div><p><strong>二分K均值(bisecting k-means)</strong></p><p>我們將每個聚類分成兩個，但僅提交一個可以最大程度降低成本的聚類。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/2516769bb7d14f939d16b9135a717dbd><p class=pgc-img-caption></p></div><p><strong>高斯混合模型（GMM）</strong></p><p>高斯混合模型是一種概率模型，它假設所有數據點都是從高斯分佈的混合中生成的。</p><p>對於K＝ 2，我們將具有2個高斯分佈G 1＝（μ1，σ2）和G 2＝（μ2，σ2）。我們從參數μ和σ的隨機初始化開始，然後計算數據點可能屬於哪個聚類的概率。然後，我們根據該概率重新計算每個高斯分佈的參數μ和σ。將數據點重新擬合到不同的聚類，並再次重新計算高斯參數。迭代一直持續到解收斂為止。讓我們使用EM算法來詳細說明此方法。</p><p><strong>期望最大化（EM）算法（GMM）</strong></p><p>EM算法在執行期望估計(E-step)和最大化(M-step)之間交替進行。E-step計算</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/486f7015213c49a6b20857f030320cc4><p class=pgc-img-caption></p></div><p>概率p是使用高斯分佈，通過使用相應聚類的xᵢ與μ之間的距離來計算的。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/672e1a8b0ca249979638f18555761a27><p class=pgc-img-caption></p></div><p>我們可以比較xᵢ的q（aᵢ）和q（bᵢ）並選擇它應該屬於哪個聚類。在所有計算之後，將訓練數據分配給a或b。這稱為hard assignment。然後，我們根據其擁有的數據點為每個聚類重新計算μ和σ。但是，這並不是GMM所做的。我們沒有為一個聚類分配數據點，而是跟蹤概率q（aᵢ）和q（bᵢ），即即數據點i是否屬於a或b的概率。一般而言，我們計算聚類分配的概率分佈而不是點估計。這稱為soft assignment。我們將使用此分佈作為權重，以求xᵢ在計算相應聚類的μ和σ中有多大影響。例如，聚類A的平均值是權重等於q（aᵢ）的所有數據點的加權平均值。概率模型通常具有更平滑的成本函數和較小的曲率。這使得訓練更穩定。通過不將xᵢ確定地分配給一個聚類，我們允許訓練更快地收斂。這是GMM的算法。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dc6398f1d2c44e688cf446b600da0e60><p class=pgc-img-caption></p></div><p>以下是適用於任何混合模型（非高斯模型）的EM算法。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/69b35640c4914c80bd1c651687749928><p class=pgc-img-caption></p></div><p>對於GMM，假定聚類j的數據分佈p（xᵢ|θⱼ）是高斯分佈。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6deea61ac151442791b6cffd716f32e1><p class=pgc-img-caption></p></div><p>為了模擬隨機變量的分佈，我們可以收集樣本來擬合GMM。相應的模型將比簡單的高斯模型更加複雜，並且仍然是密集的(只需要很少的模型參數)。這就是為什麼它在分佈建模中很受歡迎，特別是對於多模態變量。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d149c020c03f4df7a3d32dcffef621ad><p class=pgc-img-caption></p></div><p><strong>自組織映射（SOM）</strong></p><p>SOM為高密度輸入提供了一種低維表示。例如，我們希望計算39種索引顏色來表示圖像的RGB值。我們從39個節點開始，每個節點由一個與輸入維數相同的權向量表示(dim=3)。因此，對於每個節點，它都持有一個RGB值。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1f54089aa6d54dcebb2a4c3c1685b741><p class=pgc-img-caption></p></div><p>直觀地，我們隨機初始化所有節點。 然後我們遍歷訓練數據集中的所有數據點（圖像的每個像素）。 我們將節點closet定位到當前數據點RGB值。 我們將當前節點及其相鄰的RGB值更新為該RGB值。當我們遍歷訓練數據時，節點的值會發生變化，更能代表訓練數據中圖像像素的顏色。這裡是一個概述</p><ul><li>我們從圖像中隨機採樣一個數據點（一個像素）。</li><li>我們使用L2距離找到權重與輸入接近的節點。</li><li>我們更新周圍節點的權重以使其更接近輸入。但是隨著我們遠離中心，變化下降。</li><li>我們採樣下一個數據點並重復迭代。</li><li>最終，每個節點中的權重代表索引顏色的RGB值。</li></ul><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/54a15cee16d14f94ab212d134ba9fb5a><p class=pgc-img-caption></p></div><p>我們使用學習率來控制變化，並且該變化將隨著距離的增加而減少，並且隨著時間的推移也會減少。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a20d882626914ce6884604f96997f2bc><p class=pgc-img-caption></p></div><p>基於密度的聚類（DBSCAN）</p><p>K均值聚類無法發現流形。基於密度的聚類將相鄰的高密度點連接在一起形成一個聚類。直觀地說，我們逐漸向聚類添加鄰近的點。這種方法允許結構在發現相鄰以及流形時緩慢增長。如下所示，DBSCAN可以發現K均值無法發現的U形結構。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7fbc304411484402a1b9f7d13bd36f4e><p class=pgc-img-caption></p></div><p>若半徑r內有m個可達點，則數據點為core點。core點(深綠色)與相鄰的core點(淺綠色)連接形成聚類。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1ceff107bb12411d9679eeed70d78b59><p class=pgc-img-caption></p></div><p>如果我們有很多數據點，那麼計算一個數據點到另一個數據點的距離就很耗時。而是將數據點劃分為多個區域。我們僅連接位於相同或相鄰區域中的點。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3f5b50acb4fb42d1bbdf95d064d32d18><p class=pgc-img-caption></p></div><p><strong>基於密度的層次聚類</strong></p><p>基於密度的聚類最困難的部分是確定半徑<em>r</em>。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d674321650d64e999f4be59fcffa851b><p class=pgc-img-caption></p></div><p>我們可以使用自上而下的層次結構方法將大型聚類分解為多個子聚類。因此，我們可以從大的r開始，然後在分解層次結構中的聚類時逐漸減小它。</p><p><strong>層次聚類</strong></p><p>還有許多其他的層次聚類方法，包括自頂向下或自下而上。這些是算法</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8d9fb2b5968045fd8b547b13472effcb><p class=pgc-img-caption></p></div><p><strong>集成聚類（UB聚類）</strong></p><p>集成聚類使用不同的隨機種子運行m次K均值以生成m個模型。如果大多數模型都同意兩個點應屬於同一類，則可以確保它們屬於同一類。集成聚類從沒有聚類開始，</p><ul><li>如果兩個點應該在一起，但是沒有任何聚類分配，我們將為它們創建一個新的聚類。</li><li>如果分配了其中一個而沒有分配另一個，我們將未分配的放到已分配的聚類中。</li><li>如果將它們分配給不同的聚類，我們會將兩個聚類合併在一起。</li></ul><p><strong>Canopy聚類</strong></p><p>聚類是昂貴的。我們可以先應用Canopy聚類來形成某種形式的聚類，然後再使用更昂貴的方法來改進它。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/acbc50982faf4cad83eab33c60d314b9><p class=pgc-img-caption></p></div><h1>TF-IDF</h1><p>TF-IDF對搜索查詢中文檔的相關性進行評分。為了避免搜索詞被利用，如果搜索詞在文檔中普遍存在，則得分會下降。因此，如果這個詞不常見，它的得分會更高。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9fae05bc71104acd8baf2274c8e59bba><p class=pgc-img-caption></p></div><h1>決策樹</h1><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/29f1b4c7397348bfa6329464bb3720f3><p class=pgc-img-caption></p></div><p>從技術上講，我們將在每個決策樹樁上將輸入分解到兩個空間</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/06674a965472475699fc6e193bf710ce><p class=pgc-img-caption></p></div><p>決策樹也稱為分類和迴歸樹(CART)。決策樹易於解釋，易於準備數據，推理速度快。由於採用貪心法選擇決策殘差，且分支條件簡單，模型可能精度不高，方差較大。過度擬合也是決策樹中的一個常見問題。</p><p>為了進行迴歸，我們在特定閾值處拆分了特定特徵的數據。我們通過蠻力嘗試不同的特徵和閾值組合，並選擇最貪婪地減少L2誤差的特徵和閾值。樹葉處的預測將是該分支中剩餘數據點的平均值或中位數。</p><p>在分類決策樹中，我們可以使用不同的標準來選擇決策樹樁。例如，我們可以計算分類誤差，基尼係數或熵（稍後詳細說明）。樹預測將是分支數據集的多數表決。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4916e5a7763a4695a0d9a2f2fa71b261><p class=pgc-img-caption></p></div><p>在選擇決策樹樁之前，我們可以創建一個散點圖矩陣來發現屬性之間的相關性，從而使我們對如何拆分樹有一些見解。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e6a7f536e62d457c9fa0c169f911d0b4><p class=pgc-img-caption></p></div><p><strong>基尼係數</strong></p><p>如果我們知道90％的數據屬於第i類，其餘的10％屬於第j類，則可以使用相同的比率對標籤進行隨機猜測。實際上，我們可以做得很好。基尼係數（Gini index）衡量了我們使用這種方案做出預測的可能性有多大。如果數據很可能預測，則基尼係數會很低。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7c905eb5adda46a38124a8849e1d461e><p class=pgc-img-caption></p></div><p>例，假設一班有60名學生。其中40名是男性，其中22名稍後進入engineering school，而20名女學生中有8名進入engineering school。加權基尼係數為</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/35d34ee2090a4cd0957264859549c419><p class=pgc-img-caption></p></div><p>要選擇決策樹樁，我們選擇權重最低的基尼係數。如果來自每個分支的數據屬於同一類，則基尼係數等於零。</p><p><strong>信息增益</strong></p><p>我們可以選擇信息增益最高的決策樹樁。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/76cc5eb4f8e34a7894c20deacef4660d><p class=pgc-img-caption></p></div><p>換句話說，我們希望分支後的條件熵最小</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/03a8d9e7fe354bfbb2e2f70745a062a0><p class=pgc-img-caption></p></div><p>在分支之後，我們希望熵最小。</p><p>例如，我們要預測是否要打網球。如果選擇有風（true or false）作為決策樹樁，則以下是信息增益的計算。首先，考慮有風，我們計算熵。然後我們計算一下，如果不颳風。然後，我們將它們組合為條件熵H（Y | X）。然後信息增益計算為：</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/27685f3816d3488db18b10020ba97d30><p class=pgc-img-caption></p></div><p>這是每個步驟的詳細計算</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bb55861961cc4741923d590b92983c41><p class=pgc-img-caption></p></div><p>然後，我們繼續其他可能的樹樁，並貪婪地選擇具有最高值的樹樁。</p><p><strong>減少方差</strong></p><p>減少方差可在連續空間中工作。它測量拆分之前和之後的方差。我們想選擇方差減少最大的決策樹樁。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/dc1ae77937d7463eb8e4e0bfbea09bb7><p class=pgc-img-caption></p></div><p><strong>剪枝</strong></p><p>在葉子節點包含紅色或藍色點之前，以下數據點將至少佔用4級的決策樹樁。在前三個拆分中，每個分支將包含相等數量的紅點和藍點。在此示例中，直到達到第四級，我們都看不到任何進展。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ae1e55b2b2e94bd4bc6a459d4e9a93b7><p class=pgc-img-caption></p></div><p>為避免此問題，我們可以使樹更深，並使用pruning。</p><p><strong>過度擬合</strong></p><p>對於分類問題，我們拆分樹，直到每個分支的數據點屬於同一類，或它們的輸入屬性都相同，直到我們無法進一步區分。由於數據中的噪聲，這種方法很容易使機器學習模型過擬合。如前所述，我們可以在構建樹後prune它。如果測試數據的驗證性能得到改善，我們將從葉子到根部刪除決策樹樁。</p><p>其他方法包括</p><ul><li>對樹的深度有嚴格的上限。</li><li>如果數據點數降至閾值以下，則停止進一步拆分。</li><li>要求每個葉節點具有最少數量的數據點。</li><li>設置葉節點的最大數量。</li></ul><p><strong>卡方檢驗</strong></p><p>我們還可以通過卡方檢驗來驗證決策樹樁是否具有統計學意義。我們可以使用卡方來度量其父代和子代之間數據分佈的差異。我們評估差異是否主要是偶然的。在一定的置信水平(比如95%)，如果我們認為分佈的差異可能是偶然發生的，我們應該去掉樹樁。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/983d25516c7c41f38b3cc08984f8fc7c><p class=pgc-img-caption></p></div><p>例</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bc238e4798dd40d88eac4024b4607dba><p class=pgc-img-caption></p></div><p>如果卡方小於閾值T，則將刪除決策樹樁，因為數據分佈的差異在統計上並不顯著。在查找對應置信度的值T時，我們經常參考表格或卡方分佈計算器。</p><p>我們還可以在選擇決策樹樁時使用卡方值。我們將選擇價值最高的一個。</p><p><strong>弱學習者</strong></p><p>在機器學習（ML）中，我們可以構建一個弱學習器（複雜度較低的模型）以避免過度擬合。對於決策樹，我們可以限制樹的深度和輸入特徵的數量，以降低模型的複雜性。然而，這些學習者很可能帶有偏見。在機器學習（ML）中，我們可以將模型捆綁在一起進行預測。如果他們是獨立訓練的，那麼訓練後的模型之間的相關性很小，他們就不太可能犯同樣的錯誤。簡單的多數表決可以消除錯誤並創建強有力的預測。</p><p><strong>集成方法</strong></p><p>有許多方法可以創建不同的訓練模型實例。我們可以使用不同的算法、不同的種子和配置，以及在不同迭代中保存的不同模型來構建這些模型。注意避免強相關性，我們可以使用簡單多數投票來解決分類問題。對於迴歸問題，我們可以計算平均值、加權平均值或中位數值來進行預測。</p><p><strong>Stacking</strong></p><p>在stacking中，我們在第一輪中使用多種機器學習（ML）方法進行預測。然後，我們將這些預測作為輸入提供給另一個模型，以進行最終決策。</p><p><strong>Bagging</strong></p><p><strong>Bootstrapping</strong>是隨機抽樣和替換的通用技術。我們從數據集中選擇數據，但是，可以再次選擇（替換）選取的數據。即，我們一次又一次地從同一組數據中選擇數據。</p><p><strong>Bagging</strong> <strong>(Bootstrap aggregated) </strong>僅將這種技術應用於通過隨機採樣和替換來收集數據點。採樣的數據點可以重複。如果此樣本數據集的大小與原始訓練數據集的大小相同，我們應該期望原始數據集中只有63.2％的數據會出現在此樣本數據集中。</p><p>Bagging降低了訓練模型之間的相關性，因為現在輸入的樣本並不相同。如果我們還降低了樹的深度，也可以避免過度擬合。通過使用不同採樣數據集訓練的捆綁模型，我們可以期望在進行預測時具有更強大的aggregated模型。</p><p>例如，我們可以有B個bootstrap數據集，每個數據集包含n個數據點。每個bootstrap均從大小為n的原始數據集中進行替換。建立B模型後，我們可以使用它們進行B獨立預測。我們的最終答案可以是這些答案的中位數。</p><p><strong>隨機森林</strong></p><p>隨著bootstrap數據集數量的增加，由於許多數據集確實高度相關，因此集成樹的性能達到了平穩狀態。隨機森林使用上面提到的bagging以及其他技巧。每個模型僅使用部分特徵進行訓練。每個模型只使用一個特徵子集進行訓練。如果數據有K個特徵，我們將隨機選擇其中的平方根(K)。所以在訓練一個模型之前，我們隨機地預選一個特徵子集並用這個特徵子集訓練模型。這進一步降低了模型的相關性，不同的模型會找到不同的方法來確定類。將其與bagging相結合，可提高集成方法的準確性。</p><p><strong>Boosting</strong></p><p>建立第一個模型時，我們從訓練集中統一採樣數據。但是對於下一次迭代，我們將更頻繁地對具有錯誤預測的數據點進行採樣。在更好地關注我們所缺乏的地方之後，第二種模式應該在那些領域表現得更好。經過k次迭代後，便建立了k個模型。在每次迭代中，我們都會跟蹤每個模型的準確性，並以此來計算權重。在推理過程中，我們使用這些權重來計算使用這些模型進行預測的加權平均值。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/391a871417a9491a911e4b002fa56598><p class=pgc-img-caption></p></div><p><strong>AdaBoost</strong></p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/555dd30f25b0415f9fb105e774a6a7fe><p class=pgc-img-caption></p></div><p>通常，對於任何分類錯誤的數據，我們為下一次迭代設置更大的採樣權重。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/724965d5425641b698c1559f398f4fdb><p class=pgc-img-caption></p></div><p>AdaBoost訓練誤差的上限是</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bef208dc23584d79bdbbd398c4f628f6><p class=pgc-img-caption></p></div><p>雖然我們可能在每一步都學到一個弱學習者，但是如果我們學習了足夠多的機器學習模型，誤差就會顯著減少。</p><p><strong>基於梯度的模型</strong></p><p>在AdaBoost中，我們從表現不佳的數據點中學習。在基於梯度的模型中，我們對預測中的誤差進行建模。</p><p>假設我們有一個訓練數據集y = [f(100)， f(200)，…，f(800)]，用於輸入100，…到800。我們想要建立一個f的模型，我們的第一個模型是使用y的平均值來建立簡單的f模型，這是一個好的開始，但並不準確。接下來，我們計算預測中的殘差(誤差)，並用另一個模型對其進行建模。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8c3f82f1d5074bafa160eaad38ccc2a6><p class=pgc-img-caption></p></div><p>我們將繼續從殘差中構建簡單模型，而我們的預測將所有模型結果相加。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/da3f9ee4a3b14c5488a1b3c38bca2efb><p class=pgc-img-caption></p></div><p>簡而言之，第一個模型輸出y的平均值。第二個模型預測第一個模型的殘差。第三個模型預測了第二個模型的殘差。</p><p>我們將讓您查看圖中的解釋</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eed51f7ef86b4657bd961af17894f1d0><p class=pgc-img-caption></p></div><p>這裡有一種可能的方法可以建立一個迴歸樹，在樹樁條件為x>500的情況下對左邊的殘差進行建模。在這個模型中，當x>500時，它返回22，否則返回-13。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6a85b940d7514f3ca83d4b6b604a9c86><p class=pgc-img-caption></p></div><p>但還有其他的可能性。我們不是為殘差建立模型，而是隻為殘差的sign建立模型。這有點奇怪，但是當我們建立足夠多的殘差模型時，它得到了與我們之前的例子類似的結果。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dde9171e263c40f89c6f854719d3f621><p class=pgc-img-caption></p></div><p>這些解之間有什麼區別，為什麼我們將這些方法稱為基於梯度的方法呢？如果我們將這些基於梯度的模型與梯度下降進行比較，則它們看起來相似。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ad4683584bec4fe590f35edd5c2c17a0><p class=pgc-img-caption></p></div><p>如下所示，我們可以從L2或L1損失函數開始。如果我們採用這些損失函數的導數，則這些成本函數的梯度只是我們在先前示例中用於對殘差進行建模的項的負數。對於L2損失，等效值為殘差。對於L1損失，等效值是殘差的sign。因此，我們的方法只是相應選定成本函數的負梯度。這就是為什麼將其稱為基於梯度的原因。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/5c4f66f9772e403d9838fa699dbd4469><p class=pgc-img-caption></p></div><p>因此，我們可以將基於梯度的方法的概念擴展到其他成本函數，例如Huber損失。通用算法為：</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/921df640ee9147c68ba18a9503712ace><p class=pgc-img-caption></p></div><p><strong>局部集束搜索（Local beam search）</strong></p><p>搜索空間就像圍棋遊戲一樣巨大。在局部集束搜索中，我們僅探索最有前途的搜索。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/270a5650d47a474ba23714e8fae39f5b><p class=pgc-img-caption></p></div><p><strong>迴歸樹</strong></p><p>除了分類，我們還可以使用決策樹將模型分解為sub-cases，並使用迴歸模型求解每個葉節點。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/056e4be666d9417aaf6e3587fbeff40c><p class=pgc-img-caption></p></div><h1>監督和無監督機器學習</h1><p>監督學習在給定輸入（y = f（x））的情況下預測標籤時建立模型f。訓練數據集包含輸入和標籤（xᵢ，yᵢ）對。從概念上講，監督學習是對P（y | x）的研究。無監督學習可以找到訓練數據的模式和趨勢，沒有標籤，我們可以將其歸納為分佈P（x）的研究。在K均值聚類中，我們用K近似P（x）質心。無監督學習包括聚類，矩陣分解和序列模型。根據此分類，這裡有不同的算法。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f2769dbc76064c42b2682a2721c1fe12><p class=pgc-img-caption></p></div><p>在HMM中，我們對內部狀態感興趣。例如，在語音識別中，我們的觀察是音頻，內部狀態是相應的單詞。通常，此訓練數據集不提供音頻的字幕，因此可以歸類為無監督學習。但是，監督學習和非監督學習之間的界限可能是模糊的，並且通常不是很重要。</p><h1>半監督機器學習</h1><p>在某些情況下，我們可以收集大量的培訓數據，但是我們只有有限的預算來標註其中的一小部分。半監督學習使用這一小組標籤來標記那些沒有標記的。這裡有兩種不同的可能性。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7b1454000d6747bd8c88bb2b2bac9426><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/8b18f749c59d4d0994d4bc265ca4f7e9><p class=pgc-img-caption></p></div><h1>模型複雜度</h1><p>當我們增加機器學習模型複雜度時，我們可能會面臨過度擬合的風險，例如，GMM中的高斯模型數量，HMM中的隱藏狀態數量或矩陣分解中的秩等，隨著模型複雜度的增加，訓練誤差會減小。但是，當我們使用驗證數據集驗證機器學習模型時，我們將意識到驗證誤差會增加。在深度學習（DL）中，我們經常需要大量數據。因此，其數據集通常很大，我們經常將一部分數據專用於驗證。</p><p><strong>AIC和BIC</strong></p><p>與DL問題相比，在ML中，我們收集的數據量可能少得多。 如果我們有足夠的數據，則可以使用專用的驗證數據集。 否則，我們可以使用rotated hold-out數據集（交叉驗證）或從完整數據集（bootstrap）中隨機選擇驗證數據。 我們可以使用這些數據來選擇適當的模型複雜性，從而獲得最佳性能。 為了降低複雜度，我們在成本函數中添加了正則懲罰項。 以下是兩種可能的正則化。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/771b6fbe768847629d7fbc3d008d727d><p class=pgc-img-caption></p></div><p>在此公式化中，僅當成本下降大於模型參數增加的數量時，才應增加模型複雜度。 在BIC中，我們還將K乘以lnN。</p><h1>能量模型</h1><p>利用能量函數E（x），將x的概率分佈定義為</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ce335f78cdc34ddfa69e9e5b6cf18739><p class=pgc-img-caption></p></div><p>在這樣的模型中，高能量E（x）導致狀態x的可能性很小。 我們對能量函數建模，該函數使訓練數據集的P（x）最大化。 我們稱Z為配分函數。 基本上，它對0到1之間的概率分佈進行歸一化。它對x的所有可能配置的指數函數求和。 對於具有許多狀態的系統，計算Z並不容易。 通常使用近似值來解決這些問題。 接下來，讓我們討論如何對能量函數進行建模和定義。</p><p><strong>玻爾茲曼機</strong></p><p>為了提高能量模型的表示能力，我們將不可見變量（下方藍色的隱藏單元）添加到Boltzmann機器中。白色節點是可見單元。它們代表訓練數據集中的數據樣本，觀察到的狀態或特徵。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3891f377fc664de488296831f6ee53c9><p class=pgc-img-caption></p></div><p>隱藏單元是不可見的，代表我們可見單元的潛在因子。每個單元都處於二元狀態<em>sᵢ∈</em> {0,1}之一<em>。</em>單元通過邊緣Wᵢⱼ與其他單元完全連接。Wᵢⱼ的值表示兩個節點是如何關聯的。如果節點sᵢ和sⱼ正相關，我們希望Wᵢⱼ> 0。 如果它們負相關，我們希望Wᵢⱼ&lt;0。 如果有獨立的，Wᵢⱼ應為零。 從概念上講，我們希望根據其他單元及其相關性來on/off一個單元。 玻爾茲曼機的能量函數E（X）和概率分佈P（X）定義為：</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a08a0fe6190a4c9c8d71cd97e533b665><p class=pgc-img-caption></p></div><p>它以線性形式表示節點和連接的能量</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/61e6ddbccd8c43158ac661f41b58180a><p class=pgc-img-caption></p></div><p>節點是on還是off取決於其連接節點的值</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/00e43ccdff0741609b7f6abd8550c358><p class=pgc-img-caption></p></div><p>通過用數據訓練模型，我們為訓練數據集建立了能量最低的Wᵢⱼ和bᵢ值。</p><p>從某些角度看，玻爾茲曼機訓練W來提取可見單元的潛在因子。 但是，玻爾茲曼機器彼此完全互連，很難訓練。</p><p><strong>受限玻爾茲曼機（RBM）</strong></p><p>RBM從輸入v(the observable)中提取潛在特徵h。可見單元之間沒有相互聯繫，隱藏單元也一樣。它有更多的限制，但模型比玻爾茲曼機簡單得多，也更容易訓練。構型(v, h)的能量E(v, h)等於</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b028739789924aa091975f985c1059b0><p class=pgc-img-caption></p></div><p>為了訓練模型，我們優化了訓練數據的最大對數似然度（log p（v））。該目標函數（關於wᵢⱼ）的梯度等於</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3abe77e019f6470eb964374560150411><p class=pgc-img-caption></p></div><p>我們將在此梯度上使用梯度下降以稍後更新<em>wᵢⱼ</em>。</p><p>要計算下面的第一項</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/022e8520519b4cd4932cf42968a0cc26><p class=pgc-img-caption></p></div><p>v只是訓練數據集中的樣本。然後，我們使用以下公式計算隱藏節點j的概率分佈p（hⱼ| v）。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9f2b600ed4ef4b87aacf25b623a6fa2c><p class=pgc-img-caption></p></div><p>第二項是通過Gibbs採樣計算的。 我們使用梯度下降來更新模型參數wᵢⱼ</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6bf28163c24748a8aa9ce1fc83c5a874><p class=pgc-img-caption></p></div><p>第二項使用當前模型對v和h進行採樣以計算期望值。即我們需要知道p（v，h | w）。分析上很難找到它。但是我們可以使用Gibbs採樣估計這種期望。</p><p>Gibbs採樣的一般思想是，對於聯合概率p（X =x₁，x²，x₂，…，xᵢ，…），我們固定除一個參數xᵢ之外的所有X值。在某些問題中，當我們除了一個外固定其餘參數，（p，…，xᵢ，...） 可能很容易建模。我們從該分佈中抽取一個值，並將xᵢ設置為該採樣值。然後，我們選擇另一個參數，然後再次固定其餘參數。我們重複該過程很多次，並且每次獲得X樣本。</p><p>如下所示，我們將採樣的X繪製為紅色點。每個X僅更改X =（x₁，x²，x₃，…，xᵢ，…）中的一個值。如圖所示，採樣數據將類似於聯合概率p（x₁，x²，x₃，…）！</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f0a093ff3fa54117b83235bcc0352a0d><p class=pgc-img-caption></p></div><p><strong>對比散度</strong></p><p>因此，我們將結合吉布斯採樣的概念和梯度下降法來訓練RBM。這叫做對比散度。在RBM中，我們使用RBM模型對h和v進行交替採樣。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6327f51b0f4846b09f507eef645f0d47><p class=pgc-img-caption></p></div><p>首先，我們從v的任意值開始，然後我們可以計算每個隱藏節點的概率</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/97d9f6c49f7c4aaebb49d29c55d98e1c><p class=pgc-img-caption></p></div><p>然後我們對隱藏節點的值進行採樣，形成h。在第二步中，我們再次使用採樣的h對v進行採樣</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b0365c76f4544b8b88fbf7a84e322820><p class=pgc-img-caption></p></div><p>即使我們之做了幾次，也可以在RBM中生成良好的模型參數。RBM的確切算法與所描述的略有不同。以下是訓練中的算法</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5ca87e4021f846708aef30c0aa127c65><p class=pgc-img-caption></p></div><p>訓練模型參數中使用的公式為：</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/49fbb49d8b7e4f5a962f92b3687f3c77><p class=pgc-img-caption></p></div><p>其中<em>k</em>是迭代的第<em>k</em>次。</p><p><strong>自由能</strong></p><p>能量模型中另一個經常提到的是下面的自由能<em>F</em></p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a433949bdaa74ec0a1645bbf2aa96b2b><p class=pgc-img-caption></p></div><p>對數似然的梯度可以以自由能的形式表示</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4302a0f7b5a248a5bb984d62e5b09875><p class=pgc-img-caption></p></div><p>簡而言之，可以使我們可以通過構建能量模型（通過模型參數）來最大程度地提高對數似然性。</p><h1>卷積神經網絡（CNN）</h1><p>我們在每一層中使用k×k卷積來提取特徵圖。通過使用stride或max-pooling，我們逐漸減小了空間維度。因為CNN有很多好的資源，所以這裡的描述非常簡短。想知道更多細節，可參考<a class=pgc-link data-content=mp href="https://www.toutiao.com/i6618443533286113805/?group_id=6618443533286113805" target=_blank>理解神經網絡</a>和<a class=pgc-link data-content=mp href="https://www.toutiao.com/i6659707508891845128/?group_id=6659707508891845128" target=_blank>卷積神經網絡概述及示例教程</a>。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3ba55085bfb24fc7ba871a1bc369e1cc><p class=pgc-img-caption></p></div><h1>LSTM和GRU</h1><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e16a01b29f464eeca91f956e8669c15a><p class=pgc-img-caption></p></div><p>LSTM包含一系列LSTM單元。單元t在時間t負責處理輸入數據x t，並且每個單元在上一個時間步都連接到LSTM單元。每個單元具有內部單元狀態c t並輸出隱藏狀態ht。LSTM使用三個門來控制信息流。它們是forget gate，input gate和output gate。它們在圖中以符號⊗表示。每個門的形式為</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1a87ed1a0d754e408eb856c2dcbd835e><p class=pgc-img-caption></p></div><p>在分別將它們與Wx和Wh相乘之後（每個門具有不同的Wx和Wh），在當前數據輸入和先前的隱藏狀態上應用S型函數。每個門通過與門控制執行分段乘法來控制可以傳遞哪些信息。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/868104ff365943f89dbce0ee21ce7fb4><p class=pgc-img-caption></p></div><p>簡而言之，forget gate會忘記來自前一個內部單元狀態的部分信息。input gate控制將處理輸入的哪一部分和先前的隱藏狀態來創建單元狀態。output gate控制內部單元狀態ct的哪一部分將作為隱藏狀態ht輸出。</p><p>在每個時間步，我們都需要創建一個單元狀態。通過忘記舊單元狀態的一部分以及來自當前輸入和先前隱藏的附加信息來進行計算，所有這些都由下面的相應門控制。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3e608940bd2d496c8a28e9c65ce85054><p class=pgc-img-caption></p></div><p>然後，我們輸出由輸出門控制的新單元狀態</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/522fe9279cae407a9f7202713fb4b8c6><p class=pgc-img-caption></p></div><p>GRU是RSTM（遞歸神經網絡）的LSTM的替代方案。與LSTM相比，GRU不會維持單元狀態C，使用2個門而不是3個門。在每個時間步，我們都基於先前的隱藏狀態和當前輸入來計算隱藏狀態。</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a4539d9b8d2e44b7bf37f277a96833d3><p class=pgc-img-caption></p></div><h1>最後</h1><p>以下是機器學習（ML） /深度學習（ DL）項目的典型流程</p><div class=pgc-img><img alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b1eeaeb12dca4e3e8af039cbdecc1cf9><p class=pgc-img-caption></p></div></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>總結</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/59b3843e.html alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/84c47890a2c44654997e63bd5cdf0c72 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/59b3843e.html title=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等>機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>