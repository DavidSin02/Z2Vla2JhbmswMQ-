<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>為什麼說Transformer就是圖神經網絡？ | 极客快訊</title><meta property="og:title" content="為什麼說Transformer就是圖神經網絡？ - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RqSgmP9HojtMNK"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/54b66f71.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/54b66f71.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/54b66f71.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/54b66f71.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/54b66f71.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/54b66f71.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/54b66f71.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/54b66f71.html><meta property="article:published_time" content="2020-11-14T20:53:56+08:00"><meta property="article:modified_time" content="2020-11-14T20:53:56+08:00"><meta name=Keywords content><meta name=description content="為什麼說Transformer就是圖神經網絡？"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/54b66f71.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>為什麼說Transformer就是圖神經網絡？</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><p><br></p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RqSgmP9HojtMNK></div><p><br></p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Rbc1Htm93uvAQn></div><p>作者 | Chaitanya Joshi</p><p>譯者 | Kolen</p><p>出品 | AI科技大本營（ID:rgznai100）</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqLxPCXik3SR></div><h1>前言</h1><p>有些工程師朋友經常問我這樣一個問題：“圖深度學習聽起來很棒，但是現在是否有非常成功的商業案例？是否已經在實際應用中部署？”</p><p>除了那些顯而易見的案例，比如Pinterest、阿里巴巴和Twitter公司部署的推薦系統，一個稍有細微差別的成功案例就是Transformer架構的實現，它在NLP行業引起了軒然大波。</p><p>通過這篇文章，我想建立起圖神經網絡（GNNs）和Transformers之間的聯繫。具體來說，我將首先介紹NLP和GNN領域中模型架構的基本原理，然後使用公式和圖表來闡述兩者之間的聯繫，最後將討論如何讓兩者協同運作來推動這方面的研究進展。</p><p>我們先來談談模型架構的目的——表示學習。</p><h1><br></h1><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqLy93x95UEx></div><h1><br></h1><h1>NLP的表示學習</h1><p>從一個較高的層次來分析，幾乎所有的神經網絡結構都將輸入數據表示為向量（vectors）或者嵌入（embeddings）的形式，從而對數據中有用的統計和語義信息進行編碼。這些潛在或隱藏的表示方法可以用於執行一些有用的任務，例如對圖像進行分類或翻譯句子。其中，神經網絡通過接收反饋（通常是通過誤差（error）/損失（loss）函數）來學習如何構建越來越好的表示方法。</p><p>在自然語言處理（NLP）中，按照傳統方式，人們習慣將遞歸神經網絡（RNNs）以照序列的方式（即一個時間步對應一個單詞）來構建句子中每個單詞的表示。直觀地說，我們可以把RNN層想象成一個傳送帶，上面的字從左到右進行自迴歸處理。最後，我們得到句子中每個單詞的一個隱藏特徵，並將其傳遞到下一個RNN層或者用於我們選擇的NLP任務。</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RsGVfWTIiUXS3J></div><p>Transformers最初是用於機器翻譯領域，但是現在已經逐漸取代了主流NLP中的RNNs。該架構採用了一種全新的表示學習方法：完全拋棄了遞歸的方法，Transformers使用注意力機制構建每個詞的特徵，從而找出句子中所有其他單詞對上述單詞的重要性。理解了這一關鍵點我們就能明白，單詞的更新特徵僅僅是所有單詞特徵的線性變換之和，這些特徵是根據它們的重要性進行加權。</p><p>早在2017年，這個想法聽起來就非常激進，因為NLP界已經習慣了使用RNN處理文本的序列（每次一個單詞）的風格。這篇論文的標題可能是火上澆油！</p><p>Yannic Kilcher為此做了一個出色的視頻概述。</p><h1><br></h1><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RqMqMYY7LcGio6></div><h1><br></h1><h1>解析Transformer</h1><p>讓我們通過將上一節內容轉述成數學符號和向量的語言來加深對這個架構的認識。如下所示，我們將句子</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVfXDDdASpaN></div><p>中第i個詞的隱藏特徵h從</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RsGVfXV3RJuEHQ></div><p>層更新到</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVfXr6zGlBxj></div><p>層：</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVfsA2NBr9CX></div><p>其中，</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RsGVfsY2DyONdM></div><p>表示句子中的詞彙集，而</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVfspAABxz4t></div><p>是可以學習到的線性權重（分別表示注意力計算中的Query, Key 和 Value）。句子中的每個單詞並行執行注意力機制，從而可以一次性獲得它們已更新的特徵——這是Transformer相對RNNs的另一個加分點，它使得模型能夠逐字更新特徵。</p><p>我們可以通過下面這張流程圖來更好地理解注意力機制：</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVft53gMLQqU></div><p>輸入詞彙特徵和句子中其他詞彙集</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVftV1kwIFOg></div><p>，我們使用點積運算來計算出每對</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgHk6DdUZwu></div><p>的注意力權重，接著對所有的進行softmax運算。最後，把所有的</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgI9GOuKUOH></div><p>相對應的權重累加得到單詞i更新後的詞彙特徵</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgJ7FOogK5y></div><p>。句子中的每個單詞都會並行地經歷相同的流程來更新其特徵。</p><h1><br></h1><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqMZM85xvsbJ></div><h1><br></h1><h1>多頭注意力機制</h1><p>事實證明，要讓這種點積注意力機制起作用是很難的——如果隨機初始化處理得不好會使得整個學習過程失去穩定性。我們可以通過並行執行多個注意力“頭”並將結果連接起來（現在每個注意力頭都有單獨的可學習權重）來克服這個問題：</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgJP2YiZVMj></div><p>其中，</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RsGVgJfMV2UCu></div><p>是第k個注意力頭的可學習的權重，而</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgZz68oKw2z></div><p>是一個向下的投影，用以匹配跨層的</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RsGVgaV2E8bG0H></div><p>和的尺寸。</p><p>通過觀察上一層中隱藏特徵的不同的變換過程以及方面，多頭機制允許注意力機制從本質上“規避風險”。關於這點，我們將在後面詳細討論。</p><h1><br></h1><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqNMhMM9NBY></div><h1><br></h1><h1>尺度問題和前向傳播子層</h1><p>促使形成最終形態的Transformer結構的關鍵問題是，注意機制之後的詞的特徵可能在不同的尺度或重要性上：（1）這可能是由於某些詞在將其他詞的特徵累加時具有非常集中或非常分散的注意力權重。（2）在單個特徵/向量輸入級別，跨多個注意力頭（每個可能會以不同的比例輸出值）進行級聯可以導致最終向量的輸入具有一個大範圍的值。遵循傳統的機器學習思路，在上述流程中增加一個歸一化層似乎是一個合理的選擇。</p><p>Transformers使用LayerNorm克服了問題（2），LayerNorm在特徵層級上進行歸一化並學習一種仿射變換。此外，通過求特徵維度的平方根來縮放點積注意力有助於抵消問題（1）。</p><p>最後，作者提出了控制尺度問題的另一個“技巧”：具有特殊結構的考慮位置的雙層MLP。在多頭注意力之後，他們通過一個可學習的權重將投影到一個更高的維度，在該維度中，經過ReLU非線性變換，然後投影回其原始維度，然後再進行另一個歸一化操作：</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgaq8ijcyFT></div><p>說實話，我不確定超參數化前饋子層背後的確切理由是什麼，似乎也沒有人對此提出疑問！我認為LayerNorm和縮放的點積不能完全解決突出的問題，因此大型MLP是一種可以相互獨立地重新縮放特徵向量的手段。</p><p>Transformer層的最終形態如下所示：</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RsGVgbS3tIYwc1></div><p>Transformer架構也非常適合非常深的網絡，使NLP界能夠在模型參數和擴展數據這兩方面進行延伸。每個多頭注意力子層和前饋子層的輸入和輸出之間的殘差連接是堆疊Transformer層的關鍵（但為了清楚起見，在上圖中省略了）。</p><h1><br></h1><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RqMqdp52DyYCQP></div><h1><br></h1><h1>GNNs構建圖的表示</h1><p>我們暫時不討論NLP。</p><p>圖神經網絡（GNNs）或圖卷積網絡（GCNs）在圖數據中建立節點和邊的表示。它們是通過鄰域聚合（或消息傳遞）來實現的，在鄰域聚合中，每個節點從其鄰域收集特徵，以更新其周圍的局部圖結構表示。通過堆疊多個GNN層使得該模型可以將每個節點的特徵傳播到整個圖中，從其鄰居傳播到鄰居的鄰居，依此類推。</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgbt6AwFbau></div><p>以這個表情符號社交網絡為例：由GNN產生的節點特徵可用於預測性任務，例如識別最有影響力的成員或提出潛在的聯繫。</p><p>在他們最基本的形式中，GNNs通過以下方法來更新節點i在層的隱藏層特徵h（例如，），也就是先將節點自身的特徵和每個鄰居節點</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RsGVgvg3p17fka></div><p>特徵</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgvyBsOpGoL></div><p>的聚合相累加，然後再整體做一個非線性變換，如下：</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RsGVgwG4iUkUa8></div><p>其中</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RsGVgwXDgmkIUb></div><p>是GNN層的可學習的權重矩陣，而</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVgyP28ki7SS></div><p>是一個非線性變換，例如ReLU。在上述例子中，N () ={ , , , }。</p><p>鄰域節點</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVhGQEtS2ERx></div><p>上的求和可以被其他輸入大小不變的聚合函數代替，例如簡單的 均值/最大值函數或其他更強大的函數（如通過注意機制的加權和）。</p><p>這聽起來熟悉嗎？</p><p>也許這樣一條流程可以幫助建立連接：</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RsGVhH69HVeXmm></div><p>如果我們要執行多個並行的鄰域聚合頭，並且用注意力機制（即加權和）替換領域 上的求和 ，我們將獲得圖注意力網絡（GAT）。加上歸一化和前饋MLP，瞧，我們就有了Graph Transformer！</p><h1><br></h1><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rqqo5387tkSUcT></div><h1><br></h1><h1>句子就是由詞全連接而成的圖</h1><p>為了使連接更加清晰，可以將一個句子看作一個完全連接的圖，其中每個單詞都連接到其他每個單詞。現在，我們可以使用GNN來為圖（句子）中的每個節點（單詞）構建特徵，然後我們可以使用它來執行NLP任務。</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVhHPHtrJruh></div><p>廣義上來講，這就是Transformers正在做的事情：Transformers是以多頭注意力作為鄰聚合函數的GNNs。標準GNNs從其局部鄰域節點聚合特徵，而NLP的Transformers將整個句子視為局部鄰域，在每個層聚合來自每個單詞</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVhHuFsFlV2v></div><p>的特徵。</p><p>重要的是，各種特定於問題的技巧（如位置編碼、因果/掩碼聚合、學習率表和大量的預訓練）對於Transformers的成功至關重要，但在GNN界中卻很少出現。同時，從GNN的角度看Transformers可以啟發我們擺脫模型結構中的許多花哨的玩意。</p><h1><br></h1><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Rqqo547DNIp69u></div><h1><br></h1><h1>可以從Transformers和GNN中學到什麼？</h1><p>現在我們已經在Transformers和GNN之間建立了聯繫，接著讓我們來探討一些新的問題...</p><p><strong>8.1 全連接圖是NLP的最佳輸入格式嗎？</strong></p><p>在統計NLP和ML之前，Noam Chomsky等語言學家致力於發展語言結構的最新理論，如語法樹/圖。Tree LSTMs已經嘗試過這一點，但是也許Transformers/GNNs是可以讓語言理論和統計NLP的領域結合得更加緊密的更好的架構？</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVhfx5XnGv4Q></div><p><strong>8.2 如何學習到長期依賴？</strong></p><p>完全連通圖使得學習詞與詞之間的非常長期的依賴關係變得非常困難，這是完全連通圖的另一個問題。這僅僅是因為圖中的邊數與節點數成二次平方關係，即在n個單詞的句子中，Transformer/GNN將在n^2對單詞上進行計算。如果n很大，那將會是一個非常棘手的問題。</p><p>NLP界對長序列和依賴性問題的看法很有意思：例如，使注意力機制在輸入大小方面稀疏或自適應，在每一層中添加遞歸或壓縮，以及使用對局部性敏感的哈希法進行有效的注意，這些都是優化Transformers有希望的新想法。</p><p>有趣的是，還可以看到一些GNN界的想法被混入其中，例如，用於句子圖稀疏化的二進制分區似乎是另一種令人興奮的方法。</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVhgq51JhEku></div><p><strong>8.3 Transformers在學習神經網絡的句法嗎？</strong></p><p>NLP界有幾篇關於Transformers可能學到什麼的有趣論文。其基本前提是，對句子中的所有詞對使用注意力機制（目的是確定哪些詞對最有趣），可以讓Transformers學習特定任務句法之類的東西。</p><p>多頭注意力中的不同頭也可能“關注”不同的句法屬性。</p><p>從圖的角度來看，通過在完全圖上使用GNN，我們能否從GNN在每一層執行鄰域聚合的方法中恢復最重要的邊線及其可能帶來的影響？我還不太相信這種觀點。</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVhi3ANTNTli></div><p><strong>8.4 為什麼要用多頭注意力？為什麼要用注意力機制？</strong></p><p>我更贊同多頭機制的優化觀點——擁有多個注意力可以改進學習，克服不好的隨機初始化。例如，這些論文表明，Transformers頭可以在訓練後“修剪”或“刪除”，並且不會產生重大的性能影響。</p><p>多頭鄰聚合機制在GNNs中也被證明是有效的，例如在GAT使用相同的多頭注意力，MoNet使用多個高斯核來聚合特徵。雖然多頭技巧是為了穩定注意力機制而發明的，但它能否成為提煉出額外模型性能的標準？</p><p>相反，具有簡單聚合函數（如sum或max）的GNNs不需要多個聚合頭來維持穩定的訓練。如果我們不需要計算句子中每個詞對之間的成對兼容性，對Transformers來說不是很好嗎？</p><p>Transformers能從拋棄注意力中獲益嗎？Yann Dauphin和合作者最近的工作提出了另一種ConvNet架構。Transformers也可能最終會做一些類似於ConvNets的事情。</p><div classname=pgc-img><img alt=為什麼說Transformer就是圖神經網絡？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsGVhic3DR3DK></div><p><strong>8.5 為什麼Transformers這麼難訓練？</strong></p><p>閱讀新的Transformer論文讓我覺得，在確定最佳學習率表、預熱策略和衰減設置時，訓練這些模型需要一些類似於黑魔法的東西。這可能僅僅是因為模型太大，而且所研究的NLP任務非常具有挑戰性。</p><p>但是最近的結果表明，這也可能是由於結構中歸一化和殘差連接的特定組合導致的。</p><p>在這一點上我很在意，但是也讓我感到懷疑：我們真的需要代價昂貴的成對的多頭注意力結構，超參數化的MLP子層以及複雜的學習計劃嗎？</p><p>我們真的需要具有大量碳足跡的（譯者注：有人提出現在訓練一個模型相當於5輛汽車一天的排碳量）大規模模型嗎？</p><p>具有良好歸納偏差的架構難道不容易訓練嗎？</p><p>原文鏈接：</p><p>https://dwz.cn/eE9kZK6q</p><p>（本文由AI科技大本營編譯，轉載請聯繫微信1092722531）</p><p>【end】</p><p>在這次疫情防控中，無感人體測溫系統發揮了怎樣的作用？它的技術原理是什麼？無感人體測溫系統的應用場景中有哪些關鍵技術與落地困難？高精準的無感人體測溫系統的核心技術武器是什麼？對於開發者們來說，大家應該瞭解哪些技術？</p><ul><li>機器會成為神嗎？</li><li>6個步驟，告訴你如何用樹莓派和機器學習DIY一個車牌識別器！（附詳細分析）</li><li>微信迴應釘釘健康碼無法訪問；谷歌取消年度I/O開發者大會；微軟公佈Visual Studio最新路線圖</li><li>什麼是CD管道？一文告訴你如何藉助Kubernetes、Ansible和Jenkins創建CD管道！</li><li>智能合約初探：概念與演變</li><li>血虧1.5億元！微盟耗時145個小時彌補刪庫</li></ul><p><br></p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>什麼</a></li><li><a>Transformer</a></li><li><a>圖神經</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/bdedb94f.html alt=科普貼！丨你知道港珠澳大橋為什麼是彎的嗎！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d84b472b45474058bfd6bd4b87450f53 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bdedb94f.html title=科普貼！丨你知道港珠澳大橋為什麼是彎的嗎！>科普貼！丨你知道港珠澳大橋為什麼是彎的嗎！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/52ecc739.html alt=科普！看完這篇文章，你就知道港珠澳大橋為什麼是彎的了！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8f0b8a6daf6e4f9bab21e7c06cbc9111 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/52ecc739.html title=科普！看完這篇文章，你就知道港珠澳大橋為什麼是彎的了！>科普！看完這篇文章，你就知道港珠澳大橋為什麼是彎的了！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8bbd5f8a.html alt=長知識！什麼是榫卯結構？為什麼現代建築當中不常見了？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1537941825042b6f495fa56 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8bbd5f8a.html title=長知識！什麼是榫卯結構？為什麼現代建築當中不常見了？>長知識！什麼是榫卯結構？為什麼現代建築當中不常見了？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/83641773.html alt=什麼是懸挑結構？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/faebf061b82546f58e63afab5ede5fa2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/83641773.html title=什麼是懸挑結構？>什麼是懸挑結構？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a27253ff.html alt=港珠澳大橋為什麼是曲線的？關於橋那些你不知道的事 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a3c45dcf4c374ed4be36f55d44847637 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a27253ff.html title=港珠澳大橋為什麼是曲線的？關於橋那些你不知道的事>港珠澳大橋為什麼是曲線的？關於橋那些你不知道的事</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3eed9cb8.html alt=科普！港珠澳大橋為什麼是彎的！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/41bd66f3348b464c89b66628810a1aeb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3eed9cb8.html title=科普！港珠澳大橋為什麼是彎的！>科普！港珠澳大橋為什麼是彎的！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/765025c9.html alt=電纜與光纜有什麼區別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1c289a44885646ac904fbae012632866 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/765025c9.html title=電纜與光纜有什麼區別>電纜與光纜有什麼區別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c5053ab7.html alt="網站服務器是什麼 網站服務器怎麼搭建「詳解」" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/f9cfb6ae0c13447eab17646a80117f1b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c5053ab7.html title="網站服務器是什麼 網站服務器怎麼搭建「詳解」">網站服務器是什麼 網站服務器怎麼搭建「詳解」</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3b60dd43.html alt=長壽者的耳朵為什麼比別人長？原來耳朵還有這個大祕密 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/616a0000c4f325f77564 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3b60dd43.html title=長壽者的耳朵為什麼比別人長？原來耳朵還有這個大祕密>長壽者的耳朵為什麼比別人長？原來耳朵還有這個大祕密</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2c33cb41.html alt=為什麼每天都那麼累？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/S2HRVEQ6t1WhxB style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2c33cb41.html title=為什麼每天都那麼累？>為什麼每天都那麼累？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3f7c0fd3.html alt=什麼是場景？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3f7c0fd3.html title=什麼是場景？>什麼是場景？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/bd2acf38.html alt=寫場面要注意什麼？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/4e5a000167e332d06355 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bd2acf38.html title=寫場面要注意什麼？>寫場面要注意什麼？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/bdc59733.html alt="網絡詞名場面是什麼意思 名場面是什麼梗" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bdc59733.html title="網絡詞名場面是什麼意思 名場面是什麼梗">網絡詞名場面是什麼意思 名場面是什麼梗</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/31ba8cf2.html alt=[科普知識]什麼是光纖的冷接？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/c9d2cd0e63c0404f9f4cbdaae98d9606 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/31ba8cf2.html title=[科普知識]什麼是光纖的冷接？>[科普知識]什麼是光纖的冷接？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5f798649.html alt=「原創」什麼叫束裝尾纖？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/65d80c15bb5f447384bf8b734846eadc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5f798649.html title=「原創」什麼叫束裝尾纖？>「原創」什麼叫束裝尾纖？</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>