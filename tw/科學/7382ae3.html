<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習算法中的概率方法 | 极客快訊</title><meta property="og:title" content="機器學習算法中的概率方法 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p9.pstatp.com/large/pgc-image/RKwKVCYVyteRF"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/7382ae3.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/7382ae3.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/7382ae3.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/7382ae3.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/7382ae3.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/7382ae3.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/7382ae3.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/7382ae3.html><meta property="article:published_time" content="2020-10-29T20:55:39+08:00"><meta property="article:modified_time" content="2020-10-29T20:55:39+08:00"><meta name=Keywords content><meta name=description content="機器學習算法中的概率方法"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/7382ae3.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習算法中的概率方法</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><p>雷鋒網 AI 科技評論按，本文作者張皓，目前為南京大學計算機系機器學習與數據挖掘所（LAMDA）碩士生，研究方向為計算機視覺和機器學習，特別是視覺識別和深度學習。</p><p>個人主頁：http://lamda.nju.edu.cn/zhangh/。該文為其對雷鋒網 AI 科技評論的獨家供稿，未經許可禁止轉載。</p><p><strong>摘要</strong></p><p>本文介紹機器學習算法中的概率方法。概率方法會對數據的分佈進行假設，對概率密度函數進行估計，並使用這個概率密度函數進行決策。本文介紹四種最常用的概率方法：線性迴歸 (用於迴歸任務)、對數機率迴歸 (用於二分類任務)、Softmax 迴歸 (用於多分類任務) 和樸素貝葉斯分類器 (用於多分類任務)。* 前三種方法屬於判別式模型，而樸素貝葉斯分類器屬於生成式模型。（*嚴格來說，前三者兼有多種解釋，既可以看做是概率方法，又可以看做是非概率方法。）</p><p>本系列文章有以下特點: (a). 為了減輕讀者的負擔並能使儘可能多的讀者從中收益，本文試圖儘可能少地使用數學知識，只要求讀者有基本的微積分、線性代數和概率論基礎，並在第一節對關鍵的數學知識進行回顧和介紹。(b). 本文不省略任何推導步驟，適時補充背景知識，力圖使本節內容是自足的，使機器學習的初學者也能理解本文內容。(c). 機器學習近年來發展極其迅速，已成為一個非常廣袤的領域。本文無法涵蓋機器學習領域的方方面面，僅就一些關鍵的機器學習流派的方法進行介紹。(d). 為了幫助讀者鞏固本文內容，或引導讀者擴展相關知識，文中穿插了許多問題，並在最後一節進行問題的“快問快答”。</p><p><strong>1 準備知識</strong></p><p>本節給出概率方法的基本流程，後續要介紹的不同的概率方法都遵循這一基本流程。</p><p><strong>1.1 概率方法的建模流程</strong></p><p>(1). <strong>對</strong>p(y<strong> | x; θ) 進行概率假設。</strong>我們假定 p(y| x; θ)具有某種確定的概率分佈形式，其形式被參數向量</p><p>θ 唯一地確定。</p><p>(2).<strong> 對參數 θ 進行最大後驗估計</strong>。基於訓練樣例對概率分佈的參數 θ 進行最大後驗估計 (maximum a posteriori, MAP)，得到需要優化的損失函數。</p><p>最大後驗估計是指</p><p>其在最大化時考慮如下兩項：</p><p>• 參數的先驗分佈 p(θ)。最大後驗估計認為參數 θ 未知並且是一個隨機變量，其本身服從一個先驗分佈 p(θ)。這個先驗分佈蘊含了我們關於參數的領域知識。</p><p>• 基於觀測數據得到的似然 (likelihood) p(D | θ)。最大化似然是在 θ 的所有可能的取值中，找到一個能使樣本屬於其真實標記的概率最大的值。</p><p>最大後驗估計是在考慮先驗分佈 p(θ) 時最大化基於觀測數據得到的似然 (likelihood) p(D | θ)。</p><p>參數估計的兩個不同學派的基本觀點是什麼? 這實際上是參數估計 (parameter estimation) 過程，統計學中的頻率主義學派 (frequentist) 和貝葉斯學派(Bayesian) 提供了不同的解決方案 [3, 9] 。頻率主義學派認為參數雖然未知，但卻是客觀存在的固定值，因此通常使用極大似然估計來確定參數值。貝葉斯學派則認為參數是未觀察到的隨機變量，其本身也可有分佈，因此，可假定參數服從一個先驗分佈，然後基於觀察到的數據來計算參數的後驗分佈。</p><p>定理 1. 最大後驗估計的結果是優化如下形式的損失函數</p><p>Proof. 利用樣例的獨立同分布假設，</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RKwKVCYVyteRF><p>經驗風險和結構風險的含義? L(θ) 的第一項稱為經驗風險 (empirical risk)，用於描述模型與訓練數據的契合程度。第二項稱為結構風險 (structural risk) 或正則化項 (regularization term)，源於模型的先驗概率，表述了我們希望獲得何種性質的模型 (例如希望獲得複雜度較小的模型)。λ 稱為正則化常數，對兩者進行折中。</p><p>結構風險的作用? (1). 為引入領域知識和用戶意圖提供了途徑。(2). 有助於削減假設空間，從而降低了最小化訓練誤差的過擬合風險。這也可理解為一種 “罰函數法”，即對不希望得到的結果施以懲罰，從而使得優化過程趨向於希望目標。ℓp 範數是常用的正則化項。</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RKwKVClDn3z79v><p>其中先驗分佈</p><p>的參數</p><p>轉化為正則化常數 λ。</p><p>為什麼最常假設參數的先驗分佈是高斯分佈 (或最常使用</p><p>正則化)? 這是因為高斯分佈 N (µ; Σ) 是所有均值和熵存在且協方差矩陣是 Σ 的分佈中熵最大的分佈。最大熵分佈是在特定約束下具有最大不確定性的分佈。在沒有更多信息的情況下，那些不確定的部分都是 “等可能的”。在設計先驗分佈 p(θ) 時，除了我們對參數的認知 (例如均值和值域) 外，我們不想引入任何其餘的偏見 (bias)。因此最大熵先驗 (對應</p><p>正則化) 常被使用。除高斯先驗外，還可以使用不提供信息的先驗(uninformative prior)，其在一定範圍內均勻分佈，對應的損失函數中沒有結構風險這一項。</p><p>(3). 對損失函數 L(θ) 進行梯度下降優化。</p><p>梯度下降的細節留在下一節介紹。</p><p>概率方法的優缺點各是什麼? 優點: 這種參數化的概率方法使參數估計變得相對簡單。缺點: 參數估計結果的準確性嚴重依賴於所假設的概率分佈形式是否符合潛在的真實數據分佈。在現實應用中，欲做出能較好地接近潛在真實分佈的假設，往往需在一定程度利用關於應用任務本身的經驗知識，否則僅憑 “猜測”來假設概率分佈形式，很可能產生誤導性的結果。我們不一定非要概率式地解釋這個世界，在不考慮概率的情況下，直接找到分類邊界，也被稱為判別函數 (discriminant function)，有時甚至能比判別式模型產生更好的結果。</p><p><strong>1.2 梯度下降</strong></p><p>我們的目標是求解下列無約束的優化問題。</p><p>其中 L(θ) 是連續可微函數。梯度下降是一種一階 (frstorder) 優化方法，是求解無約束優化問題最簡單、最經典的求解方法之一。</p><p>梯度下降的基本思路? 梯度下降貪心地迭代式地最小化 L(θ)。梯度下降希望找到一個方向 (單位向量) v 使得 L 在這個方向下降最快，並在這個方向前進 α 的距離</p><p>定理 3. 梯度下降的更新規則是公式 5。重複這個過程，可收斂到局部極小點。</p><p>Proof. 我們需要找到下降最快的方向 v 和前進的距離α。</p><p>(1). 下降最快的方向 v。利用泰勒展開</p><p>的一階近似，</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RKwKVRcI3gOcWK><p>即下降最快的方向是損失函數的負梯度方向。</p><p>(2). 前進的距離 α。我們希望在開始的時候前進距離大一些以使得收斂比較快，而在接近最小值時前進距離小一些以不錯過最小值點。因此，我們設前進距離為損失函數梯度的一個倍數</p><p>其中 η 被稱為學習率 (learning rate)。</p><p>向公式 7 代入最優的</p><p>和</p><p>後即得。</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RKwKVZq4K4otrx><p>則稱 f 為區間 [a,b] 上的凸函數 (convex function)。當 通常是凸函數。</p><p><strong>2 線性迴歸</strong></p><p><strong>2.1 建模流程</strong></p><p>線性迴歸 (linear regression) 迴歸問題</p><p>。其建模方法包括如下三步 (參見第 1.1 節)。</p><p>(1). 對 p(y | x; θ) 進行概率假設。</p><p>我們假設</p><p>被稱為誤差項，捕獲了 (a)。特徵向量 x 中沒有包含的因素.</p><p>(b). 隨機噪聲。對不同的樣本</p><p>是獨立同分布地從中</p><p>進行採樣得到的。</p><p>線性迴歸的假設函數是</p><p>為了書寫方便，我們記</p><p>那麼公式 12 等價於</p><p>在本文其餘部分我們將沿用這一簡化記號。因此，</p><p>(2). 對參數 θ 進行最大後驗估計。</p><p>定理 7. 假設參數 θ 服從高斯先驗，對參數 θ 進行最大後驗估計等價於最小化如下損失函數</p><p>其中</p><p>被稱為平方損失 (square loss)。在線性迴歸中，平方損失就是試圖找到一個超平面</p><p>，使所有樣本到該超平面的歐式距離 (Euclidean distance) 之和最小。</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RKwKVsvGYDiK8G><p>Proof</p><p>其中，最後一行只是為了數學計算上方便，下文推導對數機率迴歸和 Softmax 迴歸時的最後一步亦然。</p><p>(3). 對損失函數 L(θ) 進行梯度下降優化。</p><p>可以容易地得到損失函數對參數的偏導數</p><p><strong>2.2 線性迴歸的閉式解</strong></p><p>線性迴歸對應的平方損失的函數形式比較簡單，可以通過求</p><p>直接得到最優解。</p><p>定理 8. 線性迴歸的閉式解為</p><p>Proof. L(θ) 可等價地寫作</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RKwKW5B61DHVBp><p>令</p><p>那麼</p><p>求解</p><p>即得。</p><p>不可逆的情況及解決方案? (1). 屬性數 d+1 多於樣例數 m。(2). 屬性之間線性相關。通過正則化項</p><p>mλI，即使</p><p>不可逆，</p><p>+ mλI 仍是可逆的。</p><p><strong>2.3 其他正則化迴歸模型</strong></p><p>事實上，上文介紹的線性迴歸模型是嶺迴歸 (ridge regression)。根據正則化項的不同，有三種常用的線性迴歸模型，見表 1。</p><p>基於 ℓ0、ℓ1 和 ℓ2 範數正則化的效果? ℓ2 範數傾向於 w 的分量取值儘量均衡，即非零分量個數儘量稠密。而 ℓ0“範數”和 ℓ1 範數則傾向於 w 的分量儘量稀疏，即非零分量個數儘量少，優化結果得到了僅採用一部分屬性的模型。也就是說，基於 ℓ0“範數”和 ℓ1 範數正則化的學習方法是一種嵌入式 (embedding) 特徵選擇方法，其特徵選擇過程和學習器訓練過程融為一體，兩者在同一個優化過程中完成。事實上，對 w 施加稀疏約束最自然的是使用 ℓ0“範數”。但 ℓ0“範數”不連續，難以優化求解。因此常採用 ℓ1 範數來近似。</p><p>為什麼 ℓ1 正則化比 ℓ2 正則化更易於獲得稀疏解？假設</p><p>，則</p><p>。我們繪製出平方損失項、ℓ1 範數和 ℓ2 範數的等值線 (取值相同的點的連線)，如圖 1 所示。LASSO 的解要在平方損失項和正則化項之間折中，即出現在圖中平方誤差項等值線和正則化項等值線的相交處。從圖中可以看出，採用 ℓ1 正則化時交點常出現在座標軸上 (w<sub>2</sub>= 0), 而採用 ℓ2 正則化時交點常出現在某個象限中 (w<sub>1</sub>，w<sub>2</sub>均不為 0)。</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RKwKWKI9P5Ol6v><p>Figure 1: ℓ1 正則化 (紅色) 比 ℓ2 正則化 (黑色) 更易於獲得稀疏解。本圖源於 [17]。</p><p>考慮一般的帶有 ℓ1 正則化的優化目標</p><p>若 ℓ(θ) 滿足 L-Lipschitz 條件，即</p><p>優化通常使用近端梯度下降 (proximal gradient descent, PGD) [1]。PGD 也是一種貪心地迭代式地最小化策略，能快速地求解基於 ℓ1 範數最小化的方法。</p><p>定理 9. 假設當前參數是</p><p>，PGD 的更新準則是</p><p>其中</p><p>Proof. 在</p><p>附近將 ℓ(θ) 進行二階泰勒展開近似</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RKwKWRB46ptBjz><p>由於 θ 各維互不影響 (不存在交叉項)，因此可以獨立求解各維。</p><p>在 LASSO 的基礎上進一步發展出考慮特徵分組結構的 Group LASSO [14] 、考慮特徵序結構的 Fused LASSO [11] 等變體。由於凸性不嚴格，LASSO 類方法可能產生多個解，該問題通過彈性網(elastic net)得以解決 [16] .</p><p><strong>2.4 存在異常點數據的線性迴歸</strong></p><p>一旦數據中存在異常點 (outlier)，由於平方損失計算的是樣本點到超平面距離的平方，遠離超平面的點會對迴歸結果產生更大的影響，如圖 2 所示。平方損失對應於假設噪聲服從高斯分佈</p><p>，一種應對異常點的方法是取代高斯分佈為其他更加重尾 (heavy tail) 的分佈，使其對異常點的容忍能力更強，例如使用拉普拉斯分佈</p><p>，如圖 3 所示。</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RKwKWZI1SGEram><p>Figure 2：存在異常點 (圖下方的三個點) 時普通線性迴歸 (紅色) 和穩健線性迴歸 (藍色)。本圖源於 [7]。</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RKwKWZZGcu7hji><p>Figure 3: 高斯分佈 N (0,1) (紅色) 和拉普拉斯分佈Lap(0,1) (藍色)。本圖源於：https://www.epixanalytics.com/modelassist/AtRisk/images/15/image632.gif</p><p>定 義 2 (拉 普 拉 斯 分 布 (Laplace distribution) Lap(µ,b))，又稱為雙邊指數分佈 (double sided exponential distribution)，具有如下的概率密度函數</p><p>該分佈均值為 µ，方差為</p><p>定理 10. 假設參數服從高斯先驗，</p><p>對參數 θ 進行最大後驗估計等價於最小化如下損失函數</p><p>Proof</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RKwKWiT8CAZsTL><p>由於絕對值函數不光滑，不便基於梯度下降對公式 33 進行優化。通過分離變量技巧，可將其轉化為二次規劃 (quadratic programming) 問題，隨後調用現有的軟件包進行求解。我們在下一章形式化 SVR 時還會再使用這個技巧。</p><p>定理 11. 最小化公式 33 等價於如下二次規劃問題，其包含 d + 1 + 2m 個變量，3m 個約束：</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RKwKWigBIFl865><p>此外，為了結合高斯分佈 (對應平凡損失) 容易優化和拉普拉斯分佈 (對應 ℓ1 損失) 可以應對異常值的優點，Huber 損失[5]在誤差接近 0 時為平方損失，在誤差比較大時接近 ℓ1 損失，如圖 4 所示。</p><p>Huber 損失處處可微，使用基於梯度的方法對 Huber 損失進行優化會比使用拉普拉斯分佈更快。</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RKwKWj4CsToSNe><p>Figure 4: ℓ2 損失 (紅色)、ℓ1 損失 (藍色) 和 Huber 損失 (綠色)。本圖源於 [7]。</p><p><strong>2.5 廣義線性模型</strong></p><p>線性迴歸利用屬性的線性組合</p><p>進行預測。除了直接利用</p><p>逼近 y 外，還可以使模型的預測值逼近 y 的衍生物。考慮單調可微函數 g，令</p><p>這樣得到的模型稱為廣義線性模型 (generalized linear model)，其中函數 g 被稱為聯繫函數 (link function)。本文介紹的線性迴歸、對數機率迴歸和 Softmax 迴歸都屬於廣義線性模型，如表 2 所示。</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RKwKWuYBpn3cRo><p>廣義線性模型的優點? (1). 形式簡單、易於建模。(2). 很好的可解釋性。</p><p>直觀表達了各屬性在預測中的重要性。</p><p>如何利用廣義線性模型解決非線性問題? (1). 引入層級結構。例如深度學習是對樣本 x 進行逐層加工，將初始的低層表示轉化為高層特徵表示後使用線性分類器。(2). 高維映射。例如核方法將 x 映射到一個高維空間 ϕ(x) 後使用線性分類器。</p><p><strong>3 對數機率迴歸</strong></p><p><strong>3.1 建模流程</strong></p><p>對數機率迴歸 (logistic regression) 應對二分類問題。其建模方法包括如下三步 (參見第 1.1 節)。</p><p><strong>(1). 對 p(y | x, θ) 進行概率假設。</strong></p><p>對二分類任務，標記</p><p>，而</p><p>產生的是實數值，於是，我們需要找到一個單調可微函數 g 將</p><p>轉化為</p><p>。最理想的是用單位階躍函數</p><p>當</p><p>大於 0 時輸出 1，小於 0 時輸出 0。但是，單位階躍函數不連續不可微，無法利用梯度下降方法進行優化。因此，我們希望找到一個能在一定程度上近似單位階躍函數並單調可微的替代函數 (surrogate function)。</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RKwKX0l5NBxDLa><p>Figure 5: 單位階躍函數 (紅色) 與對數機率函數 (黑色)。本圖源於 [17]。</p><p>如圖 5 所示，對數機率函數 (sigmoid function) 正是這樣一個常用的替代函數</p><p>我們將其視為後驗概率估計，即</p><p>那麼</p><p>兩者可以合併寫作</p><p>也就是說，y | x,θ 服從伯努利分佈 Ber(sigm</p><p>)。</p><p><strong>(2). 對參數 θ 進行最大後驗估計。</strong></p><p>定理 12. 假設參數 θ 服從高斯先驗，對參數 θ 進行最大後驗估計等價於最小化如下損失函數</p><p>其中</p><p>稱為對數機率損失 (logistic loss)。</p><p>Proof</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RKwKX77DeeRyFb><p>注意到</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RKwKXED4cE5ioh><p>因此</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RKwKXETFi5bp94><p><strong>(3). 對損失函數 L(θ) 進行梯度下降優化。</strong></p><p><strong>3.2 與廣義線性模型的關係</strong></p><p>對數機率迴歸的假設函數</p><p>等價於</p><p>，其中</p><p>被稱為機率 (odds)，反映 x 作為正例的相對可能性。</p><p>被稱為對數機率 (log odds, logit)，公式 50 實際上在用線性迴歸模型的預測結果逼近真實標記的對數機率，這是對數機率迴歸名稱的由來。</p><p>對數機率迴歸的優點? (1). 直接對分類的可能性進行建模 (假設 p(y | x, θ) 服從伯努利分佈)，無需事先假設樣本 x 的分佈，這樣避免了假設分佈不準確所帶來的問題。(2). 不僅能預測出類別，還可以得到近似概率預測，對許多需要概率輔助決策的任務很有用。(3). 對數機率的目標函數是凸函數，有很好的數學性質。</p><p>引理 13. 對數機率損失函數是凸函數。</p><p>Proof. 在</p><p>的基礎上，進一步可求得</p><p>是一個半正定矩陣。</p><p><strong>3.3</strong></p><p>的對數機率迴歸</p><p>為了概率假設方便，我們令二分類問題的標記</p><p>。有時，我們需要處理</p><p>形式的分類問題。對數機率損失函數需要進行相應的改動。</p><p><strong>(1). 對 p(y | x, θ) 進行概率假設。</strong></p><p>我們假設</p><p>那麼</p><p>兩者可以合併寫作</p><p><strong>(2). 對參數 θ 進行最大後驗估計。</strong></p><p>定理 14. 假設參數 θ 服從高斯先驗，對參數 θ 進行最大後驗估計等價於最小化如下損失函數</p><p>其中</p><p>稱為對數機率損失 (logistic loss)。</p><p>Proof</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RKwKXZcA3Hbvzz><p>(3). 對損失函數 L(θ) 進行梯度下降優化。</p><p><strong>4 Softmax 迴歸</strong></p><p><strong>4.1 建模流程</strong></p><p>Softmax 迴歸應對多分類問題，它是對數機率迴歸向多分類問題的推廣。其建模方法包括如下三步 (參見第 1.1 節)。</p><p><strong>(1). 對 p(y | x, θ) 進行概率假設。</strong></p><p>對數機率迴歸假設 p(y | x, θ) 服從伯努利分佈，Softmax 迴歸假設 p(y | x, θ) 服從如下分佈</p><p>令</p><p>假設函數可以寫成矩陣的形式</p><p><strong>(2). 對參數 θ 進行最大後驗估計。</strong></p><p>定理 15. 假設參數 θ 服從高斯先驗，對參數 θ 進行最大後驗估計等價於最小化如下損失函數</p><p>其中</p><p>稱為交叉熵損失 (cross-entropy loss)。</p><p>Proof</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RKwKXkPDy1nLhn><p><strong>(3). 對損失函數 L(θ) 進行梯度下降優化。</strong></p><p>損失函數對應於類別 k 的參數</p><p>的導數是</p><p>寫成矩陣的形式是</p><p>其中</p><p>的第 k 個元素是 1，其餘元素均為 0。對比公式 20 、49 和 67 ，損失函數的梯度有相同的數學形式</p><p>區別在於假設函數</p><p>的形式不同。事實上，所有的廣義線性模型都有類似於公式 68 的更新準則。</p><p><strong>4.2 交叉熵</strong></p><p>定義由訓練集觀察得到的分佈，稱為經驗分佈 (empirical distribution)。經驗分佈</p><p>對應於第 i 個樣例，定義</p><p>。另一方面，</p><p>是由模型估計出的概率。</p><p>定理 16. 交叉熵損失旨在最小化經驗分佈</p><p>和學得分佈</p><p>之間的交叉熵。這等價於最小化</p><p>和</p><p>之間的 KL 散度，迫使估計的分佈</p><p>近似目標分佈</p><p>。</p><p>Proof</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RKwKXxt98R5RYE><p><strong>5 樸素貝葉斯分類器</strong></p><p>樸素貝葉斯分類器 (naive Bayes classifer) 也是一種概率方法，但它是一種生成式模型。在本節，我們首先回顧生成式模型，之後介紹樸素貝葉斯分類器的建模流程。</p><p>5.1 生成式模型</p><p>判別式模型和生成式模型各是什麼? 判別式模型(discriminant model) 直接對 p(y | x) 進行建模，生成式模型 (generative model) 先對聯合分佈 p(x, y) = p(x | y)p(y) 進行建模，然後再得到</p><p>其中，p(y) 是類先驗 (prior) 概率，表達了樣本空間中各類樣本所佔的比例。p(x | y) 稱為似然 (likelihood)。p(x) 是用於歸一化的證據 (evidence)。由於其和類標記無關，該項不影響 p(y | x) 的估計</p><p>如何對類先驗概率和似然進行估計? 根據大數定律，當訓練集包含充足的獨立同分布樣本時，p(y) 可通過各類樣本出現的頻率來進行估計</p><p>而對似然 p(x | y)，由於其涉及 x 所有屬性的聯合概率，如果基於有限訓練樣本直接估計聯合概率，(1). 在計算上將會遭遇組合爆炸問題。(2). 在數據上將會遭遇樣本稀疏問題，很多樣本取值在訓練集中根本沒有出現，而“未被觀測到”與“出現概率為零”通常是不同的。直接按樣本出現的頻率來估計會有嚴重的困難，屬性數越多，困難越嚴重。</p><p>判別式模型和生成式模型的優缺點? 優缺點對比如表 3 所示。</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RKwKXyj9M6Edl4><p><strong>5.2 建模流程</strong></p><p><strong>(1). 對 p(x | y, θ) 進行概率假設。</strong></p><p>生成式模型的主要困難在於, 類條件概率 p(x | y)是所有屬性的聯合概率，難以從有限的訓練樣本直接估計而得。為避開這個障礙，樸素貝葉斯分類器採用了屬性條件獨立性假設：對已知類別，假設所有屬性相互獨立。也就是說，假設每個屬性獨立地對分類結果發生影響</p><p>此外，對連續屬性，進一步假設</p><p>因此，樸素貝葉斯分類器的假設函數是</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RKwKY7L6FrUuxq><p><strong>(2). 對參數 θ 進行最大後驗估計。</strong>參數 θ 包括了第 c 類樣本在第 j 個屬性上的高斯分佈的均值</p><p>和方差</p><p>。</p><p>定理 17. 假設參數 θ 服從不提供信息的先驗，對參數 θ 進行最大後驗估計的結果是</p><p>Proof. 代入公式 76</p><img alt=機器學習算法中的概率方法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RKwKYDKHIeNeyu><p><strong>5.3 離散屬性的參數估計</strong></p><p>樸素貝葉斯分類器可以很容易地處理離散屬性。</p><p>可估計為</p><p>然而，若某個屬性值在訓練集中沒有與某個類同時出現過，則根據公式 82 估計得到 0。代入公式 75 得到 -1。因此，無論該樣本的其他屬性是什麼，分類結果都不會是 y = c，這顯然不太合理。</p><p>為了避免其他屬性攜帶的信息被訓練集中未出現的屬性值“抹去”，在估計概率值時通常要進行平滑(smoothing)，常用拉普拉斯修正 (Laplacian correction)。具體的說，令 K 表示訓練集 D 中可能的類別數，n<sub>j</sub>表示第 j 個屬性可能的取值數，則概率估計修正為</p><p>拉普拉斯修正實際上假設了屬性值與類別均勻分佈，這是在樸素貝葉斯學習中額外引入的關於數據的先驗。在訓練集變大時，修正過程所引入的先驗的影響也會逐漸變得可忽略，使得估值漸趨向於實際概率值。</p><p>在現實任務中樸素貝葉斯有多種實現方式。例如，若任務對預測速度要求較高，則對給定訓練集，可將樸素貝葉斯分類器涉及的所有概率估值事先計算好存儲起來，這樣在進行預測時只需查表即可進行判別。若任務數據更替頻繁，則可採用懶惰學習方式，先不進行任何訓練，待收到預測請求時再根據當前數據集進行概率估值。若數據不斷增加，則可在現有估值基礎上，僅對新增樣本的屬性值所涉及的概率估值進行計數修正即可實現增量學習。</p><p>定義 3 (懶惰學習 (lazy learning))。這類學習技術在訓練階段僅僅是把樣本保存起來，訓練時間開銷是 0，待收到測試樣本後再進行處理。相應的，那些在訓練階段就對樣本進行學習處理的方法稱為急切學習(eager learning)。</p><p>定義 4 (增量學習 (incremental learning))。在學得模型後，再接收到訓練樣例時，僅需根據新樣例對模型進行更新，不必重新訓練整個模型，並且先前學得的有效信息不會被“沖掉”。</p><p><strong>5.4 樸素貝葉斯分類器的推廣</strong></p><p>樸素貝葉斯分類器採用了屬性條件獨立性假設，但在現實任務中這個假設往往很難成立。於是，人們嘗試對屬性條件獨立性假設進行一定程度的放鬆，適當考慮一部分屬性間的相互依賴關係，這樣既不需要進行完全聯合概率計算，又不至於徹底忽略了比較強的屬性依賴關係，由此產生一類半樸素貝葉斯分類器 (semi-naive Bayes classifers) 的學習方法。</p><p>獨依賴估計 (one-dependent estimator, ODE) 是最常用的一種策略，其假設每個屬性在類別之外最多依賴於一個其他屬性 (稱為父屬性)。問題的關鍵在於如何確定每個屬性的父屬性。SPODE (super-parent ODE) 假設所有屬性都依賴於同一個屬性，稱為超父 (superparent)。TAN (tree augmented naive Bayes) [4] 以屬性節點構建完全圖，任意兩結點之間邊的權重設為這兩個屬性之間的條件互信息</p><p>。之後構建此圖的最大帶權生成樹，挑選根變量，將邊置為有向，以將屬性間依賴關係約簡為樹形結構。最後加入類別結點 y，增加從 y 到每個屬性的有向邊。TAN 通過條件互信息刻畫兩屬性的條件相關性，最終保留了強相關屬性之間的依賴性。AODE (averaged ODE) [13] 嘗試將每個屬性作為超父來構建 SPODE，之後將那些具有足夠訓練數據支撐的 SPODE 集成作為最終結果。AODE 的訓練過程也是“計數”，因此具有樸素貝葉斯分類器無需模型選擇、可預計算節省預測時間、也能懶惰學習、並且易於實現增量學習。</p><p>能否通過考慮屬性間高階依賴進一步提升泛化性能? 相比 ODE, kDE 考慮最多 k 個父屬性。隨著依賴的屬性個數 k 的增加，準確進行概率估計所需的訓練樣本數量將以指數級增加。因此，若訓練數據非常充分，泛化性能有可能提升。但在有限樣本條件下，則又陷入高階聯合概率的泥沼。</p><p>更進一步，貝葉斯網 (Bayesian network)，也稱為信念網 (belief network)，能表示任意屬性間的依賴性。貝葉斯網是一種概率圖模型，藉助有向無環圖刻畫屬性間的依賴關係。</p><p>事實上，雖然樸素貝葉斯的屬性條件獨立假設在現實應用中往往很難成立，但在很多情形下都能獲得相當好的性能 [2, 8]。一種解釋是對分類任務來說，只需各類別的條件概率排序正確，無須精準概率值即可導致正確分類結果 [2]。另一種解釋是，若屬性間依賴對所有類別影響相同，或依賴關係能相互抵消，則屬性條件獨立性假設在降低計算開銷的同時不會對性能產生負面影響 [15]。樸素貝葉斯分類器在信息檢索領域尤為常用 [6]。</p><p><strong>6 快問快答</strong></p><p>隨機梯度下降和標準梯度下降的優缺點各是什麼?</p><p>• 參數更新速度。標準梯度下降需要遍歷整個訓練集才能計算出梯度，更新較慢。隨機梯度下降只需要一個訓練樣例即可計算出梯度，更新較快。</p><p>• 冗餘計算。當訓練集樣本存在冗餘時，隨機梯度下降能避免在相似樣例上計算梯度的冗餘。</p><p>• 梯度中的隨機因素/噪聲。標準梯度下降計算得到的梯度沒有隨機因素，一旦陷入局部極小將無法跳出。隨機梯度下降計算得到的梯度有隨機因素，有機會跳出局部極小繼續優化。</p><p>實際應用時，常採用隨機梯度下降和標準梯度下降的折中，即使用一部分樣例進行小批量梯度下降。此外，相比隨機梯度下降，小批量梯度下降還可以更好利用矩陣的向量化計算的優勢。</p><p>梯度下降和牛頓法的優缺點各是什麼?</p><p>• 導數階數。梯度下降只需要計算一階導數，而牛頓法需要計算二階導數。一階導數提供了方向信息(下降最快的方向)，二階導數還提供了函數的形狀信息。</p><p>• 計算和存儲開銷。牛頓法在參數更新時需要計算 Hessian 矩陣的逆，計算和存儲開銷比梯度下降更高。</p><p>• 學習率。梯度下降對學習率很敏感，而標準的牛頓法不需要設置學習率。</p><p>• 收斂速度。牛頓法的收斂速度比梯度下降更快。</p><p>• 牛頓法不適合小批量或隨機樣本。</p><p>實際應用時，有許多擬牛頓法旨在以較低的計算和存儲開銷近似 Hessian 矩陣。</p><p>線性迴歸的損失函數及梯度推導。</p><p>答案見上文。</p><p>為什麼要使用正則化，ℓ1 和 ℓ2 正則化各自對應什麼分佈，各有什麼作用?</p><p>答案見上文。</p><p>對數機率迴歸的損失函數及梯度推導。</p><p>答案見上文。</p><p>線性分類器如何擴展為非線性分類器?</p><p>答案見上文。</p><p>判別式模型和生成式模型各是什麼，各自優缺點是什麼，常見算法中哪些是判別式模型，哪些是生成式模型?</p><p>答案見上文。</p><p>貝葉斯定理各項的含義?</p><p>答案見上文。</p><p>樸素貝葉斯為什麼叫“樸素”貝葉斯?</p><p>為了避開從有限的訓練樣本直接估計 p(x | y) 的障礙，樸素貝葉斯做出了屬性條件獨立假設，該假設在現實應用中往往很難成立。</p><p><strong>References</strong></p><p>[1] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. Multiscale Modeling & Simulation, 4(4):1168–1200, 2005. 5</p><p>[2] P. M. Domingos and M. J. Pazzani. On the optimality of the simple bayesian classifer under zero-one loss. Machine Learning, 29(2-3):103–130, 1997. 12</p><p>[3] B. Efron. Bayesians, frequentists, and scientists. Journal of the American Statistical Association, 100(469):1–5, 2005. 1</p><p>[4] N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classifers. Machine Learning, 29(2-3):131–163,1997. 12</p><p>[5] P. J. Huber. Robust estimation of a location parameter. Annals of Statistics, 53(1):492–518, 1964. 6</p><p>[6] D. D. Lewis. Naive (bayes) at forty: The independence assumption in information retrieval. In Proceedings of the 10th European Conference on Machine Learning(ECML), pages 4–15, 1998. 13</p><p>[7] K. P. Murphy. Machine Learning: A Probabilistic Perspective. MIT Press, 2012. 5, 6</p><p>[8] A. Y. Ng and M. I. Jordan. On discriminative vs. generative classifers: A comparison of logistic regression and naive bayes. In Advances in Neural Information Processing Systems 14 (NIPS), pages 841–848, 2001.12</p><p>[9] F. J. Samaniegos. A Comparison of the Bayesian and Frequentist Approaches to Estimation. Springer Science & Business Media, 2010. 1</p><p>[10] R. Tibshirani. Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society. Series B (Methodological), pages 267–288, 1996. 4</p><p>[11] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. Sparsity and smoothness via the fused lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(1):91–108, 2005. 5</p><p>[12] A. N. Tikhonov and V. I. Arsenin. Solutions of Ill-posed Problems. Winston, 1977. 4</p><p>[13] G. I. Webb, J. R. Boughton, and Z. Wang. Not so naive bayes: Aggregating one-dependence estimators. Machine Learning, 58(1):5–24, 2005. 12</p><p>[14] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49–67, 2006. 5</p><p>[15] H. Zhang. The optimality of naive bayes. In Proceedings of the Seventeenth International Florida Artifcial Intelligence Research Society Conference (FLAIRS), pages 562–567, 2004. 13</p><p>[16] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320, 2005. 5</p><p>[17] 周志華. 機器學習. 清華大學出版社, 2016. 5, 7, 12</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>算法</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E5%AD%B8/860c0db2.html alt="機器學習 | 算法筆記（三）- 支持向量機算法以及代碼實現" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/6049df8f144648df96528347c71be41c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/860c0db2.html title="機器學習 | 算法筆記（三）- 支持向量機算法以及代碼實現">機器學習 | 算法筆記（三）- 支持向量機算法以及代碼實現</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>