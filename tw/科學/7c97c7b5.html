<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” | 极客快訊</title><meta property="og:title" content="「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/1530358602315057a254449"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/7c97c7b5.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/7c97c7b5.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/7c97c7b5.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/7c97c7b5.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/7c97c7b5.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/7c97c7b5.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/7c97c7b5.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/7c97c7b5.html><meta property="article:published_time" content="2020-11-14T20:56:57+08:00"><meta property="article:modified_time" content="2020-11-14T20:56:57+08:00"><meta name=Keywords content><meta name=description content="「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑”"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/7c97c7b5.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑”</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><div><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358602315057a254449><p class=pgc-img-caption></p></div><p>週末好啊各位同學，我們又見面了。</p><p>科幻名著《三體》裡有句犀利的臺詞——降低維度用於攻擊。不過，這個“降維”絕對不只是科幻界的專用名詞。</p><p>在機器學習中，你同樣得了解它。</p><p>很多初學者往往會把降維（Dimensionality reduction），特徵選擇（feature selection），以及特徵提取（feature extraction）混為一談，因為這三者都削減了進入模型的變量個數。</p><p>但降維是一個更為寬泛的概念，它包括了特徵選擇和特徵提取。</p><p>雖然降維過後，最終使用的變量個數減少了，但特徵選擇挑選的是特徵子集，也就是說，保留下來的所有特徵都在原來的特徵集中可以找到；而特徵提取所提取的是不再是特徵子集，而是原來特徵的線性（或者非線性）組合，我們經過特徵提取後的變量都是新的變量，它的本質是將原始高維空間向低維空間投影，我們所使用的特徵不僅少了，而且不再是原來的特徵。</p><p>距離是機器學習中的一個很重要的概念。每個樣本可以表示為一個向量，也就是高維空間的一個點，距離可以用來衡量樣本之間的相似度。但是在高維空間，距離的計算會變得非常困難，而我們關心的問題可能在低維空間就會得到很好的解決。但這不意味著低維空間只是對高維空間的近似，有些問題中，高維空間會增加很多噪聲，而在低維空間中會得到比高維空間更好的性能。</p><p>在上週《如何進行特徵選擇（理論篇）》的學習中，相信大家已經對特徵選擇有了足夠的認識，所以本文的“降維”特指特徵提取。</p><p>對於降維有兩種分類方式：其一，根據目標值（target）的參與與否，分為有監督降維和無監督降維；其二，根據高維空間與低維空間的關係，分為線性降維和非線性降維。</p><p>我們對每種方法分舉一例：</p><p>線性\監督</p><p>無監督</p><p>監督</p><p>線性</p><p>PCA</p><p>LDA</p><p>非線性</p><p>ISOMAP</p><p>KLDA</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153035860228957e666f787><p class=pgc-img-caption></p></div><p><strong>主成分分析（PCA）</strong></p><blockquote><p>數學準備：</p></blockquote><p><strong>1.協方差矩陣</strong>：隨機變量組成的向量，每組隨機變量的協方差構成的一個對稱矩陣，其對角元是每組隨機變量的方差</p><p><strong>2.矩陣的對角化</strong>：對於矩陣M，有可逆矩陣V，使得</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15303586021168851740122><p class=pgc-img-caption></p></div><p>成為對角矩陣，而M的特徵值對應的特徵向量組成了該可逆矩陣V。（換而言之，矩陣V的每一列對應著M的特徵向量）</p><p><strong>3.正交矩陣</strong>：轉置矩陣等於其逆矩陣（</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15303586022753d4521984a><p class=pgc-img-caption></p></div><p>），構成矩陣的列向量彼此正交。</p><p><strong>4.數據中心化</strong>：對每組隨機變量減去均值，再除以標準差。本質是將每組隨機變量變為標準的高斯分佈。</p><p>PCA（Principal component analysis）是用投影的方法將高維空間壓縮到低維。</p><p>想象一下，此時你站在路燈下面，你本身是三維的（此時此刻除去了時間維度），你的影子卻在一個二維平面上。</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358602382319e5a16b0><p class=pgc-img-caption></p></div><p>如圖，我們將二維空間的點投影到一條直線上。</p><p>但是，我們有無數個投影的方向，就像上圖我們可以找出無數條直線來進行投影，那麼哪條直線，哪個方向才是最好的呢？PCA的目標就是，找一條直線，使得投影之後的點儘可能的遠離彼此，因為點之間的互相遠離而不是相互重疊，就意味著某些距離信息被保留了下來。</p><p>在高維空間（維數D）的所有的樣本可以被表示為一個向量:</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/153035860230542764127df><p class=pgc-img-caption></p></div><p>在投影之後的低維空間（維數d），樣本也是一個向量：</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15303586023994b26dab1bb><p class=pgc-img-caption></p></div><p>向量的變化可以通過一個矩陣聯繫起來，這個矩陣我們把它叫做投影矩陣，它的作用是將一個高維向量投影到低維空間得出一個低維向量：</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15303586025856cd0dfce87><p class=pgc-img-caption></p></div><p>此時，中心化數據的優勢就體現了出來，因為經過中心化的數據，</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1530358602563be4d704fbf><p class=pgc-img-caption></p></div><p>，這就意味著數據的協方差矩陣就成了</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358602622552ab123d3><p class=pgc-img-caption></p></div><p>，投影之後的協方差矩陣就成為了</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358602576773ead440f><p class=pgc-img-caption></p></div><p>,我們的目標是使其方差最大，而協方差矩陣的對角元正是方差，所以我們只需要對其求跡：</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153035860265690fee200c7><p class=pgc-img-caption></p></div><p>換而言之，我們需要找的投影矩陣W其實是一個使</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15303586026942327f95428><p class=pgc-img-caption></p></div><p>對角化的可逆矩陣，而它的轉置等於它的逆</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358602809907d4e3647><p class=pgc-img-caption></p></div><p>。所以我們尋找W的過程，就是尋找</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15303586026942327f95428><p class=pgc-img-caption></p></div><p>的特徵向量的過程，而方差最大化的過程，也就是尋找</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15303586026942327f95428><p class=pgc-img-caption></p></div><p>最大特徵值的過程。</p><p>所以，我們只需要對</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15303586026942327f95428><p class=pgc-img-caption></p></div><p>做特徵值分解，將其特徵值排序，取到前面的d個特徵向量，彼此正交，構成了投影矩陣W，而它們所張成的低維空間，就是使得投影點方差最大的低維空間。</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603032612f2d08f7><p class=pgc-img-caption></p></div><p>如圖，這是對一個二元高斯分佈用PCA進行降維後的結果，這個平面就是由兩個最大的特徵值對應的特徵向量所張成，可以看出，特徵向量彼此正交，且首先找到的是最大的特徵值對應的特徵向量，逐步尋找第二個，第三個.....如果我們的目標空間是n維，就會取到前n個。</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153035860228957e666f787><p class=pgc-img-caption></p></div><p><strong>線性判別分析（LDA）</strong></p><blockquote><p>數學準備：</p></blockquote><p><strong>1.均值向量</strong>：由多組隨機變量組成的向量，對每一組隨機變量取均值所構成的向量。</p><p><strong>2.厄米矩陣（Hermitan ）</strong>：轉置等於其本身的矩陣，</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358602884fe82c85c5f><p class=pgc-img-caption></p></div><p>。</p><p><strong>3.廣義瑞利熵（Rayleigh quotient </strong>）：若x為非零向量，則</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153035860292016695c7c12><p class=pgc-img-caption></p></div><p>為A,B的廣義瑞利熵，它的最大值是</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153035860300923d9ccbe5f><p class=pgc-img-caption></p></div><p>的最大特徵值。</p><p><strong>4.矩陣的奇異值分解</strong>：任何實矩陣M都可以被分解成為</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15303586029999a32fff26c><p class=pgc-img-caption></p></div><p>這三個矩陣的乘積。U和V均為正交矩陣。U的列向量是</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603080ea6261244d><p class=pgc-img-caption></p></div><p>的特徵向量，V的列向量是</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603129f00892e149><p class=pgc-img-caption></p></div><p>的特徵向量，同時奇異值的大小</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603080ea6261244d><p class=pgc-img-caption></p></div><p>是的特徵值的平方根。</p><p>LDA（Linear Discriminant Analysis）的基本思想也是將高維空間的樣本投影到低維空間，使信息損失最少。</p><p>與PCA不同在於，PCA只針對樣本矩陣，希望投影到低維空間之後，樣本投影點的方差最大；但LDA不僅針對樣本矩陣，還使用了類別信息，它希望投影到低維空間後，相同樣本的方差最小（相同樣本的集中化），不同樣本的距離最大（不同樣本離散化）。</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358603310d2274c9a36><p class=pgc-img-caption></p></div><p>如圖所示，將二維空間投影到一維空間，即一條直線上。圖2相比圖1，類間樣本距離更大，類內樣本方差更小。</p><p>以二分類問題為例，我們用</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153035860327650bee5d102><p class=pgc-img-caption></p></div><p>表示兩類樣本，用</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15303586033066c48ba37cf><p class=pgc-img-caption></p></div><p>表示兩類樣本的均值向量，用</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/153035860326875829f0a88><p class=pgc-img-caption></p></div><p>來表示兩類樣本的協方差矩陣，與PCA一樣，我們假設存在一個投影矩陣W，這些量會在低維空間變成：</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358603393b092b23d46><p class=pgc-img-caption></p></div><p>其中</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/15303586034164eecc3de93><p class=pgc-img-caption></p></div><p>分別為低維空間的樣本，均值向量和協方差矩陣。在投影空間的相同樣本的方差最小，意味著</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358603503135dc71d9e><p class=pgc-img-caption></p></div><p>最小；而不同樣本的距離最大，意味著</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603565529a6dc311><p class=pgc-img-caption></p></div><p>最大。</p><p>我們定義原始空間的樣本協方差矩陣之和為</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603598396fd70244><p class=pgc-img-caption></p></div><p>,類內散度矩陣（whithin-class scatter matrix），用來刻畫原始空間上相同樣本的方差：</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15303586035950b8f52f77d><p class=pgc-img-caption></p></div><p>同時定義類間散度矩陣（between-class scatter matrix）</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/15303586036441bedb76911><p class=pgc-img-caption></p></div><p>,用來刻畫原始空間上不同樣本的距離:</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603713d175837f10><p class=pgc-img-caption></p></div><p>將以上的原則結合起來，我們的目的就變成了：</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603772fffae92969><p class=pgc-img-caption></p></div><p>根據廣義瑞利熵的形式，我們尋求最大值就變成了對</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530358603826720199618e><p class=pgc-img-caption></p></div><p>進行奇異值分解，然後選取最大的奇異值和相應的特徵向量。這些特徵向量所張成的低維空間，就是我們的目標空間。</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358603866bbeb9d71cc><p class=pgc-img-caption></p></div><p><strong>讀芯君開扒</strong></p><p><strong>課堂TIPS</strong></p><p>• 降維在表示論中屬於低維表示，本質是將原本空間壓縮到更小的空間，在這個過程中保證信息損失的最小化。與之相對的是稀疏表示，它是將原本的空間嵌入到更大的空間，在這過程中保證信息損失的最小化。</p><p>• PCA有多種理解方式，除了在低維空間使得樣本方差最大化，也可以理解為最小重構均方誤差，將問題轉化為所選低維空間重構的數據與實際數據的差。引入貝葉斯視角，還可以將PCA理解為最小化高斯先驗誤差。如果從流形的角度看，就是把數據看作一個拓撲空間的點集，在高斯概率空間內找到一個對應的線性流形。</p><p>• PCA和LDA的優化目標均可以用拉格朗日乘子法解決。PCA同樣也可以通過奇異值分解來解決。奇異值分解方法可以理解為是特徵值分解的推廣，因為特徵值分解要求矩陣為一個方陣，但奇異值分解並無此要求。</p><div class=pgc-img><img alt=「週末AI課堂」線性降維方法（理論）｜機器學習你會遇到的“坑” onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530358603866bbeb9d71cc><p class=pgc-img-caption></p></div><p>作者：唐僧不用海飛絲</p><p>如需轉載，請後臺留言，遵守轉載規範</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>週末</a></li><li><a>AI</a></li><li><a>課堂</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/1801773a.html alt="AI芯天下丨虛擬工廠 ，2025智能製造新路徑" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/9fc63c421a37451d950eb1efcde49091 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1801773a.html title="AI芯天下丨虛擬工廠 ，2025智能製造新路徑">AI芯天下丨虛擬工廠 ，2025智能製造新路徑</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/60f3eef5.html alt="全球首個3D AI主播火了 虛擬製作時代將至" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/68cc49aeb805412eb4a9d126b12284bd style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/60f3eef5.html title="全球首個3D AI主播火了 虛擬製作時代將至">全球首個3D AI主播火了 虛擬製作時代將至</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a68154e3.html alt="衡真課堂 | 電力系統中7個最常見的知識點" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/038eab503e1b47469a24f92162e48713 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a68154e3.html title="衡真課堂 | 電力系統中7個最常見的知識點">衡真課堂 | 電力系統中7個最常見的知識點</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/098d4a35.html alt=「火爐煉AI」機器學習048-Harris檢測圖像角點 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d756b20a1dbc4ab4b4f22d6b61be2043 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/098d4a35.html title=「火爐煉AI」機器學習048-Harris檢測圖像角點>「火爐煉AI」機器學習048-Harris檢測圖像角點</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8acadb42.html alt=超能課堂(209)：從CD開始的音頻編碼進化之路 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2b134f30f50f45779c576c52e79fdfa5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8acadb42.html title=超能課堂(209)：從CD開始的音頻編碼進化之路>超能課堂(209)：從CD開始的音頻編碼進化之路</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2438c41e.html alt=超能課堂(220)：數字視頻編碼的發展歷程 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/Rtg4MNgBtlHiVI style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2438c41e.html title=超能課堂(220)：數字視頻編碼的發展歷程>超能課堂(220)：數字視頻編碼的發展歷程</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/51723e35.html alt=AI也有偏見：你在機器“眼裡”是好人還是壞蛋？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RfDgA10IBHJqK9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/51723e35.html title=AI也有偏見：你在機器“眼裡”是好人還是壞蛋？>AI也有偏見：你在機器“眼裡”是好人還是壞蛋？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fbd8ef39.html alt=破解AI大腦黑盒邁出新一步！谷歌現在更懂機器，還開源了研究工具 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/6c330004a1c9ab23e6a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fbd8ef39.html title=破解AI大腦黑盒邁出新一步！谷歌現在更懂機器，還開源了研究工具>破解AI大腦黑盒邁出新一步！谷歌現在更懂機器，還開源了研究工具</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/bd423c48.html alt=口罩產業鏈徹底亂了！丙烯週末瘋狂上漲，價格兩日翻倍，PP期價暴力拉昇；多廠家發緊急聲明…… class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/Rw0qyU6Ao0YcN0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bd423c48.html title=口罩產業鏈徹底亂了！丙烯週末瘋狂上漲，價格兩日翻倍，PP期價暴力拉昇；多廠家發緊急聲明……>口罩產業鏈徹底亂了！丙烯週末瘋狂上漲，價格兩日翻倍，PP期價暴力拉昇；多廠家發緊急聲明……</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/89c5457e.html alt=建築課堂建築工程施工常用鋼筋連接方式簡介 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/cc80a949af1b43b68cbe4ee87ee37a5e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/89c5457e.html title=建築課堂建築工程施工常用鋼筋連接方式簡介>建築課堂建築工程施工常用鋼筋連接方式簡介</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/441e32c5.html alt=擰緊微課堂｜擰緊策略——螺栓連接的分類 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1529806969192cd5df56477 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/441e32c5.html title=擰緊微課堂｜擰緊策略——螺栓連接的分類>擰緊微課堂｜擰緊策略——螺栓連接的分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e07fc1ef.html alt=擰緊微課堂｜軟硬連接的定義和對於擰緊過程的影響 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1529765560055dc98759e16 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e07fc1ef.html title=擰緊微課堂｜軟硬連接的定義和對於擰緊過程的影響>擰緊微課堂｜軟硬連接的定義和對於擰緊過程的影響</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/bd213158.html alt=擰緊微課堂｜擰緊策略—摩擦力扭矩 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/152980681082224b749b890 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bd213158.html title=擰緊微課堂｜擰緊策略—摩擦力扭矩>擰緊微課堂｜擰緊策略—摩擦力扭矩</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a5851780.html alt=AI小技巧：快速製作可編輯的圖文重疊的反色文字 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3b527d934bb24c1a9f61cbb9ee6175f4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a5851780.html title=AI小技巧：快速製作可編輯的圖文重疊的反色文字>AI小技巧：快速製作可編輯的圖文重疊的反色文字</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2e9a67fa.html alt=「路橋課堂—涵洞施工」圓管涵施工技術 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/15279000002872a0b24c46a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2e9a67fa.html title=「路橋課堂—涵洞施工」圓管涵施工技術>「路橋課堂—涵洞施工」圓管涵施工技術</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>