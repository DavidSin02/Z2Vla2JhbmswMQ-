<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習所需數學知識 | 极客快訊</title><meta property="og:title" content="機器學習所需數學知識 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/851e5e3710fb4825b8bd5ccd5b407b10"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/9a024b6.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/9a024b6.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/9a024b6.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/9a024b6.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/9a024b6.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/9a024b6.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/9a024b6.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/9a024b6.html><meta property="article:published_time" content="2020-10-29T20:56:37+08:00"><meta property="article:modified_time" content="2020-10-29T20:56:37+08:00"><meta name=Keywords content><meta name=description content="機器學習所需數學知識"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/9a024b6.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習所需數學知識</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><div><p>數學</p><p>1.列舉常用的最優化方法</p><p>梯度下降法</p><p>牛頓法，</p><p>擬牛頓法</p><p>座標下降法</p><p>梯度下降法的改進型如AdaDelta，AdaGrad，Adam，NAG等。</p><p>2.梯度下降法的關鍵點</p><p>梯度下降法沿著梯度的反方向進行搜索，利用了函數的一階導數信息。梯度下降法的迭代公式為：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/851e5e3710fb4825b8bd5ccd5b407b10><p class=pgc-img-caption></p></div><p>根據函數的一階泰勒展開，在負梯度方向，函數值是下降的。只要學習率設置的足夠小，並且沒有到達梯度為0的點處，每次迭代時函數值一定會下降。需要設置學習率為一個非常小的正數的原因是要保證迭代之後的x<em>k</em>+1位於迭代之前的值x<em>k</em>的鄰域內，從而可以忽略泰勒展開中的高次項，保證迭代時函數值下降。</p><p>梯度下降法只能保證找到梯度為0的點，不能保證找到極小值點。迭代終止的判定依據是梯度值充分接近於0，或者達到最大指定迭代次數。</p><p>梯度下降法在機器學習中應用廣泛，尤其是在深度學習中。AdaDelta，AdaGrad，Adam，NAG等改進的梯度下降法都是用梯度構造更新項，區別在於更新項的構造方式不同。</p><p>3.牛頓法的關鍵點</p><p>牛頓法利用了函數的一階和二階導數信息，直接尋找梯度為0的點。牛頓法的迭代公式為：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bc73219a07244b9fa05484dffd4d5f1d><p class=pgc-img-caption></p></div><p>其中H為Hessian矩陣，g為梯度向量。牛頓法不能保證每次迭代時函數值下降，也不能保證收斂到極小值點。在實現時，也需要設置學習率，原因和梯度下降法相同，是為了能夠忽略泰勒展開中的高階項。學習率的設置通常採用直線搜索（line search）技術。</p><p>在實現時，一般不直接求Hessian矩陣的逆矩陣，而是求解下面的線性方程組：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/0d1909d917d84903a5bda99fffa020af><p class=pgc-img-caption></p></div><p>其解d稱為牛頓方向。迭代終止的判定依據是梯度值充分接近於0，或者達到最大指定迭代次數。</p><p>牛頓法比梯度下降法有更快的收斂速度，但每次迭代時需要計算Hessian矩陣，並求解一個線性方程組，運算量大。另外，如果Hessian矩陣不可逆，則這種方法失效。</p><p>4.拉格朗日乘數法</p><p>拉格朗日乘數法是一個理論結果，用於求解帶有等式約束的函數極值。對於如下問題：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f93a9e58c6b84da4bfd9032f53462770><p class=pgc-img-caption></p></div><p>構造拉格朗日乘子函數：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/0a5996f85ffb438b87444cccaf73246c><p class=pgc-img-caption></p></div><p>在最優點處對x和乘子變量的導數都必須為0：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/874937df2c94454a84498b58ecf62da1><p class=pgc-img-caption></p></div><p>解這個方程即可得到最優解。對拉格朗日乘數法更詳細的講解可以閱讀任何一本高等數學教材。機器學習中用到拉格朗日乘數法的地方有：</p><p>主成分分析</p><p>線性判別分析</p><p>流形學習中的拉普拉斯特徵映射</p><p>隱馬爾科夫模型</p><p>5.凸優化</p><p>數值優化算法面臨兩個方面的問題：局部極值，鞍點。前者是梯度為0的點，也是極值點，但不是全局極小值；後者連局部極值都不是，在鞍點處Hessian矩陣不定，即既非正定，也非負定。</p><p>凸優化通過對目標函數，優化變量的可行域進行限定，可以保證不會遇到上面兩個問題。凸優化是一類特殊的優化問題，它要求：</p><p>優化變量的可行域是一個凸集目標函數是一個凸函數</p><p>凸優化最好的一個性質是：所有局部最優解一定是全局最優解。</p><p>機器學習中典型的凸優化問題有：</p><p>線性迴歸</p><p>嶺迴歸</p><p>LASSO迴歸</p><p>Logistic迴歸</p><p>支持向量機</p><p>Softamx迴歸</p><p>6.拉格朗日對偶</p><p>對偶是最優化方法裡的一種方法，它將一個最優化問題轉換成另外一個問題，二者是等價的。拉格朗日對偶是其中的典型例子。對於如下帶等式約束和不等式約束的優化問題：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bb67c5083a684a4cb048522432441984><p class=pgc-img-caption></p></div><p>與拉格朗日乘數法類似，構造廣義拉格朗日函數：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9e61b3b9646c4772bfc2d582aa3abb18><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/51e8e51f7c9845eaa0aa9b9859f617b2><p class=pgc-img-caption></p></div><p>必須滿足</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e50bcac328b1421cab9ddbc195fff275><p class=pgc-img-caption></p></div><p>的約束。原問題為：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4f600e9a7afc4b6ba286c5ebef65fc68><p class=pgc-img-caption></p></div><p>即先固定住x，調整拉格朗日乘子變量，讓函數L取極大值；然後控制變量x，讓目標函數取極小值。原問題與我們要優化的原始問題是等價的。</p><p>對偶問題為：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/247c39a6d8ec4e5698c443bde441208b><p class=pgc-img-caption></p></div><p>和原問題相反，這裡是先控制變量x，讓函數L取極小值；然後控制拉格朗日乘子變量，讓函數取極大值。</p><p>一般情況下，原問題的最優解大於等於對偶問題的最優解，這稱為弱對偶。在某些情況下，原問題的最優解和對偶問題的最優解相等，這稱為強對偶。</p><p>強對偶成立的一種條件是Slater條件：一個凸優化問題如果存在一個候選x使得所有不等式約束都是嚴格滿足的，即對於所有的i都有<em>gi </em>(x)&lt;0，不等式不取等號，則強對偶成立，原問題與對偶問題等價。注意，Slater條件是強對偶成立的充分條件而非必要條件。</p><p>拉格朗日對偶在機器學習中的典型應用是支持向量機。</p><p>7.KKT條件</p><p>KKT條件是拉格朗日乘數法的推廣，用於求解既帶有等式約束，又帶有不等式約束的函數極值。對於如下優化問題：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/411db46a072d4d42ab15dd22c871374a><p class=pgc-img-caption></p></div><p>和拉格朗日對偶的做法類似，KKT條件構如下乘子函數：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/95256d2737584137b6da3dc8edffc6ad><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/33886724dd17446bbe6ebd08ffd04bd5><p class=pgc-img-caption></p></div><p>和</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/213a76ff2d6d460390aa7015bb269491><p class=pgc-img-caption></p></div><p>稱為KKT乘子。在最優解處</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/40305fd32b864f00b61ea5051ae497e7><p class=pgc-img-caption></p></div><p>應該滿足如下條件：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6bbaf25e27a744e2a66f3d607a265ce4><p class=pgc-img-caption></p></div><p>等式約束</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b6c58964867a45e7b565bd63a93ee546><p class=pgc-img-caption></p></div><p>和不等式約束</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f5578efbe353403f94a3277d9f924bec><p class=pgc-img-caption></p></div><p>是本身應該滿足的約束，</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0f194dbdf12349e09eb6fb7949842db3><p class=pgc-img-caption></p></div><p>和之前的拉格朗日乘數法一樣。唯一多了關於<em>gi </em>(x)的條件：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0591ffee471342fe872f09fd8c0c8d33><p class=pgc-img-caption></p></div><p>KKT條件只是取得極值的必要條件而不是充分條件。</p><p>8.特徵值與特徵向量</p><p>對於一個n階矩陣A，如果存在一個數</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/33886724dd17446bbe6ebd08ffd04bd5><p class=pgc-img-caption></p></div><p>和一個非0向量X，滿足：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cdf0debf886b4640ab2f3d2383bdb485><p class=pgc-img-caption></p></div><p>則稱</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/33886724dd17446bbe6ebd08ffd04bd5><p class=pgc-img-caption></p></div><p>為矩陣A的特徵值，X為該特徵值對應的特徵向量。根據上面的定義有下面線性方程組成立：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/116283e54f8a4ab49c73f90adda38f39><p class=pgc-img-caption></p></div><p>根據線性方程組的理論，要讓齊次方程有非0解，係數矩陣的行列式必須為0，即：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e7634923317c4f509ac2afb51d5bb0c1><p class=pgc-img-caption></p></div><p>上式左邊的多項式稱為矩陣的特徵多項式。矩陣的跡定義為主對角線元素之和：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ec07ebcf642240fd80ecd3d8ad4e1b05><p class=pgc-img-caption></p></div><p>根據韋達定理，矩陣所有特徵值的和為矩陣的跡：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/bb88b0004b4e406bb731dbfe06340efe><p class=pgc-img-caption></p></div><p>同樣可以證明，矩陣所有特徵值的積為矩陣的行列式：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f3463a1ddc864279baea9a551407b176><p class=pgc-img-caption></p></div><p>利用特徵值和特徵向量，可以將矩陣對角化，即用正交變換將矩陣化為對角陣。實對稱矩陣一定可以對角化，半正定矩陣的特徵值都大於等於0，在機器學習中，很多矩陣都滿足這些條件。特徵值和特徵向量在機器學習中的應用包括：正態貝葉斯分類器、主成分分析，流形學習，線性判別分析，譜聚類等。</p><p>9.奇異值分解</p><p>矩陣對角化只適用於方陣，如果不是方陣也可以進行類似的分解，這就是奇異值分解，簡稱SVD。假設A是一個m x n的矩陣，則存在如下分解：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ababeede0fd043e58d48c2fb2982afd4><p class=pgc-img-caption></p></div><p>其中U為m x m的正交矩陣，其列稱為矩陣A的左奇異向量；</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/77340550c6484a449a612dff11d89bc9><p class=pgc-img-caption></p></div><p>為m x n的對角矩陣，除了主對角線</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/66a34cff6d7e4da796edc6d001b4e80b><p class=pgc-img-caption></p></div><p>以外，其他元素都是0；V為n x n的正交矩陣，其行稱為矩陣A的右奇異向量。U的列為AAT的特徵向量，V的列為AT A的特徵向量。</p><p>10.最大似然估計</p><p>有些應用中已知樣本服從的概率分佈，但是要估計分佈函數的參數</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6301509f9daa4668a640faf5c222955f><p class=pgc-img-caption></p></div><p>，確定這些參數常用的一種方法是最大似然估計。</p><p>最大似然估計構造一個似然函數，通過讓似然函數最大化，求解出</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6301509f9daa4668a640faf5c222955f><p class=pgc-img-caption></p></div><p>。最大似然估計的直觀解釋是，尋求一組參數，使得給定的樣本集出現的概率最大。</p><p>假設樣本服從的概率密度函數為</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0858d76dae8546d7a9fd9c85a23aab54><p class=pgc-img-caption></p></div><p>，其中X為隨機變量，</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6301509f9daa4668a640faf5c222955f><p class=pgc-img-caption></p></div><p>為要估計的參數。給定一組樣本x<em>i</em>,<em>i </em>=1,...,<em>l</em>，它們都服從這種分佈，並且相互獨立。最大似然估計構造如下似然函數：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e0f1774484ca49e9b8ddc78e5766d2a0><p class=pgc-img-caption></p></div><p>其中x<em>i</em>是已知量，這是一個關於</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6301509f9daa4668a640faf5c222955f><p class=pgc-img-caption></p></div><p>的函數，我們要讓該函數的值最大化，這樣做的依據是這組樣本發生了，因此應該最大化它們發生的概率，即似然函數。這就是求解如下最優化問題：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/92a9ba6b7a2c462fa421486bb3b6dbbe><p class=pgc-img-caption></p></div><p>乘積求導不易處理，因此我們對該函數取對數，得到對數似然函數：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3b08d91b4fa54d75b08be6add4f23bf0><p class=pgc-img-caption></p></div><p>最後要求解的問題為：</p><div class=pgc-img><img alt=機器學習所需數學知識 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7b5e9340d68c4810b9e799ce0241b906><p class=pgc-img-caption></p></div><p>最大似然估計在機器學習中的典型應用包括logistic迴歸，貝葉斯分類器，隱馬爾科夫模型等。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>需數學</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3873d795.html alt=金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/694a9289cde541dca807f9a30d291d0d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3873d795.html title=金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例>金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>