<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>從零推導支持向量機 (SVM) | 极客快訊</title><meta property="og:title" content="從零推導支持向量機 (SVM) - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RHOqAwPHvZFTTi"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/9a564a99.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/9a564a99.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/9a564a99.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/9a564a99.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/9a564a99.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/9a564a99.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/9a564a99.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/9a564a99.html><meta property="article:published_time" content="2020-10-29T21:13:01+08:00"><meta property="article:modified_time" content="2020-10-29T21:13:01+08:00"><meta name=Keywords content><meta name=description content="從零推導支持向量機 (SVM)"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/9a564a99.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>從零推導支持向量機 (SVM)</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><p>雷鋒網 AI 科技評論按，本文作者張皓，目前為南京大學計算機系機器學習與數據挖掘所（LAMDA）碩士生，研究方向為計算機視覺和機器學習，特別是視覺識別和深度學習。</p><p>個人主頁：http://lamda.nju.edu.cn/zhangh/。該文為其給雷鋒網 AI 科技評論的獨家供稿，未經許可禁止轉載。</p><p><strong>摘要</strong></p><p>支持向量機 (SVM) 是一個非常經典且高效的分類模型。但是，支持向量機中涉及許多複雜的數學推導，並需要比較強的凸優化基礎，使得有些初學者雖下大量時間和精力研讀，但仍一頭霧水，最終對其望而卻步。本文旨在從零構建支持向量機，涵蓋從思想到形式化，再簡化，最後實現的完整過程，並展現其完整思想脈絡和所有公式推導細節。本文力圖做到邏輯清晰而刪繁就簡，避免引入不必要的概念、記號等。此外，本文並不需要讀者有凸優化的基礎，以減輕讀者的負擔。對於用到的優化技術，在文中均有介紹。</p><p>儘管現在深度學習十分流行，瞭解支持向量機的原理，對想法的形式化、簡化，及一步步使模型更一般化的過程，及其具體實現仍然有其研究價值。另一方面，支持向量機仍有其一席之地。相比深度神經網絡，支持向量機特別擅長於特徵維數多於樣本數的情況，而小樣本學習至今仍是深度學習的一大難題。</p><p><strong>1. 線性二分類模型</strong></p><p>給定一組數據</p><p>，其中</p><p>，二分類任務的目標是希望從數據中學得一個假設函數 h: R → {−1,1}，使得 h(x<sub>i</sub>) =y<sub>i</sub>，即</p><p>用一個更簡潔的形式表示是</p><p>更進一步，線性二分類模型認為假設函數的形式是基於對特徵 xi 的線性組合，即</p><p>定理 1. 線性二分類模型的目標是找到一組合適的參數 (w, b)，使得</p><p>即，線性二分類模型希望在特徵空間找到一個劃分超平面，將屬於不同標記的樣本分開。</p><p>證明.</p><p><strong>2. 線性支持向量機</strong></p><p>線性支持向量機 (SVM) [4]也是一種線性二分類模型，也需要找到滿足定理 1 約束的劃分超平面，即 (w, b)。由於能將樣本分開的超平面可能有很多，SVM 進一步希望找到離各樣本都比較遠的劃分超平面。</p><p>當面對對樣本的隨機擾動時，離各樣本都比較遠的劃分超平面對擾動的容忍能力比較強，即不容易因為樣 本的隨機擾動使樣本穿越到劃分超平面的另外一側而產生分類錯誤。因此，這樣的劃分超平面對樣本比較穩健，不容易過擬合。另一方面，離各樣本都比較遠的劃分超平面不僅可以把正負樣本分開，還可以以比較大的確信度將所有樣本分開，包括難分的樣本，即離劃分超平面近的樣本。</p><p><strong>2.1 間隔</strong></p><p>在支持向量機中，我們用間隔 (margin) 刻畫劃分超平面與樣本之間的距離。在引入間隔之前，我們需要 先知道如何計算空間中點到平面的距離。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqAwPHvZFTTi><p>定義 1 (間隔 γ ). 間隔表示距離劃分超平面最近的樣本到劃分超平面距離的兩倍，即</p><p>也就是說，間隔表示劃分超平面到屬於不同標記的最近樣本的距離之和。</p><p>定理 3. 線性支持向量機的目標是找到一組合適的參數(w, b)，使得</p><p>即，線性支持向量機希望在特徵空間找到一個劃分超平面，將屬於不同標記的樣本分開，並且該劃分超平面距離各樣本最遠。</p><p>證明. 帶入間隔定義即得。</p><p><strong>2.2 線性支持向量機基本型</strong></p><p>定理 3 描述的優化問題十分複雜，難以處理。為了能在現實中應用，我們希望能對其做一些簡化，使其變 為可以求解的、經典的凸二次規劃 (QP) 問題。</p><p>定義 2 (凸二次規劃). 凸二次規劃的優化問題是指目標函數是凸二次函數，約束是線性約束的一類優化問題。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqB7I1NcIUYU><p>由於對 (w, b) 的放縮不影響解，為了簡化優化問題，我們約束 (w, b) 使得</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RHOqB8H79yecdu><p>推論 6. 線性支持向量機基本型中描述的優化問題屬於二次規劃問題，包括 d + 1 個優化變量，m 項約束。</p><p>證明. 令</p><p>代入公式 10 即得。</p><p><strong>3. 對偶問題</strong></p><p>現在，我們可以通過調用現成的凸二次規劃軟件包來求解定理 5 描述的優化問題。不過，通過藉助拉格朗 日 (Lagrange) 函數和對偶 (dual) 問題，我們可以將問題更加簡化。</p><p><strong>3.1 拉格朗日函數與對偶形式</strong></p><p>構造拉格朗日函數是求解帶約束優化問題的重要方法。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqBFnFpcvkOk><p>證明.</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RHOqBG3IfwgrcR><p>推論 8 (KKT 條件). 公式 21 描述的優化問題在最優值處必須滿足如下條件。</p><p>證明. 由引理 7 可知，u 必須滿足約束，即主問題可行。對偶問題可行是公式 21 描述的優化問題的約束項。α<sub>i</sub>g<sub>i</sub>(u) = 0 是在主問題和對偶問題都可行的條件下的最大值。</p><p>定義 4 (對偶問題). 定義公式 19 描述的優化問題的對偶問題為</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqBGU1qjR4q5><p>引理 10 (Slater 條件). 當主問題為凸優化問題，即 f 和 g<sub>i</sub>為凸函數，h<sub>j</sub>為仿射函數，且可行域中至少有一點使不等式約束嚴格成立時，對偶問題等價於原問題。</p><p>證明. 此證明已超出本文範圍，感興趣的讀者可參考 [2]。</p><p><strong>3.2 線性支持向量機對偶型</strong></p><p>線性支持向量機的拉格朗日函數為</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqBQx3fRx6Ed><p>證明. 因為公式 26 內層對 (w,b) 的優化屬於無約束優化問題，我們可以通過令偏導等於零的方法得到 (w,b)的最優值。</p><p>將其代入公式 26，消去 (w, b)，即得。</p><p>推論 13. 線性支持向量機對偶型中描述的優化問題屬於二次規劃問題，包括 m 個優化變量，m + 2 項約束。</p><p>證明. 令</p><p>代入公式 10 即得。其中，e<sub>i</sub>是第 i 位置元素為 1，其餘位置元素為 0 的單位向量。我們需要通過兩個不等式約束</p><p>和</p><p>來得到一個等式約束。</p><p><strong>3.3 支持向量</strong></p><p>定理 14 (線性支持向量機的 KKT 條件). 線性支持向量機的 KKT 條件如下。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RHOqBatFctbE9t><p>代入引理 8 即得。</p><p>定義 5 (支持向量). 對偶變量 α<sub>i</sub>> 0 對應的樣本。</p><p>引理 15. 線性支持向量機中，支持向量是距離劃分超平面最近的樣本，落在最大間隔邊界上。</p><p>定理 16. 支持向量機的參數 (w, b) 僅由支持向量決定，與其他樣本無關。</p><p>證明. 由於對偶變量 α<sub>i</sub>> 0 對應的樣本是支持向量，</p><p>其中 SV 代表所有支持向量的集合，b 可以由互補鬆弛算出。對於某一支持向量 x<sub>s</sub>及其標記 y<sub>s，</sub>由於</p><p>實踐中，為了得到對 b 更穩健的估計，通常使用對所有支持向量求解得到 b 的平均值。</p><p>推論 17. 線性支持向量機的假設函數可表示為</p><p>證明. 代入公式 35 即得。</p><p><strong>4. 核函數</strong></p><p>至此，我們都是假設訓練樣本是線性可分的。即，存在一個劃分超平面能將屬於不同標記的訓練樣本分開。但在很多任務中，這樣的劃分超平面是不存在的。支持向量機通過核技巧 (kernel trick) 來解決樣本不是線性可分的情況 [1]。</p><p><strong>4.1 非線性可分問題</strong></p><p>既然在原始的特徵空間</p><p>不是線性可分的，支持向量機希望通過一個映射</p><p>，使得數據在新的空間</p><p>是線性可分的。</p><p>引理 18. 當 d 有限時，一定存在</p><p>，使得樣本在空間</p><p>中線性可分.</p><p>證明. 此證明已超出本文範圍，感興趣的讀者可參考計算學習理論中打散 (shatter) 的相應部分 [16]。</p><p>令 φ(x) 代表將樣本 x 映射到</p><p>中的特徵向量，參數 w 的維數也要相應變為</p><p>維，則支持向量機的基本型和對偶型相應變為：</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqBisHUMnoeW><p>其中，基本型對應於</p><p>+ 1 個優化變量，m 項約束的二次規劃問題；對偶型對應於 m 個優化變量，m + 2 項約束的二次規劃問題。</p><p><strong>4.2 核技巧</strong></p><p>注意到，在支持向量機的對偶型中，被映射到高維的特徵向量總是以成對內積的形式存在，即</p><p>如果先計算特徵在空間</p><p>的映射，再計算內積，複雜度是</p><p>。當特徵被映射到非常高維的空間，甚至是無窮維空間時，這將會是沉重的存儲和計算負擔。</p><p>核技巧旨在將特徵映射和內積這兩步運算壓縮為一步, 並且使複雜度由</p><p>降為</p><p>。即，核技巧希望構造一個核函數 κ(x<sub>i</sub>,x<sub>j</sub>)，使得</p><p>，並且 κ(xi,xj) 的計算複雜度是</p><p>。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RHOqBrhEwyGLEH><p><strong>4.3 核函數選擇</strong></p><p>通過向高維空間映射及核技巧，我們可以高效地解決樣本非線性可分問題。但面對一個現實任務，我們很 難知道應該具體向什麼樣的高維空間映射，即應該選什麼樣的核函數，而核函數選擇的適合與否直接決定整體的性能。</p><p>表 1 列出了幾種常用的核函數。通常，當特徵維數 d 超過樣本數 m 時 (文本分類問題通常是這種情況)，使用線性核；當特徵維數 d 比較小，樣本數 m 中等時，使用 RBF 核；當特徵維數 d 比較小，樣本數 m 特別大時，支持向量機性能通常不如深度神經網絡。</p><p>除此之外，用戶還可以根據需要自定義核函數，但需要滿足 Mercer 條件 [5]。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RHOqBxA93Zyux3><p>反之亦然。</p><p>新的核函數還可以通過現有核函數的組合得到，使用多個核函數的凸組合是多核學習 [9] 的研究內容。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqBxQGTp7Sz1><p><strong>4.4 核方法</strong></p><p>上述核技巧不僅使用於支持向量機，還適用於一大類問題。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RHOqBxd8ZRMbNK><p>即 Φα 比 w 有更小的目標函數值，說明 w 不是最優解，與假設矛盾。因此，最優解必定是樣本的線性組合。</p><p>此外，原版表示定理適用於任意單調遞增正則項 Ω(w)。此證明已超出本文範圍，感興趣的讀者可參考 [13]。</p><p>表示定理對損失函數形式沒有限制，這意味著對許多優化問題，最優解都可以寫成樣本的線性組合。更進 一步，</p><p>將可以寫成核函數的線性組合</p><p>通過核函數，我們可以將線性模型擴展成非線性模型。這啟發了一系列基於核函數的學習方法，統稱為核方法 [8]。</p><p><strong>5. 軟間隔</strong></p><p>不管直接在原特徵空間，還是在映射的高維空間，我們都假設樣本是線性可分的。雖然理論上我們總能找 到一個高維映射使數據線性可分，但在實際任務中，尋找到這樣一個合適的核函數通常很難。此外，由於數據中通常有噪聲存在，一味追求數據線性可分可能會使模型陷入過擬合的泥沼。因此，我們放寬對樣本的要求，即允許有少量樣本分類錯誤。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RHOqC8G1WnqI8R><p><strong>5.1 軟間隔支持向量機基本型</strong></p><p>我們希望在優化間隔的同時，允許分類錯誤的樣本出現，但這類樣本應儘可能少：</p><p>其中，</p><p>是指示函數，C 是個可調節參數，用於權衡優化間隔和少量分類錯誤樣本這兩個目標。但是，指示函數不連續，更不是凸函數，使得優化問題不再是二次規劃問題。所以我們需要對其進行簡化。</p><p>公式 60 難以實際應用的原因在於指示函數只有兩個離散取值 0/1，對應樣本分類正確/錯誤。為了能使優 化問題繼續保持為二次規劃問題，我們需要引入一個取值為連續值的變量，刻畫樣本滿足約束的程度。我們引入鬆弛變量 (slack variable) ξ<sub>i</sub>，用於度量樣本違背約束的程度。當樣本違背約束的程度越大，鬆弛變量值越大。即，</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RHOqC8w4r1ybl5><p>其中，C 是個可調節參數，用於權衡優化間隔和少量樣本違背大間隔約束這兩個目標。當 C 比較大時，我們希望更多的樣本滿足大間隔約束；當 C 比較小時，我們允許有一些樣本不滿足大間隔約束。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqC9AAE9x0Ha><p><strong>5.2 軟間隔支持向量機對偶型</strong></p><p>定理 25 (軟間隔支持向量機對偶型). 軟間隔支持向量機的對偶問題等價於找到一組合適的 α，使得</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RHOqCGy9UqAqst><p>因為內層對 (w, b, ξ) 的優化屬於無約束優化問題，我們可以通過令偏導等於零的方法得到 (w, b, ξ) 的最優值。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqCHBEwwt3i8><p>推論 26. 軟間隔支持向量機對偶型中描述的優化問題屬於二次規劃問題，包括 m 個優化變量，2m+2 項約束。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqCHO5cK9JEJ><p><strong>5.3 軟間隔支持向量機的支持向量</strong></p><p>定理 27 (軟間隔支持向量機的 KKT 條件). 軟間隔支持向量機的 KKT 條件如下.</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqCHbB5RR4Uh><p>引理 28. 軟間隔支持向量機中，支持向量落在最大間隔邊界，內部，或被錯誤分類的樣本。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqCUK2l7o0eN><p>定理 29. 支持向量機的參數 (w, b) 僅由支持向量決定，與其他樣本無關。</p><p>證明. 和線性支持向量機證明方式相同。</p><p><strong>5.4 鉸鏈損失</strong></p><p>引理 30. 公式 61 等價為</p><p>其中，第一項稱為經驗風險，度量了模型對訓練數據的擬合程度；第二項稱為結構風險，也稱為正則化項，度量了模型自身的複雜度。正則化項削減了假設空間，從而降低過擬合風險。λ 是個可調節的超參數，用於權衡經驗風險和結構風險。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqCV03vihkGv><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RHOqCVDEACHz8c><p><strong>6. 優化方法</strong></p><p><strong>6.1 SMO</strong></p><p>如果直接用經典的二次規劃軟件包求解支持向量機對偶型，由於</p><p>的存儲開銷是</p><p>，當訓練樣本很多時，這將是一個很大的存儲和計算開銷。序列最小化 (SMO) [10]是一個利用支持 向量機自身特性高效的優化算法。SMO 的基本思路是座標下降。</p><p>定義 7 (座標下降). 通過循環使用不同座標方向，每次固定其他元素，只沿一個座標方向進行優化，以達到目標函數的局部最小，見算法 1.</p><p>我們希望在支持向量機中的對偶型中，每次固定除 α<sub>i</sub>外的其他變量，之後求在 α<sub>i</sub>方向上的極值。但由於 約束</p><p>，當其他變量固定時，α<sub>i</sub>也隨著確定。這樣，我們無法在不違背約束的前提下對 α<sub>i</sub>進行優化。因此，SMO 每步同時選擇兩個變量 α<sub>i</sub>和 α<sub>j</sub>進行優化，並固定其他參數，以保證不違背約束。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RHOqCec8toPnNj><p>定理 32 (SMO 每步的優化目標). SMO 每步的優化目標為</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqCesA15kiw6><p>推論 33. SMO 每步的優化目標可等價為對 α<sub>i</sub>的單變量二次規劃問題。</p><p>證明. 由於</p><p>，我們可以將其代入 SMO 每步的優化目標，以消去變量 α<sub>j。</sub>此時，優化目標函數是對於 α<sub>i</sub>的二次函數，約束是一個取值區間 L ≤ α<sub>i</sub>≤ H。之後根據目標函數頂點與區間 [L, H] 的位置關係，可以得到 α<sub>i</sub>的最優值。理論上講，每步優化時 α<sub>i</sub>和 α<sub>j</sub>可以任意選擇，但實踐中通常取 α<sub>i</sub>為違背 KKT 條件最大的變量，而 α<sub>j</sub>取對應樣本與 α<sub>i</sub>對應樣本之間間隔最大的變量。對 SMO 算法收斂性的測試可以用過檢測是否滿足 KKT 條件得到。</p><p><strong>6.2 Pegasos</strong></p><p>我們也可以直接在原問題對支持向量機進行優化，尤其是使用線性核函數時，我們有很高效的優化算法，如 Pegasos [14]。Pegasos 使用基於梯度的方法在線性支持向量機基本型</p><p>進行優化，見算法 2。</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqCjeC75Ib5X><p><strong>6.3 近似算法</strong></p><p>當使用非線性核函數下的支持向量機時，由於核矩陣</p><p>，所以時間複雜度一定是</p><p>，因此，有許多學者致力於研究一些快速的近似算法。例如，CVM [15]基於近似最小包圍球算法，Nyström 方法[18]通過從 K 採樣出一些列來得到 K 的低秩近似，隨機傅里葉特徵[12]構造了向低維空間的隨機映射。本章介紹了許多優化算法，實際上現在已有許多開源軟件包對這些算法有很好的實現，目前比較著名的有 LibLinear[7] 和 LibSVM[3]，分別適用於線性和非線性核函數。</p><p><strong>7. 支持向量機的其他變體</strong></p><p>ProbSVM. 對數機率迴歸可以估計出樣本屬於正類的概率，而支持向量機只能判斷樣本屬於正類或負類，無法得到概率。ProbSVM[11]先訓練一個支持向量機，得到參數 (w, b)。再令</p><p>，將</p><p>當做新的訓練數據訓練一個對數機率迴歸模型，得到參數</p><p>。因此，ProbSVM 的假設函數為</p><p>對數機率迴歸模型可以認為是對訓練得到的支持向量機的微調，包括尺度 (對應 θ<sub>1</sub>) 和平移 (對應 θ<sub>0</sub>)。通常 θ<sub>1</sub>> 0，θ<sub>0</sub>≈ 0。</p><p>多分類支持向量機. 支持向量機也可以擴展到多分類問題中. 對於 K 分類問題，多分類支持向量機 [17] 有 K 組參數</p><p>，並希望模型對於屬於正確標記的結果以 1 的間隔高於其他類的結 果，形式化如下</p><img alt="從零推導支持向量機 (SVM)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RHOqCsNAgumant><p><strong>References</strong></p><p>[1] B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classifiers. In Proceedings of the Annual Workshop on Computational Learning Theory, pages 144–152, 1992. 5</p><p>[2] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004. 4</p><p>[3] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27, 2011. 10</p><p>[4] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995. 1 [5] N. Cristianini and J. Shawe-Taylor. An introduction to support vector machines and other kernel-based learning methods. Cambridge University Press, 2000. 6</p><p>[6] H. Drucker, C. J. Burges, L. Kaufman, A. J. Smola, and V. Vapnik. Support vector regression machines. In Advances in Neural Information Processing Systems, pages 155–161, 1997. 10</p><p>[7] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9(8):1871–1874, 2008. 10</p><p>[8] T. Hofmann, B. Schölkopf, and A. J. Smola. Kernel methods in machine learning. The Annals of Statistics, pages 1171–1220, 2008. 6</p><p>[9] G. R. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and M. I. Jordan. Learning the kernel matrix with semidefinite programming. Journal of Machine Learning Research, 5(1):27–72, 2004. 6 [10] J. Platt. Sequential minimal optimization: A fast algorithm for training support vector machines. Micriosoft Research, 1998. 9</p><p>[11] J. Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers, 10(3):61–74, 1999. 10</p><p>[12] A. Rahimi and B. Recht. Random features for largescale kernel machines. In Advances in Neural Information Processing Systems, pages 1177–1184, 2008. 10</p><p>[13] B. Scholkopf and A. J. Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2001. 6</p><p>[14] S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, 127(1):3–30, 2011. 9</p><p>[15] I. W. Tsang, J. T. Kwok, and P.-M. Cheung. Core vector machines: Fast SVM training on very large data sets. Journal of Machine Learning Research, 6(4):363– 392, 2005. 10</p><p>[16] V. Vapnik. The nature of statistical learning theory. Springer Science & Business Media, 2013. 5</p><p>[17] J. Weston, C. Watkins, et al. Support vector machines for multi-class pattern recognition. In Proceedings of the European Symposium on Artificial Neural Networks, volume 99, pages 219–224, 1999. 10</p><p>[18] C. K. Williams and M. Seeger. Using the nyström method to speed up kernel machines. In Advances in Neural Information Processing Systems, pages 682–688, 2001. 10</p><p>[19] 周志華. 機器學習. 清華大學出版社, 2016. 9</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>推導</a></li><li><a>SVM</a></li><li><a>向量</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E5%AD%B8/c86d7e5d.html alt=支持向量機（SVM）小結 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/34341bbffe91417b9f732c28799b78ed style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/c86d7e5d.html title=支持向量機（SVM）小結>支持向量機（SVM）小結</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/3cf7440b.html alt=理解SVM支持向量機 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3d1bb058669241a1b9254be04aac6818 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/3cf7440b.html title=理解SVM支持向量機>理解SVM支持向量機</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/dd29e0eb.html alt=理解支持向量機（SVM） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1540115493549aa21a19958 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/dd29e0eb.html title=理解支持向量機（SVM）>理解支持向量機（SVM）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e1a84312.html alt=127：平面向量(基底法，容易掉坑裡) class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/798fe5150f7e41db847e88330e26f9d1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e1a84312.html title=127：平面向量(基底法，容易掉坑裡)>127：平面向量(基底法，容易掉坑裡)</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d61d6a42.html alt=支持向量機（第八章） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/153985032133768ee02ea64 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d61d6a42.html title=支持向量機（第八章）>支持向量機（第八章）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/62900ef0.html alt=一文了解邏輯迴歸和支持向量機的異同 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1539671186198856310f156 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/62900ef0.html title=一文了解邏輯迴歸和支持向量機的異同>一文了解邏輯迴歸和支持向量機的異同</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7c54da7d.html alt=關於向量你所不知道的那1、2、3、4件事 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/3afd00000309c31b9722 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7c54da7d.html title=關於向量你所不知道的那1、2、3、4件事>關於向量你所不知道的那1、2、3、4件事</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f6469ee4.html alt=高中數學必修四-平面向量突破點（二）平面向量的線性運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/46cb0002bbea1028e85d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f6469ee4.html title=高中數學必修四-平面向量突破點（二）平面向量的線性運算>高中數學必修四-平面向量突破點（二）平面向量的線性運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/d35eb131.html alt=向量組的線性相關性 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/42878bb02cfd49a5bff27706934f48f2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/d35eb131.html title=向量組的線性相關性>向量組的線性相關性</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/ed4751b0.html alt=我理解的對正弦波正弦量、平均值、有效值的推導 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/e53f3b7344014c6e9383eb1b7ff1045d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/ed4751b0.html title=我理解的對正弦波正弦量、平均值、有效值的推導>我理解的對正弦波正弦量、平均值、有效值的推導</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/1f3617bb.html alt="GroC 組合式詞向量生成算法" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a720bf73a52548348f6095c3ce6bd920 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/1f3617bb.html title="GroC 組合式詞向量生成算法">GroC 組合式詞向量生成算法</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/98922e26.html alt="「曲線弧長，單位切向量，主單位法向量」圖解高等數學 下-07" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/4b000000f1313245125e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/98922e26.html title="「曲線弧長，單位切向量，主單位法向量」圖解高等數學 下-07">「曲線弧長，單位切向量，主單位法向量」圖解高等數學 下-07</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/33284c83.html alt=平面向量最值問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/08de805129af4d4795d0fa7a88eff4c1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/33284c83.html title=平面向量最值問題>平面向量最值問題</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/de0a6ec1.html alt=流形中的向量（或者矢量）和向量場 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/0f47062d207448fbb9e761d3eeb9e052 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/de0a6ec1.html title=流形中的向量（或者矢量）和向量場>流形中的向量（或者矢量）和向量場</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/f68e0607.html alt="用概念激活向量 (CAVs) 理解深度網絡" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RZYjVk08YSPQgO style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/f68e0607.html title="用概念激活向量 (CAVs) 理解深度網絡">用概念激活向量 (CAVs) 理解深度網絡</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>