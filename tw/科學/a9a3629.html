<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>支持向量機(SVM)的約束和無約束優化、理論和實現 | 极客快訊</title><meta property="og:title" content="支持向量機(SVM)的約束和無約束優化、理論和實現 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/1d9cd81ef1964685bc9d6f6ef3eba7ee"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/a9a3629.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/a9a3629.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/a9a3629.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/a9a3629.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/a9a3629.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/a9a3629.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/a9a3629.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/a9a3629.html><meta property="article:published_time" content="2020-10-29T21:03:51+08:00"><meta property="article:modified_time" content="2020-10-29T21:03:51+08:00"><meta name=Keywords content><meta name=description content="支持向量機(SVM)的約束和無約束優化、理論和實現"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/a9a3629.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>支持向量機(SVM)的約束和無約束優化、理論和實現</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><div><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d9cd81ef1964685bc9d6f6ef3eba7ee><p class=pgc-img-caption></p></div><p>優化是機器學習領域最有趣的主題之一。我們日常生活中遇到的大多數問題都是通過數值優化方法解決的。在這篇文章中，讓我們研究一些基本的數值優化算法，以找到任意給定函數(這對於凸函數最有效)的局部最優解。讓我們從簡單的凸函數開始，其中局部和全局最小值是相同的，然後轉向具有多個局部和全局最小值的高度非線性函數。</p><p>整個優化圍繞線性代數和微積分的基本概念展開。最近的深度學習更新引起了數值和隨機優化算法領域的巨大興趣，為深度學習網絡所展示的驚人定性結果提供了理論支持。在這些類型的學習算法中，沒有任何明確已知的優化函數，但我們只能訪問0階和1階的Oracles。Oracles是在任何給定點返回函數值（0階），梯度（1階）或Hessian（2階）的黑盒子。本本提供了對無約束和約束優化函數的基本理論和數值理解，還包括它們的python實現。</p><h1>一個點成為局部最小值的必要和充分條件：</h1><p>設f(.)是一個連續的二階可微函數。對於任一點為極小值，它應滿足以下條件:</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d872da398b114a4d8b862632f9badd89><p class=pgc-img-caption></p></div><ul><li>一階必要條件:</li></ul><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d5839c3c511c44118ea985da5959f11c><p class=pgc-img-caption></p></div><ul><li>二階必要條件：</li></ul><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/14874649780342a9ad41331725a80368><p class=pgc-img-caption></p></div><ul><li>二階充足條件：</li></ul><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5d245bec5ab84740a9e5641be27bb027><p class=pgc-img-caption></p></div><p><strong>梯度下降：</strong></p><p>梯度下降是學習算法(機器學習、深度學習或深度強化學習)領域所有進展的支柱。在這一節中，我們將看到為了更快更好的收斂，梯度下降的各種修改。讓我們考慮線性迴歸的情況，我們估計方程的係數:</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/03ff395a68b64226bed5444b6322fab8><p class=pgc-img-caption></p></div><p>假設該函數是所有特徵的線性組合。通過最小化損失函數來確定最佳係數集：</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/abf70615613440e1aa50fa22f84af6eb><p class=pgc-img-caption></p></div><p>這是線性迴歸任務的最大似然估計。最小二乘法(Ordinary Least Square)涉及到求特徵矩陣的逆，其表達式為:</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/44b1a5f62fc24386bdf35fa13fc54db5><p class=pgc-img-caption></p></div><p>對於實際問題，數據的維數很大，很容易造成計算量的激增。例如,讓我們考慮問題的圖像特徵分析:一般圖像的大小1024 x1024,這意味著特徵的數量級為10⁶。由於具有大量的特徵，這類優化問題只能通過迭代的方式來解決，這就導致了我們所熟知的梯度下降法和牛頓-拉弗森法。</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c264229e33824cbd9b1c38195c4209a3><p class=pgc-img-caption></p></div><p><strong>梯度下降算法：</strong></p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0e048247a9894de58dd3c66367e21e29><p class=pgc-img-caption></p></div><p>梯度下降算法利用先前指定的學習率（eta）在負梯度的方向（最陡的下降方向）上更新迭代（X）。學習率用於在任何給定的迭代中防止局部最小值的overshoot。</p><p>下圖顯示了函數f（x）=x²的梯度下降算法的收斂性，其中eta = 0.25</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/19a13d2437864c80b34f71cefe0c4771><p class=pgc-img-caption></p></div><p>找出一個最優eta是目前的任務，這需要事先了解函數的理解和操作域。</p><pre>import matplotlib.pyplot as pltimport numpy as np# assuming function to be x**2def get_gradient(x): return 2*xdef get_value(x): return np.sum(x**2)# python implementation of vanilla gradient descent update ruledef​ ​ def gradient_descent_update​ (x, eta): """ get_gradient is 1st order oracle """ return​ x - eta*get_gradient(x)</pre><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3f1f8d6e5dea42c7af6b722393ffed5b><p class=pgc-img-caption></p></div><p><strong>Armijo Goldstein條件梯度下降:</strong></p><p>為了減少手動設置的工作，應用Armijo Goldstein (AG)條件來查找下一個迭代的(eta)。AG條件的形式推導需要線性逼近、Lipchitz條件和基本微積分的知識。</p><p>我們定義兩個函數f1(x)和f2(x)作為兩個不同係數和的f(x)的線性逼近，其具有兩個不同的係數α和β，由下式給出：</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2e2d07d948d1407eba87d799d7cf3081><p class=pgc-img-caption></p></div><p>在AG條件的每次迭代中，滿足以下關係的特定eta：</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/dfdb74e3a7cf40f79a646148d433f978><p class=pgc-img-caption></p></div><p>找到並且相應地更新當前迭代。</p><p>下圖顯示了下一次迭代的範圍，對於函數f（x）=x²的收斂，alpha = 0.25，beta = 0.5：</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/34f69404bf124ad8a9a02db8892b52d3><p class=pgc-img-caption></p></div><p>上圖中紅色、藍色和綠色的線對應於綠色顯示的下一個可能迭代的範圍。</p><pre># python implementation of gradient descent with AG condition update ruledef gradient_descent_update_AG(x, ​ alpha​ =0.5, ​ beta​ =0.25): eta​ =0.5 max_eta​ =np.inf min_eta​ =0. value = get_value(x) grad = get_gradient(x) while​ ​ True​ : x_cand = x - (eta)*grad f = get_value(x_cand) f1 = value - eta​ *a​ lpha*np.sum(np.abs(grad)*​ *2​ ) f2 = value - eta​ *be​ ta *np.sum(np.abs(grad)*​ *2​ ) if​ f&lt;=f2 ​ and​ f&gt;=f1: return x_cand if f &lt;= f1: if eta == max_eta: eta = np.min([2.*eta, eta + max_eta/2.]) else: eta = np.min([2.*eta, (eta + max_eta)/2.]) if​ f&gt;=f2: max_eta = eta eta = (eta+min_eta)/2.0</pre><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3ac855612f08472b934c9907d409b797><p class=pgc-img-caption></p></div><p><strong>完全鬆弛條件的梯度下降：</strong></p><p>在完全鬆弛條件的情況下，新的函數g（eta）被最小化以獲得隨後用於尋找下一次迭代的eta。</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/711ef84ec7794dcda5b8bb85055d77c7><p class=pgc-img-caption></p></div><p>此方法涉及解決查找每個下一個迭代的優化問題，其執行方式如下：</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8aaa174400934f5392b3f7976404067d><p class=pgc-img-caption></p></div><pre># The python implementation for FR is shown below:def​ ​ gradient_descent_update_FR​ (x): eta = ​ 0.5 thresh = ​ 1e-6 max_eta = np.inf min_eta = ​ 0. while​ ​ True​ : x_cand = x - eta*get_gradient(x) g_diff = ​ -1.​ *get_gradient(x)*get_gradient(x_cand) if​ np.sum(np.abs(g_diff)) &lt; thresh ​ and​ eta &gt; ​ 0 ​: return​ x_cand if g_diff &gt; 0: if eta == max_eta: eta = np.min([2.*eta, eta + max_eta/2.]) else: eta = np.min([2.*eta, (eta + max_eta)/2.]) else: max_eta = eta eta = (min_eta + eta)/2.0</pre><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1f4bcc16596f4a908271d29c68baaa7d><p class=pgc-img-caption></p></div><p><strong>隨機梯度下降</strong></p><p>我們知道，在實際設置中，數據的維數會非常大，這使得對所有特徵進行進一步的梯度計算非常昂貴。在這種情況下，隨機選擇一批點(特徵)並計算期望的梯度。整個過程的收斂只是在預期的意義上。</p><p>在數學上它意味著：隨機選擇一個點（p）來估計梯度。</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/551f9471f42648b1a8af5c02e292178e><p class=pgc-img-caption></p></div><p>在上面的迭代中，wt可以被視為噪聲。只有當E（wt）趨於0時，該迭代才會導致局部最優。</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0b15e35938b44ccb984b7881424e9c15><p class=pgc-img-caption></p></div><p>同樣，可以看出wt的方差也是有限的。通過以上證明，保證了SGD的收斂性。</p><pre># SGD implementation in pythondef​ ​ SGD​ (self, X, Y, batch_size, thresh=​ 1 ​ ): loss = ​ 100 step = ​ 0 if​ self.plot: losses = [] while​ loss &gt;= thresh: # mini_batch formation index = np.random.randint(​ 0 ​ , len(X), size = batch_size) trainX, trainY = np.array(X)[index], np.array(Y)[index]  self.forward(trainX) loss = self.loss(trainY) self.gradients(trainY) # update self.w0 -= np.squeeze(self.alpha*self.grad_w0) self.weights -= np.squeeze(self.alpha*self.grad_w)  if self.plot: losses.append(loss) if​ step % ​ 1000​ == ​ 999​ : ​  print​ ​ "Batch number: {}"​ .format(step)+​ " current loss: {}"​ .format(loss) step += ​ 1 if​ self.plot : self.visualization(X, Y, losses) pass</pre><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bba5993dc7c640f185258a08380503c3><p class=pgc-img-caption></p></div><p><strong>AdaGrad</strong></p><p>Adagrad是一個優化器，可幫助自動調整優化問題中涉及的每個特徵的學習率。這是通過跟蹤所有梯度的歷史來實現的。該方法也僅在期望意義上收斂。</p><pre>def​ ​ AdaGrad​ (self, X, Y, batch_size,thresh=0.5​ ,epsilon=1e-6​ ): loss = ​ 100 step = ​ 0 if​ self.plot: losses = [] G = np.zeros((X.shape[​ 1 ​ ], X.shape[​ 1 ​ ])) G0 = ​ 0 while​ loss &gt;= thresh: # mini_batch formation index = np.random.randint(​ 0 ​ , len(X), size = batch_size) trainX, trainY = np.array(X)[index], np.array(Y)[index] self.forward(trainX) loss = self.loss(trainY) self.gradients(trainY) G += self.grad_w.T*self.grad_w G0 += self.grad_w0**​ 2 den = np.sqrt(np.diag(G)+epsilon) delta_w = self.alpha*self.grad_w / den delta_w0 = self.alpha*self.grad_w0 / np.sqrt(G0 + epsilon) # update parameters self.w0 -= np.squeeze(delta_w0) self.weights -= np.squeeze(delta_w) if self.plot: losses.append(loss) if​ step % ​ 500​ == ​ 0 ​ : ​  print​ ​ "Batch number: {}".format (step)+​ " current loss: {}"​.format(loss)  step += ​ 1 if​ self.plot : self.visualization(X, Y, losses) pass</pre><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ec15027957d6444fb83a9b21d946d598><p class=pgc-img-caption></p></div><h1>讓我們轉向基於Hessian的方法，牛頓和擬牛頓方法：</h1><p>基於hessian的方法是基於梯度的二階優化方法，幾何上涉及到梯度和曲率信息來更新權重，因此收斂速度比僅基於梯度的方法快得多，牛頓方法的更新規則定義為:</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f02e0b37347e404aac353e2bd0978171><p class=pgc-img-caption></p></div><p>該算法的收斂速度遠快於基於梯度的方法。數學上,梯度下降法的收斂速度正比於O (1 / t),而對於牛頓法它正比於O (1 / t²)。但是對於高維數據，為每個迭代估計二階oracle的計算成本很高，這導致使用一階oracle模擬二階oracle。這就給出了擬牛頓算法。這類擬牛頓法中最常用的算法是BFGS和LMFGS算法。在這一節中，我們只討論BFGS算法，它涉及到對函數Hessian的rank one矩陣更新。該方法的總體思想是隨機初始化Hessian，並使用rank one更新規則在每次迭代中不斷更新Hessian。數學上可以表示為:</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/89b5d6410f504bd0802357d4ba10199f><p class=pgc-img-caption></p></div><pre># python implementation for BFGSdef​ ​ BFGS_update​ (H, s, y): smooth = ​ 1e-3 s = np.expand_dims(s, axis= ​ -1​ ) y = np.expand_dims(y, axis= ​ -1​ ) rho = ​ 1.​ /(s.T.dot(y) + smooth) p = (np.eye(H.shape[​ 0 ​ ]) - rho*s.dot(y.T)) return​ p.dot(H.dot(p.T)) + rho*s.dot(s.T)def​ ​ quasi_Newton​ (x0, H0, num_iter=​ 100​ , eta=​ 1 ​ ): xo, H = x0, H0 for​ _ ​ in​ range(num_iter): xn = xo - eta*H.dot(get_gradient(xo)) y = get_gradient(xn) - get_gradient(xo) s = xn - xo H = BFGS_update(H, s, y) xo = xn return​ xo</pre><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9043876e3ab745af8cc4bb1590844a21><p class=pgc-img-caption></p></div><h1>約束優化</h1><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1fd1f56b2eb149af864a9fb932c9e853><p class=pgc-img-caption></p></div><p>現在，是時候討論一些圍繞約束優化(包括問題的制定和解決策略)的關鍵概念了。本節還將討論一種稱為SVM(支持向量機)的算法的理論和Python實現。當我們在現實生活中遇到問題時，提出一個理想的優化函數是相當困難的，有時是不可行的，所以我們通過對問題施加額外的約束來放鬆優化函數，優化這個約束設置將提供一個近似，我們將儘可能接近問題的實際解決方案，但也是可行的。求解約束優化問題的方法有拉格朗日公式法、懲罰法、投影梯度下降法、內點法等。在這一節中，我們將學習拉格朗日公式和投影梯度下降法。本節還將詳細介紹用於優化CVXOPT的開源工具箱，並介紹使用此工具箱的SVM實現。</p><p>約束優化問題的一般形式：</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/80a0485f0a1e4530abd25bdd7ce90266><p class=pgc-img-caption></p></div><p>其中f(x)為目標函數，g(x)、h(x)分別為不等式約束和等式約束。如果f(x)是凸的約束條件形成一個凸集，(即g(x)為凸函數h(x)為仿射函數），該優化算法保證收斂於全局最小值。對於其他問題，它收斂於局部極小值。</p><p><strong>投影梯度下降</strong></p><p>求解約束優化設置的第一步(也是最明顯的一步)是對約束集進行迭代投影。這是求解約束優化問題中最強大的算法。這包括兩個步驟(1)在最小化(下降)方向上找到下一個可能的迭代，(2)在約束集上找到迭代的投影。</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/895fb1bee23441f39388472c3cde5420><p class=pgc-img-caption></p></div><pre># projected gradient descent implementationdef​ ​ projection_oracle_l2​ (w, l2_norm): return​ (l2_norm/np.linalg.norm(w))*wdef​ ​ projection_oracle_l1​ (w, l1_norm): # first remember signs and store them. Modify w signs = np.sign(w) w = w*signs  # project this modified w onto the simplex in first orthant. d=len(w) if​ np.sum(w)&lt;=l1_norm: return​ w*signs for​ i ​ in​ range(d): w_next = w+​ 0 w_next[w&gt;​ 1e-7​ ] = w[w&gt;​ 1e-7​ ] - np.min(w[w&gt;​ 1e-7​ ]) if​ np.sum(w_next)&lt;=l1_norm: w = ((l1_norm - np.sum(w_next))*w + (np.sum(w) - l1_norm)*w_next)/(np.sum(w)-np.sum(w_next)) return​ w*signs else​ : w=w_nextdef​ ​ main​ (): eta=​ 1.0 /smoothness_coeff for​ l1_norm ​ in​ np.arange(​ 0 ​ , ​ 10​ , ​ 0.5​ ): w=np.random.randn(data_dim) for​ i ​ in​ range(​ 1000​ ): w = w - eta*get_gradient(w) w = projection_oracle_l1(w, l1_norm) pass</pre><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/7edf28ca75794435ae74c44046c4b5b1><p class=pgc-img-caption></p></div><p><strong>瞭解拉格朗日公式</strong></p><p>在大多數優化問題中，找到約束集上迭代的投影是一個困難的問題(特別是在複雜約束集的情況下)。它類似於在每次迭代中解決優化問題，在大多數情況下，優化問題是非凸的。在現實中，人們試圖通過解決對偶問題而不是原始問題來擺脫約束。</p><p>在深入拉格朗日對偶和原始公式之前，讓我們先了解一下KKT條件及其意義</p><ul><li>對於任意點為具有等式約束的局部/全局最小值:</li></ul><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4fc7f515558843a7882a7f155482108d><p class=pgc-img-caption></p></div><ul><li>類似地，不等式約束:</li></ul><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7a0dc25bf3864a89b9140214dd297712><p class=pgc-img-caption></p></div><p>這兩個條件可以很容易地通過將單位圓看作一個約束集來觀察。在第一部分中，我們只考慮一個(\mu)符號不重要的邊界，這是等式約束的結果。在第二種情況下，考慮一個單位圓的內部集合，其中-ve符號表示(\lambda)，表示可行解區域。</p><p>KKT (Karush-Kuhn-Tucker)條件被認為是一階必要條件，當一個點被認為是一個平穩點(局部極小點、局部極大點、鞍點)時，該條件必須滿足。x ^ * 是局部最小值：</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d5deb7ddb2784994b72a8ff9b4d95263><p class=pgc-img-caption></p></div><p>LICQ條件:所有活動約束</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ba46179089304a7d9d48c42f13d15832><p class=pgc-img-caption></p></div><p>它們應該是線性無關的。</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/013c6c5c165b462586cfb6980f7d4f8b><p class=pgc-img-caption></p></div><p><strong>拉格朗日函數</strong></p><p>對於任何函數f (x)與不等式約束g_i (x)≤0和等式約束h_i (x) = 0,拉格朗日L (x \λ\μ)</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cd3555b8fd794d0ea6fed73e79e84902><p class=pgc-img-caption></p></div><p>優化函數：</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/13054fb19dbb4ac688ee541cd21e6a53><p class=pgc-img-caption></p></div><p>以上優化相當於：</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a6309387c53447019d54c6f7e30a6c1b><p class=pgc-img-caption></p></div><p>上面的公式被遊戲理論的主張稱為原始問題（p ^ *）（即第二個玩家將總是有更好的優化機會），可以很容易地看出：</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d665aa90bde447e997c9a7005bb88126><p class=pgc-img-caption></p></div><p>這個公式被稱為對偶問題(d ^ *)。</p><p>當且僅當目標函數為凸且約束集為凸時，原始公式和對偶公式的最優值相同。這項要求的證明理由如下:</p><p>函數是凸的，這意味著</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1b12aa17ef2744a6ae3f3138ec003d26><p class=pgc-img-caption></p></div><h1>SVM（支持向量機）</h1><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9977d74ad95d4dd0aad4d2153cd0cd02><p class=pgc-img-caption></p></div><p>SVM屬於用於分類和迴歸問題的監督機器學習算法類。SVM易於擴展，可以解決線性和非線性問題（通過使用基於核的技巧）。在大多數問題中，我們無法對兩類不同的數據進行單獨的區分，因此我們需要在決策邊界的構建中提供一點餘地，這很容易用SVM來表示。支持向量機的思想是在兩組不同的數據點之間創建分離的超平面。一旦獲得了分離超平面，對其中一個類中的數據點(測試用例)進行分類就變得相對容易了。支持向量機甚至可以很好地用於高維數據。與其他機器學習（ML）模型相比，svm的優點是內存效率高、準確、快速。我們來看看支持向量機的數學</p><p>SVM原始問題</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ea18a226ae984fd7a8749f2e746c2f76><p class=pgc-img-caption></p></div><p>使用拉格朗日算法的SVM對偶問題</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/acd6f4be852b45ab860174fccca257c0><p class=pgc-img-caption></p></div><p>Derivation of Dual</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e38afdcc638c4def8c5d2443f4aa8ed8><p class=pgc-img-caption></p></div><h1>CVXOPT</h1><p>在本節中，我們將討論使用CVXOPT庫在python中實現上述SVM對偶算法。</p><p>CVXOPT通用格式的問題</p><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a587e128be194c36ae2ca49586689408><p class=pgc-img-caption></p></div><pre># SVM using CVXOPTimport​ numpy ​ as​ npfrom​ cvxopt ​ import​ matrix,solversdef​ ​ solve_SVM_dual_CVXOPT​ (x_train, y_train, x_test): """ Solves the SVM training optimisation problem (the Arguments: x_train: A numpy array with shape (n,d), denoting \R^d. y_train: numpy array with shape (n,) Each element x_train: A numpy array with shape (m,d), denoting dual) using cvxopt.  n training samples in takes +1 or -1 m test samples in \R^d. Limits: n&lt;200, d&lt;100, m&lt;1000  Returns: y_pred_test : A numpy array with shape (m,). This is the result of running the learned model on the test instances x_test. Each element is +1 or -1. """ n, d = x_train.shape c = ​ 10​ ​ # let max \lambda value be 10 y_train = np.expand_dims(y_train, axis=​ 1 ​ )*​ 1. P = (y_train * x_train).dot((y_train * x_train).T) q = -1.​ *np.ones((n, ​ 1 ​ )) G = np.vstack((np.eye(n)*​ -1​ ,np.eye(n))) h = np.hstack((np.zeros(n), np.ones(n) * c)) A = y_train.reshape(​ 1 ​ , ​ -1​ ) b = np.array([​ 0.0​ ]) P = matrix(P); q = matrix(q) G = matrix(G); h = matrix(h) A = matrix(A); b = matrix(b) sol = solvers.qp(P, q, G, h, A, b) lambdas = np.array(sol[​ 'x'​ ]) w = ((y_train * lambdas).T.dot(x_train)).reshape(​ -1​ , ​ 1 ​ ) b = y_train - np.dot(x_train, w) prediction = ​ lambda​ x, w, b: np.sign(np.sum(w.T.dot(x) + b)) y_test = np.array([prediction(x_, w, b) ​ for​ x_ ​ in​ x_test]) return​ y_testif​ __name__ == ​ "__main__"​ : # Example format of input to the functions n=​ 100 m=​ 100 d=​ 10 x_train = np.random.randn(n,d) x_test = np.random.randn(m,d) w = np.random.randn(d) y_train = np.sign(np.dot(x_train, w)) y_test = np.sign(np.dot(x_test, w)) y_pred_test = solve_SVM_dual_CVXOPT(x_train, y_train, x_test) check1 = np.sum(y_pred_test == y_test) print​ (​ "Score: {}/{}"​ .format(check1, len(y_Test)))</pre><div class=pgc-img><img alt=支持向量機(SVM)的約束和無約束優化、理論和實現 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b761e859b46a44af9c4bcc391126e11f><p class=pgc-img-caption></p></div></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>SVM</a></li><li><a>約束</a></li><li><a>無約束</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/e28a9d89.html alt=電力市場改革的約束及邊界條件 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e28a9d89.html title=電力市場改革的約束及邊界條件>電力市場改革的約束及邊界條件</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0a53677e.html alt=時序約束是如何影響數字系統的，具體如何做時序分析？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/S7TMgbZ6pelMuY style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0a53677e.html title=時序約束是如何影響數字系統的，具體如何做時序分析？>時序約束是如何影響數字系統的，具體如何做時序分析？</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/9a564a99.html alt="從零推導支持向量機 (SVM)" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RHOqAwPHvZFTTi style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/9a564a99.html title="從零推導支持向量機 (SVM)">從零推導支持向量機 (SVM)</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/f7769191.html alt=什麼是支持向量機（SVM）？-Python class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/64c948f7266c4ee4a77f9cb634eb3274 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/f7769191.html title=什麼是支持向量機（SVM）？-Python>什麼是支持向量機（SVM）？-Python</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/cb9490d5.html alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d9a5ff954d45460fa5bb43d259dd388e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/cb9490d5.html title="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu">支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/c86d7e5d.html alt=支持向量機（SVM）小結 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/34341bbffe91417b9f732c28799b78ed style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/c86d7e5d.html title=支持向量機（SVM）小結>支持向量機（SVM）小結</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/2a085964.html alt=機器學習：支持向量機（SVM） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1530360893829fb265274ef style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/2a085964.html title=機器學習：支持向量機（SVM）>機器學習：支持向量機（SVM）</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/3cf7440b.html alt=理解SVM支持向量機 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3d1bb058669241a1b9254be04aac6818 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/3cf7440b.html title=理解SVM支持向量機>理解SVM支持向量機</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/dd29e0eb.html alt=理解支持向量機（SVM） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1540115493549aa21a19958 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/dd29e0eb.html title=理解支持向量機（SVM）>理解支持向量機（SVM）</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/61ae03dd.html alt=面試必備：支持向量機（SVM）重要知識點總結 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/61ae03dd.html title=面試必備：支持向量機（SVM）重要知識點總結>面試必備：支持向量機（SVM）重要知識點總結</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b9d020fe.html alt=常見零件的聯接及其約束你知道嗎？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5a46aee604f14f08b449e89e21077473 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b9d020fe.html title=常見零件的聯接及其約束你知道嗎？>常見零件的聯接及其約束你知道嗎？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b1ba3caf.html alt=教你學Python30-支持向量機SVM基礎 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/07db0cc84ff24bd5ae7a4632bd7f97b3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b1ba3caf.html title=教你學Python30-支持向量機SVM基礎>教你學Python30-支持向量機SVM基礎</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/75515be.html alt=緊箍咒的真正作用——唐僧的自我約束 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/b2f24ea08bc94bcbb70fed42aa65670e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/75515be.html title=緊箍咒的真正作用——唐僧的自我約束>緊箍咒的真正作用——唐僧的自我約束</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/f3ff910.html alt="力學中概念問題——約束  約束反力" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ad6519e30e5b444c888b8de73d7ce4a0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/f3ff910.html title="力學中概念問題——約束  約束反力">力學中概念問題——約束 約束反力</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/3fdad20.html alt=規則沒有定好，對孩子約束力會大大降低，制定規則需要注意這些 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/c8310b6d-af12-4a31-84c9-73019215122c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/3fdad20.html title=規則沒有定好，對孩子約束力會大大降低，制定規則需要注意這些>規則沒有定好，對孩子約束力會大大降低，制定規則需要注意這些</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>