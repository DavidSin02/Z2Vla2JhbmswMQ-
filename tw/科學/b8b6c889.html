<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020 | 极客快訊</title><meta property="og:title" content="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/S1YPO2YkDdit"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/b8b6c889.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/b8b6c889.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/b8b6c889.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/b8b6c889.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/b8b6c889.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/b8b6c889.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/b8b6c889.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/b8b6c889.html><meta property="article:published_time" content="2020-11-14T20:57:31+08:00"><meta property="article:modified_time" content="2020-11-14T20:57:31+08:00"><meta name=Keywords content><meta name=description content="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/b8b6c889.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><img alt="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S1YPO2YkDdit><p><strong>本</strong><strong>文介紹的是ICML 2020 論文《</strong><strong>PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization </strong><strong>》，論文作</strong><strong>者</strong><strong>來自倫敦帝國理工學院和谷歌。</strong></p><p>作者 | 劉傑鵬</p><p>編輯 | 叢 末</p><img alt="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S3xL2IIHQpfmbu><p>論文地址：https://arxiv.org/pdf/1912.08777.pdf</p><p>論文代碼：https://github.com/google-research/pegasus</p><p><strong>1</strong></p><p></p><h1 toutiao-origin=h3>導語</h1><p>近些年 Transformers 在海量語料上進行自監督預訓練再到下游各種NLP任務(當然也包括文本摘要)上微調的方案已取得巨大成功。但是，尚未有針抽象文本摘要(abstractive text summarization)定製預訓練目標。此外，目前抽象文本摘要任務也缺乏跨領域的系統評價。</p><p>為此，本文提出了一種新的自監督預訓練目標：GSG(Gap Sentences Generation)，以適配 Transformer-based 的 encoder-decoder 模型在海量文本語料上預訓練。在 PEGASUS 中， 將輸入文檔中的“重要句子”刪除或者遮蔽，再利用剩餘的句子在輸出中生成這些被刪除或遮蔽的句子。從輸入和輸出看，該目標與文本摘要類似。</p><p>本文以 12 個文本摘要數據集(包括新聞、科學、故事、使用說明、電子郵件、專利和立法議案)對最好的 PEGASUS 模型進行全面測試。實驗結果是：PEGASUS 刷新 12 個數據集的 ROUGE 得分記錄。另外，PEGASUS 模型在處理低資源摘要數據集也顯示出驚人的性能，在 6 個數據集上僅以 1000 個樣本就超過了之前的最先進結果。最後，本文還對 PEGASUS 模型生成的摘要結果進行人工評測，結果表明本文的模型在多個數據集上達到與人工摘要相媲美的性能。</p><p><strong>2</strong></p><p></p><h1 toutiao-origin=h3>前言</h1><p>抽象文本摘要是一項極具挑戰的自然語言處理任務，因為這要求理解長篇文章、壓縮資訊以及生成語言。目前主流的解決方案是用 seq2seq，讓神經網路學習把輸入序列映射到輸出序列。這些 seq2seq 模型最初是使用 RNN，但因為基於 Transformer encoder-decoder 的各種模型在處理長序列中的依賴關係表現更好，所以逐漸更受青睞。</p><p>各種 Transformer 模型與自監督預訓練技術(如 BERT、GPT-2、 RoBERTa、XLNet、ALBERT、T5、ELECTRA)相結合，已被證明是學習生成通用語言的強大框架。之前的工作中，預訓練使用的自監督目標對下游應用有一定程度的不可知性，即不考慮下游任務，如此有利於模型通用性的學習。本文認為如果預訓練的自監督目標更接近最終的任務，那麼最終的下游任務能取得更好的結果。</p><p>實驗證明，將輸入文檔中部分句子遮蔽掉，用剩餘的句子生成被遮蔽掉句子的這種預訓練目標很適用於文本摘要任務。這種預訓練目標確實適合於抽象摘要，因為它非常類似於下游任務，從而促進模型對整個文檔的理解和類似摘要的生成。需要指出的是，選擇重要句子比隨機選擇或者選擇前幾句的結果性能都要好。</p><p>在 C4 語料上預訓練出的最好 PEGASUS 模型，參數只有 568M，但在 12 個評測數據集上評測能夠比肩此前最優結果，甚至超越它們刷新紀錄。另外，本文為進一步提升最先進結果，引入了一個新收集的文本語料庫，該語料庫由新聞類文章組成包括 XSum 和 CNN/DailyMail 摘要數據集，統稱為 HugeNews。此外，將本文的模型應用了低資源文本摘要任務上時，實驗結果表明本文的模型能夠非常快速適用於少量監督對的微調，並僅以 1000 個樣本即在 6 個數據集中斬獲桂冠。最後，還將文本模型的結果與人工摘要結果做對比，結果表明本文的模型可以達到與人工摘要相媲美的效果。</p><p>總結下本文的貢獻：</p><p>(1)提出了一個新的自監督的預訓練目標(GSG)用於抽象摘要任務，並研究相應的句子選擇策略。</p><p>(2)用多個領域的摘要任務數據集對 GSG 進行廣泛評測，並仔細地選擇最佳的模型設置，訓練一個參數量僅為 568M 的 PEGASUS 模型。該模型在全部的 12 個下游數據集上能夠超過或與當前最先進水平持平。</p><p>(3)對於低資源任務數據集，通過微調 PEGASUS 模型，可以在廣泛的領域實現良好的抽象摘要效果。在多個任務上，僅需 1000 個樣本就超過了以前的最先進的結果。</p><p>(4)對模型結果進行人工評估，結果表明在 XSum, CNN/DailyMail 和 Reddit TIFU 上的摘要效果與人工摘要比肩。</p><p><strong>3</strong></p><p></p><h1 toutiao-origin=h3>模型</h1><p><strong>1、預訓練目標 GSG</strong></p><blockquote class=pgc-blockquote-abstract toutiao-origin=p>本文假設預訓練自監督的目標越接近最終的任務則結果性能越好。在 PEGASUS 預訓練中，將文件裡的幾個完整句子刪除，而模型的目標就是要恢復這些句子，換句話說，用來預訓練的輸入是有缺失部分句子的文檔，而輸出則是缺失句子的串連。這是一項難以置信的艱鉅任務，甚至對人人類來說也是不可能的，我們並不期望模型能完美地解決它。然而，這樣一個具有挑戰性的任務促使模型學習到關於語言的知識和這個世界的一般事實，以及如何從整個文檔中提取信息，以便生成類似於微調摘要任務的輸出。這種自監督的優點是，可以創建與文檔一樣多的示例，而不需要任何人工註釋，而這通常是純監督系統的阿喀琉斯之踵。</blockquote><p>Figure 1 展示了 GSG 和 MLM 如何同時作用到一個樣本。在實驗中發現，MLM 任務在大的預訓練 Steps 中並不能提升下游任務，所以在最終的 PEGASUS_{LARGE}版中捨棄了 MLM 任務。</p><img alt="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S3xL2Ir8WPL0ua><p>實驗發現，選擇重要的句子來遮蔽效果最好，讓自監督示例的輸出結果更像摘要。那麼怎麼選擇重要的句子？根據 ROUGE 度量標準，通過查找那些與文檔的其他部分最相似的句子，自動地識別出這些句子。ROUGE 計算兩個文本的 n-gram 重疊，從而得到文本之間的相似性(ROUGE-1、ROUGE-2 和 ROUGE-L 是三種常見的變體)。句子選擇策略如 Figure 2 所示：</p><img alt="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S3xL2JT2y2b4Ix><p>Ind 表示獨立計算每個句子得分再選擇 top 個句子(其對立面是 Seq，通過貪婪地最大化所選句子之間的 ROUGE1-F1)，Orig 表示採用原始的 n-grams(其對立面是 Uniq，句子中的 n-grams 去重)。所以，組合方案有 4 種。</p><p><strong>2、預訓練語料和下游任務</strong></p><p>與 T5 類似,本文預訓練所用的海量語料也是通過網絡爬取。接著在 12 個抽象摘要數據集上微調 PEGASUS，以 ROUGE 得分來看取得當下最好結果，但參數量只有 T5 的 5%。參與評測的 12 個數據集是多樣的的，包括新聞文章、科學論文、專利、短篇小說、電子郵件、法律文件和使用指南，這表明模型框架適用於廣泛的主題，具有一定通用性。</p><p>預訓練的語料具體如下：</p><p>（1）C4，這是 T5 中引入的語料</p><p>（2）HugeNews，這是本文新引入的</p><p>下游任務具體如下：</p><p>(1)XSum(2)CNN/DailyMail(3)NEWSROOM(4)Multi-News(5)Gigaword(6)arXiv(7)PubMed(8)BIGPATENT(9)WikiHow(10)Reddit TIFU(11)AESLC(12)BillSum</p><p><strong>4</strong></p><p></p><h1 toutiao-origin=h3>實驗結果</h1><p>PEGASUS_{base}版：</p><p>參數量為 223M，L=12，H=768，F=3072，A=12，batch size=256。</p><p>PEGASUS_{large}版：</p><p>參數量為 568M，L=16，H=1024，F=4096，A=16，batch size=8192。</p><p><strong>1、消融研究</strong></p><p>模型的消融研究基 於PEGASUS_{BASE}，研究對象：預訓練語料、預訓練目標、詞典尺寸。</p><p>預訓練語料的影響如 Figure 3 所示：</p><img alt="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S3xL2K2AG6cdig><p>從上圖可以看出在兩個新聞相關的下游任務上用 HugeNews 預訓練效果更好，而另兩個非新聞類的任務 WikiHow 和 Reddit TIFU 則用 C4 預訓練效果更好。這表明，當預訓練的語料和下游任務更相關時，預訓練的模型可以更有效地遷移到下游任務。</p><p>預訓練目標的影響如 Figure 4 所示：</p><img alt="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S3xL2xTb2hoNd><p>Figure 4a 可以看出 Ind-Orig 的方案最佳，Seq-Uniq 次之。Figure 4a 展示了 gap-sentences 比例(GSR)的影響。實驗表明 GSR 低於 50% 較好，在 CNN/DailyMail 數據集上 15% 的比例可以得到最優結果。而 XSum/Reddit TIFU 和 WikiHow 的最佳值分別是 30% 和 45%。</p><p>Figure 5 展示了詞典大小的影響：</p><img alt="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S3xL2yo1aboDDw><p>在新聞類數據集中 Unigram 和 BPE 效果相差不大，而在非新聞數據集中 Unigram 則優於 BPE，特別是在 WikiHow 數據集上。在 XSum 和 CNN/DailyMail 上，Unigram 96kROUGE 得分最高。在 WikiHow 和 Reddit TIFU 數據集上對應的最佳選擇分別是 Unigram 128k 和 64k。</p><p><strong>2、Larger 模型效果</strong></p><p>基於之前的實驗，在 Large 版的模型中選用的是 GSG(Ind-Orig)預訓練目標(不帶有MLM)、Unigram 詞典大小 96k。</p><p>Table 1和 Table 2展示了 PEGASUS_{BASE} 和 PEGASUS_{LARGE}在下游任務上的表現。PEGASUS_{BASE}在多項任務上超過當前最優結果，PEGASUS_{LARGE} 則在全部下游任務超越當下最優結果。</p><img alt="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S3xL2zL6K8A3yf><img alt="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S3xL30HCrCE8Bp><p><strong>3、處理低資源數據集</strong></p><p>經過大型語料預訓練的 PEGASUS 模型，該模型不需要大量的樣本進行微調，就可以獲得接近最先進的性能。Figure 6 展示了 PEGASUS 模型在 8 個數據集下使用不同樣本數進行微調的結果。</p><img alt="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S3xL314H7Y8kf4><p>Large 版只要用 100 個樣本進行微調就可以得到與 Base 版在 20k 到 200k 樣本上進行監督訓練相近的結果。Large 版在其中的 6 個任務上以 1000 個微調樣本量就超越了之前的最優結果。在只有 1000 個微調樣本的情況下，在大多數任務中都比使用完整監督數據的強基線(Transformer 編碼器-解碼器)執行得更好，在某些情況下，強基線(Transformer 編碼器-解碼器)使用的是多個數量級的樣本。這種“樣本效率”極大地提高了文本摘要模型的有用性，因為它顯著地降低了監督數據收集的規模和成本，而在摘要的情況下，監督數據收集的成本是非常昂貴的。</p><p><strong>4、人工評測</strong></p><p>雖然使用像 ROUGE 這樣的自動度量標準在模型開發過程中作為度量標準有用，但是該標準提供的信息有限，比如無法獲悉文本摘要的流暢性或者與人類性能相比較結果如何。為此，本文還進行了一次人工評估，要求評分者將文本的模型摘要結果與人工摘要進行比較(不知道哪個是哪個)。這與圖靈測試有一些相似之處。</p><img alt="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S1rigla1BhSf8f><p>使用 3 個不同的數據集進行了實驗，發現人工評分者並不總是喜歡人工摘要，有時候也會更傾向於本文模型的摘要。特別是，在 XSum 和 CNN/Dailymail 數據集被大量研究的情況下，該模型僅使用 1000 個示例就達到了與人工摘要相媲美的結果。這表明，使用大量的監督樣本不再是文本摘要所必須的了。</p><p><strong>5</strong></p><p></p><h1 toutiao-origin=h3>總結</h1><p>本文提出 PEGASUS，一種基於 gap-sentences 生成的序列-序列模型，它為抽象文本摘要任務定製預訓練目標 GSG。此外本文研究了數種 gap-sentence 的選擇方法，確定了主句的選擇是最優策略。同時證明了預訓練語料庫、gap-sentences 比率、詞彙量所帶來的影響，並設置了最佳配置以在所有 12 個下游數據集上獲得最先進的結果。本文還表明，PEGASUS 模型能夠非常快速地適應新的摘要數據集，僅需 1000 個樣本就可以獲得較好的結果。最後在多個數據集上對比本文模型摘要與人工摘要，證明本文模型在多個數據集上可以取得與人類媲美的結果。</p><p><em>本文作者：劉傑鵬，畢業於華中科技大學，研究方向機器閱讀理解、文本生成等。</em></p><img alt="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S3xL3sRDuuq5K5><p><strong class=highlight-text toutiao-origin=span>ACL 2020原定於2020年7月5日至10日在美國華盛頓西雅圖舉行，因新冠肺炎疫情改為線上會議。為促進學術交流，方便國內師生提早了解自然語言處理（NLP）前沿研究，AI 科技評論將推出「ACL 實驗室系列論文解讀」內容，同時歡迎更多實驗室參與分享，敬請期待！</strong></p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>帝國</a></li><li><a>聯手</a></li><li><a>ICML</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/ada3c70c.html alt=帝國盛世的危機：維多利亞死後的英國為何迅速衰落 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/85d304cd5d254113b63b8f046f5caa0f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ada3c70c.html title=帝國盛世的危機：維多利亞死後的英國為何迅速衰落>帝國盛世的危機：維多利亞死後的英國為何迅速衰落</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/810dab5a.html alt=羅馬帝國的衰落 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/810dab5a.html title=羅馬帝國的衰落>羅馬帝國的衰落</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/38791397.html alt=紅色帝國＂暴風雪＂號的使命 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/e764e08d8c7442abb2a40e3b4ef98e8a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/38791397.html title=紅色帝國＂暴風雪＂號的使命>紅色帝國＂暴風雪＂號的使命</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/11486dbc.html alt=羅馬帝國時期的三大蠻族的祖先，主要遷移過哪些地方 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/fbf9f59099804004a907891306dcd569 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/11486dbc.html title=羅馬帝國時期的三大蠻族的祖先，主要遷移過哪些地方>羅馬帝國時期的三大蠻族的祖先，主要遷移過哪些地方</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e6e2f12e.html alt=小說：她躲了他五年，帝國總裁直接在酒店房間堵她，她卻不知 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/ffdc0000260dfc9de34f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e6e2f12e.html title=小說：她躲了他五年，帝國總裁直接在酒店房間堵她，她卻不知>小說：她躲了他五年，帝國總裁直接在酒店房間堵她，她卻不知</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fac9923b.html alt=小說：帝國總裁撞見萌寶媽，緊抓不放，“跟我去民政局領證。” class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/ff32000036841bd8fcf7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fac9923b.html title=小說：帝國總裁撞見萌寶媽，緊抓不放，“跟我去民政局領證。”>小說：帝國總裁撞見萌寶媽，緊抓不放，“跟我去民政局領證。”</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3f51fe23.html alt=年收600億方便麵帝國，賣遍全國，康師傅究竟還是不是那個味兒 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/c5a7e44c893744e8b09306ea0a8ab07d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3f51fe23.html title=年收600億方便麵帝國，賣遍全國，康師傅究竟還是不是那個味兒>年收600億方便麵帝國，賣遍全國，康師傅究竟還是不是那個味兒</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1a91b1f8.html alt=康師傅老闆不姓康，揭祕泡麵背後的魏氏帝國 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/cdfe48eaa3364fe4b781c6ae08f885a7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1a91b1f8.html title=康師傅老闆不姓康，揭祕泡麵背後的魏氏帝國>康師傅老闆不姓康，揭祕泡麵背後的魏氏帝國</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b11edaf9.html alt="聯手HERE，高德進軍地圖海外市場 | CES 2020" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RmzgE048umzR6f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b11edaf9.html title="聯手HERE，高德進軍地圖海外市場 | CES 2020">聯手HERE，高德進軍地圖海外市場 | CES 2020</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b5037302.html alt="ICML 2019 Workshop短文 | TuckER：基於張量因式分解的知識圖譜補全" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/RVQ722d8Fi4fDf style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b5037302.html title="ICML 2019 Workshop短文 | TuckER：基於張量因式分解的知識圖譜補全">ICML 2019 Workshop短文 | TuckER：基於張量因式分解的知識圖譜補全</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f7df70d5.html alt="嶗山區金家嶺街道：黨群聯手戮力同心 打好疫情阻擊戰" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f7df70d5.html title="嶗山區金家嶺街道：黨群聯手戮力同心 打好疫情阻擊戰">嶗山區金家嶺街道：黨群聯手戮力同心 打好疫情阻擊戰</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9bde3cab.html alt=大華樂橙佈局民用智能安防！聯手阿里、華為推智能雲門鎖 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/1540389038739b86f8bf536 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9bde3cab.html title=大華樂橙佈局民用智能安防！聯手阿里、華為推智能雲門鎖>大華樂橙佈局民用智能安防！聯手阿里、華為推智能雲門鎖</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1fb22a23.html alt="布加迪與弗勞恩霍夫聯手 利用3D打印設備製造鈦金屬製動鉗" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/65b40013401a0e17d98b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1fb22a23.html title="布加迪與弗勞恩霍夫聯手 利用3D打印設備製造鈦金屬製動鉗">布加迪與弗勞恩霍夫聯手 利用3D打印設備製造鈦金屬製動鉗</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/568587ac.html alt=《重案六組》第五部來了，六組老人再度聯手推出《天下無詐》！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7c3e2dfa396a4a738a1f730f22001887 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/568587ac.html title=《重案六組》第五部來了，六組老人再度聯手推出《天下無詐》！>《重案六組》第五部來了，六組老人再度聯手推出《天下無詐》！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6fb83afc.html alt=全球民間最大教育組織聯手百年童書協會：這些書告訴孩子創造性思維如何改變世界 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RT8JhxnF4hpOvY style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6fb83afc.html title=全球民間最大教育組織聯手百年童書協會：這些書告訴孩子創造性思維如何改變世界>全球民間最大教育組織聯手百年童書協會：這些書告訴孩子創造性思維如何改變世界</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>