<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>IT大數據學習分享：關於機器學習的知識點（1） | 极客快訊</title><meta property="og:title" content="IT大數據學習分享：關於機器學習的知識點（1） - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/dfic-imagehandler/d4954510-db08-4291-9757-377b5c9ae457"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/bab82bac.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/bab82bac.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/bab82bac.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/bab82bac.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/bab82bac.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/bab82bac.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/bab82bac.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/bab82bac.html><meta property="article:published_time" content="2020-11-14T20:56:35+08:00"><meta property="article:modified_time" content="2020-11-14T20:56:35+08:00"><meta name=Keywords content><meta name=description content="IT大數據學習分享：關於機器學習的知識點（1）"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/bab82bac.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>IT大數據學習分享：關於機器學習的知識點（1）</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><blockquote><p>作者用超過1.2萬字的篇幅，總結了自己學習機器學習過程中遇到知識點。“入門後，才知道機器學習的魅力與可怕。”希望正在閱讀本文的你，也能在機器學習上學有所成。作者 塵戀</p></blockquote><p><br></p><div class=pgc-img><img alt=IT大數據學習分享：關於機器學習的知識點（1） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/dfic-imagehandler/d4954510-db08-4291-9757-377b5c9ae457><p class=pgc-img-caption></p></div><p><br></p><h1 class=pgc-h-arrow-right>準備</h1><p>機器學習是什麼，人工智能的子類，深度學習的父類。</p><p><strong>機器學習：</strong>使計算機改進或是適應他們的行為，從而使他們的行為更加準確。也就是通過數據中學習，從而在某項工作上做的更好。</p><p>引用王鈺院士在2008年會議的一句話，假定W是給定世界的有限或者無限的所有對象的集合，Q是我們能夠或得到的有限數據，Q是W的一個很小的真子集，機器學習就是根據世界的樣本集來推算世界的模型，使得模型對於整體世界來說為真。</p><p><strong>機器學習的兩個驅動：</strong>神經網絡，數據挖掘。</p><p><strong>機器學習的分類：</strong></p><p><strong>監督學習</strong>：提供了包含正確回答的訓練集，並以這個訓練集為基礎，算法進行泛化，直到對所有的可能輸入都給出正確回答，這也稱在範例中學習。</p><p><strong>無監督學習：</strong>沒有提供正確回答，算法試圖鑑別出輸入之間的相似，從而將同樣的輸入歸為一類，這種方法稱密度學習。</p><p><strong>強化學習：</strong>介於監督和無監督之間，當答案不正確時，算法被告知，如何改正則不得而知，算法需要去探索，試驗不同情況，直到得到正確答案，強化學習有時稱為伴隨評論家的學習，因為他只對答案評分，而不給出改進建議。</p><p><strong>進化學習</strong>：將生物學的進化看成一個學習過程，我們研究如何在計算機中對這一過程進行建模，採用適應度的概念，相當於對當前解答方案好壞程度的評分。(不是所有機器學習書籍都包含進化學習)</p><p><strong>優點：</strong>泛化，對於未曾碰到的輸入也能給出合理的輸出。</p><p><strong>監督學習：</strong>迴歸、分類。</p><p><strong>機器學習過程：</strong></p><p>數據的收集和準備<br>特徵選擇<br>算法選擇<br>參數和模型選擇<br>訓練<br>評估</p><p><strong>專業術語：</strong></p><p>輸入：輸入向量x作為算法輸入給出的數據</p><p>突觸：wij是節點i和節點j之間的加權連接，類似於大腦中的突觸，排列成矩陣W</p><p>輸出：輸出向量y，可以有n個維度</p><p>目標：目標向量t，有n個維度，監督學習所需要等待額外數據，提供了算法正在學習的“正確答案”</p><p>維度：輸入向量的個數</p><p>激活函數：對於神經網絡，g(·)是一種數學函數，描述神經元的激發和作為對加權輸入的響應</p><p>誤差：E是根據y和t計算網絡不準確性的函數</p><p>權重空間：當我們的輸入數據達到200維時，人類的限制使得我們無法看見，我們最多隻能看到三維投影，而對於計算機可以抽象出200個相互正交的軸的超平面進行計算，神經網絡的參數是將神經元連接到輸入的一組權重值，如將神經元的權重視為一組座標，即所謂的權重空間</p><p>維度災難：隨著維度的增加，單位超球面的體積也在不斷增加，2d中，單位超球面為圓，3d中則為求，而更高的維度便稱為超球面，Vn = (2π/n)*Vn-2，於是當n>2π時，體積開始縮小，因此可用數據減少，意味著我們需要更多的數據，當數據到達100維以上時，單位數據變得極小，進而需要更多的數據，從而造成維度災難</p><p><strong>維度和體積的關係：</strong></p><p></p><div class=pgc-img><img alt=IT大數據學習分享：關於機器學習的知識點（1） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/31636b4b28e044fa94df485a4d7dd7ba><p class=pgc-img-caption></p></div><p></p><p><strong>機器學習算法測試：</strong></p><p>算法成功程度是預測和一直目標進行比較，對此我們需要一組新的數據，測試集。</p><p>當對算法進行訓練時，過度的訓練將會導致過擬合，即擬合曲線與數據完美擬合，但是失去了泛化能力，為檢測過擬合我們需要用測試集進行驗證，稱為統計中的交叉驗證，它是模型選擇中的一部門：為模型選擇正確的參數，以便儘可能的泛化。</p><p>數據的準備，我們需要三組數據集，訓練算法的訓練集，跟蹤算法學習效果的驗證集，用於產生最終結果的測試集，數據充足情況便執行50:25:25或60:20:20的劃分，數據集分配應隨機處理，當數據請核實板塊，則採用流出方法或多折交叉驗證。</p><p>混淆矩陣是檢測結果是否良好的分類，製作一個方陣，其包含水平和垂直方向上所有可能的類，在(i，j)處的矩陣元素告訴我們在目標中有多少模式被放入類i中，主對角線上任何東西都是正確答案，主對角線元素之和除以所有元素的和，從而得到的百分比便是精度。</p><p>精度指標：真正例是被正確放入類1，假正例是被錯誤放入類1，而真反例是被正確放入類2，假反例是被錯誤放入類2。</p><p></p><div class=pgc-img><img alt=IT大數據學習分享：關於機器學習的知識點（1） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5ad591c98bf24639b1397e53f83ff858><p class=pgc-img-caption></p></div><p></p><p>敏感率=#TP/(#TP+#FN) 特異率=#TN/(#TN+#FP)<br>查準率=#TP/(#TP+#FP) 查全率=#TP/(#TP+#FN)<br>F1 = 2*(查準率*查全率)/(查準率+查全率)</p><p><strong>受試者工作曲線：</strong>y軸真正例率，x軸假正例率，線下區面積：AUC。</p><p>數<strong>據與概率的轉換：</strong>通過貝葉斯法則處理聯合概率P(C,X)和條件概率P(X|C)得出P(C|X)，MAP問題是訓練數據中最可能的類是什麼。將所有類的最終結果考慮在內的方法稱為貝葉斯最優分類。</p><p><strong>損失矩陣：</strong>指定類Ci被分為類Cj所涉及的風險。</p><p><strong>基本統計概念：</strong>協方差，度量兩個變量的依賴程度。</p><p>Cov({xi},{yi})=E({xi} – u)E({yi} – v)</p><p><strong>權衡偏差與方差：</strong>偏差-方差困境：更復雜的模型不一定能產生更好的結果;模型糟糕可能由於兩個原因，模型不準確而與數據不匹配，或者不精確而有極大的不穩定性。第一種情況稱為偏差，第二種情況稱為方差。</p><h1 class=pgc-h-arrow-right>01 神經元、神經網絡和線性判別</h1><p><strong>1. 魯棒性</strong></p><p>魯棒是Robust的音譯，也就是健壯和強壯的意思。它是在異常和危險情況下系統生存的關鍵。比如說，計算機軟件在輸入錯誤、磁盤故障、網絡過載或有意攻擊情況下，能否不死機、不崩潰，就是該軟件的魯棒性。</p><p><strong>2. 神經網絡</strong></p><p>神經網絡模仿的便是生物學中的神經網絡，通過輸入進而判定神經元激活否。</p><p>將一系列的神經元放置在一起，假設數據存在模式。通過神經元一些已知的樣例，我們希望他能夠發現這種模式，並且正確預測其他樣例，稱為模式識別。為了讓神經網絡能夠學習，我們需要改變神經元的權重和閾值進而得到正確的結果，歷史上的第一個神經網絡——感知器。</p><p><strong>3. Hebb法則</strong></p><p>突觸連接強度的變化和兩個相連神經元激活得相關性成比例，如果兩個神經元始終同時激活，那麼他們之間連接的強度會變大，反之，如果兩個神經元從來不同時激活，那麼他們之間的連接會消失。也被成為長時效增強法則和神經可塑性。</p><p><strong>4. McCulloch和Pitts神經元</strong></p><p>建模，一組輸入加權wi相當於突觸，一個加法器把輸入信號相加(等價於收集電荷的細胞膜)，一個激活函數，決定細胞對於當前的輸入是否激活，輸入乘於權重的和與閾值進行判斷，大於則激活，否則抑制。侷限性：現實中的神經元不給出單一的輸出相應，而是給出一個點位序列，一種連續的方式給出分等級的輸出。神經元不會根據電腦的時鐘脈衝去順序更新，而是隨機的異步更新。</p><p><strong>5. 感知器</strong></p><p></p><div class=pgc-img><img alt=IT大數據學習分享：關於機器學習的知識點（1） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a43c2a6725b14426b1741b7baa19612b><p class=pgc-img-caption></p></div><p></p><p>▲感知器神經網絡</p><p><strong>權重更新規則</strong></p><p>Wij &lt;- Wij – n(yi – ti)*xi</p><p>N為學習效率，過大會造成網絡不穩定，過小會造成學習時間久;yi為神經元輸出，ti為神經元目標，xi為神經元輸入，Wij為權重。</p><p><strong>感知器學習算法</strong></p><p>分為兩部分，訓練階段和再現階段。</p><p></p><div class=pgc-img><img alt=IT大數據學習分享：關於機器學習的知識點（1） onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/38fdd6235d314ab48650b257610ccd79><p class=pgc-img-caption></p></div><p></p><p><strong>6. 線性可分性</strong></p><p>一條直線將神經元激活的和不激活的神經元劃分開來，這條直線稱為決策邊界，也稱為判別函數，在三維空間該決策邊界為平面，更高維則為超平面。</p><p><strong>7. 感知器收斂定理</strong></p><p>感知器以1/γ*γ為界，其中γ為分離超平面與最接近的數據點之間的距離。</p><p>只要把數據映射到正確的維度空間，那麼總是可以用一個線性函數來把兩個類別區分開，為了較有效率的解決這個問題，有一整類的方法稱為核分類器，也是支持向量機的基礎。</p><p><strong>8. 數據項預處理</strong></p><p>特徵選擇，我們每次去掉一個不同的特徵，然後試著在所得的輸入子集上訓練分類器，看結果是否有所提高，如果去掉某一個特徵能使得結果有所改進，那麼久徹底去掉他，在嘗試能否去掉其他的特徵，這是一個測試輸出與每一個特徵的相關性的過於簡單方法。</p><p><strong>9. 線性迴歸</strong></p><p>迴歸問題是用一條線去擬合數據，而分類問題是尋找一條線來劃分不同類別。迴歸方法，引入一個指示變量，它簡單的標識每一個數據點所屬的類別。現在問題就變成了用數據去預測指示變量，第二種方法是進行重複的迴歸，每一次對其中的一個類別，指示值為1代表樣本屬於該類別，0代表屬於其他類別。</p><h1 class=pgc-h-arrow-right>02 維度簡約</h1><p><strong>1. 降維的三種算法</strong></p><p>特徵選擇法：仔細查找可見的並可以利用的特徵而無論他們是否有用，把它與輸出變量關聯起來</p><p>特徵推導法：通過應用數據遷移，即通過可以用矩陣來描述的平移和旋轉來改變圖標的座標系，從而用舊的特徵推導出新的特徵，因為他允許聯合特徵，並且堅定哪一個是有用的，哪一個沒用</p><p>聚類法：把相似的數據點放一起，看能不能有更少的特徵</p><p><strong>2. 特徵選擇方法</strong></p><p>建設性方法：通過迭代不斷加入，測試每一個階段的錯誤以瞭解某個特徵加入時是否會發生變化。破壞性方法是去掉應用在決策樹上的特徵。</p><p><strong>主成分分析(PCA)</strong></p><p>主成分的概念是數據中變化最大的方向。算法首先通過減去平均值來把數據集中， 選擇變化最大的方向並把它設為座標軸，然後檢查餘下的變化並且找一個座標軸使得它垂直於第一個並且覆蓋儘可能多的變化。</p><p>不斷重複這個方法直到找到所有可能的座標軸。這樣的結果就是所有的變量都是沿著直角座標系的軸，並且協方差矩陣是對角的——每個新變量都與其他變量無關，而只與自己有關。一些變化非常小的軸可以去掉不影響數據的變化性。</p><p><strong>具體算法</strong></p><p></p><div class=pgc-img><img alt=IT大數據學習分享：關於機器學習的知識點（1） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e7faabb2768240f88703c3f17b718ed5><p class=pgc-img-caption></p></div><p></p><p><strong>3. 基於核的PCA算法</strong></p><p></p><div class=pgc-img><img alt=IT大數據學習分享：關於機器學習的知識點（1） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2170eccabb3d47f7b6250933a6ad9989><p class=pgc-img-caption></p></div><p></p><p><strong>4. 因素分析</strong></p><p>觀察數據是否可以被少量不相關的因素或潛在的變量解釋，目的用於發現獨立因素和測量每一個因素固有的誤差。</p><p><strong>5. 獨立成分分析(ICA)</strong></p><p>統計成分是獨立的，即對於E[bi,bj] = E[bi]E[bj]與及bi是不相關的。</p><p><strong>6. 局部線性嵌入算法</strong></p><p>找出每個點的鄰近點(即前k個近的點):計算每對點間的距離。找到前k個小的距離。對於其他點，令Wij=0.對每個點xi:創建一個鄰近點的位置表z,計算zi=zi-xi。</p><p>根據約束條件計算令等式(6.31)最小的權矩陣W:計算局部協方差C=ZZ^T，其中Z是zi組成的矩陣。利用CW=I計算W,其中I是N*N單位矩陣。對於非鄰近點，令Wij=0。</p><p>對W/∑W設置其他元素計算使得等式(6.32)最小的低維向量 yi:創建M=(I-W)T(I-W).計算M的特徵值和特徵向量。根據特徵值的大小給特徵向量排序。對應於第q小的特徵值，將向量y的第q行設置為第q+1 個特徵向量(忽略特徵值為0)</p><p></p><div class=pgc-img><img alt=IT大數據學習分享：關於機器學習的知識點（1） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8feca6c276a5410ba683efebce30da54><p class=pgc-img-caption></p></div><p></p><p><strong>7. 多維標度算法</strong></p><p></p><div class=pgc-img><img alt=IT大數據學習分享：關於機器學習的知識點（1） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/86a9c06adaa845289f105842c88b2c53><p class=pgc-img-caption></p></div><p></p><p><strong>8. ISOMAP算法</strong></p><p></p><div class=pgc-img><img alt=IT大數據學習分享：關於機器學習的知識點（1） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6de99dbc9b754a04ac38601539f72a7d><p class=pgc-img-caption></p></div><p></p><h1 class=pgc-h-arrow-right>03 概率學習</h1><p><strong>1. 期望最大算法(EM)</strong></p><p>額外加入位置變量，通過這些變量最大化函數。</p><p><strong>2. 高斯混合模型的期望最大算法</strong></p><p></p><div class=pgc-img><img alt=IT大數據學習分享：關於機器學習的知識點（1） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d2704059656e428bbb4cdd1c9c2023e4><p class=pgc-img-caption></p></div><p></p><p><strong>3. 通常的期望最大化算法</strong></p><p></p><div class=pgc-img><img alt=IT大數據學習分享：關於機器學習的知識點（1） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ce3cde331597406096ad4aa61474a06f><p class=pgc-img-caption></p></div><p></p><p><strong>4. 信息準則</strong></p><p>除了通過模型選擇確定停止學習的時間，前期採用驗證集思想，而信息準則則是確定一些方法從而期待這個訓練過的模型可以表現的多好。</p><p>艾卡信息準則：AIC = ln(C)-k<br>貝葉斯信息準則：BIC = 2ln(C)-klnN</p><p>K是模型中參數的數目，N是訓練樣本的數量，C是模型的最大似然。以上兩種方法都是奧卡姆剃刀的一種形式。</p><p><strong>5. 奧卡姆剃刀</strong></p><p>如無必要，勿增實體，即簡單有效原理。</p><p><strong>6. 最近鄰法</strong></p><p>如果沒有一個描述數據的模型，那麼最好的事情就是觀察相似的數據並且把他們選擇成同一類。</p><p><strong>7. 核平滑法</strong></p><p>用一個和(一堆點的權重函數)來根據輸入的距離來決定每一個數據點有多少權重。當兩個核都會對離當前輸入更近的點給出更高的權重，而當他們離當前輸入點越遠時，權重會光滑的減少為0，權重通過λ來具體化。</p><p><strong>8. KD-Tree</strong></p><p>在一個時刻選擇一個維度並且將它分裂成兩個，從而創建一顆二進制樹，並且讓一條直線通過這個維度裡點的座標的中位數。這與決策樹的差別不大。數據點作為樹的樹葉。</p><p>製作樹與通常的二進制樹的方法基本相同：我們定義一個地方來分裂成兩種選擇——左邊和右邊， 然後沿著它們向下。可以很自然地想到用遞歸的方法來寫算法。</p><p>選擇在哪分裂和如何分裂使得KD-Tree是不同的。在每一步只有一個維度分裂，分裂的地方是通過計算那一維度的點的中位數得到的，並且在那畫一條直線。通常，選擇哪一個維度分裂要麼通過不同的選擇要麼隨機選擇。</p><p>算法向下搜索可能的維度是基於到目前為止樹的深度，所以在二維裡，它要麼是水平的要麼是垂直的分裂。組成這個方法的核心是簡單地選代選取分裂的函數，找到那個座標的中位數的值，並且根據那個值來分裂點。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>學習</a></li><li><a>大數據</a></li><li><a>關於機器</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/35db2356.html alt="大數據深度學習的新利器: 快速神經網絡訓練:P-network" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d172925963f2465aa131058c05cd72f9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/35db2356.html title="大數據深度學習的新利器: 快速神經網絡訓練:P-network">大數據深度學習的新利器: 快速神經網絡訓練:P-network</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f58222f0.html alt=學習大數據採集，分析，存儲才是正確的方式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/515ef50ad4694ef5aef5cf5aa4c3c4e4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f58222f0.html title=學習大數據採集，分析，存儲才是正確的方式>學習大數據採集，分析，存儲才是正確的方式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c08f7fcf.html alt=寶信軟件：公司聚焦大數據、人工智能、虛擬製造等七大關鍵技術的研究 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c08f7fcf.html title=寶信軟件：公司聚焦大數據、人工智能、虛擬製造等七大關鍵技術的研究>寶信軟件：公司聚焦大數據、人工智能、虛擬製造等七大關鍵技術的研究</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html alt=直流鍋爐給水控制學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/eba10edcc8d14d9f8cde6fd5b212d90e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html title=直流鍋爐給水控制學習>直流鍋爐給水控制學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html alt=HTMLCSS學習筆記（六）——元素類型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb5988349894ce9bf568c6418f85b7d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html title=HTMLCSS學習筆記（六）——元素類型>HTMLCSS學習筆記（六）——元素類型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html alt="web前端（從零開始），每天更新學習筆記 HTML5元素分類" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/46d70004fcd55e1ddad3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html title="web前端（從零開始），每天更新學習筆記 HTML5元素分類">web前端（從零開始），每天更新學習筆記 HTML5元素分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html alt="MySQL 學習筆記" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html title="MySQL 學習筆記">MySQL 學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html alt=深入學習MySQL事務：ACID特性的實現原理「轉」 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/cdc702d66d6943499997d11e931425eb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html title=深入學習MySQL事務：ACID特性的實現原理「轉」>深入學習MySQL事務：ACID特性的實現原理「轉」</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html alt=如何學習模擬IC設計？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html title=如何學習模擬IC設計？>如何學習模擬IC設計？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html alt=小猿圈python學習-三大特性之多態 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ad0e8e3777854337abeb7c779ad79a04 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html title=小猿圈python學習-三大特性之多態>小猿圈python學習-三大特性之多態</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/03a295fc.html alt=地理學習5——地球的運動（地球的公轉及其地理意義） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7b2b74c871eb40beb8ee143627d29611 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/03a295fc.html title=地理學習5——地球的運動（地球的公轉及其地理意義）>地理學習5——地球的運動（地球的公轉及其地理意義）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ebad378f.html alt=繼續學習打卡，還真心學不會了，努力，堅持 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/f36d6d47a06840aaaf78138853b9d9d1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ebad378f.html title=繼續學習打卡，還真心學不會了，努力，堅持>繼續學習打卡，還真心學不會了，努力，堅持</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>