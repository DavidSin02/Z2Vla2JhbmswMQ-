<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習與線性代數 - 特殊矩陣 | 极客快訊</title><meta property="og:title" content="機器學習與線性代數 - 特殊矩陣 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/e9b67653bc8f44999d1b863010f5dc6a"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/c31f9da4.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/c31f9da4.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/c31f9da4.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/c31f9da4.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/c31f9da4.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/c31f9da4.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/c31f9da4.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/c31f9da4.html><meta property="article:published_time" content="2020-11-14T20:52:19+08:00"><meta property="article:modified_time" content="2020-11-14T20:52:19+08:00"><meta name=Keywords content><meta name=description content="機器學習與線性代數 - 特殊矩陣"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/c31f9da4.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習與線性代數 - 特殊矩陣</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><div><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e9b67653bc8f44999d1b863010f5dc6a><p class=pgc-img-caption></p></div><p>在線性代數中，有一些特殊的矩陣具有易於分析和操作的特性。它們的特徵向量可能具有特定的特徵值或特殊關係。還有一些方法可以將一個矩陣分解成這些“更簡單”的矩陣。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/88e7b256aa854aef9b74a2dd393696ea><p class=pgc-img-caption></p></div><p>操作複雜性的降低提高了可伸縮性。然而，即使這些矩陣都是特殊的，它們也不是罕見的。在機器學習和許多應用程序中，我們經常需要處理它們。</p><h1>對角矩陣</h1><p>對角矩陣S使所有非對角元素等於零。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/42ddbe5ba73a40cea24c7f2764c5288e><p class=pgc-img-caption></p></div><p>許多分解方法都有一個分解後的矩陣是對角矩陣。由於矩陣只包含對角元素，我們有時用向量來表示它。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/46cf6082aadd4ef2ae1bc67c9f4415b7><p class=pgc-img-caption></p></div><p>一般矩陣的逆不容易計算。但是求對角矩陣的逆很簡單。我們可以用1/m替換對角線元素。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0fa764305ea64824a20e6eab0747b9aa><p class=pgc-img-caption></p></div><p>如果其中一個矩陣是對角的，矩陣乘法就簡單多了。但是當任何對角元素等於0或者對角矩陣不是方形的時候，它的逆就不存在。但是，在一些方法中，偽逆矩陣（keep the inverse of 0 as 0）可以用作替代。</p><h1>正交矩陣</h1><p>正交矩陣Q是滿足下列要求的方形矩陣</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cfb6b64543144036ba111f55cdf1113f><p class=pgc-img-caption></p></div><p>Q中的所有列（v 1 ，...，v 6 ，...）都是正交的，即對於i≠j，vᵢᵀvⱼ= 0，vᵢ都是單位向量。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ff642f1eab3c4096b5473382ceb5f769><p class=pgc-img-caption></p></div><p>這聽起來像是一個嚴格的要求但是對於一些矩陣，比如對稱矩陣，我們可以選擇特徵向量在分解過程中是正交的。</p><p>以下矩陣是正交的。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5870bf0ea2a545de9345731046d4be4f><p class=pgc-img-caption></p></div><p>像對角矩陣一樣，它的逆也很容易計算 - 正交矩陣的逆是它的轉置。這是正交矩陣非常方便的一個關鍵原因。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7862fee2daa540bd82196f8a7e99578c><p class=pgc-img-caption></p></div><p>證明：</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9f33b5006ed447c998d7b1cc366cfe8c><p class=pgc-img-caption></p></div><p>如果我們用正交矩陣乘以x, x中的誤差不會被放大。這種行為對於保持數值穩定性是非常理想的。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/2df28519b1754fd095f7df49c128a3fd><p class=pgc-img-caption></p></div><h1>對稱矩陣</h1><p>如果矩陣的轉置等於自身，則矩陣是對稱的。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b57cc8a388d94e41a7b02ca2b5ee5a30><p class=pgc-img-caption></p></div><p>例如，</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/63f62502ecc5423b8a3029ce758c6c4f><p class=pgc-img-caption></p></div><p>對稱矩陣是線性代數和機器學習中最重要的矩陣之一。在機器學習(ML),我們經常使用矩陣保存<em>f(vᵢ , vⱼ)</em>。這些函數通常是對稱的，f(x, y) = f(y, x)，因此對應的矩陣是對稱的。例如在機器學習中，f可以測量數據點之間的特徵距離，或者計算特徵的協方差。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1be1daf1d95d459f94c0787b8522e9ca><p class=pgc-img-caption></p></div><p><strong>對稱矩陣屬性</strong></p><p>對稱矩陣S是n×n方形矩陣。</p><ul><li>它的逆也是對稱的。</li><li>S的所有特徵值都是實數(不是複數)。</li><li>即使重複的特徵值，我們也可以選擇S的 n個本徵向量為正交。</li><li>可以通過將矩陣A與其轉置 - AᵀA或AAᵀ（通常AᵀA ≠ AAᵀ）相乘來形成對稱矩陣。在機器學習中，以零為中心的協方差矩陣就是這種形式。</li></ul><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f81622d73884449694a846dc37f99c5b><p class=pgc-img-caption></p></div><ul><li>如果 A的列是線性無關的，則 AᵀA是可逆的。</li><li>每個對稱矩陣小號可以進行對角化（因式分解）與Q由正交的特徵向量形成vᵢ的小號和Λ是對角矩陣保持所有的特徵值。</li><li>每個對稱矩陣S可以被對角化（分解），其中Q由S的正交特徵向量vi形成，Λ是對角矩陣的所有特徵值。</li></ul><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e9b71241273a4cbb8f6b532a83f50c70><p class=pgc-img-caption></p></div><p>上面的等式可以改寫為</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/064f899b41c141ac9097a34599c6899e><p class=pgc-img-caption></p></div><p>其中v是單位向量。因此，特徵值項λᵢ主導了上述每個項的重要性。事實上，如果它太小，我們可以完全放棄相應的項λᵢvᵢvᵢᵀ。</p><p>該分解特性和“ S具有n個正交特徵向量”是對稱矩陣的兩個重要特性。</p><p><strong>正交特徵向量</strong></p><p>特徵向量不是唯一的。但通常，我們可以“選擇”一組特徵向量來滿足某些特定條件。如前所述，對稱矩陣的特徵向量可以選擇為正交。如果S是對稱矩陣，則其特徵值λ和μ滿足以下條件。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ae31324c671346c5873a848064f25f53><p class=pgc-img-caption></p></div><p>證明</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3a4207c1dcaf46eba70d33c652a9b9e8><p class=pgc-img-caption></p></div><p>從這種情況來看，如果λ和μ具有不同的值，則等效性迫使內積為零。因此，x和y是正交的，並且很容易將它們歸一化為具有單位長度 - 正交。這證明了如果它們的相應特徵值不同，我們可以選擇S的特徵向量是正交的。即使有重複的特徵值，對於對稱矩陣仍然如此。</p><p>證明 - 第2部分（可選）</p><p>對於n×n對稱矩陣，我們總能找到n個獨立的正交特徵向量。最大的特徵值是</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1bbd8cdc868149cc9cb7ed8045b1f438><p class=pgc-img-caption></p></div><p>為了求最大值，我們令r(x)的導數為0。經過一些處理，得到</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9a38c298094543a7b9b2160e2f8dd71d><p class=pgc-img-caption></p></div><p>即，當x是特徵向量且特徵值最大時，r(x)的比值最大。通過歸納，我們可以推導出我們可以用正交於前一個的特徵向量找到下一個最高的特徵值。這只是證明的高級描述。</p><p><strong>譜定理（Spectral theorem）</strong></p><p>讓我們總結一下。每個n×n對稱矩陣S具有n個實特徵值λᵢ，其中有n個正交特徵向量vᵢ。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/af3454afb0994f67af4f0ed0ff4d088a><p class=pgc-img-caption></p></div><p>這些特徵值可以形成對角矩陣Λ as <em>diag(λ)</em>。我們還可以將特徵向量vᵢ連接到V，即，</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2cce874663314eebb614de7101472edc><p class=pgc-img-caption></p></div><p>我們將V重命名為Q.因為Q是正交的，所以它是可逆的並且Qᵀ= Q-1。因此，對稱矩陣S可以分解為</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/e18556bcdb7b44619c1e14a0a5a0ac77><p class=pgc-img-caption></p></div><p>這是譜定理。因為找到轉置比逆轉更容易，所以在線性代數中非常需要對稱矩陣。</p><h1>正定矩陣</h1><p>正定矩陣具有所有正特徵值。它是對稱的。這聽起來很不尋常，但現實生活中的許多矩陣都是肯定的。下面的術語計算具有狀態x的系統的能量（<strong>energy</strong>）。如果S是正定的，它保證能量保持為正，除非x為零。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/c2a762a334084608a7a38bc3acc3ad8f><p class=pgc-img-caption></p></div><p>在許多應用中，我們假設能量是正的，因此，相應的S應該是正定的。</p><p>測試正定性有許多等效條件。如果以下任何測試為真，則對稱矩陣S為正定的：</p><p>1.所有特徵值> 0，</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/ceaa342ae7934b1a91b4bf4481b08d3a><p class=pgc-img-caption></p></div><p>2.所有左上角的行列式> 0，</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/06730bf168f44356a3071c0591a71a8b><p class=pgc-img-caption></p></div><p>3.所有pivots > 0，</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d091021e7942461c8e4ebb6bad2fa1a9><p class=pgc-img-caption></p></div><p>4.能量（<strong>energy</strong>）> 0，除了x = 0，</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9acf855cfcef4f17abe8ae4bdbafff46><p class=pgc-img-caption></p></div><p>5. S可以由一個列向量無關的矩陣a構成。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/87324c00559e4d94a3490ccc891ab3c7><p class=pgc-img-caption></p></div><p>驗證所有特徵值是正的需要很多工作。因此，條件2或3是更常見的測試。例如，正pivots 意味著正特徵值（反之亦然）。另一方面，如果我們通過上述測試之一證明矩陣是正定的，我們保證它擁有上述所有屬性。</p><p>證明</p><p>在本節中，我們將證明上面的一些屬性。如果S是正定的，則​​所有λ都是正的。因此，相應狀態x的計算能量為正（x = 0除外）。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/78ae61e032b84afd8ff3cf8f19b75610><p class=pgc-img-caption></p></div><p>如果S由AᵀA組成，則S在能量測試下為正。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6170d620117d41109a3976726c7ad1e1><p class=pgc-img-caption></p></div><p>除了正定，我們還有半正定，負定和半負定。半正定用“≥”替換上面的所有“>”條件（例如，它的特徵值是大於或等於0 ），負定和半負定與正定和半正定相反。</p><p><strong>Minimum</strong></p><p>在微積分中，我們將f的一階導數設置為零以找到其臨界點。然而，這樣的點可以是最大值，最小值或鞍點。許多機器學習模型以二次形式xAᵀx表示其成本函數。知道這個函數是否是凸函數是很重要的。因為如果它是凸的，我們知道局部最小值也是全局最小值。如果A是正定的，則​​二次函數是凸的。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5922f677ff7a4829a50aa5d9ab89c56b><p class=pgc-img-caption></p></div><p>對於任何函數f，我們計算下面的Hessian矩陣。如果A是正定的，則​​相應的點是局部最小值。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e47a885905464820a28e34c4805a2d97><p class=pgc-img-caption></p></div><h1>協方差矩陣</h1><p>在機器學習中，我們非常有興趣找到特徵之間的相關性。下圖顯示了重量和高度之間的正相關關係。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/e4f0bb8d32dc472b8148bbe9a4687c6e><p class=pgc-img-caption></p></div><p>在機器學習中，我們用協方差矩陣Σ建模關係。</p><div class=pgc-img><img alt="機器學習與線性代數 - 特殊矩陣" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/ce566368de72452ebd6b73a7c6cd727b><p class=pgc-img-caption></p></div><p>協方差矩陣是半正定的。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>性代數</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5a7c0dad.html alt=深度學習中的線性代數 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/dfic-imagehandler/677561e3-e0ec-4693-bd88-0cd182b21a17 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5a7c0dad.html title=深度學習中的線性代數>深度學習中的線性代數</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>