<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu | 极客快訊</title><meta property="og:title" content="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/d9a5ff954d45460fa5bb43d259dd388e"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/cb9490d5.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/cb9490d5.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/cb9490d5.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/cb9490d5.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/cb9490d5.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/cb9490d5.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/cb9490d5.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/cb9490d5.html><meta property="article:published_time" content="2020-10-29T21:13:00+08:00"><meta property="article:modified_time" content="2020-10-29T21:13:00+08:00"><meta name=Keywords content><meta name=description content="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/cb9490d5.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><p><strong>一、支持向量機（SVM: support vector machine）</strong></p><p>support vector machine (SVM): a support vector machine is supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they’re able to categorize new text.</p><p>支持向量機是一種使用分類算法解決二分類問題的監督學習模型。</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d9a5ff954d45460fa5bb43d259dd388e></div><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9720d4607ce44300874672354a4f6cba></div><p>上圖中不同顏色的點表示不同的類型（class)，找到一個平面使得兩類support vector支持向量之間的距離(margin)最大。之間的距離通過計算後得</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0d23244c76404e97a9514781b17385b8></div><p>，分類問題就變成了一個求解</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d8b876faba264de7983633b503dd7778></div><p>最大值的過程，即求解</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f7d0a14f09dc4bedb36c954c6b18bdc9></div><p>的最小值的過程。具體理論計算網上找找會有很多寫得很全面的文章。理論我就不多贅述啦。下面還是直接來代碼比較實際一點。希望看完對大家有些幫助。</p><p>上面圖片中用直線就可以將這些點劃分兩個區域了，但在實際很多情況下，只是 線性劃分並不能將這兩個不同顏色的類別劃分開來。如下右圖，這種無法用線性劃分的，我們應該可以用一條封閉的曲線將兩個類別劃分開來。接下來本文會用代碼針對線性和非線性的情況分別進行劃分。</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f12f51aab80e4da7a3008e88a8c46d92></div><p><strong>二、支持向量機處理線性關係問題：‍</strong></p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c673ab8e8eb14fe9b6054109aaa5dbd1></div><p>關注公眾號查看，體驗更佳哦～</p><ul><li><br></li></ul><p>%matplotlib inline</p><p>import numpy as np</p><p>import matplotlib.pyplot as plt</p><p>from scipy import stats</p><p># use Seaborn plotting defaults</p><p>import seaborn as sns; sns.set()</p><p>from sklearn.datasets.samples_generator import make_blobs</p><p>X, y = make_blobs(n_samples=[10,20,30], centers=[[0,0],[1,1],[2,2]], random_state=0, cluster_std=0.30)</p><p>plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/fbd4b28e04b845fc89b98ef84ccd49ed></div><ul><li><br></li></ul><p>X, y = make_blobs(n_samples=50, centers=2,random_state=0, cluster_std=0.60)</p><p>plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/33632f7b9ff14ff5bb3c40f973b626f3></div><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/ca5603eaf0c84dc4a4a650cfbc783696></div><p><strong>sklearn.datasets.samples_generator.Make_blobs</strong></p><p>(n_sample=100,n_features=2,*,centers=None,cluster_std=1.0,random_state=None,renturn_centers=False,...)‍</p><p><strong>Make_blobs: </strong>generate isotropic Gaussian blobs for clustering。生成各向同性的高斯斑點用來進行聚類。</p><p><strong>N_samples</strong>: int,optional(default=2)</p><p>N_samples為數組時，則序列中的每個元素表示每個簇(cluster)的樣本數。此時對應的center裡面也需要寫出每個簇(cluster)中心點的座標。比如在N_sample=[10,20,30]的時候，表示有三個簇(cluster)，他們每個簇點的個數分別為10個，20個和30個，此時在對應的centers參數中就要設置這三個簇(cluster)的中心座標，這裡設置為centers=[[0,0],[1,1],[2,2]]，那麼三個中心點（x,y）對應的座標分別為（0，0），（1，1），（2，2），在上面的圖中分別用紅，橙，黃顏色的點來表示。</p><p>n_samples為整數時，表示一共有多少個散點，那麼這些點在所有的集群之間平均分佈，n_sample=50,而centers=2，那麼這50個點在兩個中心之間平均分佈，每個中心所在的cluster的點的個數為50/2=25。</p><p><strong>Centers</strong>: int or array of shape[n_center, n_features],optional</p><p>產生的中心點的數量，或者固定的中心點的位置座標。</p><p>如果N_samples是整數但是又沒有定義centers的個數，那麼自定義為3個centers。</p><p>如果n_samples是矩陣的形式，那麼centers必須為設置為空值（None)或者和n_sample長度一樣的矩陣。</p><p><strong>random_state</strong>: int, randomstate instance, default=None</p><p>這個參數在之前文章裡面講過的，要讓你的隨機的結果復現，就需要設置一個這個的參數。</p><p><strong>Return_centers</strong>:bool,optional(default=False)</p><p>該參數為True時，make_blobs的返回值中會返回每個集群的中心點。</p><p><strong>Cluster_std</strong>: floot or sequence of floats, optional(default=1.0)</p><p>The standard deviation of the clusters. 集群的標準差，標準差越小，數據越集中於自己的中心點。</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3883f14109824324bdae5dd8ed74ae4c></div><p><strong>Make_blobs 返回的值：</strong></p><p><strong>X</strong>: array of shape [n_samples, n_features]</p><p>The generated samples.產生的點的x,y座標。</p><p><strong>Y</strong>: array of shape[n_samples]</p><p>The integer labels for cluster membership of each sample.產生的每個點的標籤，紅顏色和黃顏色各一個標籤，對應的值非0即1。如果有多個標籤，假設有n個，那麼這個y值的取值為[0,1...n-1]。</p><p><strong>Centers </strong>: array , shape[n_centers,n_feature]</p><p>The centers of each cluster. Only returned if return_center=True。每個簇（cluster)中心點（centers)的座標，只有make_blobs裡面的return_centers參數設置為True時才會返回Centers的座標。Random_state取不同值時，中心點的座標也會隨著不同，所以有時候為了讓結果復現，也就是產生的中心點的座標與上一次使用make_blobs function產生的一樣，這裡最好設置一下random_state。</p><ul><li><br></li></ul><p>from sklearn.datasets.samples_generator import make_blobs</p><p>X, y,C = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std = 0.30, return_centers =True)</p><p>plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dc2dd62e8ea0402e8f67d103c0b77756></div><p>在這個例子中要將這兩個cluster分開來，可以直接do by hand，</p><ul><li><br></li></ul><p>xfit = np.linspace(-1, 3.5)</p><p>plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')</p><p>plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)</p><p>for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:</p><p>plt.plot(xfit, m * xfit + b, '-k')</p><p>plt.xlim(-1, 3.5)</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/244efb84941e45dbae1066334b9e0b36></div><p><strong>Numpy.linspace</strong></p><p>(start,stop,num=50,endpoint=True,retstep=False,dtype=None,axis=0)‍</p><p><strong>numpy.linspace() </strong>返回區間在[start,stop]，間隔均勻的num個數據點，</p><p><strong>Start</strong>:array_like, the starting value of the sequence.區間的左端點。</p><p><strong>Stop</strong>: array_like the end value of the sequence.區間的右端點。</p><p><strong>Num</strong>: 在[start,stop]區間內產生的點（sample)的個數，自定義為50個。</p><p>Matplotlib.pyplot.scatter 畫散點。</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/6e3d4eaa885040808ba63424065b9f36></div><p><strong>Matplotlib.pyplot.plot</strong>(scalex=True,scaley=Ture,**kwargs)‍</p><p><strong>Fmt</strong>=’[marker][line][color]’</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/708505ae452243789a1bb38a65215ab3></div><p>可以看到要將兩組數據用直線分開可以有很多不同的直線，上圖中畫了三條直線，均可以將這兩個(cluster)完全分開，這三條直線的（斜率，截距）分別為（1，0.65），（0.5，1.6）和（-0.2，2.9）這些直線的寬度都是零，我們可以給這些直線畫一個給定寬度的margin，使灰色區域的邊界正好接觸到最近的cluster中的點。</p><ul><li><br></li></ul><p>xfit = np.linspace(-1, 3.5)</p><p>plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')</p><p>for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:</p><p>yfit = m * xfit + b</p><p>plt.plot(xfit, yfit, '-k')</p><p>plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none', color='#AAAAAA',</p><p>alpha=0.4)</p><p>plt.xlim(-1, 3.5);</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/23c58659870c4a25a29eac2f74777ba8></div><p>那麼這三條直線中，哪條直線是最優的解呢？我們希望對於新加入的點，仍能能有較好的劃分能力，在三條直線中，斜率為1或-0.2的兩條直線容易把新加入的點劃分到另一方區域內。</p><p>比如對於圖中紅色的x,斜率為-0.2的那條直線容易把它劃分到黃顏色點的簇(cluster)中,而對於其他新加入的點，如果新加入的點的橫座標(x)超過三條直線交叉點的橫座標，此時很容易被斜率為1的直線將屬於紅色的簇（cluster)的點劃分到黃色的cluster中，由此看來，只有中間那條線具有很好的容錯能力，與它平行的兩條灰色區域邊界所圍成的區域比其他兩條直線擁有更大的margin。</p><p>在支持向量機(support vector machines)中，這條能夠使灰色區域擁有最大margin的直線（上圖可以看到最中間這條直線的灰色部分寬度最寬）就會被選擇成為我們的最優模型。由此，支持向量機（support vector machines)可以看作是這種最大化margin時的estimator(中文叫算子)的一個例子。</p><p>下面我們使用scikit-learn的support vector classifier（SVC） 在上面那些數據上來訓練一個SVM 模型。為了避免過於擬合或擬合不夠充分，會在求解</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/46dca08bb2c94d81b2467f8c2cf2c77d></div><p>最小值的過程種引入懲罰參數C。</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/31c7d215a28247e59d132eea8d16abd9></div><p>C>0稱為懲罰參數，C值越大，容許的誤差越小，C值過大時，容易出現過擬合，過小則出現欠擬合。在這個例子中，對於兩個簇(cluster)，可以明顯區分邊界時，我們要求這個邊界非常地有原則，涇渭分明，也就是邊界very hard，此時可以把C設置為一個非常大的值。</p><ul><li><br></li></ul><p>from sklearn.svm import SVC # "Support vector classifier"</p><p>model = SVC(kernel='linear', C=1E10)</p><p>model.fit(X, y)</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/95eb47c6f690408498c666775693a8a1></div><p><strong>sklearn.svm.SVC</strong></p><p>(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)‍</p><p><strong>SVC:</strong>support Vector Classification.</p><p><strong>C</strong>:float, default=1.0 C為剛剛提及的懲罰參數，這裡為滿足劃分地邊界非常清晰，所以給C取值為C=1E10。</p><p><strong>Kernel</strong>{'linear','poly','rbf','sigmoid','precomputed'},default='rbf'。</p><p>Specifies the kernel type to be used in the algorithm.</p><p>對於輸入空間中的非線性分類問題，可以通過非線性變換將它轉化為某個維特徵空間中的線性分類問題，在高維特徵空間中學習線性支持向量機。 由於在線性支持向量機學習的對偶問題裡，目標函數核分類決策函數都只涉及實例核實例之間的內積，所以不需要顯式地指定非線性變換，而是用<strong>核函數</strong>替換當中的內積。目前這裡還是線性劃分，所以kernel=’linear’,關於非線性的，下面待會兒會講到。</p><p>Kernel函數的選擇可以參考下面這個連接：</p><p>http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/</p><p><strong>Degree</strong>:int,default=3</p><p>這個參數僅適用於polynomial kernel多項式的維度，一次函數，二次函數，三次函數等，對於其他kernel，這個參數不適用。</p><p><strong>Coef0</strong>:float, default=0.0</p><p>kernel function 中的獨立項，僅對 kernel = 'poly'或 'sigmoid'時才有用。</p><p><strong>gamma{‘scale’, ‘auto’} or float, default=’scale’</strong></p><p>Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.</p><p>gamma這個參數是否具體存在要看你使用的是什麼樣的核函數（kernel）,在kernel參數的介紹中那個鏈接裡面可以發現有些kernel裡面是含有gamma這個參數的。</p><p>以Gaussian Kernel為例子做個簡短的說明：</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3654645b0b004b2496cffdc3ce9b1df5></div><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d2da392665b94a738a19c26657394d92></div><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e8f6c90fcd474d85b49d364f922a4691></div><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/48cb6c0f1bcc4c95a2e55be92e2b10b6></div><p>圖中</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cfb3efd4759e4d219896d624fda890ff></div><p>值越大，高斯分佈越扁平，對於所有的X更為雨露均沾，</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/33797db68d0f4505b7c1e5766660d302></div><p>（gamma）值越小，那麼這個模型它就不能提取到數據裡面更為複雜的部分，也是更具有特徵性的那些部分，因為向量機的影響範圍包含了整個訓練集。與此同時，對於單一的training example，它的影響範圍越廣，它的訓練和預測的速度也會慢下來。</p><p>如果gamma值非常大，支持向量的影響範圍的半徑（radius of the area of the influence)將會只包括支持向量本身，這時防止過擬合而設立的C值起到的作用就會變得很小。</p><p>Gamma 和C的取值也需要自己trade-off 一下，下圖為在不同的gamma和C取值下，模型對兩個cluster的分類情況。</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a5443c6721d846e9a8e44f495967e475></div><p>sklearn.svm.<strong>SVC</strong>(gamma=)裡面，如果gamma='scale'(default)，那麼在計算時，gamma的值為1/（n_features*X.var())。如果 gamma='auto'，計算時gamma的值為1/n_features。</p><p>為了更好地可視化，下面定義一種快速的function用來畫SVM的決策邊界（decision boundaries）.</p><ul><li><br></li></ul><p>def plot_svc_decision_function(model, ax=None, plot_support=True):</p><p>'''Plot the decision function for a two-dimensional SVC'''</p><p>if ax is None:</p><p>ax = plt.gca()</p><p>xlim = ax.get_xlim()</p><p>ylim = ax.get_ylim()</p><p># create grid to evaluate model</p><p>x = np.linspace(xlim[0], xlim[1], 30)</p><p>y = np.linspace(ylim[0], ylim[1], 30)</p><p>Y, X = np.meshgrid(y, x)</p><p>xy = np.vstack([X.ravel(), Y.ravel()]).T</p><p>P = model.decision_function(xy).reshape(X.shape)</p><p># plot decision boundary and margins</p><p>ax.contour(X, Y, P, colors='k',</p><p>levels=[-1, 0, 1], alpha=0.5,</p><p>linestyles=['--', '-', '--'])</p><p># plot support vectors</p><p>if plot_support:</p><p>ax.scatter(model.support_vectors_[:, 0],</p><p>model.support_vectors_[:, 1],</p><p>s=300, linewidth=1, facecolors='none');</p><p>ax.set_xlim(xlim)</p><p>ax.set_ylim(ylim)</p><p>Matplotlib.pyplot.gca: get the current Axes instance on the current figure matching the given keyword arguments, or create one。</p><ul><li><br></li></ul><p>plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')</p><p>plot_svc_decision_function(model)</p><p>畫出如下圖：</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/908de0a0223c4f908d99f6c829974963></div><p>上圖可以看出得到的分割線使得這兩群點中間的margin最大。有些點正好在直線上面。在Scikit-Learn中，那些正好在邊界的點組成了我們的support vector，並且存儲在support_vectors_attribute中：</p><ul><li><br></li></ul><p>model.support_vectors_</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/4e36e3d7686848808c0d13fdbaea1321></div><p>對於非線性關係，可以通過定義多項式和高斯基本函數，將數據投影到高維度空間，由此可以使用線性classifier擬合非線性關係。</p><p><strong>三、支持向量機處理非線性關係問題</strong></p><ul><li><br></li></ul><p>from sklearn.datasets.samples_generator import make_circles</p><p>X, y = make_circles(100, factor=.1, noise=.1)</p><p>clf = SVC(kernel='linear').fit(X, y)</p><p>plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')</p><p>plot_svc_decision_function(clf, plot_support=False)</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/204b12f2acb04c5ead48f099c3765697></div><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8d0831dfe82f4a38b0b7df3eac12f0f2></div><p><strong>Sklearn.datasets.make_circles</strong></p><p>(n_sample=100,*,shuffle=True,noise=None,random_state=None,factor=0.8)‍</p><p><strong>Make_circles</strong>用來在二維平面上產生一個大的circle，該circle裡面包含一個小一點的circle.</p><p><strong>N_samples</strong>: 如果為偶數，外圈和內圈點的數量一樣多；如果為奇數，內圈比外圈多一個點。</p><p><strong>Factor</strong>:0&lt;double&lt;1(default=.8)</p><p>Scale factor between inner and outer circle.</p><p>在其他情況不變時，Factor 設為不同值的圖如下：</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b172b94d731347e4a34ba3d3598cb837></div><p><strong>Noise</strong>: double or None (default=None)</p><p>Standard deviation of Gaussian noise added to the data.</p><p>在其他參數不變的情況下，noise設為不同值時的圖像如下：</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c4987536e8a54090b2313fe260dbd5bd></div><p>由上面一系列圖片可以看到，使用線性（直線）去劃分非線性（中間一個圈，外面一個圈）的數據得到的結果並不理想，在二維平面上使用線性的可能永遠無法將這兩類點分開，那我們如何將這些點投影到更高維度才足以使用線性分開呢。舉個簡單的投影例子，我們可以使用計算一個集中在中間的 radial basis function。</p><blockquote><p>radial basis function: a radial basis function(RBF) is a real-valued function whose value depends only on the distance between the input and some fixed point, either the origin. --wiki</p></blockquote><p>對於在有限維度向量空間中線性不可分的樣本，我們將其映射到更高維度的向量空間裡，再通過間隔最大化的方式，學習得到支持向量機，就是非線性 SVM。</p><p>我們用 x 表示原來的樣本點，用</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/77a1c27a2d0548bf889082715fc14f3b></div><p>表示 x 映射到特徵新的特徵空間後到新向量。那麼分割超平面可以表示為：</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9daa3bde79e04381ae3e033a13a8de92></div><p>。</p><p>對於非線性 SVM 的對偶問題就變成了，在三維空間，僅使用一個平面就能將其很好地分類：</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/66ad69c0903045768aa363b63390e1b6></div><ul><li><br></li></ul><p>r = np.exp(-(X ** 2).sum(1))</p><p>然後你可以使用下面的代碼查看：</p><ul><li><br></li></ul><p>from mpl_toolkits import mplot3d</p><p>def plot_3D(elev=30, azim=30, X=X, y=y):</p><p>ax = plt.subplot(projection='3d')</p><p>ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')</p><p>ax.view_init(elev=elev, azim=azim)</p><p>ax.set_xlabel('x')</p><p>ax.set_ylabel('y')</p><p>ax.set_zlabel('r')</p><p>interact(plot_3D, elev=[-90,30, 90], azip=(-180, 180), X=fixed(X), y=fixed(y));</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6d5f5d3c5b8b47dd9cd7d6b68dba5b8e></div><p>也可以自己計算一個z座標，然後畫一個三維的，比如剛剛講的把上面的二維散點投影到三維，我們可以給一個z軸，使這些散點的z軸的值與二維圖像上的散點到原座標（0，0）的距離呈正關係，這裡我們定義：</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/37daa440b8a347d9a5027ea4d99f1d11></div><p>X為上面那些散點的x,y座標，下面代碼使用上式生成z座標。</p><ul><li><br></li></ul><p>import pandas as pd</p><p>df=pd.DataFrame(data=X,columns={'x','y'})</p><p>df['z']=(df['x']**2+df['y']**2)**0.5</p><p>然後使用plotly畫三維圖像（記得先在anaconda prompt裡面安裝：pip install plotly）：</p><ul><li><br></li></ul><p>import plotly</p><p>import plotly.graph_objs as go</p><p>plotly.offline.init_notebook_mode()</p><p>trace=[go.Scatter3d(x=df['x'].tolist(),y=df['y'].tolist(),z=df['z'].tolist(),</p><p>mode='markers',marker=dict(size=6,color=y,colorscale='Viridis'))]</p><p>fig=go.Figure(data=trace)</p><p>fig.show()</p><p>color=y中y就是之前畫的100個二維散點的時候給他們的分類標籤，0或1值，這裡對不同的類用不同的顏色區別。那麼這個從x,y得到z的過程就是一個核函數的功能。</p><p>錄了一個小小的video，小夥伴們感受一下將二維投影到三維後的效果：</p><p>上圖可以看到投影之後我們很好地將這兩類不同顏色地點分開來，只需要在中間插入一個平面（比如說z=0.5）就可以很easy地將兩類分開。</p><p>如果我們這個投影project的basic function沒有選擇好，那麼這個分離的效果可能就不那麼明顯了。</p><p>比如下面這種情況就需要用polynomial kernel來mapping（映射）</p><p>插入一個video：</p><p>每對點的前後轉換過程叫做內核轉換。</p><p>這種策略存在潛在的問題，投影N個點到N維，會隨著N的增大使得運算量相當龐大，有一種過程可以替代掉這種一對一的映射，把它叫做kernel trick, 比如上面講的</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e3ccfcb04105491db1718d2ed350eccf></div><p>公式，SVM裡面內置了這樣的kernel trick, 這也是SVM強有力的原因之一。</p><p>下面看看如何使用SVM的內置kernel trick，其中kernel='rbf'。</p><ul><li><br></li></ul><p>clf = SVC(kernel='rbf', C=1E6)</p><p>clf.fit(X, y)</p><ul><li><br></li></ul><p>plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')</p><p>plot_svc_decision_function(clf)</p><p>plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],</p><p>s=300, lw=1, facecolors='none')</p><p>Kernel='rbf'中的rbf：radial basic function，使用方式和其他的model一樣，大致形式都是model.fit(X,y)。</p><p>SVC.fit(self,X,y[,sample_weight]): fit the SVM model according to the given training data.</p><p>通過使用這種kernelized 的SVM模型，學習到了非線性決策邊界，得到的下圖很完美地區分了這兩組非線性關係的點。</p><p><strong></strong></p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b8a66fd31147458c81b915f4aa124fd8></div><p><strong>四、關於軟邊界</strong></p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b80c293d5f084079b4ec674f1783d3af></div><p>左圖為硬邊界，右圖為軟邊界，軟就是模糊，不清楚的意思。硬邊界有明顯的界限，而軟邊界裡面，就會有部分互相參雜，那麼互相參雜多少，這個容錯的範圍多大，也是由參數來確定的。</p><p>剛剛提及的都是邊界很清晰的，可以將兩類劃分，有時候，這兩種不同顏色的散點的邊界並沒有那麼清晰，允許一部分點互相摻雜。比如當你碰到下面的這種情況時：</p><ul><li><br></li></ul><p>X, y = make_blobs(n_samples=100, centers=2,random_state=0, cluster_std=1.2)</p><p>plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/49394ba7bd9244a597a00e7b8cd4ef84></div><p>通過mak_blobs人為產生了如上圖的數據，下面用SVC線性劃分這兩個類別，因為邊界不是很清晰，所以這裡控制懲罰程度的C的值，不會像最初那樣設置得很大，而這個容錯的範圍也由C的值確定，下面是SVM裡面C分別為10和0.1的結果的圖像：</p><ul><li><br></li></ul><p>X, y = make_blobs(n_samples=100, centers=2,random_state=0, cluster_std=0.8)</p><p>fig, ax = plt.subplots(1, 2, figsize=(16, 6))</p><p>fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)</p><p>for axi, C in zip(ax, [10.0, 0.1]):</p><p>model = SVC(kernel='linear', C=C).fit(X, y)</p><p>axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')</p><p>plot_svc_decision_function(model, axi)</p><p>axi.scatter(model.support_vectors_[:, 0],</p><p>model.support_vectors_[:, 1],</p><p>s=300, lw=1, facecolors='none');</p><p>axi.set_title('C = {0:.1f}'.format(C), size=14)</p><div class=pgc-img><img alt="支持向量機SVM(Support Vector Machine) Ⅰ原創 Yu" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d29a47448b8c4c7caaca569b9d12949f></div><p>C=0.1有部分點進入了兩個cluster的support vector之間，而當C=10的時候沒有。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>SVM</a></li><li><a>Support</a></li><li><a>Vector</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E5%AD%B8/dba9bec7.html alt="支持向量機(Support Vector Machine, SVM)（一）" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1521427148259f393e175fb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/dba9bec7.html title="支持向量機(Support Vector Machine, SVM)（一）">支持向量機(Support Vector Machine, SVM)（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/9a564a99.html alt="從零推導支持向量機 (SVM)" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RHOqAwPHvZFTTi style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/9a564a99.html title="從零推導支持向量機 (SVM)">從零推導支持向量機 (SVM)</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/f7769191.html alt=什麼是支持向量機（SVM）？-Python class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/64c948f7266c4ee4a77f9cb634eb3274 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/f7769191.html title=什麼是支持向量機（SVM）？-Python>什麼是支持向量機（SVM）？-Python</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/c86d7e5d.html alt=支持向量機（SVM）小結 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/34341bbffe91417b9f732c28799b78ed style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/c86d7e5d.html title=支持向量機（SVM）小結>支持向量機（SVM）小結</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/2a085964.html alt=機器學習：支持向量機（SVM） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1530360893829fb265274ef style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/2a085964.html title=機器學習：支持向量機（SVM）>機器學習：支持向量機（SVM）</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/3cf7440b.html alt=理解SVM支持向量機 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3d1bb058669241a1b9254be04aac6818 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/3cf7440b.html title=理解SVM支持向量機>理解SVM支持向量機</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/dd29e0eb.html alt=理解支持向量機（SVM） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1540115493549aa21a19958 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/dd29e0eb.html title=理解支持向量機（SVM）>理解支持向量機（SVM）</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/61ae03dd.html alt=面試必備：支持向量機（SVM）重要知識點總結 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/61ae03dd.html title=面試必備：支持向量機（SVM）重要知識點總結>面試必備：支持向量機（SVM）重要知識點總結</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b1ba3caf.html alt=教你學Python30-支持向量機SVM基礎 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/07db0cc84ff24bd5ae7a4632bd7f97b3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b1ba3caf.html title=教你學Python30-支持向量機SVM基礎>教你學Python30-支持向量機SVM基礎</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9221757a.html alt="Android矢量圖Vector Drawable使用及適配" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9221757a.html title="Android矢量圖Vector Drawable使用及適配">Android矢量圖Vector Drawable使用及適配</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/a9a3629.html alt=支持向量機(SVM)的約束和無約束優化、理論和實現 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1d9cd81ef1964685bc9d6f6ef3eba7ee style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/a9a3629.html title=支持向量機(SVM)的約束和無約束優化、理論和實現>支持向量機(SVM)的約束和無約束優化、理論和實現</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b1d127a.html alt="通俗易懂 | SVM之拉格朗日乘子法" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/efeb67c543e14721883f83826c9adbd9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b1d127a.html title="通俗易懂 | SVM之拉格朗日乘子法">通俗易懂 | SVM之拉格朗日乘子法</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>