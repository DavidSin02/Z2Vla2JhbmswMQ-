<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>最詳細的迴歸算法介紹，一遍就能看懂 | 极客快訊</title><meta property="og:title" content="最詳細的迴歸算法介紹，一遍就能看懂 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/2e50a35979274f80963d07fca7d71962"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/ccc1469.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/ccc1469.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/ccc1469.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/ccc1469.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/ccc1469.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/ccc1469.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/ccc1469.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/ccc1469.html><meta property="article:published_time" content="2020-10-29T21:03:59+08:00"><meta property="article:modified_time" content="2020-10-29T21:03:59+08:00"><meta name=Keywords content><meta name=description content="最詳細的迴歸算法介紹，一遍就能看懂"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/ccc1469.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>最詳細的迴歸算法介紹，一遍就能看懂</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><h1 class=pgc-h-arrow-right><strong>Regression：Case Study</strong></h1><blockquote><p><strong>迴歸</strong>-案例研究</p></blockquote><h1 class=pgc-h-arrow-right><strong>問題的導入：預測寶可夢的CP值</strong></h1><p>Estimating the Combat Power(CP) of a pokemon after evolution</p><p>我們期望根據已有的寶可夢進化前後的信息，來預測某隻寶可夢進化後的cp值的大小</p><h1 class=pgc-h-arrow-right><strong>確定Senario、Task和Model</strong></h1><h1 class=pgc-h-arrow-right><strong>Senario</strong></h1><p>首先根據已有的data來確定Senario，我們擁有寶可夢進化前後cp值的這樣一筆數據，input是進化前的寶可夢(包括它的各種屬性)，output是進化後的寶可夢的cp值；因此我們的data是labeled，使用的Senario是<strong>Supervised Learning</strong></p><h1 class=pgc-h-arrow-right><strong>Task</strong></h1><p>然後根據我們想要function的輸出類型來確定Task，我們預期得到的是寶可夢進化後的cp值，是一個scalar，因此使用的Task是<strong>Regression</strong></p><h1 class=pgc-h-arrow-right><strong>Model</strong></h1><p>關於Model，選擇很多，這裡採用的是<strong>Non-linear Model</strong></p><h1 class=pgc-h-arrow-right><strong>設定具體參數</strong></h1><p>： 表示一隻寶可夢，用下標表示該寶可夢的某種屬性</p><p>：表示該寶可夢進化前的cp值</p><p>： 表示該寶可夢是屬於哪一種物種，比如妙瓜種子、皮卡丘...</p><p>：表示該寶可夢的hp值即生命值是多少</p><p>： 代表該寶可夢的重重量</p><p>： 代表該寶可夢的高度</p><p>： 表示我們要找的function</p><p>： 表示function的output，即寶可夢進化後的cp值，是一個scalar</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2e50a35979274f80963d07fca7d71962><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>Regression的具體過程</strong></h1><h1 class=pgc-h-arrow-right><strong>回顧一下machine Learning的三個步驟：</strong></h1><ul><li>定義一個model即function set</li><li>定義一個goodness of function損失函數去評估該function的好壞</li><li>找一個最好的function</li></ul><h1 class=pgc-h-arrow-right><strong>Step1：Model (function set)</strong></h1><p>如何選擇一個function的模型呢？畢竟只有確定了模型才能調參。這裡沒有明確的思路，只能憑經驗去一種種地試</p><h1 class=pgc-h-arrow-right><strong>Linear Model 線性模型</strong></h1><p><br></p><p>y代表進化後的cp值，代表進化前的cp值，w和b代表未知參數，可以是任何數值</p><p>根據不同的w和b，可以確定不同的無窮無盡的function，而這個抽象出來的式子就叫做model，是以上這些具體化的function的集合，即function set</p><p>實際上這是一種<strong>Linear Model</strong>，但只考慮了寶可夢進化前的cp值，因而我們可以將其擴展為：</p><p>====</p><p><strong>x~i~</strong>： an attribute of input X ( x~i~ is also called <strong>feature</strong>，即特徵值)</p><p><strong>w~i~</strong>：weight of x~i~</p><p><strong>b</strong>： bias</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/0be689ebf4bd4d768b704a55184b57ce><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>Step2：Goodness of Function</strong></h1><h1 class=pgc-h-arrow-right><strong>參數說明</strong></h1><p>：用上標來表示一個完整的object的編號，表示第i只寶可夢(下標表示該object中的component)</p><p>：用表示一個實際觀察到的object輸出，上標為i表示是第i個object</p><p>注：由於regression的輸出值是scalar，因此裡面並沒有component，只是一個簡單的數值；但是未來如果考慮structured Learning的時候，我們output的object可能是有structured的，所以我們還是會需要用上標下標來表示一個完整的output的object和它包含的component</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a48823e17c9d4d54a5559d525e74fb5e><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>Loss function 損失函數</strong></h1><p>為了衡量function set中的某個function的好壞，我們需要一個評估函數，即==Loss function==，損失函數，簡稱L；loss function是一個function的function</p><p>input：a function；</p><p>output：how bad/good it is</p><p>由於，即f是由b和w決定的，因此input f就等價於input這個f裡的b和w，因此==Loss function實際上是在衡量一組參數的好壞==</p><p>之前提到的model是由我們自主選擇的，這裡的loss function也是，最常用的方法就是採用類似於方差和的形式來衡量參數的好壞，即預測值與真值差的平方和；這裡真正的數值減估測數值的平方，叫做估測誤差，Estimation error，將10個估測誤差合起來就是loss function</p><p>如果越大，說明該function表現得越不好；越小，說明該function表現得越好</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/39db633b8d734ac59552210ae969cb86><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>Loss function可視化</strong></h1><p>下圖中是loss function的可視化，該圖中的每一個點都代表一組(w,b)，也就是對應著一個function；而該點的顏色對應著的loss function的結果L(w,b)，它表示該點對應function的表現有多糟糕，顏色越偏紅色代表Loss的數值越大，這個function的表現越不好，越偏藍色代表Loss的數值越小，這個function的表現越好</p><p>比如圖中用紅色箭頭標註的點就代表了b=-180 , w=-2對應的function，即，該點所在的顏色偏向於紅色區域，因此這個function的loss比較大，表現並不好</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/dab907ece660492dae7bc120fc4e4d55><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>Step3：Pick the Best Function</strong></h1><p>我們已經確定了loss function，他可以衡量我們的model裡面每一個function的好壞，接下來我們要做的事情就是，從這個function set裡面，挑選一個最好的function</p><p>挑選最好的function這一件事情，寫成formulation/equation的樣子如下：</p><p>，或者是</p><p>也就是那個使最小的或，就是我們要找的或(有點像極大似然估計的思想)</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d935e98f61aa461da623227a35d78c96><p class=pgc-img-caption></p></div><p>利用線性代數的知識，可以解得這個closed-form solution，但這裡採用的是一種更為普遍的方法——==gradient descent(梯度下降法)==</p><h1 class=pgc-h-arrow-right><strong>Gradient Descent 梯度下降</strong></h1><p>上面的例子比較簡單，用線性代數的知識就可以解；但是對於更普遍的問題來說，gradient descent的厲害之處在於，只要是可微分的，gradient descent都可以拿來處理這個，找到表現比較好的parameters</p><h1 class=pgc-h-arrow-right><strong>單個參數的問題</strong></h1><p>以只帶單個參數w的Loss Function L(w)為例，首先保證是<strong>可微</strong>的</p><p>我們的目標就是找到這個使Loss最小的，實際上就是尋找切線L斜率為0的global minima最小值點(注意，存在一些local minima極小值點，其斜率也是0) 有一個暴力的方法是，窮舉所有的w值，去找到使loss最小的，但是這樣做是沒有效率的；而gradient descent就是用來解決這個效率問題的 * 首先隨機選取一個初始的點 (當然也不一定要隨機選取，如果有辦法可以得到比較接近的表現得比較好的當初始點，可以有效地提高查找的效率) * 計算在的位置的微分，即，幾何意義就是切線的斜率 * 如果切線斜率是negative負的，那麼就應該使w變大，即往右踏一步；如果切線斜率是positive正的，那麼就應該使w變小，即往左踏一步，每一步的步長step size就是w的改變量 w的改變量step size的大小取決於兩件事 * 一是現在的微分值有多大，微分值越大代表現在在一個越陡峭的地方，那它要移動的距離就越大，反之就越小； * 二是一個常數項η，被稱為==learning rate==，即學習率，它決定了每次踏出的step size不只取決於現在的斜率，還取決於一個事先就定好的數值，如果learning rate比較大，那每踏出一步的時候，參數w更新的幅度就比較大，反之參數更新的幅度就比較小 如果learning rate設置的大一些，那機器學習的速度就會比較快；但是learning rate如果太大，可能就會跳過最合適的global minima的點 * 因此每次參數更新的大小是 η，為了滿足斜率為負時w變大，斜率為正時w變小，應當使原來的w減去更新的數值，即 ηηηη</p><p>此時對應的斜率為0，我們找到了一個極小值local minima，這就出現了一個問題，當微分為0的時候，參數就會一直卡在這個點上沒有辦法再更新了，因此通過gradient descent找出來的solution其實並不是最佳解global minima</p><p>但幸運的是，在linear regression上，是沒有local minima的，因此可以使用這個方法</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a9544f6007e842268d07beb6a7830616><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>兩個參數的問題</strong></h1><p>今天要解決的關於寶可夢的問題，是含有two parameters的問題，即</p><p>當然，它本質上處理單個參數的問題是一樣的</p><ul><li>首先，也是隨機選取兩個初始值，和</li><li>然後分別計算這個點上，L對w和b的偏微分，即 和</li><li>更新參數，當迭代跳出時，對應著極小值點 ηηηηηη</li></ul><p>實際上，L 的gradient就是微積分中的那個梯度的概念，即</p><p>可視化效果如下：(三維座標顯示在二維圖像中，loss的值用顏色來表示)</p><p>橫座標是b，縱座標是w，顏色代表loss的值，越偏藍色表示loss越小，越偏紅色表示loss越大</p><p><strong>每次計算得到的梯度gradient，即由和組成的vector向量，就是該等高線的法線方向(對應圖中紅色箭頭的方向)；而ηη的作用就是讓原先的朝著gradient的方向即等高線法線方向前進，其中η(learning rate)的作用是每次更新的跨度(對應圖中紅色箭頭的長度)；經過多次迭代，最終gradient達到極小值點</strong></p><p>注：這裡兩個方向的η(learning rate)必須保持一致，這樣每次更新座標的step size是等比例縮放的，保證座標前進的方向始終和梯度下降的方向一致；否則座標前進的方向將會發生偏移</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/16fae44b0b8b48aba2e229c199502ef1><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>Gradient Descent的缺點</strong></h1><p>gradient descent有一個令人擔心的地方，也就是我之前一直提到的，它每次迭代完畢，尋找到的梯度為0的點必然是極小值點，local minima；卻不一定是最小值點，global minima</p><p>這會造成一個問題是說，如果loss function長得比較坑坑窪窪(極小值點比較多)，而每次初始化的取值又是隨機的，這會造成每次gradient descent停下來的位置都可能是不同的極小值點；而且當遇到梯度比較平緩(gradient≈0)的時候，gradient descent也可能會效率低下甚至可能會stuck卡住；也就是說通過這個方法得到的結果，是看人品的(滑稽</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/48f6de48cfd1404982c0a3aedf6ed257><p class=pgc-img-caption></p></div><p>但是！在==linear regression==裡，loss function實際上是<strong>convex</strong>的，是一個<strong>凸函數</strong>，是沒有local optimal局部最優解的，他只有一個global minima，visualize出來的圖像就是從裡到外一圈一圈包圍起來的橢圓形的等高線(就像前面的等高線圖)，因此隨便選一個起始點，根據gradient descent最終找出來的，都會是同一組參數</p><h1 class=pgc-h-arrow-right><strong>回到pokemon的問題上來</strong></h1><h1 class=pgc-h-arrow-right><strong>偏微分的計算</strong></h1><p>現在我們來求具體的L對w和b的偏微分</p><h1 class=pgc-h-arrow-right><strong>How's the results?</strong></h1><p>根據gradient descent，我們得到的中最好的參數是b=-188.4, w=2.7</p><p>我們需要有一套評估系統來評價我們得到的最後這個function和實際值的誤差error的大小；這裡我們將training data裡每一隻寶可夢 進化後的實際cp值與預測值之差的絕對值叫做，而這些誤差之和Average Error on Training Data為</p><blockquote><p>What we really care about is the error on new data (testing data)</p></blockquote><p>當然我們真正關心的是generalization的case，也就是用這個model去估測新抓到的pokemon，誤差會有多少，這也就是所謂的testing data的誤差；於是又抓了10只新的pokemon，算出來的Average Error on Testing Data為；可見training data裡得到的誤差一般是要比testing data要小，這也符合常識</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5ea0648401af4498b73b8c09344b6287><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>How can we do better?</strong></h1><p>我們有沒有辦法做得更好呢？這時就需要我們重新去設計model；如果仔細觀察一下上圖的data，就會發現在原先的cp值比較大和比較小的地方，預測值是相當不準的</p><p>實際上，從結果來看，最終的function可能不是一條直線，可能是稍微更復雜一點的曲線</p><h1 class=pgc-h-arrow-right><strong>考慮的model</strong></h1><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/894a1ec831d247eab2e0d9061b0e3481><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>考慮的model</strong></h1><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/92431444e70d4738bc2a2a0b65f748c7><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>考慮的model</strong></h1><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/51cfd92268c24aeaa61ee3fc69d87452><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>考慮的model</strong></h1><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f87980536ab64366b8e86ce510de8e92><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>5個model的對比</strong></h1><p>這5個model的training data的表現：隨著的高次項的增加，對應的average error會不斷地減小；實際上這件事情非常容易解釋，實際上低次的式子是高次的式子的特殊情況(令高次項對應的為0，高次式就轉化成低次式)</p><p>也就是說，在gradient descent可以找到best function的前提下(多次式為Non-linear model，存在local optimal局部最優解，gradient descent不一定能找到global minima)，function所包含的項的次數越高，越複雜，error在training data上的表現就會越來越小；但是，我們關心的不是model在training data上的error表現，而是model在testing data上的error表現</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a61e15e9166f4c3f97d7a99251500e55><p class=pgc-img-caption></p></div><p>在training data上，model越複雜，error就會越低；但是在testing data上，model複雜到一定程度之後，error非但不會減小，反而會暴增，在該例中，從含有項的model開始往後的model，testing data上的error出現了大幅增長的現象，通常被稱為<strong>overfitting過擬合</strong></p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/db973842ede74a4090c02ada49ee0293><p class=pgc-img-caption></p></div><p>因此model不是越複雜越好，而是選擇一個最適合的model，在本例中，包含的式子是最適合的model</p><h1 class=pgc-h-arrow-right><strong>進一步討論其他參數</strong></h1><h1 class=pgc-h-arrow-right><strong>物種的影響</strong></h1><p>之前我們的model只考慮了寶可夢進化前的cp值，這顯然是不對的，除了cp值外，還受到物種的影響</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/733204af7c8b486e86f4452feab38c1c><p class=pgc-img-caption></p></div><p>因此我們重新設計model：</p><p>也就是根據不同的物種，設計不同的linear model(這裡)，那如何將上面的四個if語句合併成一個linear model呢？</p><p>這裡引入δ條件表達式的概念，當條件表達式為true，則δ為1；當條件表達式為false，則δ為0，因此可以通過下圖的方式，將4個if語句轉化成同一個linear model</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3dd6a9867bba441fb1ebedd50bf20c4d><p class=pgc-img-caption></p></div><p>有了上面這個model以後，我們分別得到了在training data和testing data上測試的結果：</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5922564fd3904523bc2717d9b466a436><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>Hp值、height值、weight值的影響</strong></h1><p>考慮所有可能有影響的參數，設計出這個最複雜的model：</p><p>算出的training error=1.9，但是，testing error=102.3！<strong>這麼複雜的model很大概率會發生overfitting</strong>(按照我的理解，overfitting實際上是我們多使用了一些input的變量或是變量的高次項使曲線跟training data擬合的更好，但不幸的是這些項並不是實際情況下被使用的，於是這個model在testing data上會表現得很糟糕)，overfitting就相當於是那個範圍更大的韋恩圖，它包含了更多的函數更大的範圍，代價就是在準確度上表現得更糟糕</p><h1 class=pgc-h-arrow-right><strong>regularization解決overfitting(L2正則化解決過擬合問題)</strong></h1><blockquote><p>regularization可以使曲線變得更加smooth，training data上的error變大，但是 testing data上的error變小。有關regularization的具體原理說明詳見下一部分</p></blockquote><p>原來的loss function只考慮了prediction的error，即；而regularization則是在原來的loss function的基礎上加上了一項，就是把這個model裡面所有的的平方和用λ加權(其中i代表遍歷n個training data，j代表遍歷model的每一項)</p><p>也就是說，<strong>我們期待參數越小甚至接近於0的function，為什麼呢？</strong></p><p>因為參數值接近0的function，是比較平滑的；所謂的平滑的意思是，當今天的輸入有變化的時候，output對輸入的變化是比較不敏感的</p><p>舉例來說，對這個model，當input變化，output的變化就是，也就是說，如果越小越接近0的話，輸出對輸入就越不sensitive敏感，我們的function就是一個越平滑的function；說到這裡你會發現，我們之前沒有把bias——b這個參數考慮進去的原因是<strong>bias的大小跟function的平滑程度是沒有關係的</strong>，bias值的大小隻是把function上下移動而已</p><p><strong>那為什麼我們喜歡比較平滑的function呢？</strong></p><p>如果我們有一個比較平滑的function，由於輸出對輸入是不敏感的，測試的時候，一些noises噪聲對這個平滑的function的影響就會比較小，而給我們一個比較好的結果</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/50179a1b041c4acc95431b85814fd100><p class=pgc-img-caption></p></div><p><strong>注：這裡的λ需要我們手動去調整以取得最好的值</strong></p><p>λ值越大代表考慮smooth的那個regularization那一項的影響力越大，我們找到的function就越平滑</p><p>觀察下圖可知，當我們的λ越大的時候，在training data上得到的error其實是越大的，但是這件事情是非常合理的，因為當λ越大的時候，我們就越傾向於考慮w的值而越少考慮error的大小；但是有趣的是，雖然在training data上得到的error越大，但是在testing data上得到的error可能會是比較小的</p><p>下圖中，當λ從0到100變大的時候，training error不斷變大，testing error反而不斷變小；但是當λ太大的時候(>100)，在testing data上的error就會越來越大</p><p>==我們喜歡比較平滑的function，因為它對noise不那麼sensitive；但是我們又不喜歡太平滑的function，因為它就失去了對data擬合的能力；而function的平滑程度，就需要通過調整λ來決定==，就像下圖中，當λ=100時，在testing data上的error最小，因此我們選擇λ=100</p><p>注：這裡的error指的是</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d6b8c198833c42b8895e5c0f5c51507a><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>conclusion總結</strong></h1><h1 class=pgc-h-arrow-right><strong>關於pokemon的cp值預測的流程總結：</strong></h1><ul><li>根據已有的data特點(labeled data，包含寶可夢及進化後的cp值)，確定使用supervised learning監督學習</li><li>根據output的特點(輸出的是scalar數值)，確定使用regression迴歸(linear or non-linear)</li><li>考慮包括進化前cp值、species、hp等各方面變量屬性以及高次項的影響，我們的model可以採用這些input的一次項和二次型之和的形式，如： 而為了保證function的平滑性，loss function應使用regularization，即，注意bias——參數b對function平滑性無影響，因此不額外再次計入loss function(y的表達式裡已包含w、b)</li><li>利用gradient descent對regularization版本的loss function進行梯度下降迭代處理，每次迭代都減去L對該參數的微分與learning rate之積，假設所有參數合成一個vector：，那麼每次梯度下降的表達式如下： 梯度 當梯度穩定不變時，即為0時，gradient descent便停止，此時如果採用的model是linear的，那麼vector必然落於global minima處(凸函數)；如果採用的model是Non-linear的，vector可能會落於local minima處(此時需要採取其他辦法獲取最佳的function) 假定我們已經通過各種方法到達了global minima的地方，此時的vector：所確定的那個唯一的function就是在該λ下的最佳，即loss最小</li><li>這裡λ的最佳數值是需要通過我們不斷調整來獲取的，因此令λ等於0，10，100，1000，...不斷使用gradient descent或其他算法得到最佳的parameters：，並計算出這組參數確定的function——對training data和testing data上的error值，直到找到那個使testing data的error最小的λ，(這裡一開始λ=0，就是沒有使用regularization時的loss function) 注：引入評價的error機制，令error=，分別計算該對training data和testing data(more important)的大小 先設定λ->確定loss function->找到使loss最小的->確定function->計算error->重新設定新的λ重複上述步驟->使testing data上的error最小的λ所對應的所對應的function就是我們能夠找到的最佳的function</li></ul><h1 class=pgc-h-arrow-right><strong>本章節總結：</strong></h1><ul><li>Pokémon: Original CP and species almost decide the CP after evolution</li><li>There are probably other hidden factors</li><li>Gradient descent More theory and tips in the following lectures</li><li>Overfitting and Regularization</li><li>We finally get average error = 11.1 on the testing data</li><li>How about new data? Larger error? Lower error?(larger->need validation)</li><li>Next lecture: Where does the error come from? More theory about overfitting and regularizationThe concept of validation(用來解決new data的error高於11.1的問題)</li></ul><h1 class=pgc-h-arrow-right><strong>附：Regularization(L1 L2 正則化解決overfitting)</strong></h1><blockquote><p>Regularization -> redefine the loss function</p></blockquote><p>關於overfitting的問題，很大程度上是由於曲線為了更好地擬合training data的數據，而引入了更多的高次項，使得曲線更加“蜿蜒曲折”，反而導致了對testing data的誤差更大</p><p>回過頭來思考，我們之前衡量model中某個function的好壞所使用的loss function，僅引入了真實值和預測值差值的平方和這一個衡量標準；我們想要避免overfitting過擬合的問題，就要使得高次項對曲線形狀的影響儘可能小，因此我們要在loss function裡引入高次項(非線性部分)的衡量標準，也就是將高次項的係數也加權放進loss function中，這樣可以使得訓練出來的model既滿足預測值和真實值的誤差小，又滿足高次項的係數儘可能小而使曲線的形狀比較穩定集中</p><p>以下圖為例，如果loss function僅考慮了這一誤差衡量標準，那麼擬合出來的曲線就是紅色虛線部分(過擬合)，而過擬合就是所謂的model對training data過度自信, 非常完美的擬合上了這些數據, 如果具備過擬合的能力, 那麼這個方程就可能是一個比較複雜的非線性方程 , 正是因為這裡的和使得這條虛線能夠被彎來彎去, 所以整個模型就會特別努力地去學習作用在和上的c、d參數. <strong>但是在這個例子裡，我們期望模型要學到的卻是這條藍色的曲線. 因為它能更有效地概括數據</strong>.而且只需要一個就能表達出數據的規律.</p><p>或者是說, 藍色的線最開始時, 和紅色線同樣也有c、d兩個參數, 可是最終學出來時, c 和 d 都學成了0, 雖然藍色方程的誤差要比紅色大, 但是概括起數據來還是藍色好</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4d1e3ffb089849d3a7bb50b3c3ad3d08><p class=pgc-img-caption></p></div><p>這也是我們通常採用的方法，我們不可能一開始就否定高次項而直接只採用低次線性表達式的model，因為有時候真實數據的確是符合高次項非線性曲線的分佈的；而如果一開始直接採用高次非線性表達式的model，就很有可能造成overfitting，在曲線偏折的地方與真實數據的誤差非常大。我們的目標應該是這樣的：</p><p><strong>在無法確定真實數據分佈的情況下，我們儘可能去改變loss function的評價標準</strong></p><ul><li><strong>我們的model的表達式要儘可能的複雜，包含儘可能多的參數和儘可能多的高次非線性項；</strong></li><li><strong>但是我們的loss function又有能力去控制這條曲線的參數和形狀，使之不會出現overfitting過擬合的現象；</strong></li><li><strong>在真實數據滿足高次非線性曲線分佈的時候，loss function控制訓練出來的高次項的係數比較大，使得到的曲線比較彎折起伏；</strong></li><li><strong>在真實數據滿足低次線性分佈的時候，loss function控制訓練出來的高次項的係數比較小甚至等於0，使得到的曲線接近linear分佈</strong></li></ul><p>那我們如何保證能學出來這樣的參數呢? 這就是 L1 L2 正規化出現的原因.</p><p>之前的loss function僅考慮了這一誤差衡量標準，而<strong>L1 L2正規化</strong>就是在這個loss function的後面多加了一個東西，即model中跟高次項係數有關的表達式；</p><ul><li>L1正規化即加上λ這一項，loss function變成，即n個training data裡的數據的真實值與預測值差值的平方和加上λ權重下的model表達式中所有項係數的絕對值之和</li><li>L2正規化即加上這一項，loss function變成，即n個training data裡的數據的真實值與預測值差值的平方和加上λ權重下的model表達式中所有項係數的平方和</li></ul><p>相對來說，L2要更穩定一些，L1的結果則不那麼穩定，如果用p表示正規化程度，上面兩式可總結如下：</p><div class=pgc-img><img alt=最詳細的迴歸算法介紹，一遍就能看懂 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c01ed3db56324036a6acc29c32ea7c8e><p class=pgc-img-caption></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>詳細</a></li><li><a>介紹</a></li><li><a>能看懂</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/cacb18ce.html alt=一篇活性炭詳細的介紹，淨水器安裝必備！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/15257660505672449544e5d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cacb18ce.html title=一篇活性炭詳細的介紹，淨水器安裝必備！>一篇活性炭詳細的介紹，淨水器安裝必備！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4b82f3f.html alt=拉絲機詳細的介紹 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/fbc24fc5ae1444dd989e3f0ab153a16b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4b82f3f.html title=拉絲機詳細的介紹>拉絲機詳細的介紹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4adca96d.html alt=不鏽鋼旋轉樓梯—不鏽鋼旋轉樓梯的原理和尺寸介紹 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1532917894689e20edadba9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4adca96d.html title=不鏽鋼旋轉樓梯—不鏽鋼旋轉樓梯的原理和尺寸介紹>不鏽鋼旋轉樓梯—不鏽鋼旋轉樓梯的原理和尺寸介紹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b11a0727.html alt=今天介紹的電纜型號是：RVVP class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/db5ce8c1-b1ad-424b-a931-d371d5aad449 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b11a0727.html title=今天介紹的電纜型號是：RVVP>今天介紹的電纜型號是：RVVP</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cae5a51a.html alt=「施工技巧」詳細瞭解光纜、終端盒、尾纖的接法和光纖各種接口 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/decf8edb02a34404b92b86681378575f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cae5a51a.html title=「施工技巧」詳細瞭解光纜、終端盒、尾纖的接法和光纖各種接口>「施工技巧」詳細瞭解光纜、終端盒、尾纖的接法和光纖各種接口</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/47bcd6a6.html alt=詳細瞭解光纜、終端盒、尾纖的接法和光纖各種接口 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/dcdae811e636496d948cf5a745f470fc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/47bcd6a6.html title=詳細瞭解光纜、終端盒、尾纖的接法和光纖各種接口>詳細瞭解光纜、終端盒、尾纖的接法和光纖各種接口</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e145517c.html alt=詳細解析變壓器如何接地？接地的主要作用是什麼？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/dfic-imagehandler/ab7c98a3-daad-418d-8b72-e584bca53b42 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e145517c.html title=詳細解析變壓器如何接地？接地的主要作用是什麼？>詳細解析變壓器如何接地？接地的主要作用是什麼？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c36f4ae6.html alt=手推公式：LSTM單元梯度的詳細的數學推導 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/68dcd30ac0c0469a9bb85ea2bc9f3e8c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c36f4ae6.html title=手推公式：LSTM單元梯度的詳細的數學推導>手推公式：LSTM單元梯度的詳細的數學推導</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6c932f22.html alt=策略梯度的簡明介紹 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1527998215130569eb83799 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6c932f22.html title=策略梯度的簡明介紹>策略梯度的簡明介紹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c99d724e.html alt=給大家介紹幾種常見的齒輪，學機械的收藏了 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/dfic-imagehandler/b864f3ca-e8bf-47f4-a940-027e7a96e4a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c99d724e.html title=給大家介紹幾種常見的齒輪，學機械的收藏了>給大家介紹幾種常見的齒輪，學機械的收藏了</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ab2d9a76.html alt=齒輪的前世今生，你瞭解齒輪嗎，詳細講解齒輪乾貨知識大全 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/8efc9a2a67ca4b86816d4ccc86ebeddb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ab2d9a76.html title=齒輪的前世今生，你瞭解齒輪嗎，詳細講解齒輪乾貨知識大全>齒輪的前世今生，你瞭解齒輪嗎，詳細講解齒輪乾貨知識大全</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/37860fee.html alt=「小恩學堂」壁掛爐核心部件介紹第4期——循環水泵 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/5e7a000263651c843347 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/37860fee.html title=「小恩學堂」壁掛爐核心部件介紹第4期——循環水泵>「小恩學堂」壁掛爐核心部件介紹第4期——循環水泵</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a14a021d.html alt=火力發電廠主要設備及其作用介紹 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a14a021d.html title=火力發電廠主要設備及其作用介紹>火力發電廠主要設備及其作用介紹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/82ec221c.html alt="向三歲孩子介紹地球系列之一 - 天空，大地，水元素分類遊戲" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5e731406b21a45718433a22396b5282b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/82ec221c.html title="向三歲孩子介紹地球系列之一 - 天空，大地，水元素分類遊戲">向三歲孩子介紹地球系列之一 - 天空，大地，水元素分類遊戲</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/44c014a1.html alt=SpringBoot事務詳細簡介 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/44c014a1.html title=SpringBoot事務詳細簡介>SpringBoot事務詳細簡介</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>