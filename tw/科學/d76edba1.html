<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>Transformers是一種圖神經網絡 | 极客快訊</title><meta property="og:title" content="Transformers是一種圖神經網絡 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/6f96ef1cefa442138d3480deb3dff8f9"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/d76edba1.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/d76edba1.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/d76edba1.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/d76edba1.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/d76edba1.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/d76edba1.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/d76edba1.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/d76edba1.html><meta property="article:published_time" content="2020-11-14T20:53:56+08:00"><meta property="article:modified_time" content="2020-11-14T20:53:56+08:00"><meta name=Keywords content><meta name=description content="Transformers是一種圖神經網絡"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/d76edba1.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>Transformers是一種圖神經網絡</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><blockquote><p>作者：Chaitanya Joshi</p><p>編譯：ronghuaiyang</p></blockquote><h1 class=pgc-h-arrow-right>導讀</h1><blockquote><p>這個觀點的目的是構建Transformer結構背後的NLP上的直覺，以及與圖神經網絡的聯繫。</p></blockquote><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6f96ef1cefa442138d3480deb3dff8f9><p class=pgc-img-caption></p></div><p>工程師朋友經常問我：“圖深度學習”聽起來很厲害，但有什麼大的商業成功的故事嗎？它有沒有被部署在實際app中？</p><p>除了Pinterest、阿里巴巴和Twitter的推薦系統外，一個非常小的成功就是Transformer結構，這個結構帶來了NLP的風暴。</p><p>通過這篇文章，我想在Graph Neural Networks (GNNs)和transformer之間建立聯繫。我會討論NLP和GNN社區中，模型架構背後的直覺，使用方程和圖把這兩者建立聯繫，並討論如何把這兩個放到一起工作來得到進展。</p><p>讓我們先談談模型架構的目的 —— 表示學習。</p><h1 class=pgc-h-arrow-right>NLP的表示學習</h1><p>在較高的層次上，所有的神經網絡架構都將輸入數據構建為向量/嵌入的“表示”，它們編碼和數據有關的有用的統計和語義信息。這些潛在的或隱藏的表示可以用於執行一些有用的操作，比如對圖像進行分類或翻譯句子。神經網絡通過接收反饋(通常是通過誤差/損失函數)來“學習”，建立越來越好的表示。</p><p>對於自然語言處理(NLP)，通常，遞歸神經網絡(RNNs)以順序的方式構建句子中每個單詞的表示，即，一次一個單詞。直觀地說，我們可以把RNN層想象成一條傳送帶，上面的文字從左到右進行遞歸處理。最後，我們得到了句子中每個單詞的隱藏特徵，我們將其傳遞給下一個RNN層或用於我們的NLP任務。</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6272b282bab74f7ba70319caf4b01f87><p class=pgc-img-caption></p></div><p>RNNs以順序的方式構建句子中每個單詞的表示。一次一個單詞。另一方面，Transformers 使用一種注意機制來判斷句子中其他詞對上述詞的重要性。</p><p><strong>Transformers</strong>最初是為機器翻譯而引入的，現在已經逐漸取代了主流NLP中的RNNs。這個架構使用了一種全新的方式來進行表示學習：不需要進行遞歸，Transformers使用注意力機制構建每個單詞的特徵，找出句子中的其他詞對於前面之前的詞的重要程度。知道了這一點，單詞的特徵更新就是所有其他單詞特徵的線性變換的總和，並根據它們的重要性進行加權。</p><hr><h1 class=pgc-h-arrow-right>拆解Transformer</h1><p>讓我們通過將前一段翻譯成數學符號和向量的語言來發展關於架構的直覺。我們在句子S中從第l層到第l+1層更新第i個單詞的隱藏特徵h：</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f37520d34600443cbba08c62bf74f8b9><p class=pgc-img-caption></p></div><p>其中j∈S表示句子中的詞集，Q、K、V為可學習的線性權值(分別表示注意力計算的<strong>Q</strong>uery、<strong>K</strong>ey和<strong>V</strong>值)。對於Transformers，注意力機制是對句子中的每個單詞並行執行的，RNNs是一個詞一個詞的進行更新。</p><p>通過以下pipeline，我們可以更好地瞭解注意力機制：</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d03d9a56116d4f9eaa9ec4704f50a543><p class=pgc-img-caption></p></div><p>用這個詞的特徵h_i ^ℓ^和句子中的其他的詞的特徵h_j ^ℓ^，∀j∈S，我們通過點積為每一對(i, j)計算注意力權重w~ij~，然後對所有的j計算softmax。最後，對所有的h_j ^ℓ^進行加權求和，產生更新的詞的特徵h_i ^{ℓ+ 1}。句子中的每個詞都並行地經歷同一個pipeline，以更新其特徵。</p><h1 class=pgc-h-arrow-right>多頭注意力機制</h1><p>要讓這種點積注意力機制發揮作用是很棘手的，因為隨機初始化會破壞學習過程的穩定性。我們可以通過並行地執行多個“頭”的注意力，並連接結果來克服這個問題(現在每個頭都有獨立的可學習的權重)：</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7bb7d560217940269b43abdf952b2aff><p class=pgc-img-caption></p></div><p>其中，Q^k^，K^k^，V^k^是第k個注意力頭的可學習的權重，O是降維投影，為了匹配特徵的維度。</p><p>多個頭可以讓注意力機制從本質上“對衝賭注”，觀察前一層隱藏特徵的不同的轉換或不同的方面。我們稍後會詳細討論。</p><hr><h1 class=pgc-h-arrow-right>尺度問題以及前饋層</h1><p>Transformer架構的一個關鍵問題是，詞的特徵在注意力機制之後可能是不同尺度，(1)這是由於當對其他的單詞特徵進行求和的時候，這些單詞的權重可能有非常shape的分佈。(2)在個體的特徵向量層面上，拼接多個注意力頭可能輸出不同的尺度的值，導致最後的值具有很寬的動態範圍。照傳統的ML的經驗，在pipeline中添加一歸一化層似乎是合理的。</p><p>Transformer通過<strong>LayerNorm</strong>克服了問題，它在特徵級別歸一化和學習仿射變換。此外，通過特徵維度的平方根來縮放點積注意力有助於抵消問題(1)。</p><p>最後，作者提出了另一個控制尺度問題的“技巧”：<strong>一個position-wise的2層MLP</strong>。多頭的注意力之後，通過一個可學習的權重，他們把向量*h_i ^{ℓ+ 1}*投影到更高的維度上，然後通過ReLU再投影回原來的尺寸，再接另一個歸一化：</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/2a6b426c505f4e719fd8cbd8c14bafd7><p class=pgc-img-caption></p></div><blockquote><p>老實說，我不確定這個過於參數化的前饋子層背後的確切直覺是什麼，而且似乎也沒有人對它提出問題！我認為LayerNorm和縮放的點積並沒有完全解決這個問題，所以大的MLP是一種獨立地重新縮放特徵向量的hack。</p></blockquote><hr><p>Transformer層的最終看起來是這樣的：</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3e540d2ef999485db5d7e5bb120ca011><p class=pgc-img-caption></p></div><p>Transformer架構也非常適合深度網絡，使得NLP社區在模型參數和數據方面都能進行擴展。每個多頭注意子層和前饋子層的輸入和輸出之間的<strong>殘差連接</strong>是疊加Transformer層的關鍵(但為了清楚起見在圖中省略)。</p><h1 class=pgc-h-arrow-right>用GNNs構建圖的表示</h1><p>讓我們暫時離開NLP。</p><p>圖神經網絡(GNNs)或圖卷積網絡(GCNs)構建圖數據中的節點和邊的表示。它們通過<strong>鄰域聚合</strong>(或消息傳遞)來實現，其中每個節點從其鄰域收集特徵，以更新其周圍的局部的圖結構的表示。堆疊幾個GNN層使模型能夠在整個圖中傳播每個節點的特徵 —— 從它的鄰居傳播到鄰居的鄰居，等等。</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/411049b2e57a4b50b39c1f8f91661b7f><p class=pgc-img-caption></p></div><p>以這個表情社交網絡為例：GNN產生的節點特徵可以用於預測任務，如識別最有影響力的成員或提出潛在的聯繫</p><p>在其最基本的形式中，GNN在第ℓ層通過對節點自身的特徵和鄰居節點的特徵非線性變換的方式進行聚合，更新節點i的隱藏特徵h：</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/09cadc03ef5b417b8e31568c3a1ca3bd><p class=pgc-img-caption></p></div><p>其中U、V為GNN層的可學習權重矩陣，σ為ReLU等非線性變換。</p><p>鄰居節點的總和*j∈N(i)*可以被其他輸入大小不變的<strong>聚合函數</strong>代替，比如簡單的mean/max，或者更強大的函數，比如通過<strong>注意力機制</strong>的加權和。</p><p>聽起來耳熟嗎？</p><p>也許一個pipeline有助於建立聯繫：</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c790ded10b534432b84011b44117c7cd><p class=pgc-img-caption></p></div><blockquote><p>如果我們將多個並行的鄰域頭進行聚合，並將鄰域j的求和替換為注意力機制，即加權和，我們就得到<strong>圖注意力網絡</strong> (GAT)。加上歸一化和前饋MLP，看，我們得到一個<strong>圖Transformer</strong>！</p></blockquote><hr><h1 class=pgc-h-arrow-right>句子是完全聯通的詞圖</h1><p>為了使這種聯繫更加明確，可以把一個句子看作是一個完全連通的圖，其中每個單詞都與其他單詞相連。現在，我們可以使用GNN為圖(句子)中的每個節點(單詞)構建特徵，然後我們可以使用它執行NLP任務。</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c8bcde227d2a452386595c3f251f2023><p class=pgc-img-caption></p></div><p>廣義上說，這就是Transformers 正在做的事情：它們是帶有多頭注意力的GNN，作為鄰居的聚合函數。標準的GNNs從其局部鄰居節點j∈N(i)中聚合特徵，而NLP的Transformers將整個句子S作為局部鄰居，從每一層的每個單詞j∈S中聚合特徵。</p><p>重要的是，各種針對特定問題的技巧 —— 比如位置編碼、因果/隱藏聚合、學習率策略和預訓練 ——對Transformers 的成功至關重要，但很少在GNN社區中出現。同時，從GNN的角度來看Transformers可以讓我們擺脫架構中的許多花哨的東西。</p><h1 class=pgc-h-arrow-right>我們可以相互學到點什麼？</h1><p>現在我們已經在Transformers和GNN之間建立了聯繫，讓我來談談……</p><h1 class=pgc-h-arrow-right>全連通圖是NLP的最佳輸入格式嗎？</h1><p>在統計NLP和ML之前，像Noam Chomsky這樣的語言學家專注於發展語言結構的正式理論，比如<strong>語法樹/圖</strong>。Tree LSTMs已經嘗試過了，但是也許transformer/GNNs是更好的架構，可以讓語言理論和統計NLP的世界更靠近？</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/becd01dec9724f1d85309c4172d73968><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>如何學習長期的依賴？</h1><p>完全連通圖的另一個問題是，它們讓學習單詞之間的長期依賴關係變得很困難。這僅僅是因為圖中的邊的數量是如何隨著節點的數量以平方量級增長的。在一個有n個單詞的句子中，一個Transformer/GNN將對n^2^個單詞對進行計算。對於非常大的n，就沒辦法處理了。</p><p>NLP社區對長序列和依賴問題的看法很有趣：讓注意力力機制變得稀疏或者可以自適應輸入的大小，對每一層添加遞歸或壓縮，使用局部敏感哈希來獲得有效的注意力，都是改善Transformer的有前途的新想法。</p><p>看到來自GNN社區的想法加入其中將是很有趣的，例如使用劃分二部圖的方式用於句子<strong>圖稀疏化</strong>似乎是另一種令人興奮的方法。</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8009625d85e14e25b082ebfeb0ae0ca0><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>Transformers在學習“神經語法嗎” ？</h1><p>有幾個有趣的文章來自NLP社區，是有關Transformers可能正在學習的內容。它的基本前提是，對句子中的所有詞進行關注，以確定對哪些詞最感興趣，從而使“Transformers”能夠學習一些類似於特定任務語法的東西。在多頭注意力中，不同的頭也可能“看”不同的句法屬性。</p><p>用圖的術語來說，通過在全圖上使用GNN，我們能從GNN如何在每一層執行鄰居的聚合來恢復最重要的邊緣(以及它們可能包含的內容)嗎？對於這個觀點，我並不那麼信服。</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7205049f22594954835fa8cf10014632><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>為什麼是多頭注意力？為什麼是注意力？</h1><p>我更贊同多頭機制的優化視圖 —— 擁有多個注意力頭<strong>改進了學習</strong>並克服了<strong>錯誤的隨機初始化</strong>。例如，這些論文表明Transformer 頭可以在訓練被“修剪”或刪除，而不會對性能產生顯著影響。</p><p>多頭鄰居聚合機制在GNNs中也被證明是有效的，例如，GAT使用相同的多頭注意力和MoNet使用多個高斯核聚合特徵。雖然是為了穩定注意力機制而發明的，但是多頭機制會成為壓榨模型性能的標準操作嗎？</p><p>相反，具有簡單聚合函數(如sum或max)的GNN不需要多個聚合頭進行穩定的訓練。如果我們不需要計算句子中每個詞對之間的配對兼容性，對Transformers來說不是很好嗎？</p><p>Transformers能從完全擺脫注意力中獲益嗎？Yann Dauphin和合作者的最近工作提出了一種替代的ConvNet的架構。Transformers也可能最終會做一些類似的事情。</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/213cc118b1864128b724e672acc413e3><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>為什麼訓練Transformers 這麼難？</h1><p>閱讀Transformer的新論文讓我覺得，在確定最佳學習率策略、熱身策略和衰變設置時，訓練這些模型需要類似於“黑魔法”的東西。這可能只是因為模型太大了，而NLP的研究任務太具有挑戰性了。</p><p>最近的研究結果認為，也可能是因為歸一化的具體排列和架構內的殘差連接的原因。</p><div class=pgc-img><img alt=Transformers是一種圖神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/aa364a35d5aa458cbd8b6981069b8475><p class=pgc-img-caption></p></div><p>在這一點上，我很憤怒，但這讓我懷疑：我們真的需要多個頭的昂貴的兩兩的注意力嗎，過分參數化的MLP層，和複雜的學習率策略嗎？</p><p>我們真的需要擁有這麼大的模型嗎？</p><p>對於手頭的任務來說，具有良好的歸納偏差的架構不應該更容易訓練嗎？</p><h1 class=pgc-h-arrow-right>進一步的閱讀</h1><p>這個博客並不是第一個將GNNs和Transformers聯繫起來的博客：以下是Arthur Szlam關於注意力/記憶網絡、GNNs和Transformers之間的歷史和聯繫的精彩演講：https://ipam.wistia.com/medias/1zgl4lq6nh。同樣，DeepMind的明星雲集的position paper引入了圖網絡框架，統一了所有這些想法。DGL團隊有一個關於把seq2seq問題轉化為GNN的很好的教程：https://docs.dgl.ai/en/latest/tutorials/models/4_old_wines/7_transformer.html</p><p>英文原文：https://towardsdatascience.com/transformers-are-graph-neural-networks-bca9f75412aa</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>Transformers</a></li><li><a>一種</a></li><li><a>圖神經</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/f4df1c1a.html alt="有一種音頻編碼技術叫MQA，聲音質感優於TIDAL HiFi（CD等級）" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/6383bc881ccf4674aa3074a767f7f743 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f4df1c1a.html title="有一種音頻編碼技術叫MQA，聲音質感優於TIDAL HiFi（CD等級）">有一種音頻編碼技術叫MQA，聲音質感優於TIDAL HiFi（CD等級）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2161eb1b.html alt=一種可以除去砷的綠色方法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2161eb1b.html title=一種可以除去砷的綠色方法>一種可以除去砷的綠色方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/30ef85ba.html alt=「今日賓川」有一種青春叫軍訓，那些年我們一起軍訓的日子 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1534765722006c4a614cbda style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/30ef85ba.html title=「今日賓川」有一種青春叫軍訓，那些年我們一起軍訓的日子>「今日賓川」有一種青春叫軍訓，那些年我們一起軍訓的日子</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6facd8bb.html alt=一種限滑可鎖的差速器總成-託森差速器 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/377c51f9d3b3443796bd2aeacc50bf97 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6facd8bb.html title=一種限滑可鎖的差速器總成-託森差速器>一種限滑可鎖的差速器總成-託森差速器</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5ec4d0cb.html alt="上海光機所提出一種基於濾波PRBS信號相位調製的線寬展寬方法，實現光纖激光<5GHz線寬1.27kW輸出" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ea110b92b2c84fa8bbf54234ec0e1ee3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5ec4d0cb.html title="上海光機所提出一種基於濾波PRBS信號相位調製的線寬展寬方法，實現光纖激光<5GHz線寬1.27kW輸出">上海光機所提出一種基於濾波PRBS信號相位調製的線寬展寬方法，實現光纖激光&lt;5GHz線寬1.27kW輸出</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0e7787ae.html alt="陳邕  素描既是一種獨立的藝術表現形式" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/0967d56ed7a845338717c864fefa6729 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0e7787ae.html title="陳邕  素描既是一種獨立的藝術表現形式">陳邕  素描既是一種獨立的藝術表現形式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/bf1b008b.html alt=世界上有一種動物的耙耙是方形的 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6823/2478391326 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bf1b008b.html title=世界上有一種動物的耙耙是方形的>世界上有一種動物的耙耙是方形的</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e9b5e60d.html alt=承認錯誤也是一種勇敢 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1523331859560d5a2bd44e6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e9b5e60d.html title=承認錯誤也是一種勇敢>承認錯誤也是一種勇敢</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1875bfc7.html alt=生活中，敢於承認自己的錯誤，是一種成熟的表現 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/614f4c5c-9032-4849-91af-41b95d34c617 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1875bfc7.html title=生活中，敢於承認自己的錯誤，是一種成熟的表現>生活中，敢於承認自己的錯誤，是一種成熟的表現</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5a3b6b4a.html alt=一種電纜在線檢測技術 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RNJA92g7bwUBB3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5a3b6b4a.html title=一種電纜在線檢測技術>一種電纜在線檢測技術</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a2237f28.html alt=合併多列的好幾種方法，至少得學會一種吧 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7355bd523d6541009351ae8e42abe179 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a2237f28.html title=合併多列的好幾種方法，至少得學會一種吧>合併多列的好幾種方法，至少得學會一種吧</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/bdcc4e56.html alt=一種高級的DoS攻擊-Hash碰撞攻擊 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/c1c0537cde484d3daea887e3ef059355 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bdcc4e56.html title=一種高級的DoS攻擊-Hash碰撞攻擊>一種高級的DoS攻擊-Hash碰撞攻擊</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f9e5d67b.html alt=獬豸，一種明辨是非的神獸，古代的法官都想擁有一頭 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1524523867698b4fdfe7cbc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f9e5d67b.html title=獬豸，一種明辨是非的神獸，古代的法官都想擁有一頭>獬豸，一種明辨是非的神獸，古代的法官都想擁有一頭</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f8b10366.html alt=吉林大學：一種新型倒置器件結構，顯著提高量子點效率 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/e1b88d3bb4404ac7a6e3b01dc126a066 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f8b10366.html title=吉林大學：一種新型倒置器件結構，顯著提高量子點效率>吉林大學：一種新型倒置器件結構，顯著提高量子點效率</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/99643c8f.html alt=宋慧喬：客氣，是一種距離，也是一種自我保護 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/novel-images/65c2fd0a217cf4e7d8a1894d2310f75c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/99643c8f.html title=宋慧喬：客氣，是一種距離，也是一種自我保護>宋慧喬：客氣，是一種距離，也是一種自我保護</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>