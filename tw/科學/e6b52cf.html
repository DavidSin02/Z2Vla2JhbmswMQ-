<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>​監督學習中的損失函數及應用研究 | 极客快訊</title><meta property="og:title" content="​監督學習中的損失函數及應用研究 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/ed8f4ad9d95c46439c42078229cec6ad"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/e6b52cf.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/e6b52cf.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/e6b52cf.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/e6b52cf.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/e6b52cf.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/e6b52cf.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/e6b52cf.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/e6b52cf.html><meta property="article:published_time" content="2020-10-29T20:55:39+08:00"><meta property="article:modified_time" content="2020-10-29T20:55:39+08:00"><meta name=Keywords content><meta name=description content="​監督學習中的損失函數及應用研究"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/e6b52cf.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>​監督學習中的損失函數及應用研究</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><h1 class=pgc-h-center-line><strong>​監督學習中的損失函數及應用研究</strong></h1><p><br></p><p><strong>鄧建國, 張素蘭, 張繼福, 荀亞玲, 劉愛琴</strong></p><p><strong>太原科技大學計算機科學與技術學院</strong></p><p><br></p><p><strong>摘要：</strong>監督學習中的損失函數常用來評估樣本的真實值和模型預測值之間的不一致程度，一般用於模型的參數估計。受應用場景、數據集和待求解問題等因素的制約，現有監督學習算法使用的損失函數的種類和數量較多，而且每個損失函數都有各自的特徵，因此從眾多損失函數中選擇適合求解問題最優模型的損失函數是相當困難的。研究了監督學習算法中常用損失函數的標準形式、基本思想、優缺點、主要應用以及對應的演化形式，探索了它們適用的應用場景和可能的優化策略。本研究不僅有助於提升模型預測的精確度，而且也為構建新的損失函數或改進現有損失函數的應用研究提供了一個新的思路。</p><h1 class=pgc-h-center-line>1 引言</h1><p>隨著人工智能在智能製造、智慧農業以及智慧教育等領域的廣泛應用，機器學習變得越來越普及，並逐漸成為人工智能研究的重點內容。機器學習以數據為研究內容，使計算機能夠自動地從數據中學習規律，並利用規律預測未知數據。機器學習分為監督學習、無監督學習和強化學習。作為機器學習的一個重要類別，監督學習與機器學習同時產生，並伴隨著機器學習逐步發展起來。監督學習常用於解決分類或迴歸問題，是目前研究和應用較為廣泛的一種機器學習方法。由於監督學習具有很好的分類和標記能力，被廣泛應用於計算機視覺、自然語言、語音識別、目標檢測、藥物發現和基因組學等多個領域。</p><p>監督學習、無監督學習和強化學習都是從數據集中尋找規律，不同的是監督學習從有標記的訓練數據集中學習規律，並利用學到的規律預測訓練集外的數據的標記，不能預測數據集本身的潛在規律，這在一定程度上限制了監督學習的應用範圍。但是，現有的機器學習算法中大多還是基於監督學習的，甚至部分無監督學習和強化學習算法也是基於監督學習並受監督學習思想啟發發展起來的。另外，儘管機器學習在多個領域取得了令人矚目的成績，但現有機器學習算法的很多結論是通過實驗或經驗獲得的，還有待理論的深入研究與支持。現有機器學習算法無法從根本上解決機器學習面臨的技術壁壘，這導致機器學習無法跨越弱人工智能，仍然要依賴監督學習。</p><p>監督學習利用有標記的樣本調整模型參數，使模型具有正確預測未知數據的能力，其目的是讓計算機學習一組有標記的訓練數據集，進而獲得新的知識或技能，這就要求計算機不斷學習樣本數據，並依據樣本真實值與預測值之間的損失調整模型參數，提升模型的判別能力。顯然，衡量樣本真實值和預測值不一致程度的損失函數是監督學習研究的重點內容。損失函數是統計學、經濟學和機器學習等領域的基礎概念，它將隨機事件或與其相關的隨機變量的取值映射為非負實數，用來表示該隨機事件的風險或損失的函數。在監督學習中，損失函數表示單個樣本真實值與模型預測值之間的偏差，其值通常用於衡量模型的性能。現有的監督學習算法不僅使用了損失函數，而且求解不同應用場景的算法會使用不同的損失函數。研究表明，即使在相同場景下，不同的損失函數度量同一樣本的性能時也存在差異。可見，損失函數的選用是否合理直接決定著監督學習算法預測性能的優劣。</p><p>在實際問題中，損失函數的選取會受到許多約束，如機器學習算法的選擇、是否有離群點、梯度下降的複雜性、求導的難易程度以及預測值的置信度等。目前，沒有一種損失函數能完美處理所有類型的數據。在同等條件下，模型選取的損失函數越能擴大樣本的類間距離、減小樣本的類內距離，模型預測的精確度就越高。實踐表明，在同一模型中，與求解問題數據相匹配的損失函數往往對提升模型的預測能力起著關鍵作用。因此，如果能正確理解各種損失函數的特性，分析它們適用的應用場景，針對特定問題選取合適的損失函數，就可以進一步提高模型的預測精度。</p><p><br></p><h1 class=pgc-h-center-line>2 損失函數</h1><p>監督學習問題是在假設空間F中選取模型f作為決策函數，對於給定的輸入x，用損失函數L(Y,f(x))度量該樣本經決策函數f計算後的輸出預測值f(x)與樣本真實值Y之間的不一致程度。損失函數是經驗風險函數的核心部分，也是結構風險函數的重要組成部分。結構風險最小化策略認為結構風險最小的模型是最優模型，因此求最優模型，就是求解最優化問題：</p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ed8f4ad9d95c46439c42078229cec6ad><p class=pgc-img-caption></p></div><p>，其中，J(f)為模型的複雜度，λ為係數（λ≥0）。顯然，監督學習問題被轉化為一個經驗風險或結構風險函數的最優化問題。</p><p>在監督學習中，損失函數用於評估單個樣本經模型計算後輸出的預測值與真實值的不一致程度。它是一個非負實值函數，主要特點為：恆非負；誤差越小，函數值越小；收斂快。損失函數的值直接影響著模型的預測性能，損失函數值越小，模型的預測性能就越好。另外，作為樣本間相似度度量標準，損失函數用來刻畫樣本真實值與預測值之間的關係，如果損失值小於某一值，則認為樣本是相似的，否則認為是不相似的。監督學習算法中的損失函數如圖1所示。</p><p><br></p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4c3819caff6d4d3889c0e34f7e3037ae><p class=pgc-img-caption>圖1 監督學習算法中的損失函數</p></div><p><br></p><p>損失函數的標準數學形式（以下簡稱標準公式）不僅種類多，而且每類損失函數又在其標準形式的基礎上演化出許多演化形式。0-1損失函數是最簡單的損失函數，在其基礎上加入參數控制損失範圍，形成感知機損失函數；加入安全邊界，演化為鉸鏈損失函數。為解決多分類問題，在鉸鏈損失函數的基礎上，加入參數k，組合成top -k鉸鏈損失函數。將對數損失函數與softmax函數的特性結合，構成softmax損失函數；與概率分佈相似性融合，構成交叉熵（cross entropy）損失函數。另外，組合不同損失函數的標準形式或演化形式又形成新的損失函數。可見，損失函數的發展不是孤立的，而是隨著應用研究的發展進行變革的。</p><p>本文依據損失函數度量方式的不同，將主要損失函數分為基於距離度量的損失函數和基於概率分佈度量 的損失函數。同時，進一步研究了每一類損失函數的基本思想、優缺點、演化形式及演化動機，總結了它們的應用場景、更適用的數據集、可能的優化方向，使監督學習的應用研究儘可能選取最優損失函數，以提高模型預測的精確度。同時，給出了監督學習算法中使用頻次低的損失函數和組合損失函數。</p><p><br></p><h1 class=pgc-h-center-line><strong>3 主要損失函數</strong></h1><p><br></p><h1 class=pgc-h-center-line><strong>3.1 基於距離度量的損失函數</strong></h1><p><br></p><p>基於距離度量的損失函數通常將輸入數據映射到基於距離度量的特徵空間上，如歐氏空間、漢明空間等，將映射後的樣本看作空間上的點，採用合適的損失函數度量特徵空間上樣本真實值和模型預測值之間的距離。特徵空間上兩個點的距離越小，模型的預測性能越好，常用的基於距離度量的損失函數見表1。</p><p><br></p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/29b19ed4438848f4b3d3f9f736676efc><p class=pgc-img-caption></p></div><p><br></p><p>（1）平方損失函數</p><p>平方損失 （squared loss）函數最早是從天文學和地理測量學領域發展起來的，後來，由於歐氏距離在各個領域的廣泛使用，平方損失函數日益受到研究人員的關注。在迴歸問題中，平方損失用於度量樣本點到迴歸曲線的距離，通過最小化平方損失使樣本點可以更好地擬合迴歸曲線。在機器學習的經典算法（反向傳播算法、循環神經網絡、流形學習、隨機森林和圖神經網絡）中，常選用平方損失及其演化形式作為模型對誤檢測樣本進行懲罰的依據。</p><p>由於平方損失函數具有計算方便、邏輯清晰、評估誤差較準確以及可求得全局最優解等優點，一直受到研究人員的廣泛關注，其演化形式也越來越多。基於平方損失演化的損失函數有加權平方損失函數、和方誤差（sum squared error，SSE）函數、均方誤差（mean squared error，MSE）函數、L2損失（L2 loss）函數、均方根誤差（root mean squared error，RMSE）函數、x2檢驗（chi-square test）函數、triple損失函數和對比損失（contrastive loss）函數，見表2。</p><p><br></p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7234308984474d5b994e6d3ab1c273a1><p class=pgc-img-caption></p></div><p><br></p><p>加權平方損失函數通過加權修改樣本真實值與預測值之間的誤差，使樣本到擬合曲線的距離儘可能小，即找到最優擬合曲線。在正負樣本比例相差很大時，SSE通過計算擬合數據和原始數據對應點的誤差平方和，使正樣本點更加靠近擬合曲線，與MSE相比，SSE可以更好地表達誤差。MSE的思想是使各個訓練樣本點到最優擬合曲線的距離最小，常用於評價數據的變化程度。MSE的值越小，表示預測模型描述的樣本數據具有越好的精確度。由於無參數、計算成本低和具有明確物理意義等優點，MSE已成為一種優秀的距離度量方法。儘管MSE在圖像和語音處理方面表現較弱，但它仍是評價信號質量的標準，在迴歸問題中，MSE常被作為模型的經驗損失或算法的性能指標。L2損失又被稱為歐氏距離，是一種常用的距離度量方法，通常用於度量數據點之間的相似度。由於L2損失具有凸性和可微性，且在獨立、同分布的高斯噪聲情況下，它能提供最大似然估計，使得它成為迴歸問題、模式識別、圖像處理中最常使用的損失函數。RMSE直觀地揭示了模型的輸出預測值與樣本真實值的離散程度，常被作為迴歸算法的性能度量指標。儘管與平均絕對誤差（MAE）相比，RMSE計算更復雜且易偏向更高的誤差，但由於其是平滑可微的函數，且更容易進行運算，目前仍是許多模型默認的度量標準。x2檢驗也被稱為x2統計，常用於計算圖像直方圖之間的距離。triple損失函數是一個三元損失函數，使用時需設計樣本本身、相似的正樣本和不相似的負樣本三方數據，既耗時又對性能敏感。triple損失函數不能對每一個單獨的樣本進行約束，由於其具有類間距離大於類內距離的特性，首次出現在基於卷積神經網絡（convolutional neural network，CNN）的人臉識別任務中便取得了令人滿意的效果。對比損失函數是一個成對損失函數，在使用時除需樣本本身外，還需一個對比數據，可見，它也不能對每一個單獨的樣本進行約束。對比損失函數不僅能降維，而且降維後成對樣本的相似性保持不變，可以很好地表達成對樣本的匹配程度，另外，它能擴大類間距離，縮小類內距離，在人臉驗證算法中，常被作為人臉判斷的依據。</p><p>總之，平方損失函數不僅常用於迴歸問題，而且也可用於分類或標註問題，實現離散問題的預測。因為它對離群點比較敏感，所以它不適合離群點較多的數據集。在實際應用中，由於模型泛化問題等原因，一般不常使用平方損失函數的標準形式，而更多使用它的演化形式。另外，由於平方損失函數是可微的凸函數，常與之搭配的優化方法為隨機梯度下降或牛頓法。儘管平方損失或基於平方損失的演化損失函數已被廣泛應用於圖像自動標註、圖像重建、對象計數及圖像檢索等領域，並取得了令人滿意的效果，但在選取時也應根據具體問題的實現細節用其優勢避其劣勢。</p><p>（2）絕對損失函數</p><p>絕對損失（ absolute loss）函數是最常見的一種損失函數，它不僅形式簡單，而且能很好地表達真實值和預測值之間的距離。絕對損失對離群點有很好的魯棒性，但它在殘差為零處卻不可導。絕對損失的另一個缺點是更新的梯度始終相同，也就是說，即使很小的損失值，梯度也很大，這樣不利於模型的收斂。針對它的收斂問題，一般的解決辦法是在優化算法中使用變化的學習率，在損失接近最小值時降低學習率。儘管絕對損失本身的缺陷限制了它的應用範圍，但基於絕對損失的演化形式卻受到了更多的關注。在有噪聲標籤的分類問題中，基於平均絕對誤差構建的神經網絡具有良好的噪聲容忍能力，在單幅圖像超分辨率重建中，選用絕對損失函數可使重建的圖像失真更少，smooth L1損失在目標檢測問題中可有效解決梯度爆炸問題。</p><p>基於絕對損失的演化損失函數包括平均絕對誤差函數、平均相對誤差（mean relative error，MRE）函數、L1損失（L1 loss）函數、Chebyshev損失函數、Minkowski損失函數、smooth L1損失函數、huber損失函數和分位數損失（quantile loss）函數，見表3。</p><p><br></p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b3c3447ab2634352a7af41565d12b427><p class=pgc-img-caption></p></div><p><br></p><p>MAE表達預測誤差的實際情況，只衡量預測誤差的平均模長，不考慮方向，一般作為迴歸算法的性能指標。MRE既指明誤差的大小，又指明其正負方向。一般來說，與MAE相比，它更能反映評估的可信程度。L1損失又稱為曼哈頓距離，表示殘差的絕對值之和。Chebyshev損失也稱切比雪夫距離或L∞度量，是向量空間中的一種度量方法。Minkowski損失也被稱為閔氏距離或閔可夫斯基距離，是歐氏空間中的一種度量方法，常被看作歐氏距離和曼哈頓距離的一種推廣。smooth L1損失是由Girshick R在Fast R-CNN中提出的，主要用在目標檢測中防止梯度爆炸。huber損失是平方損失和絕對損失的綜合，它克服了平方損失和絕對損失的缺點，不僅使損失函數具有連續的導數，而且利用MSE梯度隨誤差減小的特性，可取得更精確的最小值。儘管huber損失對異常點具有更好的魯棒性，但是，它不僅引入了額外的參數，而且選擇合適的參數比較困難，這也增加了訓練和調試的工作量。分位數損失的思想是通過分位數γ懲罰高估和低估的預測值，使其更接近目標值的區間範圍，當設置多個γ值時，將得到多個預測模型，當γ=0.5時，分位數損失相當於MAE。分位數損失易構建能夠預測輸出值範圍的模型，與MAE相比，它可減少數據預處理的工作量。基於分位數損失的迴歸學習，不僅適用於正態分佈的殘差預測問題，而且對於具有變化方差或非正態分佈殘差的預測問題，也能給出合理的預測區間。在神經網絡模型、梯度提升迴歸器和基於樹模型的區間預測問題中，選取分位數損失評估模型的性能往往會取得更好的預測結果。分位數損失函數選取合適的分位數比較困難，一般情況下，分位值的選取取決於求解問題對正誤差和反誤差的重視程度，應根據實驗結果進行反覆實驗後再選取。另外，分位數損失值在0附近的區間內存在導數不連續的問題。</p><p>總之，在實際問題中，一般的數據集或多或少存在離群數據，當離群數據較多或需要考慮離群數據時，利用絕對損失及其演化損失對異常點魯棒性的特點，可以取得更好的效果。也就是說，絕對損失及其演化損失更適用於有較多離群點的數據集。</p><p>（3）0-1損失函數</p><p>0-1損失（zero-one loss）函數是一種較為簡單的損失函數，常用於分類問題。它不考慮預測值和真實值的誤差程度，是一種絕對分類方法。其思想是以分隔線為標準，將樣本集中的數據嚴格區分為0或1。由於沒有考慮噪聲對現實世界數據的影響因素，對每個誤分類樣本都施以相同的懲罰，預測效果不佳，甚至出現嚴重誤分類情況，這在很大程度上限制了0-1損失函數的應用範圍。另外，0-1損失函數是一種不連續、非凸且不可導函數，優化困難，這進一步限制了它的應用範圍。基於0-1損失的分類思想，出現了最常見的分類模型——K最近鄰（K nearest neighbor，KNN）。雖然0-1損失很少出現在監督學習算法中，但它的分類思想為之後出現的其他分類算法奠定了基礎。由於0-1損失函數直觀簡單，也易理解，研究人員在0-1損失的基礎上引入參數，進一步放寬分類標準，將其演化為感知機損失（perceptron loss）。</p><p>感知機損失是0-1損失改進後的結果，它採用參數克服了0-1損失分類的絕對性。與0-1損失相比，它的分類結果更可靠。感知機損失被廣泛應用於圖像風格化、圖像復原等問題中，通過使用預訓練的深度網絡對圖像進行多層語義分解，在相關問題上取得了較好的效果，其形式為：</p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b356a71f03a24dc08f6f8ed809ebf910><p class=pgc-img-caption></p></div><p>，其中t為參數，在感知機算法中t=0.5。</p><p>總之，儘管0-1損失函數存在誤分類的情況，但是，當所有樣本完全遠離分隔線時，0-1損失函數可能是最好的選擇，也就是說，0-1損失函數在對稱或均勻噪聲的數據集上具有較好的魯棒性。</p><p>（4）鉸鏈損失函數</p><p>鉸鏈損失（hinge loss）也被稱為合頁損失，它最初用於求解最大間隔的二分類問題。鉸鏈損失函數是一個分段連續函數，當Y和f(x)的符號相同時，預測結果正確；當Y和f(x)的符號相反時，鉸鏈損失隨著f(x)的增大線性增大。鉸鏈損失函數最著名的應用是作為支持向量機（support vector machine，SVM）的目標函數，其性質決定了SVM具有稀疏性，也就是說，分類正確但概率不足1和分類錯誤的樣本被識別為支持向量，用於劃分決策邊界，其餘分類完全正確的樣本沒有參與模型求解。SVM基本模型是定義在特徵空間上間隔最大的線性分類器，當採用核技術後， SVM可轉化為非線性分類器。鉸鏈損失函數是一個凸函數，因此，鉸鏈損失函數可應用於機器學習領域的很多凸優化方法中。</p><p>基於鉸鏈損失的演化損失函數包括邊界鉸鏈損失函數、坡道損失（ramp loss）函數、Crammerand鉸鏈損失函數、Weston鉸鏈損失函數、二分類支持向量機損失函數、多分類支持向量機損失函數、多分類支持向量機平方損失函數和top-k鉸鏈損失函數，見表4。</p><p>邊界鉸鏈損失表示期望正確預測的得分高於錯誤預測的得分，且高出邊界值margin，它主要用於訓練兩個樣本之間的相似關係，而非樣本的類別得分。在坡道損失函數中，s為截斷點的位置，一般情況下，s的值取決於類別個數c，</p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3ed5aeb83b644fbd9bf2dc9b3dda4cba><p class=pgc-img-caption></p></div><p>，它在x=1和x=s兩處不可導。Crammerand鉸鏈損失函數是由Crammerand Singer提出的一種針對線性分類器的損失函數。Weston鉸鏈損失函數是由Weston和Watkins提出的一種損失函數。二分類支持向量機損失函數可看作L2正則化與鉸鏈損失之和。多分類支持向量機損失函數只考慮在正確值附近的那些值，其他的均作為0處理，即只關注那些可能造成影響的點（或支持向量），因此，具有較好的魯棒性。與多分類支持向量機損失函數相比，多分類支持向量機平方損失函數的懲罰更強烈。top-k鉸鏈損失函數在k個測試樣本預測為正的約束下，使所有訓練實例的鉸鏈損失最小化。</p><p><br></p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0e7e56c91ba24e149ab7b1acadea97da><p class=pgc-img-caption></p></div><p><br></p><p>總之，鉸鏈損失及其演化損失函數常作為人臉識別、文本分類、筆跡識別和圖像自動標註領域的損失函數，用於度量圖像的相似性或向量空間中向量的距離，對錯誤越大的樣本，它施以越嚴重的懲罰，這可能使它對噪聲敏感，從而降低模型的泛化能力。</p><p>（5）中心損失函數</p><p>中心損失（c enter loss）函數採用了歐氏距離思想，為每個類的深層特徵學習一箇中心（一個與特徵維數相同的向量），但在全部樣本集上計算類中心相當困難，常用的做法是把整個訓練集劃分成若干個小的訓練集（mini-batch），並在mini-batch樣本範圍內進行類中心計算。另外，在更新類中心時常增加一個類似學習率的參數α，用於處理採樣太少或者有離群點的情況。中心損失函數主要用於減小類內距離，表面上只是減少了類內距離，但實際上間接增大了類間距離，它一般不單獨使用，常與softmax損失函數搭配使用，其分類效果比只用softmax損失函數更好。</p><p>基於中心損失的演化損失函數有三元中心損失（triplet-center loss， TCL）函數，TCL函數使樣本與其對應的中心之間的距離比樣本與其最近的負中心之間的距離更近，其形式為：</p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/028212316b774308ace5f1ad26816849><p class=pgc-img-caption></p></div><p>。</p><p>總之，中心損失函數常用於神經網絡模型中，實現類內相聚、類間分離。在特徵學習時，當期望特徵不僅可分，而且必須差異大時，通常使用中心損失函數減小類內變化，增加不同類特徵的可分離性。在實際應用中，由於中心損失函數本身考慮類內差異,因此中心損失函數應與主要考慮類間的損失函數搭配使用，如softmax損失、交叉熵損失等。</p><p>（6）餘弦損失</p><p>餘弦損失（cos ine loss）也被稱為餘弦相似度，它用向量空間中兩個向量夾角的餘弦值衡量兩個樣 本的差異。與歐氏距離相比，餘弦距離對具體數值的絕對值不敏感，而更加註重兩個向量在方向上的差異。在監督學習中，餘弦相似度常用於計算文本或標籤的相似度。它常與詞頻逆向文件頻率（term frequency-inverse document frequency，TF-IDF）算法結合使用，用於文本挖掘中的文件比較。在數據挖掘領域，餘弦損失常用於度量集群內部的凝聚力。</p><p>基於餘弦損失的演化損失函數有改進的餘弦距離核函數，它是由李為等人在與文本相關的說話人確認技術中提出的，用來區分說話人身份及文本內容的差異，其形式為：</p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a2c82f5a327b4725a7853e66f36ba5fa><p class=pgc-img-caption></p></div><p>，其中，λ為揚聲器模型， u為說話人。</p><p><br></p><h1 class=pgc-h-center-line><strong>3.2 基於概率分佈度量的損失函數</strong></h1><p><br></p><p>基於概率分佈度量的損失函數是將樣本間的相似性轉化為隨機事件出現的可能性，即通過度量樣本的真實分佈與它估計的分佈之間的距離，判斷兩者的相似度，一般用於涉及概率分佈或預測類別出現的概率的應用問題中，在分類問題中尤為常用。監督學習算法中，常用的基於概率分佈度量的損失函數見表5。</p><p><br></p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/39d427bf23c043819367d39db566584a><p class=pgc-img-caption></p></div><p><br></p><p>（1）對數損失函數</p><p>對數損失（logarithm loss）也被稱為對數似然損失，它使用極大似然估計的思想，表示樣本x在類別y的情形下，使概率P(y|x)達到最大值。因為概率的取值範圍為[0,1]，使得log(P(y|x))取值為((−∞,0))，為保證損失為非負，對數損失的形式為對數的負值。對數損失函數是邏輯迴歸、神經網絡以及一些期望極大估計的模型經常使用的損失函數，它通過懲罰錯誤的分類，實現對分類器的精確度量化和模型參數估計，對數損失函數常用於度量真實條件概率分佈與假定條件概率分佈之間的差異。</p><p>基於對數損失函數的演化形式包括邏輯迴歸損失（logistic regression loss）函數、加權對數損失函數、對數雙曲餘弦損失（log cosh loss）函數、softmax損失函數、二分類對數損失函數和巴氏距離（Bhattacharyya distance）函數，見表6。</p><p><br></p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b3567af8d2d8416da66dd53ce426bf3d><p class=pgc-img-caption></p></div><p><br></p><p>邏輯迴歸損失函數假設樣本服從伯努利分佈，利用極大似然估計的思想求得極值，它常作為分類問題的損失函數。加權對數損失函數主要應用在類別樣本數目差距非常大的分類問題中，如邊緣檢測問題（邊緣像素的重要性比非邊緣像素大，可針對性地對樣本進行加權）。對數雙曲餘弦損失函數基本上等價於MSE函數，但又不易受到異常點的影響，是更加平滑的損失函數，它具有huber損失函數的所有優點，且二階處處可導。softmax損失函數是卷積神經網絡處理分類問題時常用的損失函數。巴氏距離函數用於度量兩個連續或離散概率分佈的相似度，它與衡量兩個統計樣本或種群之間的重疊量的巴氏係數密切相關。在直方圖相似度計算中，選用巴氏距離函數會獲得很好的效果，但它的計算很複雜。</p><p>總之，在分類學習中，當預測問題使用已知的樣本分佈，找到最有可能導致這種分佈的參數值時，應選取對數損失函數或其演化損失函數作為預測問題的損失函數。在基於深度神經網絡的分類或標註問題中，一般在輸出層使用softmax作為損失函數，對數損失函數是迴歸、決策樹、深度神經網絡常使用的損失函數。</p><p>（2）KL散度函數</p><p>KL散度（ Kullback-Leibler divergence）也被稱為相對熵，是一種非對稱度量方法，常用於度量兩個概率分佈之間的距離。KL散度也可以衡量兩個隨機分佈之間的距離，兩個隨機分佈的相似度越高的，它們的KL散度越小，當兩個隨機分佈的差別增大時，它們的KL散度也會增大，因此KL散度可以用於比較文本標籤或圖像的相似性。基於KL散度的演化損失函數有JS散度函數。JS散度也稱JS距離，用於衡量兩個概率分佈之間的相似度，它是基於KL散度的一種變形，消除了KL散度非對稱的問題，與KL散度相比，它使得相似度判別更加準確。JS散度函數的形式為：</p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/61ec6b1b125c45eda7737d9bf8e0daef><p class=pgc-img-caption></p></div><p>，其中，</p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3b4e7d2323d84e1ead9fa4724cd26ffa><p class=pgc-img-caption></p></div><p>為KL散度。</p><p>KL 散度及其演化損失主要用於衡量兩個概率分佈之間的相似度，常作為圖像低層特徵和文本標籤相似度的度量標準。KL散度在成像分析、流體動力學、心電圖等臨床實驗室檢測、生物應用的網絡分析、細胞生物學等領域有廣泛的應用。</p><p>（3）交叉熵損失</p><p>交叉熵是信息論中的一個概念，最初用於估算平均編碼長度，引入機器學習後，用於評估當前訓練得到的概率分佈與真實分佈的差異情況。為了使神經網絡的每一層輸出從線性組合轉為非線性逼近，以提高模型的預測精度，在以交叉熵為損失函數的神經網絡模型中一般選用tanh、sigmoid、softmax或ReLU作為激活函數。</p><p>基於交叉熵損失的演化損失函數包括平均交叉熵損失函數、二分類交叉熵損失函數、二分類平衡交叉熵損失函數、多分類交叉熵損失函數和focal損失函數，見表7。</p><p><br></p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f5589af1bd2c4e268cf34628a9cf17a8><p class=pgc-img-caption></p></div><p><br></p><p>二分類交叉熵損失函數對於正樣本而言，輸出概率越大，損失越小；對於負樣本而言，輸出概率越小，損失越小。二分類平衡交叉熵損失函數與二分類交叉熵損失函數相比，它的優勢在於引入了平衡參數β∈[0,1]，可實現正負樣本均衡，使預測值更接近於真實值。</p><p>交叉熵損失函數刻畫了實際輸出概率與期望輸出概率之間的相似度，也就是交叉熵的值越小，兩個概率分佈就越接近，特別是在正負樣本不均衡的分類問題中，常用交叉熵作為損失函數。目前，交叉熵損失函數是卷積神經網絡中最常使用的分類損失函數，它可以有效避免梯度消散。</p><p>（4）softmax損失函數</p><p>從標準形式上看，softmax損失函數應歸到對數損失的範疇，在監督學習中，由於它被廣泛使用，所以單獨形成一個類別。softmax損失函數本質上是邏輯迴歸模型在多分類任務上的一種延伸，常作為CNN模型的損失函數。softmax損失函數的本質是將一個k維的任意實數向量x映射成另一個k維的實數向量，其中，輸出向量中的每個元素的取值範圍都是(0,1)，即softmax損失函數輸出每個類別的預測概率。由於softmax損失函數具有類間可分性，被廣泛用於分類、分割、人臉識別、圖像自動標註和人臉驗證等問題中，其特點是類間距離的優化效果非常好，但類內距離的優化效果比較差。</p><p>基於softmax損失函數的演化損失函數包括softer softmax函數、NSL （normalized softmax loss）函數、LMCL（large margin cosine loss）函數、L-softmax函數、A-softmax函數、A Msoftmax（additive margin softmax）函數以及正則化softmax函數，見表8。</p><p><br></p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4b1be47385554475ab716565bfc1fa74><p class=pgc-img-caption></p></div><p><br></p><p>softer softmax 損失函數是Hinton G等人為了解決模型給誤分類標籤分配的概率被softmax損失忽略的問題而提出的。NSL損失函數利用兩個特徵向量之間的餘弦相似度評估兩個樣本之間的相似性，使後驗概率只依賴於角度的餘弦值，由此產生的模型學習了角空間中可分離的特徵，提高了特徵學習能力。為了構建一個更大邊距的分類器，期望</p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5a96e02732dd4cdb80be1bf90755cb42><p class=pgc-img-caption></p></div><p>且</p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7efd60db6bcc412c95f8eccba7ddfb5f><p class=pgc-img-caption></p></div><p>，m≥0為控制餘弦邊矩大小的參數，Wang H等人提出了LMCL損失函數。與傳統的歐幾里得邊距相比，角的餘弦值與softmax具有內在的一致性，基於此思想，在原始softmax基礎上選用角邊距度量兩個樣本的相似性， Liu W等人提出了L-softmax損失函數。受L-softmax損失函數啟發，在它的基礎上，Liu W等人增加了條件</p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/34466a1370d24faf8d212e9408ee1c7a><p class=pgc-img-caption></p></div><p>，B=0和cos(mθ1)>cos(θ2)，使得預測僅取決於W和x之間的角度θ，提出了A-softmax損失函數。受L-softmax損失函數的啟發， Wang F等人提出了AM-softmax損失函數，它使前後向傳播變得更加簡單。正則化softmax損失函數加入刻畫模型的複雜度指標的正則化，可以有效地避免過擬合問題。</p><p>softmax損失函數具有類間可分性，在多分類和圖像標註問題中，常用它解決特徵分離問題。在基於卷積神經網絡的分類問題中，一般使用softmax損失函數作為損失函數，但是softmax損失函數學習到的特徵不具有足夠的區分性，因此它常與對比損失或中心損失組合使用，以增強區分能力。</p><p><br></p><h1 class=pgc-h-center-line><strong>4 其他損失函數</strong></h1><p><br></p><p>其他損失函數主要包括指數損失（ exponential loss）函數、漢明距離函數、dice損失函數、餘弦損失+softmax函數和softmax+LDloss函數，見表9。</p><p><br></p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4cf4e065496f4ed48a61de5b8d32127f><p class=pgc-img-caption></p></div><p>與主要損失函數相比，其他損失函數在監督學習中使用的頻次比較低，其中指數損失函數是AdaBoost算法中常用的損失函數。它與鉸鏈損失函數和交叉熵損失函數相比，對錯誤分類施加的懲罰更大，這使得它的誤差梯度也較大，因此在使用梯度下降算法優化時，在極小值處求解速度也較快。漢明距離用於計算兩個向量的相似度，即通過比較兩個向量的每一位是否相同來計算漢明距離。dice損失函數是一種集合相似性度量函數，通常用於計算兩個樣本的相似性，常作為文本比較或圖像分割類問題的損失函數，尤其適用於處理圖像的前景區域和背景區域相差較大的圖像分割問題。餘弦損失+softmax函數是利用餘弦和softmax的特性組合而成的，見第3.2節中的LMCL損失函數。softmax+LDloss函數是黃旭等人在融合判別式深度特徵學習的圖像識別算法中引入線性判別分析（linear discriminant analysis，LDA）思想構建的損失函數，該算法使softmax+LDloss參與卷積神經網絡的訓練，實現儘可能最小化類內特徵距離和最大化類間特徵距離的目標，以提高特徵的鑑別能力，進而改善圖像的識別性能。</p><p><br></p><h1 class=pgc-h-center-line><strong>5 損失函數在監督學習中的應用</strong></h1><p><br></p><p>在監督學習中，損失函數的選取和演化通常是伴隨著監督學習算法的發展而演變的。監督學習算法是通過計算機學習有標記的訓練數據集，使模型能對未知數據進行預測的機器學習任務。在機器學習發展的過程中，產生了許多監督學習算法，其中，典型算法有深度神經網絡（deep neural network，DNN）、 決策樹（ decision tree，DT）、樸素貝葉斯分類、線性迴歸（linear regression）、 邏輯迴歸、支持向量機、K最近鄰和AdaBoost等，這些監督學習算法仍是當前研究和應用的重點內容。</p><p><br></p><h1 class=pgc-h-center-line><strong>5.1 深度神經網絡中的應用</strong></h1><p><br></p><p>深度神經網絡是深度學習的一個重要分支，由於深度神經網絡具有更多的網絡層數和參數個數，所以能夠提取更多的數據特徵，獲取更好的學習效果。在基於深度神經網絡的應用中，損失函數作為度量樣本真實值和模型預測值之間差異的工具，主要解決視覺處理、自然語言和語音識別等領域的問題。在視覺識別應用中，中心損失函數可區分人臉的差異、提高人臉識別準確度；softmax損失函數結合fisher損失函數提升不同模態下人臉數據的關聯程度；改進的triplet損失函數解決了跨攝像機人員再識別問題。在圖像標註應用中，利用JS散度生成獨特的圖像標籤，採用交叉熵損失函數構建分層標籤的圖像分類模型，選用餘弦損失函數表達語義相似性。在視覺檢索應用中，通過L2損失函數構建用於大規模視覺搜索的深度Hash方法。在圖像重建方面，採用均方誤差函數重建出準確度更高的圖像。在圖像分割應用中，在softmax分類器後增加dice損失函數，以提高圖像分割精度。在語音識別應用中，使用A-softmax損失函數提高端到端系統的性能。在自然語言處理應用中，使用加權交叉熵生成富含情感的單詞。</p><p><br></p><h1 class=pgc-h-center-line><strong>5.2 決策樹中的應用</strong></h1><p><br></p><p>決策樹是一個預測模型，一般依據KL散度大小，將樣本屬性分解成樹狀結構，並用於新樣本分類。決策樹的3種典型實現為ID3、CART和C4.5。決策樹的計算複雜度低，可解釋性強，對中間值的缺失不敏感，這使得決策樹至今在一些問題上仍被使用。在基於決策樹的應用中，引入C4.5決策樹方法處理流量分類問題，利用訓練數據集中的KL散度構建分類模型，並通過對分類模型的簡單查找實現未知網絡流樣本的分類，理論分析和實驗結果表明，使用C4.5決策樹處理流量分類問題在分類穩定性方面具有明顯的優勢。在視覺位置識別問題中，將漢明距離嵌入二叉搜索樹中，解決描述符匹配和圖像檢索問題。</p><p><br></p><h1 class=pgc-h-center-line><strong>5.3 樸素貝葉斯分類中的應用</strong></h1><p><br></p><p>樸素貝葉斯分類算法是一個典型的機器學習算法，它假定樣本的各個特徵之間是相互獨立的，在大量樣本下會有較好的表現，不適用於與輸入向量的特徵條件有關聯的場景。由於樸素貝葉斯分類算法實現簡單，並有堅實的數學理論作為支撐，因此在很多領域有廣泛的應用，如垃圾郵件過濾、文本分類等。在基於樸素貝葉斯分類的應用中，採用餘弦損失度量成對標籤相似度提高標籤在整個圖像相似性分配上的一致性，漢明損失函數和0-1損失函數在多標籤分類中分類能力不同，0-1損失函數在有限樣本下具有良好的分類效果。</p><p><br></p><h1 class=pgc-h-center-line><strong>5.4 線性迴歸中的應用</strong></h1><p><br></p><p>線性迴歸是監督學習中經典的迴歸模型之一，它是利用數理統計中的迴歸分析，確定兩個或兩個以上變量間相互依賴的定量關係的一種統計分析方法。線性迴歸分為一元線性迴歸和多元線性迴歸。因為線性迴歸形式簡單、易於建模，常用於解決連續值預測的問題。在基於線性迴歸的應用中，引入KL散度估計簇間距離，不僅能夠獲得更精確的分割結果，而且對噪聲和初始輪廓具有更強的魯棒性；在存在異常值或重尾誤差分佈的情況下，基於指數平方損失函數的線性迴歸估計方法比最小二乘數估計方法更有效；採用均方誤差函數可精確估計交通擁擠時的車輛數量；自適應huber損失函數不僅解決了原始huber損失函數沒有封閉解、難以優化的問題，而且在曲線擬合、帶噪聲標籤圖像標註、經典迴歸問題和人群計數應用方面有很大的優勢。</p><p><br></p><h1 class=pgc-h-center-line><strong>5.5 邏輯迴歸中的應用</strong></h1><p><br></p><p>邏輯迴歸是一個應用非常廣泛的機器學習算法，它將數據擬合到一個邏輯函數中，預測事件發生的概率。邏輯迴歸使用極大似然估計思想，常選用對數損失或交叉熵損失作為損失函數，可用於解決二分類或多分類問題。在基於邏輯迴歸的應用中，採用基於對數損失的核函數度量兩個樣本之間的相似性設計的邏輯迴歸模型，與核邏輯迴歸分析和支持向量機相比，不僅可達到更好的分類精度，而且有更好的時間效率；在研究分類不平衡對軟件缺陷影響的問題中，採用對數損失函數的邏輯迴歸模型的性能更加穩定；採用最小化對數損失構建的點擊通過率（clickthrough rate，CTR）預測模型，比傳統的點擊率預測模型以及最新的基於深度學習的預測模型的性能更好。</p><p><br></p><h1 class=pgc-h-center-line><strong>5.6 支持向量機中的應用</strong></h1><p><br></p><p>支持向量機基於最大化分類間隔的原則，通過核函數巧妙地將線性不可分問題轉化為線性可分問題，並且具有非常好的泛化性能。它使用鉸鏈損失函數計算經驗風險，並在求解系統中加入了正則化項以優化結構風險，是一個具有稀疏性和穩健性的分類器，它在文本分類和圖像標註領域有廣泛的應用。在基於支持向量機的應用中，採用JS散度計算兩個特徵向量之間的相似度，在原始SVM基礎上，引入概率加權策略構建多個分類器，設計了基於SVM的多特徵融合的圖像標註方法；採用餘弦損失函數計算向量空間模型（vector space model）中向量的相似度，提出了內容和標籤相融合的圖像標註方法；在標籤混淆情況下，將公差參數引入鉸鏈損失函數中，可提高標籤分類準確率。</p><p><br></p><h1 class=pgc-h-center-line><strong>5.7 K最近鄰算法中的應用</strong></h1><p><br></p><p>K最近鄰（KNN）算法是簡單的機器學習算法之一，該算法的思想是如果一個樣本在特徵空間中的K個最相鄰的樣本中的大多數屬於某一個類別，則該樣本也屬於這個類別，並具有這個類別樣本的特性。KNN算法在進行類別決策時，只與極少量的相鄰樣本有關，因此，它適用於類域有交叉或重疊較多的待分類樣本集。在基於KNN算法的應用中，採用鉸鏈損失函數度量兩幅圖像之間的相似性進行歸納學習，採用餘弦損失函數度量兩幅圖像標籤的相似性進行推理學習，並將歸納學習和推理學習結合，形成統一的距離度量學習框架；在經典的2PKNN算法中，採用鉸鏈損失函數度量兩幅圖像的相似性，解決了類別不平衡和弱標記問題；利用漢明距離度量KNN搜索上的可伸縮性，實現了跨模態相似度搜索。</p><p><br></p><h1 class=pgc-h-center-line><strong>5.8 AdaBoost算法中的應用</strong></h1><p><br></p><p>AdaBoost算法是基於Boosting思想的集成學習算法，使用指數函數作為損失函數，其核心思想是對同一個訓練集訓練不同的弱學習器，然後將多個弱學習器進行集成，構造一個精度非常高的強學習器。AdaBoost算法被廣泛應用於計算機視覺和目標檢測領域，在人臉檢測方面的表現尤為出眾。在基於AdaBoost算法的應用中，通過學習圖像局部區域的相似性得到一組非線性弱學習器，使用弱學習器訓練圖像局部特徵獲得低維的、獨有的特徵描述子，選用漢明距離度量特徵描述子之間的相似度，提高了圖像局部區域的匹配精度。利用改進的指數損失函數與平方損失函數的加權組合代替傳統AdaBoost指數損失函數構建模型，解決傳統的AdaBoost算法在訓練樣本存在異常值時導致的分類效果不理想的問題。</p><p><br></p><h1 class=pgc-h-center-line><strong>6 結束語</strong></h1><p><br></p><p>本文依據損失函數對樣本的評估方式，將監督學習的主要損失函數分為基於距離度量的損失函數和基於概率分佈度量的損失函數，並分析了每個損失函數的基本思想、優缺點、主要應用，在此基礎上，總結了每個損失函數更適合的應用場景或可能的優化方向。另外，在其他損失函數中給出了在監督學習算法中出現但使用頻次相對較低的損失函數和組合函數。在監督學習中，損失函數作為度量數據真實值與預測值之間相似度的工具，它直接影響著模型的預測性能，顯然，損失函數的選取或改進是監督學習領域研究的主要內容。</p><p>本文提到的主要損失函數在監督學習算法中使用頻次比較高，對於具體的研究問題，這些損失函數不一定是最好的損失函數，可能存在其他更合適的損失函數。其原因是模型的產生與解決的具體問題有關，同一模型在同類問題中性能差異不大，但在相似類問題或不同類問題中，性能差異可能較大，甚至出現不適合的情形。導致此類問題產生的原因除模型本身問題外，另一個主要原因是模型默認的損失函數。儘管每個模型都有默認的損失函數，但在使用模型解決實際問題時，應考慮問題與損失函數的內在聯繫，尤其是使用當下流行的遷移學習解決問題時，更應考慮問題與損失函數的關係。</p><p>雖然監督學習已產生多年，但在機器學習中仍佔有重要地位，其主要原因是監督學習模型在解決問題方面的精確度一直在不斷提高，甚至在某些方面的能力已經超越人類。模型精確度提升的原因除模型結構等因素不斷改進外，另一主要原因是不斷優化的損失函數在度量預測值與真實值差異方面的能力在不斷提升，甚至在不改變模型結構的情形下，只調整損失函數就可以達到很好的效果。相比於模型結構的升級換代，損失函數的優化工作量要少得多，可見優化損失函數也是提升模型性能的有效手段。優化損失函數除在現有標準形式的基礎上擴展延伸外，還應考慮組合不同損失函數形成組合損失函數。組合損失函數是將現有的損失函數經過四則運算，構造一類新的損失函數，筆者認為組合損失函數構成的一般原則是基於問題性質，以損失函數最小為目標，確定預測模型中各單項預測的加權係數，並將各個單獨的損失函數組合成一個新的損失函數，利用新的損失函數度量樣本之間的差異。</p><p>構建監督學習算法的目的是解決實際問題，而實際問題中的主要內容是數據。在監督學習模型中，損失函數通過度量數據來衡量模型的性能，可見損失函數與數據直接相關。不同類的問題中數據間差異較大，甚至是不相關的，而相似類問題中的數據差異往往不大，甚至存在共同特性，這為實際問題選定的模型選擇合適的損失函數提供了參考。筆者認為應首先考慮問題中數據的性質，然後找到解決此類問題效果好的模型，在選定模型的基礎上，進一步考慮損失函數與數據的內在關係，進而選擇更合適的損失函數。</p><p>儘管損失函數在模型中的地位比較重要，但它畢竟只是模型的一個部分，要使模型有更好的預測性能，在實際問題中還需要考慮其他影響模型性能的因素，如數據特徵歸一化方法、樣本間相似度度量方法、相近類別分離技術以及組合損失函數等。總之，監督學習是一個系統工程，除研究損失函數外，還應考慮數據集、模型結構及優化策略等，只有圍繞損失函數全面考慮影響它的因素，才會取得更好的預測效果。</p><p>通常情況下，損失函數的選取應從以下方面考慮：</p><p>● 選擇最能表達數據的主要特徵構建基於距離或基於概率分佈度量的特徵空間；</p><p>● 選擇合理的特徵歸一化方法，使特徵向量轉換後仍能保持原來數據的核心內容；</p><p>● 選取合理的損失函數，在實驗的基礎上，依據損失不斷調整模型的參數，使其儘可能實現類別區分；</p><p>● 合理組合不同的損失函數，發揮每個損失函數的優點，使它們能更好地度量樣本間的相似性；</p><p>● 將數據的主要特徵嵌入損失函數，提升基於特定任務的模型預測精確度。</p><p>下一步可能的工作有兩方面：一是在現階段研究的基礎上，進一步研究監督學習算法中的損失函數，以期能找到更普適的度量樣本特徵的損失函數；二是研究無監督學習和強化學習中的損失函數，總結其中典型的損失函數，並嘗試將其引入監督學習應用中，以提高模型預測的精確度。</p><p>本文在常用損失函數的基礎上，為構建新的損失函數或改進現有損失函數的應用研究提供了一個新的思路，不僅有助於研究人員針對研究問題輕鬆選擇適合問題的損失函數，而且還可根據待求解問題改進或優化現有的損失函數。總之，監督學習是當前的一個熱門研究領域，也是未來很有前途的一個研究方向，對監督學習算法中的損失函數研究具有重大的理論意義和應用前景，是一個具有實用價值的研究課題。</p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8050803ec6224d19b0106e046b49d2b7><p class=pgc-img-caption></p></div><p><br></p><p><strong>《大數據》期刊</strong></p><p>《大數據（Big Data Research，BDR）》雙月刊是由中華人民共和國工業和信息化部主管，人民郵電出版社主辦，中國計算機學會大數據專家委員會學術指導，北京信通傳媒有限責任公司出版的中文科技核心期刊。</p><div class=pgc-img><img alt=​監督學習中的損失函數及應用研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/32794d686ad0496f872e061689964110><p class=pgc-img-caption></p></div><h1 class=pgc-h-center-line>關注《大數據》期刊微信公眾號，獲取更多內容</h1></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>監督學習</a></li><li><a>損失</a></li><li><a>函數</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/e087ca41.html alt=偏導數和函數的梯度 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/9d20a1e4cbff42a094d57df057fe9597 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e087ca41.html title=偏導數和函數的梯度>偏導數和函數的梯度</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5fc113b1.html alt=梯度原理：梯度在每一點上都指向函數增長最快的方向 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/cdb8db41d5024f38a2e490e66baebdb4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5fc113b1.html title=梯度原理：梯度在每一點上都指向函數增長最快的方向>梯度原理：梯度在每一點上都指向函數增長最快的方向</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/408d3387.html alt=EXCEL:VLOOKUP函數綜合運用，實現供應商每月數據自動查詢 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/03e79d133c994f9f8a386b20b04b3da1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/408d3387.html title=EXCEL:VLOOKUP函數綜合運用，實現供應商每月數據自動查詢>EXCEL:VLOOKUP函數綜合運用，實現供應商每月數據自動查詢</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/526a98f3.html alt=Excel條件求和函數那麼多，高手一直都在用這一個，而你卻沒聽過 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/b2fd76f54f9b46c8bd79500ba4dac2fa style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/526a98f3.html title=Excel條件求和函數那麼多，高手一直都在用這一個，而你卻沒聽過>Excel條件求和函數那麼多，高手一直都在用這一個，而你卻沒聽過</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2664a94c.html alt=excel中的DSUM函數——條件求和原來如此簡單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/0dc7aeaa9b61467e86dbaeae6dfeaaa4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2664a94c.html title=excel中的DSUM函數——條件求和原來如此簡單>excel中的DSUM函數——條件求和原來如此簡單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/48f18e97.html alt=excel常用函數用法解析第四篇——COLUMN、ROW函數 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4c3daf45857e4f169def37fc08d652fd style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/48f18e97.html title=excel常用函數用法解析第四篇——COLUMN、ROW函數>excel常用函數用法解析第四篇——COLUMN、ROW函數</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b9de5637.html alt=比Sum函數好用10倍，它才是Excel求和函數中的NO.1 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/d492e720108a4daf94a3411b6868bd6f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b9de5637.html title=比Sum函數好用10倍，它才是Excel求和函數中的NO.1>比Sum函數好用10倍，它才是Excel求和函數中的NO.1</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1dfdbb27.html alt=這4張圖如果不懂，自噴至少損失8分！（詳解報警閥！） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5fe122eff5694ace8861fcc8d3dab174 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1dfdbb27.html title=這4張圖如果不懂，自噴至少損失8分！（詳解報警閥！）>這4張圖如果不懂，自噴至少損失8分！（詳解報警閥！）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1f318889.html alt=風機倒塌損失1000W+，原來金屬疲勞危害這麼大 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/602eadc6915f45c28bbbb6a73405d88b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1f318889.html title=風機倒塌損失1000W+，原來金屬疲勞危害這麼大>風機倒塌損失1000W+，原來金屬疲勞危害這麼大</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cc21918b.html alt=難點解析丨求一次函數中參數的值（6種解法） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/f611ff11a88043ddb35b0ca455618731 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cc21918b.html title=難點解析丨求一次函數中參數的值（6種解法）>難點解析丨求一次函數中參數的值（6種解法）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9ed0ee97.html alt=邏輯函數中IF函數判斷是與不是，NOT函數對參數值求反 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4757b6dd22c84a46b4bfaa7ee6165d20 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9ed0ee97.html title=邏輯函數中IF函數判斷是與不是，NOT函數對參數值求反>邏輯函數中IF函數判斷是與不是，NOT函數對參數值求反</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/75a5bee4.html alt=你不知道的階乘與gamma函數 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7d184211b19a4de18223836f70286b08 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/75a5bee4.html title=你不知道的階乘與gamma函數>你不知道的階乘與gamma函數</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/23bfdd0e.html alt=11個函數，8種操作，詳述日期、時間的提取判定 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5de047b4187246eb9f853770badb197c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/23bfdd0e.html title=11個函數，8種操作，詳述日期、時間的提取判定>11個函數，8種操作，詳述日期、時間的提取判定</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6df935a0.html alt=Excel中最常用的函數公式——提取出生年月：TEXT+MID函數 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/de02ae9e43c0456fad1f5a53a8524523 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6df935a0.html title=Excel中最常用的函數公式——提取出生年月：TEXT+MID函數>Excel中最常用的函數公式——提取出生年月：TEXT+MID函數</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7a1acbc3.html alt=Excel函數太麻煩？提升效率，教你輕鬆提取表格內的日期信息 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/19ba957bdbb24f7697e357b7c31479aa style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7a1acbc3.html title=Excel函數太麻煩？提升效率，教你輕鬆提取表格內的日期信息>Excel函數太麻煩？提升效率，教你輕鬆提取表格內的日期信息</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>