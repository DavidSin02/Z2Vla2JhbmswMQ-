<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>在Python中計算神經網絡的雅可比矩陣 | 极客快訊</title><meta property="og:title" content="在Python中計算神經網絡的雅可比矩陣 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/acc2a9f4315f4fe8a3c6e03201f66725"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/f380664.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/f380664.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/f380664.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/f380664.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/f380664.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/f380664.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/f380664.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/f380664.html><meta property="article:published_time" content="2020-10-29T21:01:39+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:39+08:00"><meta name=Keywords content><meta name=description content="在Python中計算神經網絡的雅可比矩陣"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/f380664.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>在Python中計算神經網絡的雅可比矩陣</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><div><p>通常，神經網絡是一個多變量，矢量值函數，如下所示：</p><div class=pgc-img><img alt=在Python中計算神經網絡的雅可比矩陣 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/acc2a9f4315f4fe8a3c6e03201f66725><p class=pgc-img-caption></p></div><p>函數f有一些參數θ(神經網絡的權重),它將一個N維向量x(即貓圖片的N像素)映射到一個m維矢量(例如，x屬於M個不同類別中的每個類別的概率):</p><div class=pgc-img><img alt=在Python中計算神經網絡的雅可比矩陣 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a6165c8d00564d0384036323611be3e3><p class=pgc-img-caption></p></div><p>在訓練過程中，通常會給輸出附加一個標量損失值——分類的一個典型例子是預測類概率的交叉熵損失。當使用這樣的標量損失時，M = 1，然後通過執行（隨機）梯度下降來學習參數，在此期間重複計算相對於θ的損失函數的梯度。因此，在訓練期間計算標量輸出值相對於網絡參數的梯度是很常見的，並且所有常見的機器學習庫都可以這樣做，通常使用自動微分非常有效。</p><p>然而,在推理時網絡的輸出通常是一個向量(例如,類概率)。在這種情況下，查看網絡的雅可比矩陣可能會很有趣。在這篇文章中,我解釋一下什麼是雅可比矩陣,然後我探索和比較一些可能實現用Python來完成。</p><h1>什麼是雅可比矩陣，為什麼我們會關心？</h1><p>我們稱y為f的輸出向量。f的雅可比矩陣包含y的每個元素的偏導數，相對於輸入x的每個元素：</p><div class=pgc-img><img alt=在Python中計算神經網絡的雅可比矩陣 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fd76cd21e4384391884f264d0daadde3><p class=pgc-img-caption></p></div><p>該矩陣告訴我們神經網絡輸入的local perturbations將如何影響輸出。在某些情況下，這些信息可能很有價值。例如，在用於創造性任務的機器學習（ML）系統中，讓系統為用戶提供一些交互式反饋可以很方便，告訴他們修改每個輸入維度將如何影響每個輸出類。</p><h1>Tensorflow</h1><p>讓我們一起嘗試使用Tensorflow。首先，我們需要一個示例網絡來玩。在這裡，我只對在測試時計算現有網絡f的雅可比行列式感興趣，所以我不專注於訓練。假設我們有一個簡單的網絡[affine → ReLU → affine → softmax]。我們首先定義一些隨機參數：</p><pre>import numpy as npN = 500 # Input sizeH = 100 # Hidden layer sizeM = 10 # Output sizew1 = np.random.randn(N, H) # first affine layer weightsb1 = np.random.randn(H) # first affine layer biasw2 = np.random.randn(H, M) # second affine layer weightsb2 = np.random.randn(M) # second affine layer bias</pre><div class=pgc-img><img alt=在Python中計算神經網絡的雅可比矩陣 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d38e258a3cad4144812929a9e776a9b8><p class=pgc-img-caption></p></div><p>使用Keras，我們按如下方式實施我們的神經網絡：</p><pre>import tensorflow as tffrom tensorflow.keras.layers import Densesess = tf.InteractiveSession()sess.run(tf.initialize_all_variables())model = tf.keras.Sequential()model.add(Dense(H, activation='relu', use_bias=True, input_dim=N))model.add(Dense(O, activation='softmax', use_bias=True, input_dim=O))model.get_layer(index=0).set_weights([w1, b1])model.get_layer(index=1).set_weights([w2, b2])</pre><div class=pgc-img><img alt=在Python中計算神經網絡的雅可比矩陣 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5c47bad01b124d9eb92de1f7a0c6201d><p class=pgc-img-caption></p></div><p>現在讓我們嘗試計算這個神經網絡模型的雅可比行列式。不幸的是，Tensorflow目前沒有提供一種開箱即用的雅可比矩陣計算方法。方法tf.gradients(y,xs)為xs的每一個x都返回sum(dy / dx)，在我們的例子中，是n維的矢量，不是我們想要的。然而，通過計算每個yi的梯度矢量，我們仍然可以計算出雅可比矩陣，並且將輸出分組為矩陣:</p><pre>def jacobian_tensorflow(x):  jacobian_matrix = [] for m in range(M): # We iterate over the M elements of the output vector grad_func = tf.gradients(model.output[:, m], model.input) gradients = sess.run(grad_func, feed_dict={model.input: x.reshape((1, x.size))}) jacobian_matrix.append(gradients[0][0,:])  return np.array(jacobian_matrix)</pre><div class=pgc-img><img alt=在Python中計算神經網絡的雅可比矩陣 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c23921834983444fa69c05e3db2ed801><p class=pgc-img-caption></p></div><p>我們用數值微分來檢驗雅可比矩陣的正確性。下面的函數is_jacobian_correct()採用一個函數來計算雅可比矩陣和前饋函數f:</p><pre>def is_jacobian_correct(jacobian_fn, ffpass_fn): """ Check of the Jacobian using numerical differentiation """ x = np.random.random((N,)) epsilon = 1e-5 """ Check a few columns at random """ for idx in np.random.choice(N, 5, replace=False): x2 = x.copy() x2[idx] += epsilon num_jacobian = (ffpass_fn(x2) - ffpass_fn(x)) / epsilon computed_jacobian = jacobian_fn(x)  if not all(abs(computed_jacobian[:, idx] - num_jacobian) &lt; 1e-3):  return False return Truedef ffpass_tf(x): """ The feedforward function of our neural net """  xr = x.reshape((1, x.size)) return model.predict(xr)[0]is_jacobian_correct(jacobian_tensorflow, ffpass_tf)&gt;&gt; True</pre><div class=pgc-img><img alt=在Python中計算神經網絡的雅可比矩陣 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5ded31e5cad14e7cbeba98cb65c42a97><p class=pgc-img-caption></p></div><p>非常好，這是正確的。讓我們看看這個計算需要多長時間：</p><pre>tic = time.time()jacobian_tf = jacobian_tensorflow(x0, verbose=False)tac = time.time()print('It took %.3f s. to compute the Jacobian matrix' % (tac-tic))&gt;&gt; It took 0.658 s. to compute the Jacobian matrix</pre><div class=pgc-img><img alt=在Python中計算神經網絡的雅可比矩陣 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/da38007bbf3a42aaa39f60f0a30826bd><p class=pgc-img-caption></p></div><p>用CPU需要大約650毫秒。650 ms對於這樣的示例來說太慢了，特別是如果我們在測試時考慮到交互式使用，那麼是否可以做得更好呢？</p><h1>Autograd</h1><p>Autograd是一個非常好的Python庫。要使用它，我們首先必須使用Autograd的封裝Numpy 指定我們的前饋函數f：</p><pre>import autograd.numpy as anpdef ffpass_anp(x): a1 = anp.dot(x, w1) + b1 # affine a1 = anp.maximum(0, a1) # ReLU a2 = anp.dot(a1, w2) + b2 # affine  exps = anp.exp(a2 - anp.max(a2)) # softmax out = exps / exps.sum() return out</pre><div class=pgc-img><img alt=在Python中計算神經網絡的雅可比矩陣 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ea4426a673a2480caeeea755fb77c959><p class=pgc-img-caption></p></div><p>首先，讓我們通過比較之前的Tensorflow前饋函數ffpass_tf()來檢查這個函數是否正確。</p><pre>out_anp = ffpass_anp(x0)out_keras = ffpass_tf(x0)np.allclose(out_anp, out_keras, 1e-4)&gt;&gt; True</pre><p>好的,我們有相同的函數f。現在讓我們計算雅可比矩陣。Autograd很簡單：</p><pre>from autograd import jacobiandef jacobian_autograd(x): return jacobian(ffpass_anp)(x)is_jacobian_correct(jacobian_autograd, ffpass_np)&gt;&gt; True</pre><div class=pgc-img><img alt=在Python中計算神經網絡的雅可比矩陣 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f63400bd5683474f9128b9fbcc831361><p class=pgc-img-caption></p></div><p>看起來很正確。需要多長時間呢？</p><pre>%timeit jacobian_autograd(x0)&gt;&gt; 3.69 ms ± 135 µs</pre><p>我們的Tensorflow實現大約需要650毫秒，Autograd需要3.7毫秒，在這種情況下我們的速度提高了約170倍。當然，使用Numpy指定一個模型並不是很方便，因為Tensorflow和Keras提供了許多有用的函數和開箱即用的訓練設施......，但是現在我們越過這一步並使用Numpy編寫我們的網絡，我們是不是會讓它更快呢？如果你看一下Autograd的jacobian()函數的實現，它仍然是在函數輸出的維度上映射的。這是一個提示，我們可以通過直接依靠Numpy更好的矢量化來改善我們的結果。</p><h1>NumPy</h1><p>如果我們想要一個適當的Numpy實現，我們必須指定每個層的forward 和backward passes，以便自己實現backprop。下面的示例網絡包含 - affine，ReLU和softmax。這裡的層的實現是非常通用的。</p><p>基本上，backward pass現在傳播包含每個網絡輸出的梯度的矩陣（或者，在一般情況下，張量），我們使用Numpy高效矢量化操作：</p><pre>def affine_forward(x, w, b): """ Forward pass of an affine layer :param x: input of dimension (I, ) :param w: weights matrix of dimension (I, O) :param b: biais vector of dimension (O, ) :return output of dimension (O, ), and cache needed for backprop """ out = np.dot(x, w) + b cache = (x, w) return out, cachedef affine_backward(dout, cache): """ Backward pass for an affine layer. :param dout: Upstream Jacobian, of shape (M, O) :param cache: Tuple of: - x: Input data, of shape (I, ) - w: Weights, of shape (I, O) :return the jacobian matrix containing derivatives of the M neural network outputs with respect to this layer's inputs, evaluated at x, of shape (M, I) """ x, w = cache dx = np.dot(dout, w.T) return dxdef relu_forward(x): """ Forward ReLU """ out = np.maximum(np.zeros(x.shape), x) cache = x return out, cachedef relu_backward(dout, cache): """ Backward pass of ReLU :param dout: Upstream Jacobian :param cache: the cached input for this layer :return: the jacobian matrix containing derivatives of the M neural network outputs with respect to this layer's inputs, evaluated at x. """ x = cache dx = dout * np.where(x &gt; 0, np.ones(x.shape), np.zeros(x.shape)) return dxdef softmax_forward(x): """ Forward softmax """ exps = np.exp(x - np.max(x)) s = exps / exps.sum() return s, s def softmax_backward(dout, cache): """ Backward pass for softmax :param dout: Upstream Jacobian :param cache: contains the cache (in this case the output) for this layer """ s = cache ds = np.diag(s) - np.outer(s, s.T) dx = np.dot(dout, ds) return dx</pre><div class=pgc-img><img alt=在Python中計算神經網絡的雅可比矩陣 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5d153fa3c9504db490a91599efb921d7><p class=pgc-img-caption></p></div><p>現在我們已經定義了層，讓我們在forward 和backward passes中使用它們：</p><pre>def forward_backward(x): layer_to_cache = dict() # for each layer, we store the cache needed for backward pass # Forward pass a1, cache_a1 = affine_forward(x, w1, b1) r1, cache_r1 = relu_forward(a1) a2, cache_a2 = affine_forward(r1, w2, b2) out, cache_out = softmax_forward(a2) # backward pass dout = np.diag(np.ones(out.size, )) # the derivatives of each output w.r.t. each output. dout = softmax_backward(dout, cache_out) dout = affine_backward(dout, cache_a2) dout = relu_backward(dout, cache_r1) dx = affine_backward(dout, cache_a1)  return out, dx</pre><div class=pgc-img><img alt=在Python中計算神經網絡的雅可比矩陣 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/94fbf45f00314a49a6cb5c7c2d485d92><p class=pgc-img-caption></p></div><p>前饋輸出是否正確？</p><pre>out_fb = forward_backward(x0)[0]out_tf = ffpass_tf(x0)np.allclose(out_fb, out_tf, 1e-4)&gt;&gt; True</pre><p>我們的雅可比矩陣是否正確？</p><pre>is_jacobian_correct(lambda x: forward_backward(x)[1], ffpass_tf)&gt;&gt; True</pre><p>需要多長時間呢？</p><pre>%timeit forward_backward(x0)&gt;&gt; 115 µs ± 2.38 µs</pre><p>在Autograd需要3.7 ms的情況下，我們現在只需要115μs。</p><h1>結論</h1><p>我已經探索了幾種可能的方法來計算雅可比矩陣，在CPU上使用Tensorflow，Autograd和Numpy。每種方法都有不同的優缺點。如果您已準備好指定層的forward 和backward passes，您可以直接使用Numpy獲得更好性能 。</p><p>我發現這很有用，因為網絡需要在測試時有效運行是很常見的。在這些情況下，花一點時間來確保它們的實現是有效的是值得的。當需要以交互方式計算雅可比矩陣時，這更是如此。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>Python</a></li><li><a>中計算</a></li><li><a>神經</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E9%81%8A%E6%88%B2/e5aa1a1.html alt=使用Python優化神經網絡參數的遺傳算法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=http://p9.pstatp.com/large/pgc-image/ff8157f76c514af2b80a27432ddb690c style=border-radius:25px></a>
<a href=../../tw/%E9%81%8A%E6%88%B2/e5aa1a1.html title=使用Python優化神經網絡參數的遺傳算法>使用Python優化神經網絡參數的遺傳算法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/91eece92.html alt="只需 45 秒，Python 給故宮畫一組手繪圖！" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/df1bd890ffee4a439e9f5142ae42c102 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/91eece92.html title="只需 45 秒，Python 給故宮畫一組手繪圖！">只需 45 秒，Python 給故宮畫一組手繪圖！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8aab963e.html alt=Python手繪圖瞭解一下！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/746c9e6e214b48b2a0215fc9e151cdc8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8aab963e.html title=Python手繪圖瞭解一下！>Python手繪圖瞭解一下！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cfd854c6.html alt=故宮下雪了！我用Python給它畫了一組手繪圖，僅用45秒（附代碼） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/774d2f0a372f48c589ec84dd3a164dd9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cfd854c6.html title=故宮下雪了！我用Python給它畫了一組手繪圖，僅用45秒（附代碼）>故宮下雪了！我用Python給它畫了一組手繪圖，僅用45秒（附代碼）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html alt=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/65c4000bda98898dcdbb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html title=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼>谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b433c17e.html alt="四十、Python模塊random: 偽隨機數據生成與隨機元素抽取" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/25f231d6-c1ce-4cf8-8988-5da509a0c26a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b433c17e.html title="四十、Python模塊random: 偽隨機數據生成與隨機元素抽取">四十、Python模塊random: 偽隨機數據生成與隨機元素抽取</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/04486eba.html alt=Python爬蟲使用selenium爬取群成員信息（全自動實現自動登陸） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/e11e69f643584941aaa2b71ee6ed3d7f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/04486eba.html title=Python爬蟲使用selenium爬取群成員信息（全自動實現自動登陸）>Python爬蟲使用selenium爬取群成員信息（全自動實現自動登陸）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/86b46e6d.html alt=Python爬蟲教程，利用Python採集QQ群成員信息 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/6d6cce76ad48405c9dbb960d4617bcef style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/86b46e6d.html title=Python爬蟲教程，利用Python採集QQ群成員信息>Python爬蟲教程，利用Python採集QQ群成員信息</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7739dc7a.html alt=Python數據可視化Matplotlib，如何在一副圖像中顯示多組柱形圖？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/5d4a0000046e1bea8b90 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7739dc7a.html title=Python數據可視化Matplotlib，如何在一副圖像中顯示多組柱形圖？>Python數據可視化Matplotlib，如何在一副圖像中顯示多組柱形圖？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2bc1496a.html alt=為了更好的深度神經網絡視覺，只需添加反饋（循環） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/17fccfd7096d44eeb3921bbd0dc29a13 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2bc1496a.html title=為了更好的深度神經網絡視覺，只需添加反饋（循環）>為了更好的深度神經網絡視覺，只需添加反饋（循環）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/35db2356.html alt="大數據深度學習的新利器: 快速神經網絡訓練:P-network" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d172925963f2465aa131058c05cd72f9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/35db2356.html title="大數據深度學習的新利器: 快速神經網絡訓練:P-network">大數據深度學習的新利器: 快速神經網絡訓練:P-network</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f93ce821.html alt=失戀很痛？那你是沒被三叉神經痛過 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/cd16c354613244babe0f75d0aa3c7931 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f93ce821.html title=失戀很痛？那你是沒被三叉神經痛過>失戀很痛？那你是沒被三叉神經痛過</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/05b8f164.html alt=Python中的多進程 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/49dd44b999924b69bd3396709ecacaf4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/05b8f164.html title=Python中的多進程>Python中的多進程</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/389d4437.html alt="Python常用算法學習(5) 樹二叉樹（原理+代碼）-最全總結" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d3d5fd4ef98c4a8e8dc9095eeef052a6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/389d4437.html title="Python常用算法學習(5) 樹二叉樹（原理+代碼）-最全總結">Python常用算法學習(5) 樹二叉樹（原理+代碼）-最全總結</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0fa274c2.html alt="七十九、Python | Leetcode 二叉樹系列（上篇）" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a6c9fa6432a3419a8a60bd334f978a11 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0fa274c2.html title="七十九、Python | Leetcode 二叉樹系列（上篇）">七十九、Python | Leetcode 二叉樹系列（上篇）</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>