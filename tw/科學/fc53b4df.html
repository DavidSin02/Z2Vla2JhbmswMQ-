<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>一文詳盡之支持向量機算法 | 极客快訊</title><meta property="og:title" content="一文詳盡之支持向量機算法 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/eb3419f34fb5463eb06573b2573e25cb"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/fc53b4df.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/fc53b4df.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/fc53b4df.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/fc53b4df.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/fc53b4df.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/fc53b4df.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e5%ad%b8/fc53b4df.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e5%ad%b8/fc53b4df.html><meta property="article:published_time" content="2020-10-29T21:13:00+08:00"><meta property="article:modified_time" content="2020-10-29T21:13:00+08:00"><meta name=Keywords content><meta name=description content="一文詳盡之支持向量機算法"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E5%AD%B8/fc53b4df.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>一文詳盡之支持向量機算法</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E5%AD%B8.html>科學</a></span></div><div class=post-content><p>寄語：本文介紹了SVM的理論，細緻說明了“間隔”和“超平面 ” 兩個概念；隨後， 闡述 了 如何最大化間隔並區分了軟硬間隔SVM；同時，介紹了SVC問題的應用。最後，用SVM 乳腺癌診斷 經典數據集，對SVM進行了深入的理解。</p><p>支持向量機（support vector machines, SVM）是一種二分類模型，它的基本模型是定義在特徵空間上的間隔最大的線性分類器，間隔最大使它有別於感知機。</p><p>SVM的的學習策略就是間隔最大化，可形式化為一個求解凸二次規劃的問題，也等價於正則化的合頁損失函數的最小化問題。SVM的的學習算法就是求解凸二次規劃的最優化算法。</p><p>下圖為SVM的分類效果顯示，可以發現，不管是線性還是非線性，SVM均表現良好。</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eb3419f34fb5463eb06573b2573e25cb><p class=pgc-img-caption></p></div><h2 class=pgc-h-arrow-right>學習框架</h2><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0370ab8b384c42518e6c705314a7c477><p class=pgc-img-caption></p></div><p>後臺回覆 <strong>SVM </strong>可下載SVM學習框架高清導圖</p><h2 class=pgc-h-arrow-right>SVM理論</h2><p>支持向量機(Support Vector Machine：SVM)的目的是用訓練數據集的間隔最大化找到一個最優分離超平面。</p><p>下邊用一個例子來理解下間隔和分離超平面兩個概念。現在有一些人的身高和體重數據，將它們繪製成散點圖，是這樣的：</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1095b20f24a748ba9b42632e8265da01><p class=pgc-img-caption></p></div><p>如果現在給你一個未知男女的身高和體重，你能分辨出性別嗎？直接將已知的點劃分為兩部分，這個點落在哪一部分就對應相應的性別。那就可以畫一條直線，直線以上是男生，直線以下是女生。</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/48df6fbabb9c4eb9bc694d6c85b1be47><p class=pgc-img-caption></p></div><p>問題來了，現在這個是一個二維平面，可以畫直線，如果是三維的呢？該怎麼畫?我們知道一維平面是點，二維平面是線，三維平面是面。</p><p>對的，那麼注意，今天的第一個概念：超平面是平面的一般化：</p><ul><li>在一維的平面中，它是點</li><li>在二維的平面中，它是線</li><li>在三維的平面中，它是面</li><li>在更高的維度中，我們稱之為超平面</li></ul><p>注意：後面的直線、平面都直接叫超平面了。</p><p>繼續剛才的問題，我們剛才是通過一個分離超平面分出了男和女，這個超平面唯一嗎？很明顯，並不唯一，這樣的超平面有若干個。</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1b298a4d9cc64441aa187bb1fcc2c810><p class=pgc-img-caption></p></div><p>那麼問題來了，既然有若干個，那肯定要最好的，這裡最好的叫最優分離超平面。如何在眾多分離超平面中選擇一個最優分離超平面？下面這兩個分離超平面，你選哪個？綠色的還是黑色的？</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5bc1ef3498b648d58fa1d07478d73af5><p class=pgc-img-caption></p></div><p>對，當然是黑色的，可是原理是什麼？很簡單，原理有兩個，分別是：</p><ul><li>正確的對訓練數據進行分類</li><li>對未知數據也能很好的分類</li></ul><p>黑色的分離超平面能夠對訓練數據很好的分類，當新增未知數據時，黑色的分離超平面泛化能力也強於綠色。深究一下，為什麼黑色的要強於綠色？原理又是什麼？</p><p>其實很簡單：最優分離超平面其實是和兩側樣本點有關，而且只和這些點有關。怎麼理解這句話呢，我們看張圖：</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/02835409566244988139798e94719570><p class=pgc-img-caption></p></div><p>其中當間隔達到最大，兩側樣本點的距離相等的超平面為最優分離超平面。注意，今天的第二個概念：對應上圖，Margin對應的就是最優分離超平面的間隔，此時的間隔達到最大。</p><p>一般來說，間隔中間是無點區域，裡面不會有任何點（理想狀態下）。給定一個超平面，我們可以就算出這個超平面與和它最接近的數據點之間的距離。那麼間隔（Margin）就是二倍的這個距離。</p><p>如果還是不理解為什麼這個分離超平面就是最優分離超平面，那你在看這張圖。</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/864d5c6b75d1455d81fb78666b71a5d4><p class=pgc-img-caption></p></div><p>在這張圖裡面間隔MarginB小於上張圖的MarginA。當出現新的未知點，MarginB分離超平面的泛化能力不如MarginA，用MarginB的分離超平面去分類，錯誤率大於MarginA</p><p>總結一下</p><p>支持向量機是為了通過間隔最大化找到一個最優分離超平面。在決定分離超平面的時候，只有極限位置的那兩個點有用，其他點根本沒有大作用，因為只要極限位置離得超平面的距離最大，就是最優的分離超平面了。</p><h2 class=pgc-h-arrow-right>如何確定最大化間隔</h2><p>如果我們能夠確定兩個平行超平面，那麼兩個超平面之間的最大距離就是最大化間隔。 看個 圖你就都明白了：</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/acc28826b36b4e408e318d0cea758c0d><p class=pgc-img-caption></p></div><p>左右兩個平行超平面將數據完美的分開，我們只需要計算上述兩個平行超平面的距離即可。所以，我們找到最大化間隔：</p><ul><li>找到兩個平行超平面，可以劃分數據集並且兩平面之間沒有數據點</li><li>最大化上述兩個超平面</li></ul><p>1. 確定兩個平行超平面</p><p>怎麼確定兩個平行超平面？我們知道一條直線的數學方程是：y-ax+b=0，而超平面會被定義成類似的形式：</p><p>推廣到n維空間，則超平面方程中的w、x分別為：</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3db4c0dcb7ad43e8a2bf08cb9410bb8f><p class=pgc-img-caption></p></div><p>如何確保兩超平面之間沒有數據點？我們的目的是通過兩個平行超平面對數據進行分類，那我們可以這樣定義兩個超平面。</p><p>對於每一個向量x i ：滿足：</p><p>或者</p><p>也就是這張圖：所有的紅點都是1類，所有的藍點都是−1類。</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/acc28826b36b4e408e318d0cea758c0d><p class=pgc-img-caption></p></div><p>整理一下上面的兩個超平面：</p><p>不等式兩邊同時乘以 yi，-1類的超平面yi=-1，要改變不等式符號，合併後得</p><p>ok，記住上面的約束條件。</p><p><strong>2. 確定間隔</strong></p><p>如何 求兩個平行超平面的間隔呢？我們可以先做這樣一個假設：</p><ul><li>是滿足約束 的超平面</li><li>是滿足約束 的超平面</li><li>是 上的一點</li></ul><p>則 到平面 的垂直距離 就是我們要的間隔。</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b91a1905d40d434f9b386fe9a9613c27><p class=pgc-img-caption></p></div><p>這個間隔是可以通過計算出來的，推導還需要一些步驟，直接放結果了就：</p><p>其中||w||表示w的二範數，求所有元素的平方和，然後在開方。比如，二維平面下：</p><p>可以發現，w 的模越小，間隔m 越大</p><h2 class=pgc-h-arrow-right><strong>3. 確定目標</strong></h2><p>我們的間隔最大化，最後就成了這樣一個問題：</p><p>了其中w和b，我們的最優分離超平面就確定了，目的也就達到了。</p><p>上面的最優超平面問題是一個凸優化問題，可以轉換成了拉格朗日的對偶問題，判斷是否滿足KKT條件，然後求解。 上一句話包含的知識是整個SVM的核心，涉及到大量的公式推導。</p><p>此處 略過 推導的步驟，若想了解推導過程可直接百度。你只需要知道它的目的就是為了找出一個最優分離超平面。 就假設我們已經解出了最大間隔，找到了最優分離超平面，它是這樣的：</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/475c2ebf90954a8397b09e6e1af1580e><p class=pgc-img-caption></p></div><p>除去上面我們對最大間隔的推導計算，剩下的部分其實是不難理解的。 從上面過程，我們可以發現，其實最終分類超平面的確定依賴於部分極限位置的樣本點，這叫做支持向量。</p><p>由於支持向量在確定分離超平面中起著決定性作用，所有將這類模型叫做支持向量機。</p><p>我們在上面圖中的點都是線性可分的，也就是一條線（或一個超平面）可以很容易的分開的。 但是實際情況不都是這樣，比如有的女生身高比男生高，有的男生體重比女生都輕， 像這種存在噪聲點分類，應該怎麼處理？</p><h2 class=pgc-h-arrow-right>針對樣本的SVM</h2><p>1. 硬間隔線性SVM</p><p>上面例子中提到的樣本點都是線性可分的，我們就可以通過分類將樣本點完全分類準確，不存在分類錯誤的情況，這種叫硬間隔，這類模型叫做硬間隔線性SVM。</p><p>2. 軟間隔線性SVM</p><p>同樣的，可以通過分類將樣本點不完全分類準確，存在少部分分類錯誤的情況，這叫軟間隔，這類模型叫做軟間隔線性SVM。</p><p>不一樣的是，因為有分類錯誤的樣本點，但我們仍需要將錯誤降至最低，所有需要添加一個乘法項來進行浮動，所有此時求解的最大間隔就變成了這樣：</p><p>硬間隔和軟間隔都是對線性可分的樣本點進行分類，那如果樣本點本身就不線性可分？ 舉個例子： 下面這幅圖</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/af324b01cac94d14bd59aa7c48471ac4><p class=pgc-img-caption></p></div><p>樣本點並不是線性可分的，這種問題應該怎麼處理呢？ 解決這個問題之前，先看一下這個小短視頻：</p><p>視頻中是將平面中的樣本點映射到三維空間中，使用一個平面將樣本線性可分。</p><p>所以 我們需要一種方法，可以將樣本從原始空間映射到一個更高緯的空間中，使得樣本在新的空間中線性可分，即： 核函數。 在非線性SVM中，核函數的選擇關係到SVM的分類效果。</p><p>幸好的是，我們有多種核函數：線性核函數、多項式核函數、高斯核函數、sigmoid核函數等等，甚至你還可以將這些核函數進行組合，以達到最優線性可分的效果</p><p>核函數瞭解到應該就差不多了，具體的實現我們在下一節的實戰再說。</p><h2 class=pgc-h-arrow-right>多分類SVM</h2><p>前面提到的所有例子最終都指向了二分類，現實中可不止有二分類，更多的是多分類問題。 那麼多 分類應該怎麼分呢？ 有兩種方法： 一對多和一對一。</p><p><strong>1. 一對多法</strong></p><p>一對多法講究的是將所有的分類分成兩類：一類只包含一個分類，另一類包含剩下的所有分類</p><p>舉個例子：現在有A、B、C、D四種分類，根據一對多法可以這樣分：</p><ul><li>①：樣本A作為正集，B、C、D為負集</li><li>②：樣本B作為正集，A、C、D為負集</li><li>③：樣本C作為正集，A、B、D為負集</li><li>④：樣本D作為正集，A、B、C為負集</li></ul><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/faf6107becd1404782cbef5671aa1a03><p class=pgc-img-caption></p></div><p>該方法分類速度較快，但訓練速度較慢，添加新的分類，需要重新構造分 類器。</p><p><strong>2. 一對一法</strong></p><p>一對一法講究的是從所有分類中只取出兩類，一個為正類一個為父類</p><p>再舉個例子：現在有A、B、C三種分類，根據一對一法可以這樣分：</p><ul><li>①分類器：樣本A、B</li><li>②分類器：樣本A、C</li><li>③分類器：樣本B、C</li></ul><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c90f50f709a2481eb4565b1d6fa20290><p class=pgc-img-caption></p></div><p>該方法的優點是：當新增一類時，只需要訓練與該類相關的分類器即可，訓練速度較快。缺點是：當類的種類K很多時，分類器個數K(K-1)/2會很多，訓練和測試時間較慢。</p><h2 class=pgc-h-arrow-right>SVC，Support Vector Classification</h2><p>我們知道針對樣本有線性SVM和非線性SVM。 同樣的在sklearn 中提供的這兩種的實現，分別是： LinearSVC和SVC。</p><p>SVC : Support Vector Classification 用支持向量機處理分類問題</p><p>SV R : Support Vector R egressi on 用支持向量機 處理迴歸問題</p><p><strong>1. SVC和LinearSVC</strong></p><p>LinearSVC是線性分類器，用於處理線性分類的數據，且只能使用線性核函數。 SVC是非線性分類器，即可以 使用線性核函數進行線性劃分，也可以使用高維核函數 進行非線性劃分。</p><p><strong>2. SVM的使用</strong></p><p>在sklearn 中，一句話調用SVM，</p><pre><code>from sklearn import svm</code></pre><p>主要說一下SVC的創建，因為它的參數比較重要</p><pre><code>model = svm.SVC(kernel='rbf', C=1.0, gamma=0.001)</code></pre><pre><code>分別解釋一下三個重要參數：</code></pre><ul><li>kernel代表核函數的選擇，有四種選擇，默認rbf（即高斯核函數）</li><li>參數C代表目標函數的懲罰係數，默認情況下為 1.0</li><li>參數gamma代表核函數的係數，默認為樣本特徵數的倒數</li></ul><p>其中kernel代表的四種核函數分別是：</p><ul><li>linear：線性核函數，在數據線性可分的情況下使用的</li><li>poly：多項式核函數，可以將數據從低維空間映射到高維空間</li><li>rbf：高斯核函數，同樣可以將樣本映射到高維空間，但所需的參數較少，通常性能不錯</li><li>sigmoid：sigmoid核函數，常用在神經網絡的映射中</li></ul><p>SVM的使用就介紹這麼多，來實戰測試一下。</p><h2 class=pgc-h-arrow-right>經典數據集實戰</h2><p><strong>1. 數據集</strong></p><p>SVM的經典數據集：乳腺癌診斷。 醫療人員採集了患者乳腺腫塊經過細針穿刺 (FNA) 後的數字化圖像，並且對這些數字圖像進行了特徵提取，這些特徵可以描述圖像中的細胞核呈現。 通過這些特徵可以將腫瘤分成良性和惡性。</p><p>本次數據一共569條 、32個字段，先來看一下具體數據字段吧 ：</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8c63e1a05994495dab3089d012f016b7><p class=pgc-img-caption></p></div><p>其中mean結尾的代表平均值、se結尾的代表標準差、worst結尾代表最壞值（這裡具體指腫瘤的特徵最大值）。 所有其實主要有10個特徵字段，一個id字段，一個預測類別字段。 我們的目的是通過給出的特徵字段來預測腫瘤是良性還是惡性。</p><p><strong>2. 數據EDA</strong></p><p>EDA:Exploratory Data Analysis探索性數據分析， 先 來看數據的 分佈情況：</p><pre><code>df_data.info()</code></pre><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0ccf11e01a884efb86bb3a957893a3ec><p class=pgc-img-caption></p></div><p>一共569條、32個字段。 32個字段中1個object類型，一個int型id，剩下的都是float 類型。 另外： 數據中不存在缺失值。</p><p>大膽猜測一下，object類型可能是類別型數據，即最終的預測類型，需要進行處理，先記下 。 再來看連續性數據的統計數據：</p><pre><code>df_data.describe()</code></pre><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/342d76d26f694b4583c6f65ad4b3ca18><p class=pgc-img-caption></p></div><p>好像也沒啥問題（其實因為這個數據本身比較規整），可 直接開始特徵工程吧。</p><p>3. 特徵工程</p><p>首先就是將類別數據連續化</p><pre><code>"""2. 類別特徵向量化"""le = preprocessing.LabelEncoder()le.fit(df_data['diagnosis'])df_data['diagnosis'] = le.transform(df_data['diagnosis'])</code></pre><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a8d0dd4f620d40c79b6d952277e0ebd0><p class=pgc-img-caption></p></div><p>再來觀察每一個特徵的三個指標：均值、標準差和最大值。 優先選擇均值，最能體現該指特徵的整體情況。</p><pre><code> """3. 提取特徵""" # 提取所有mean 字段和label字段df_data_X = df_data.filter(regex='_mean')df_data_y = df_data['diagnosis']</code></pre><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c07aff84d68e4bdda6d0e9d5b000f47e><p class=pgc-img-caption></p></div><p>現在還有十個特徵，我們通過熱力圖來看一下特徵之間的關係 。</p><pre><code>#熱力圖查看特徵之間的關係sns.heatmap(df_data[df_data_X.columns].corr(), linewidths=0.1, vmax=1.0, square=True,            cmap=sns.color_palette('RdBu', n_colors=256),            linecolor='white', annot=True)plt.title('the feature of corr')plt.show()</code></pre><p>熱力圖是這樣的：</p><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e150c990d2ee4248ad335a64a9a6eff4><p class=pgc-img-caption></p></div><p>我們發現radius_mean、perimeter_mean和area_mean這三個特徵強相關，那我們只保留一個就行了。 這裡保留熱力圖裡面得分最高的perimeter_mean。</p><p>最後一步， 因為是連續數值，最好對其進行標準化。 標準化之後的數據是這樣的：</p><pre><code> df_data_X = df_data_X.drop(['radius_mean', 'area_mean'], axis=1)"""5. 進行特徵歸一化/縮放"""scaler = preprocessing.StandardScaler()df_data_X = scaler.fit_transform(df_data_X)return df_data_X, df_data_y</code></pre><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a3d95334594c4cc5b515ee50d354756b><p class=pgc-img-caption></p></div><p><strong>4. 訓練模型</strong></p><p>上面已經做好了特徵工程，直接塞進模型看看效果怎麼樣。 因為並不知道數據樣本到底是否線性可分，所有我們都來試一下兩種算法。 先來看看LinearSVC 的效果</p><pre><code>"""1.1. 第一種模型驗證方法"""    # 切分數據集    X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.2)    # 創建SVM分類器    model = svm.LinearSVC()    # 用訓練集做訓練    model.fit(X_train, y_train)    # 用測試集做預測    pred_label = model.predict(X_test)    print('準確率: ', metrics.accuracy_score(pred_label, y_test))</code></pre><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e0c00a91d76845cfb8edb542a4d3379f><p class=pgc-img-caption></p></div><p>效果很好，簡直好的不行，在此，並沒有考慮 準確率。</p><p>ok，還有SVC的效果。 因為SVC需要設置參數，直接通過網格搜索讓機器自己找到最優參數， 效果更好。</p><pre><code>"""2. 通過網格搜索尋找最優參數"""    parameters = {        'gamma': np.linspace(0.0001, 0.1),        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],    }    model = svm.SVC()    grid_model = GridSearchCV(model, parameters, cv=10, return_train_score=True)    grid_model.fit(X_train, y_train)    # 用測試集做預測    pred_label = grid_model.predict(X_test)    print('準確率: ', metrics.accuracy_score(pred_label, y_test))    # 輸出模型的最優參數    print(grid_model.best_params_)</code></pre><div class=pgc-img><img alt=一文詳盡之支持向量機算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0b7d8b31841647b38aeaccd9757d8a1d><p class=pgc-img-caption></p></div><p>可以看出，最終模型還是選擇rbf高斯核函數，果然實至名歸。 主要是通過數據EDA+特徵工程完成了數據方面的工作，然後通過交叉驗證+網格搜索確定了最優模型和最優參數。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>詳盡</a></li><li><a>向量</a></li><li><a>算法</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/ed4abe39.html alt=算法小專欄：散列表（二） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a831970b0ccf4e4cbe591777ebd3f2a3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ed4abe39.html title=算法小專欄：散列表（二）>算法小專欄：散列表（二）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/38aea254.html alt=七大查找算法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/15393515221731c57aa8da1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/38aea254.html title=七大查找算法>七大查找算法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/51e4e55b.html alt=掌握算法-散列 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7fe8d19cb78241e999d77102bee7e16c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/51e4e55b.html title=掌握算法-散列>掌握算法-散列</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e1a84312.html alt=127：平面向量(基底法，容易掉坑裡) class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/798fe5150f7e41db847e88330e26f9d1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e1a84312.html title=127：平面向量(基底法，容易掉坑裡)>127：平面向量(基底法，容易掉坑裡)</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d2c8de48.html alt=C#算法系列（1）——二叉樹 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d2c8de48.html title=C#算法系列（1）——二叉樹>C#算法系列（1）——二叉樹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d241e092.html alt=算法--二叉樹（平衡二叉樹、搜索二叉樹、完全二叉樹） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/9b2026e861ad49e88e1e124dc67edb32 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d241e092.html title=算法--二叉樹（平衡二叉樹、搜索二叉樹、完全二叉樹）>算法--二叉樹（平衡二叉樹、搜索二叉樹、完全二叉樹）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/bf5399b7.html alt=算法題—完全二叉樹 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/ef7d8ad6eaca4524a71e1e5d1277532b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bf5399b7.html title=算法題—完全二叉樹>算法題—完全二叉樹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d61d6a42.html alt=支持向量機（第八章） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/153985032133768ee02ea64 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d61d6a42.html title=支持向量機（第八章）>支持向量機（第八章）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/62900ef0.html alt=一文了解邏輯迴歸和支持向量機的異同 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1539671186198856310f156 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/62900ef0.html title=一文了解邏輯迴歸和支持向量機的異同>一文了解邏輯迴歸和支持向量機的異同</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1b7b6866.html alt=基於一致性算法的微網分佈式有功均衡控制 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/39fa00034e1eb30ffce3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1b7b6866.html title=基於一致性算法的微網分佈式有功均衡控制>基於一致性算法的微網分佈式有功均衡控制</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a61341cf.html alt=深入淺出排序算法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/c898a3bab26542c2965d2bc5bebf9bd8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a61341cf.html title=深入淺出排序算法>深入淺出排序算法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8257f1b4.html alt=程序員那些必須掌握的排序算法(上) class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/c32d187d725f4bd59225a5d09a38cb37 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8257f1b4.html title=程序員那些必須掌握的排序算法(上)>程序員那些必須掌握的排序算法(上)</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/500d3e1f.html alt="算法 － 七大排序算法詳細介紹" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9275d53c0e5f432294fce6dd4cfef236 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/500d3e1f.html title="算法 － 七大排序算法詳細介紹">算法 － 七大排序算法詳細介紹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e9a28bf0.html alt=算法之旅｜冒泡排序法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/37e00004d03a88913f18 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e9a28bf0.html title=算法之旅｜冒泡排序法>算法之旅｜冒泡排序法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1ca491e3.html alt=算法之旅｜快速排序法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/3b0e0000a716d98c3cba style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1ca491e3.html title=算法之旅｜快速排序法>算法之旅｜快速排序法</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>