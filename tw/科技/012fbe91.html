<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>Transformers 庫中常見的用例 | 极客快訊</title><meta property="og:title" content="Transformers 庫中常見的用例 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><meta property="article:published_time" content="2020-11-14T21:01:26+08:00"><meta property="article:modified_time" content="2020-11-14T21:01:26+08:00"><meta name=Keywords content><meta name=description content="Transformers 庫中常見的用例"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/012fbe91.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>Transformers 庫中常見的用例</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>本章介紹使用Transformers庫時最常見的用例。可用的模型允許許多不同的配置，並且在用例中具有很強的通用性。這裡介紹了最簡單的方法，展示了諸如問答、序列分類、命名實體識別等任務的用法。</p><p>這些示例利用Auto Model，這些類將根據給定的checkpoint實例化模型，並自動選擇正確的模型體系結構。有關詳細信息，請查看：AutoModel文檔。請隨意修改代碼，使其更具體，並使其適應你的特定用例。</p><ul><li>為了使模型能夠在任務上良好地執行，必須從與該任務對應的checkpoint加載模型。這些checkpoint通常是在大量數據上預先訓練的，並針對特定任務進行微調。這意味著：並非所有模型都針對所有任務進行了微調。如果要對特定任務的模型進行微調，可以利用examples目錄中的run\$task.py腳本。</li><li>微調模型是在特定的數據集上微調的。此數據集可能與你的用例和域重疊，也可能不重疊。如前所述，你可以利用示例腳本來微調模型，也可以創建自己的訓練腳本。</li></ul><p>為了對任務進行推理，庫提供了幾種機制：</p><ul><li>管道是非常易於使用的抽象，只需要兩行代碼。</li><li>直接將模型與Tokenizer(PyTorch/TensorFlow)結合使用來使用模型的完整推理。這種機制稍微複雜，但是更強大。</li></ul><p>這裡展示了兩種方法。</p><blockquote><p>請注意，這裡介紹的所有任務都利用了在預訓練模型針對特定任務進行微調後的模型。加載未針對特定任務進行微調的checkpoint時，將只加載transformer層，而不會加載用於該任務的附加層，從而隨機初始化該附加層的權重。這將產生隨機輸出。</p></blockquote><h1 class=pgc-h-arrow-right>序列分類</h1><p>序列分類是根據已經給定的類別然後對序列進行分類的任務。序列分類的一個例子是GLUE數據集，它就是完全基於該任務的。如果你想在GLUE序列分類任務上微調模型，可以利用run_GLUE.py或run_tf_GLUE.py腳本。</p><p>下面是一個使用管道進行情緒分析的例子：識別該序列是積極的還是消極的。它利用sst2上的微調模型，這是一個GLUE任務。</p><pre><code>from transformers import pipelinenlp = pipeline(&#34;sentiment-analysis&#34;)print(nlp(&#34;I hate you&#34;))print(nlp(&#34;I love you&#34;))</code></pre><p>這將返回一個標籤(“積極”或“消極”)和一個分數，如下所示：</p><pre><code>[{&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.9991129}][{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.99986565}]</code></pre><p>下面是一個使用模型進行序列分類的示例，以確定兩個序列是否是彼此的解釋。該過程如下：</p><ul><li>從checkpoint名稱實例化一個tokenizer和一個模型。該模型被識別為一個BERT模型，並用存儲在checkpoint中的權重加載它。</li><li>從這兩句話中構建一個序列，使用正確的特定於模型的分隔符標記類型id和注意力掩碼(encode()和encode_plus()處理這個問題)</li><li>將這個序列傳遞到模型中，以便將其分類到兩個可用的類中的一個：0(不是解釋)和1(是解釋)</li><li>計算結果的softmax獲取類的概率</li><li>打印結果</li></ul><p>Pytorch代碼</p><pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassificationimport torchtokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased-finetuned-mrpc&#34;)model = AutoModelForSequenceClassification.from_pretrained(&#34;bert-base-cased-finetuned-mrpc&#34;)classes = [&#34;not paraphrase&#34;, &#34;is paraphrase&#34;]sequence_0 = &#34;The company HuggingFace is based in New York City&#34;sequence_1 = &#34;Apples are especially bad for your health&#34;sequence_2 = &#34;HuggingFace&#39;s headquarters are situated in Manhattan&#34;paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, return_tensors=&#34;pt&#34;)not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=&#34;pt&#34;)paraphrase_classification_logits = model(**paraphrase)[0]not_paraphrase_classification_logits = model(**not_paraphrase)[0]paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]print(&#34;Should be paraphrase&#34;)for i in range(len(classes)):    print(f&#34;{classes[i]}: {round(paraphrase_results[i] * 100)}%&#34;)print(&#34;\nShould not be paraphrase&#34;)for i in range(len(classes)):    print(f&#34;{classes[i]}: {round(not_paraphrase_results[i] * 100)}%&#34;)</code></pre><p>TensorFlow代碼</p><pre><code>from transformers import AutoTokenizer, TFAutoModelForSequenceClassificationimport tensorflow as tftokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased-finetuned-mrpc&#34;)model = TFAutoModelForSequenceClassification.from_pretrained(&#34;bert-base-cased-finetuned-mrpc&#34;)classes = [&#34;not paraphrase&#34;, &#34;is paraphrase&#34;]sequence_0 = &#34;The company HuggingFace is based in New York City&#34;sequence_1 = &#34;Apples are especially bad for your health&#34;sequence_2 = &#34;HuggingFace&#39;s headquarters are situated in Manhattan&#34;paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, return_tensors=&#34;tf&#34;)not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=&#34;tf&#34;)paraphrase_classification_logits = model(paraphrase)[0]not_paraphrase_classification_logits = model(not_paraphrase)[0]paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]print(&#34;Should be paraphrase&#34;)for i in range(len(classes)):    print(f&#34;{classes[i]}: {round(paraphrase_results[i] * 100)}%&#34;)print(&#34;\nShould not be paraphrase&#34;)for i in range(len(classes)):    print(f&#34;{classes[i]}: {round(not_paraphrase_results[i] * 100)}%&#34;)</code></pre><p>這將輸出以下結果：</p><pre><code>Should be paraphrasenot paraphrase: 10%is paraphrase: 90%Should not be paraphrasenot paraphrase: 94%is paraphrase: 6%</code></pre><h1 class=pgc-h-arrow-right>抽取式問答</h1><p>抽取式問答是從給定問題的文本中抽取答案的任務。問答數據集的一個例子是SQuAD數據集，它完全基於該任務。如果你想在團隊任務中微調模型，可以利用run_SQuAD.py。</p><p>下面是一個使用管道進行問答的示例：從給定問題的文本中提取答案。它利用了一個小隊的微調模型。</p><pre><code>from transformers import pipelinenlp = pipeline(&#34;question-answering&#34;)context = r&#34;&#34;&#34;Extractive Question Answering is the task of extracting an answer from a text given a question. An example of aquestion answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tunea model on a SQuAD task, you may leverage the `run_squad.py`.&#34;&#34;&#34;print(nlp(question=&#34;What is extractive question answering?&#34;, context=context))print(nlp(question=&#34;What is a good example of a question answering dataset?&#34;, context=context))</code></pre><p>這將返回從文本中提取的答案，一個置信度，以及“開始”和“結束”值，這些值是提取的答案在文本中的位置。</p><pre><code>{&#39;score&#39;: 0.622232091629833, &#39;start&#39;: 34, &#39;end&#39;: 96, &#39;answer&#39;: &#39;the task of extracting an answer from a text given a question.&#39;}{&#39;score&#39;: 0.5115299158662765, &#39;start&#39;: 147, &#39;end&#39;: 161, &#39;answer&#39;: &#39;SQuAD dataset,&#39;}</code></pre><p>下面是一個使用模型和Tokenizer回答問題的示例。該過程如下：</p><ul><li>從checkpoint名稱實例化一個tokenizer和一個模型。該模型被識別為一個BERT模型，並用存儲在checkpoint中的權重加載它。</li><li>定義一段文本和幾個問題。</li><li>遍歷問題並根據文本和當前問題構建一個序列，使用正確的模型特定分隔符標記類型id和注意力掩碼將此序列傳遞到模型中。這將輸出整個序列標記(問題和文本)的開始位置和結束位置的一系列分數。</li><li>計算結果的softmax以獲得從標記的開始位置和停止位置對應的概率</li><li>將這些標記轉換為字符串。</li><li>打印結果</li></ul><p>Pytorch代碼</p><pre><code>from transformers import AutoTokenizer, AutoModelForQuestionAnsweringimport torchtokenizer = AutoTokenizer.from_pretrained(&#34;bert-large-uncased-whole-word-masking-finetuned-squad&#34;)model = AutoModelForQuestionAnswering.from_pretrained(&#34;bert-large-uncased-whole-word-masking-finetuned-squad&#34;)text = r&#34;&#34;&#34; Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purposearchitectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and NaturalLanguage Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability betweenTensorFlow 2.0 and PyTorch.&#34;&#34;&#34;questions = [    &#34;How many pretrained models are available in Transformers?&#34;,    &#34;What does Transformers provide?&#34;,    &#34;Transformers provides interoperability between which frameworks?&#34;,]for question in questions:    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=&#34;pt&#34;)    input_ids = inputs[&#34;input_ids&#34;].tolist()[0]    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)    answer_start_scores, answer_end_scores = model(**inputs)    answer_start = torch.argmax(        answer_start_scores    )  # Get the most likely beginning of answer with the argmax of the score    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))    print(f&#34;Question: {question}&#34;)    print(f&#34;Answer: {answer}\n&#34;)</code></pre><p>TensorFlow代碼</p><pre><code>from transformers import AutoTokenizer, TFAutoModelForQuestionAnsweringimport tensorflow as tftokenizer = AutoTokenizer.from_pretrained(&#34;bert-large-uncased-whole-word-masking-finetuned-squad&#34;)model = TFAutoModelForQuestionAnswering.from_pretrained(&#34;bert-large-uncased-whole-word-masking-finetuned-squad&#34;)text = r&#34;&#34;&#34; Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purposearchitectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and NaturalLanguage Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability betweenTensorFlow 2.0 and PyTorch.&#34;&#34;&#34;questions = [    &#34;How many pretrained models are available in Transformers?&#34;,    &#34;What does Transformers provide?&#34;,    &#34;Transformers provides interoperability between which frameworks?&#34;,]for question in questions:    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=&#34;tf&#34;)    input_ids = inputs[&#34;input_ids&#34;].numpy()[0]    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)    answer_start_scores, answer_end_scores = model(inputs)    answer_start = tf.argmax(        answer_start_scores, axis=1    ).numpy()[0]  # Get the most likely beginning of answer with the argmax of the score    answer_end = (        tf.argmax(answer_end_scores, axis=1) + 1    ).numpy()[0]  # Get the most likely end of answer with the argmax of the score    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))    print(f&#34;Question: {question}&#34;)    print(f&#34;Answer: {answer}\n&#34;)</code></pre><p>這將輸出預測答案後的問題：</p><pre><code>Question: How many pretrained models are available in Transformers?Answer: over 32 +Question: What does Transformers provide?Answer: general - purpose architecturesQuestion: Transformers provides interoperability between which frameworks?Answer: tensorflow 2 . 0 and pytorch</code></pre><h1 class=pgc-h-arrow-right>語言建模</h1><p>語言建模是將一個模型與一個特定領域的語料庫相匹配的任務。所有流行的基於transformer的模型都是使用語言建模的變體來訓練的，例如掩碼語言建模的BERT、因果語言建模的GPT-2。</p><p>語言建模在預訓練之外也很有用，例如將模型分佈轉換為特定領域：使用在非常大的語料庫上訓練的語言模型，然後將其微調到新聞數據集或科學論文上，例如LysandreJik/arxiv nlp(https://huggingface.co/lysandre/arxiv-nlp)。</p><h1 class=pgc-h-arrow-right>掩碼語言建模</h1><p>掩碼語言建模是用掩碼標記對序列中的標記進行掩碼，並提示模型用適當的標記填充該掩碼的任務。這允許模型同時處理右上下文(掩碼右側的標記)和左上下文(掩碼左側的標記)。這樣的訓練為需要雙向背景的下游任務(如SQuAD)奠定了堅實的基礎。</p><p>下面是使用管道來替換序列中的掩碼的示例：</p><pre><code>from transformers import pipelinenlp = pipeline(&#34;fill-mask&#34;)print(nlp(f&#34;HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.&#34;))</code></pre><p>這將在Tokenizer詞彙表中輸出填充了掩碼的序列、置信度得分以及標記id：</p><pre><code>[    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a tool that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.15627853572368622, &#39;token&#39;: 3944},    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a framework that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.11690319329500198, &#39;token&#39;: 7208},    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a library that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.058063216507434845, &#39;token&#39;: 5560},    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a database that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.04211743175983429, &#39;token&#39;: 8503},    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a prototype that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.024718601256608963, &#39;token&#39;: 17715}]</code></pre><p>下面是一個使用模型和Tokenizer進行掩碼語言建模的示例。該過程如下：</p><ul><li>從checkpoint名稱實例化一個tokenizer和一個模型。該模型被識別為一個DistilBERT模型，並用存儲在checkpoint中的權重加載它。</li><li>定義一個帶掩碼標記的序列，不使用單詞而是選擇tokenizer.mask_token進行放置(進行掩碼)。</li><li>將該序列編碼為id，並在該id列表中找到掩碼標記的位置。</li><li>在掩碼標記的索引處檢索預測：此張量與詞彙表的大小相同，值是每個標記的分數。模型對他認為在這種情況下可能出現的標記會給出更高的分數。</li><li>使用PyTorch topk或TensorFlow top_k方法檢索前5個標記。</li><li>用預測的標記替換掩碼標記並打印結果</li></ul><p>Pytorch代碼</p><pre><code>from transformers import AutoModelWithLMHead, AutoTokenizerimport torchtokenizer = AutoTokenizer.from_pretrained(&#34;distilbert-base-cased&#34;)model = AutoModelWithLMHead.from_pretrained(&#34;distilbert-base-cased&#34;)sequence = f&#34;Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.&#34;input = tokenizer.encode(sequence, return_tensors=&#34;pt&#34;)mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]token_logits = model(input)[0]mask_token_logits = token_logits[0, mask_token_index, :]top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()for token in top_5_tokens:    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))</code></pre><p>TensorFlow代碼</p><pre><code>from transformers import TFAutoModelWithLMHead, AutoTokenizerimport tensorflow as tftokenizer = AutoTokenizer.from_pretrained(&#34;distilbert-base-cased&#34;)model = TFAutoModelWithLMHead.from_pretrained(&#34;distilbert-base-cased&#34;)sequence = f&#34;Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.&#34;input = tokenizer.encode(sequence, return_tensors=&#34;tf&#34;)mask_token_index = tf.where(input == tokenizer.mask_token_id)[0, 1]token_logits = model(input)[0]mask_token_logits = token_logits[0, mask_token_index, :]top_5_tokens = tf.math.top_k(mask_token_logits, 5).indices.numpy()for token in top_5_tokens:    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))</code></pre><p>這將打印五個序列，其中前五個標記由模型預測：</p><pre><code>Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.</code></pre><h1 class=pgc-h-arrow-right>因果語言建模</h1><p>因果語言建模是根據一系列的標記來預測標記的任務。在這種情況下，模型只關注左邊的上下文(掩碼左邊的標記)。這樣的訓練對於生成任務來說是有作用的。</p><p>目前還沒有進行因果語言建模/生成的管道。 下面是一個使用Tokenizer和模型的示例。利用generate()方法按照PyTorch中的初始序列生成標記，並在TensorFlow中創建一個簡單的循環。</p><p>Pytorch代碼</p><pre><code>from transformers import AutoModelWithLMHead, AutoTokenizertokenizer = AutoTokenizer.from_pretrained(&#34;gpt2&#34;)model = AutoModelWithLMHead.from_pretrained(&#34;gpt2&#34;)sequence = f&#34;Hugging Face is based in DUMBO, New York City, and is&#34;input = tokenizer.encode(sequence, return_tensors=&#34;pt&#34;)generated = model.generate(input, max_length=50)resulting_string = tokenizer.decode(generated.tolist()[0])print(resulting_string)</code></pre><p>TensorFlow代碼</p><pre><code>from transformers import TFAutoModelWithLMHead, AutoTokenizerimport tensorflow as tftokenizer = AutoTokenizer.from_pretrained(&#34;gpt2&#34;)model = TFAutoModelWithLMHead.from_pretrained(&#34;gpt2&#34;)sequence = f&#34;Hugging Face is based in DUMBO, New York City, and is&#34;generated = tokenizer.encode(sequence)for i in range(50):    predictions = model(tf.constant([generated]))[0]    token = tf.argmax(predictions[0], axis=1)[-1].numpy()    generated += [token]resulting_string = tokenizer.decode(generated)print(resulting_string)</code></pre><p>這將從原始序列輸出(希望)的對應字符串，使用top_p/tok_k分佈獲取generate()採樣的結果：</p><pre><code>Hugging Face is based in DUMBO, New York City, and is a live-action TV series based on the novel by JohnCarpenter, and its producers, David Kustlin and Steve Pichar. The film is directed by!</code></pre><h1 class=pgc-h-arrow-right>命名實體識別</h1><p>命名實體識別(NER)是根據類別對標記進行分類的任務，例如將標記標識為個人、組織或位置。命名實體識別數據集的一個例子是CoNLL-2003數據集，它完全基於該任務。如果你想對NER任務的模型進行微調，可以利用ner/run_ner.py(PyTorch)、ner/run_pl_ner.py(利用PyTorch lightning)或ner/run_tf_ner.py(TensorFlow)腳本。</p><p>下面是一個使用管道進行命名實體識別的示例，試圖將標記標識為屬於9個類之一：</p><ul><li>O, 不是命名實體</li><li>B-MIS, 一個雜項實體的開頭</li><li>I-MIS, 雜項實體</li><li>B-PER, 一個人名的開頭</li><li>I-PER, 人名</li><li>B-ORG, 一個組織的開頭</li><li>I-ORG, 組織</li><li>B-LOC, 一個地點的開頭</li><li>I-LOC, 地點</li></ul><p>它利用CoNLL-2003上一個經過微調的模型，由dbmdz的@stefan-it進行了微調。</p><pre><code>from transformers import pipelinenlp = pipeline(&#34;ner&#34;)sequence = &#34;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&#34; \           &#34;close to the Manhattan Bridge which is visible from the window.&#34;print(nlp(sequence))</code></pre><p>這將輸出上面定義的9個類中標識為實體的所有單詞的列表。以下是預期結果：</p><pre><code>[    {&#39;word&#39;: &#39;Hu&#39;, &#39;score&#39;: 0.9995632767677307, &#39;entity&#39;: &#39;I-ORG&#39;},    {&#39;word&#39;: &#39;##gging&#39;, &#39;score&#39;: 0.9915938973426819, &#39;entity&#39;: &#39;I-ORG&#39;},    {&#39;word&#39;: &#39;Face&#39;, &#39;score&#39;: 0.9982671737670898, &#39;entity&#39;: &#39;I-ORG&#39;},    {&#39;word&#39;: &#39;Inc&#39;, &#39;score&#39;: 0.9994403719902039, &#39;entity&#39;: &#39;I-ORG&#39;},    {&#39;word&#39;: &#39;New&#39;, &#39;score&#39;: 0.9994346499443054, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;York&#39;, &#39;score&#39;: 0.9993270635604858, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;City&#39;, &#39;score&#39;: 0.9993864893913269, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;D&#39;, &#39;score&#39;: 0.9825621843338013, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;##UM&#39;, &#39;score&#39;: 0.936983048915863, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;##BO&#39;, &#39;score&#39;: 0.8987102508544922, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;Manhattan&#39;, &#39;score&#39;: 0.9758241176605225, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;Bridge&#39;, &#39;score&#39;: 0.990249514579773, &#39;entity&#39;: &#39;I-LOC&#39;}]</code></pre><p>注意“Hugging Face”是如何被確定為一個組織，“New York City”，“DUMBO”和“Manhattan Bridge”是如何被確定為地點的。</p><p>下面是一個使用模型和Tokenizer進行命名實體識別的示例。 該過程如下：</p><ul><li>從checkpoint名稱實例化一個tokenizer和一個模型。該模型被識別為一個BERT模型，並用存儲在checkpoint中的權重加載它。</li><li>定義用於訓練模型的標籤列表。</li><li>定義一個包含已知實體的序列，例如“Hugging Face”作為一個組織，“New York City”作為一個位置。</li><li>將單詞拆分為標記，以便它們可以映射到預測。我們使用一個小技巧，首先對序列進行完全的編碼和解碼，這樣就留下了一個包含特殊標記的字符串。</li><li>將該序列編碼為id(自動添加特殊標記)。</li><li>通過將輸入傳遞到模型並獲得第一個輸出來檢索預測。這將導致每個標記在9個可能的類上分佈。我們使用argmax來檢索每個標記最可能的類。</li><li>將每個標記及其預測到一起並打印出來。</li></ul><p>Pytorch代碼</p><pre><code>from transformers import AutoModelForTokenClassification, AutoTokenizerimport torchmodel = AutoModelForTokenClassification.from_pretrained(&#34;dbmdz/bert-large-cased-finetuned-conll03-english&#34;)tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased&#34;)label_list = [    &#34;O&#34;,       # 不是命名實體    &#34;B-MISC&#34;,  # 一個雜項實體的開頭    &#34;I-MISC&#34;,  # 雜項    &#34;B-PER&#34;,   # 一個人名的開頭    &#34;I-PER&#34;,   # 人名    &#34;B-ORG&#34;,   # 一個組織的開頭    &#34;I-ORG&#34;,   # 組織    &#34;B-LOC&#34;,   # 一個地點的開頭    &#34;I-LOC&#34;    # 地點]sequence = &#34;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&#34; \           &#34;close to the Manhattan Bridge.&#34;# Bit of a hack to get the tokens with the special tokenstokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))inputs = tokenizer.encode(sequence, return_tensors=&#34;pt&#34;)outputs = model(inputs)[0]predictions = torch.argmax(outputs, dim=2)print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].tolist())])</code></pre><p>TensorFlow代碼</p><pre><code>from transformers import TFAutoModelForTokenClassification, AutoTokenizerimport tensorflow as tfmodel = TFAutoModelForTokenClassification.from_pretrained(&#34;dbmdz/bert-large-cased-finetuned-conll03-english&#34;)tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased&#34;)label_list = [    &#34;O&#34;,       # 不是命名實體    &#34;B-MISC&#34;,  # 一個雜項實體的開頭    &#34;I-MISC&#34;,  # 雜項    &#34;B-PER&#34;,   # 一個人名的開頭    &#34;I-PER&#34;,   # 人名    &#34;B-ORG&#34;,   # 一個組織的開頭    &#34;I-ORG&#34;,   # 組織    &#34;B-LOC&#34;,   # 一個地點的開頭    &#34;I-LOC&#34;    # 地點]sequence = &#34;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&#34; \           &#34;close to the Manhattan Bridge.&#34;#用特殊的標記來獲取標記的一點技巧tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))inputs = tokenizer.encode(sequence, return_tensors=&#34;tf&#34;)outputs = model(inputs)[0]predictions = tf.argmax(outputs, axis=2)print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())])</code></pre><p>這將輸出映射到其預測的每個標記的列表。與管道不同的是，這裡每個標記都有一個預測，因為我們沒有刪除“O”類，這意味著在該標記上找不到特定的實體。以下數組應為輸出：</p><pre><code>[(&#39;[CLS]&#39;, &#39;O&#39;), (&#39;Hu&#39;, &#39;I-ORG&#39;), (&#39;##gging&#39;, &#39;I-ORG&#39;), (&#39;Face&#39;, &#39;I-ORG&#39;), (&#39;Inc&#39;, &#39;I-ORG&#39;), (&#39;.&#39;, &#39;O&#39;), (&#39;is&#39;, &#39;O&#39;), (&#39;a&#39;, &#39;O&#39;), (&#39;company&#39;, &#39;O&#39;), (&#39;based&#39;, &#39;O&#39;), (&#39;in&#39;, &#39;O&#39;), (&#39;New&#39;, &#39;I-LOC&#39;), (&#39;York&#39;, &#39;I-LOC&#39;), (&#39;City&#39;, &#39;I-LOC&#39;), (&#39;.&#39;, &#39;O&#39;), (&#39;Its&#39;, &#39;O&#39;), (&#39;headquarters&#39;, &#39;O&#39;), (&#39;are&#39;, &#39;O&#39;), (&#39;in&#39;, &#39;O&#39;), (&#39;D&#39;, &#39;I-LOC&#39;), (&#39;##UM&#39;, &#39;I-LOC&#39;), (&#39;##BO&#39;, &#39;I-LOC&#39;), (&#39;,&#39;, &#39;O&#39;), (&#39;therefore&#39;, &#39;O&#39;), (&#39;very&#39;, &#39;O&#39;), (&#39;##c&#39;, &#39;O&#39;), (&#39;##lose&#39;, &#39;O&#39;), (&#39;to&#39;, &#39;O&#39;), (&#39;the&#39;, &#39;O&#39;), (&#39;Manhattan&#39;, &#39;I-LOC&#39;), (&#39;Bridge&#39;, &#39;I-LOC&#39;), (&#39;.&#39;, &#39;O&#39;), (&#39;[SEP]&#39;, &#39;O&#39;)]</code></pre></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>Transformers</a></li><li><a>用例</a></li><li><a>中常</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/95bc1c3b.html alt=施工中常見8種鋼筋分類 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ee33c2f5893849fbb7ed9dd98f7cba72 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/95bc1c3b.html title=施工中常見8種鋼筋分類>施工中常見8種鋼筋分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b15bfb03.html alt=施工中常見的鋼筋你需要知道的知識 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/e912e61511874e9db8e79597811ebabd style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b15bfb03.html title=施工中常見的鋼筋你需要知道的知識>施工中常見的鋼筋你需要知道的知識</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/61d128bc.html alt=岩土工程中常見的“問題土” class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/8803daf3aacd493fb1c4c99d8aebda28 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/61d128bc.html title=岩土工程中常見的“問題土”>岩土工程中常見的“問題土”</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9b8bf240.html alt=正交法——測試用例設計方法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/78c1ce95b7c344d0bf5c3d8c5d7463ab style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9b8bf240.html title=正交法——測試用例設計方法>正交法——測試用例設計方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/81df39fc.html alt=測試用例設計方法(二)——正交實驗法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a1f74afae8fe4b7080cb8188d32254b8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/81df39fc.html title=測試用例設計方法(二)——正交實驗法>測試用例設計方法(二)——正交實驗法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f117353f.html alt=軟件測試用例之正交試驗法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1535955562035888ded133e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f117353f.html title=軟件測試用例之正交試驗法>軟件測試用例之正交試驗法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1f6fbe66.html alt=生活中常見的英文縮寫，你學會了嗎？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/153416357927380a4b4f9b2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1f6fbe66.html title=生活中常見的英文縮寫，你學會了嗎？>生活中常見的英文縮寫，你學會了嗎？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ccee16d3.html alt=功能測試用例設計技術方法——《因果圖分析法》 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1522490029316e07f49ff30 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ccee16d3.html title=功能測試用例設計技術方法——《因果圖分析法》>功能測試用例設計技術方法——《因果圖分析法》</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b4aca8e2.html alt=編寫測試用例的幾種方法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7336aa5d97ae46b5a4fd215429afb09b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b4aca8e2.html title=編寫測試用例的幾種方法>編寫測試用例的幾種方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/19b46c1c.html alt=4種製造業大數據分析用例分享 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/79c1667cafcc423b880e472ac32735a6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/19b46c1c.html title=4種製造業大數據分析用例分享>4種製造業大數據分析用例分享</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6dff48fd.html alt="CSS 偽元素的一些罕見用例" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/489c31a395a84b9aaa450264e0e83ec0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6dff48fd.html title="CSS 偽元素的一些罕見用例">CSS 偽元素的一些罕見用例</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9d573b2b.html alt=Dai穩定幣的十大用例及其優勢 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RivR6n1AU5yy8h style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9d573b2b.html title=Dai穩定幣的十大用例及其優勢>Dai穩定幣的十大用例及其優勢</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/36c77ab8.html alt="UML - 建模基礎 - 用例驅動" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/10d1a9db8fa34ac5924bff940d5b2c8f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/36c77ab8.html title="UML - 建模基礎 - 用例驅動">UML - 建模基礎 - 用例驅動</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4aa66f67.html alt=UML用例圖總結 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3881c18a31494f37ba1ea7008da33889 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4aa66f67.html title=UML用例圖總結>UML用例圖總結</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/796dc49a.html alt=“用例規劃”到底該怎麼做？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RatayRtH3ElLfi style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/796dc49a.html title=“用例規劃”到底該怎麼做？>“用例規劃”到底該怎麼做？</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>