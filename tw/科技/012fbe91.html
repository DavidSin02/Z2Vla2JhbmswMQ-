<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>Transformers åº«ä¸­å¸¸è¦‹çš„ç”¨ä¾‹ | æå®¢å¿«è¨Š</title><meta property="og:title" content="Transformers åº«ä¸­å¸¸è¦‹çš„ç”¨ä¾‹ - æå®¢å¿«è¨Š"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/012fbe91.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/012fbe91.html><meta property="article:published_time" content="2020-11-14T21:01:26+08:00"><meta property="article:modified_time" content="2020-11-14T21:01:26+08:00"><meta name=Keywords content><meta name=description content="Transformers åº«ä¸­å¸¸è¦‹çš„ç”¨ä¾‹"><meta name=author content="æå®¢å¿«è¨Š"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/012fbe91.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>ğŸ¤“ æå®¢å¿«è¨Š Geek Bank</a></h1><p class=description>ç‚ºä½ å¸¶ä¾†æœ€å…¨çš„ç§‘æŠ€çŸ¥è­˜ ğŸ§¡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>çŒœä½ å–œæ­¡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=ç§‘æŠ€>ç§‘æŠ€</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=éŠæˆ²>éŠæˆ²</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=ç§‘å­¸>ç§‘å­¸</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>Transformers åº«ä¸­å¸¸è¦‹çš„ç”¨ä¾‹</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>ç§‘æŠ€</a></span></div><div class=post-content><p>æœ¬ç« ä»‹ç´¹ä½¿ç”¨Transformersåº«æ™‚æœ€å¸¸è¦‹çš„ç”¨ä¾‹ã€‚å¯ç”¨çš„æ¨¡å‹å…è¨±è¨±å¤šä¸åŒçš„é…ç½®ï¼Œä¸¦ä¸”åœ¨ç”¨ä¾‹ä¸­å…·æœ‰å¾ˆå¼·çš„é€šç”¨æ€§ã€‚é€™è£¡ä»‹ç´¹äº†æœ€ç°¡å–®çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†è«¸å¦‚å•ç­”ã€åºåˆ—åˆ†é¡ã€å‘½åå¯¦é«”è­˜åˆ¥ç­‰ä»»å‹™çš„ç”¨æ³•ã€‚</p><p>é€™äº›ç¤ºä¾‹åˆ©ç”¨Auto Modelï¼Œé€™äº›é¡å°‡æ ¹æ“šçµ¦å®šçš„checkpointå¯¦ä¾‹åŒ–æ¨¡å‹ï¼Œä¸¦è‡ªå‹•é¸æ“‡æ­£ç¢ºçš„æ¨¡å‹é«”ç³»çµæ§‹ã€‚æœ‰é—œè©³ç´°ä¿¡æ¯ï¼Œè«‹æŸ¥çœ‹ï¼šAutoModelæ–‡æª”ã€‚è«‹éš¨æ„ä¿®æ”¹ä»£ç¢¼ï¼Œä½¿å…¶æ›´å…·é«”ï¼Œä¸¦ä½¿å…¶é©æ‡‰ä½ çš„ç‰¹å®šç”¨ä¾‹ã€‚</p><ul><li>ç‚ºäº†ä½¿æ¨¡å‹èƒ½å¤ åœ¨ä»»å‹™ä¸Šè‰¯å¥½åœ°åŸ·è¡Œï¼Œå¿…é ˆå¾èˆ‡è©²ä»»å‹™å°æ‡‰çš„checkpointåŠ è¼‰æ¨¡å‹ã€‚é€™äº›checkpointé€šå¸¸æ˜¯åœ¨å¤§é‡æ•¸æ“šä¸Šé å…ˆè¨“ç·´çš„ï¼Œä¸¦é‡å°ç‰¹å®šä»»å‹™é€²è¡Œå¾®èª¿ã€‚é€™æ„å‘³è‘—ï¼šä¸¦éæ‰€æœ‰æ¨¡å‹éƒ½é‡å°æ‰€æœ‰ä»»å‹™é€²è¡Œäº†å¾®èª¿ã€‚å¦‚æœè¦å°ç‰¹å®šä»»å‹™çš„æ¨¡å‹é€²è¡Œå¾®èª¿ï¼Œå¯ä»¥åˆ©ç”¨examplesç›®éŒ„ä¸­çš„run\$task.pyè…³æœ¬ã€‚</li><li>å¾®èª¿æ¨¡å‹æ˜¯åœ¨ç‰¹å®šçš„æ•¸æ“šé›†ä¸Šå¾®èª¿çš„ã€‚æ­¤æ•¸æ“šé›†å¯èƒ½èˆ‡ä½ çš„ç”¨ä¾‹å’ŒåŸŸé‡ç–Šï¼Œä¹Ÿå¯èƒ½ä¸é‡ç–Šã€‚å¦‚å‰æ‰€è¿°ï¼Œä½ å¯ä»¥åˆ©ç”¨ç¤ºä¾‹è…³æœ¬ä¾†å¾®èª¿æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥å‰µå»ºè‡ªå·±çš„è¨“ç·´è…³æœ¬ã€‚</li></ul><p>ç‚ºäº†å°ä»»å‹™é€²è¡Œæ¨ç†ï¼Œåº«æä¾›äº†å¹¾ç¨®æ©Ÿåˆ¶ï¼š</p><ul><li>ç®¡é“æ˜¯éå¸¸æ˜“æ–¼ä½¿ç”¨çš„æŠ½è±¡ï¼Œåªéœ€è¦å…©è¡Œä»£ç¢¼ã€‚</li><li>ç›´æ¥å°‡æ¨¡å‹èˆ‡Tokenizer(PyTorch/TensorFlow)çµåˆä½¿ç”¨ä¾†ä½¿ç”¨æ¨¡å‹çš„å®Œæ•´æ¨ç†ã€‚é€™ç¨®æ©Ÿåˆ¶ç¨å¾®è¤‡é›œï¼Œä½†æ˜¯æ›´å¼·å¤§ã€‚</li></ul><p>é€™è£¡å±•ç¤ºäº†å…©ç¨®æ–¹æ³•ã€‚</p><blockquote><p>è«‹æ³¨æ„ï¼Œé€™è£¡ä»‹ç´¹çš„æ‰€æœ‰ä»»å‹™éƒ½åˆ©ç”¨äº†åœ¨é è¨“ç·´æ¨¡å‹é‡å°ç‰¹å®šä»»å‹™é€²è¡Œå¾®èª¿å¾Œçš„æ¨¡å‹ã€‚åŠ è¼‰æœªé‡å°ç‰¹å®šä»»å‹™é€²è¡Œå¾®èª¿çš„checkpointæ™‚ï¼Œå°‡åªåŠ è¼‰transformerå±¤ï¼Œè€Œä¸æœƒåŠ è¼‰ç”¨æ–¼è©²ä»»å‹™çš„é™„åŠ å±¤ï¼Œå¾è€Œéš¨æ©Ÿåˆå§‹åŒ–è©²é™„åŠ å±¤çš„æ¬Šé‡ã€‚é€™å°‡ç”¢ç”Ÿéš¨æ©Ÿè¼¸å‡ºã€‚</p></blockquote><h1 class=pgc-h-arrow-right>åºåˆ—åˆ†é¡</h1><p>åºåˆ—åˆ†é¡æ˜¯æ ¹æ“šå·²ç¶“çµ¦å®šçš„é¡åˆ¥ç„¶å¾Œå°åºåˆ—é€²è¡Œåˆ†é¡çš„ä»»å‹™ã€‚åºåˆ—åˆ†é¡çš„ä¸€å€‹ä¾‹å­æ˜¯GLUEæ•¸æ“šé›†ï¼Œå®ƒå°±æ˜¯å®Œå…¨åŸºæ–¼è©²ä»»å‹™çš„ã€‚å¦‚æœä½ æƒ³åœ¨GLUEåºåˆ—åˆ†é¡ä»»å‹™ä¸Šå¾®èª¿æ¨¡å‹ï¼Œå¯ä»¥åˆ©ç”¨run_GLUE.pyæˆ–run_tf_GLUE.pyè…³æœ¬ã€‚</p><p>ä¸‹é¢æ˜¯ä¸€å€‹ä½¿ç”¨ç®¡é“é€²è¡Œæƒ…ç·’åˆ†æçš„ä¾‹å­ï¼šè­˜åˆ¥è©²åºåˆ—æ˜¯ç©æ¥µçš„é‚„æ˜¯æ¶ˆæ¥µçš„ã€‚å®ƒåˆ©ç”¨sst2ä¸Šçš„å¾®èª¿æ¨¡å‹ï¼Œé€™æ˜¯ä¸€å€‹GLUEä»»å‹™ã€‚</p><pre><code>from transformers import pipelinenlp = pipeline(&#34;sentiment-analysis&#34;)print(nlp(&#34;I hate you&#34;))print(nlp(&#34;I love you&#34;))</code></pre><p>é€™å°‡è¿”å›ä¸€å€‹æ¨™ç±¤(â€œç©æ¥µâ€æˆ–â€œæ¶ˆæ¥µâ€)å’Œä¸€å€‹åˆ†æ•¸ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š</p><pre><code>[{&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.9991129}][{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.99986565}]</code></pre><p>ä¸‹é¢æ˜¯ä¸€å€‹ä½¿ç”¨æ¨¡å‹é€²è¡Œåºåˆ—åˆ†é¡çš„ç¤ºä¾‹ï¼Œä»¥ç¢ºå®šå…©å€‹åºåˆ—æ˜¯å¦æ˜¯å½¼æ­¤çš„è§£é‡‹ã€‚è©²éç¨‹å¦‚ä¸‹ï¼š</p><ul><li>å¾checkpointåç¨±å¯¦ä¾‹åŒ–ä¸€å€‹tokenizerå’Œä¸€å€‹æ¨¡å‹ã€‚è©²æ¨¡å‹è¢«è­˜åˆ¥ç‚ºä¸€å€‹BERTæ¨¡å‹ï¼Œä¸¦ç”¨å­˜å„²åœ¨checkpointä¸­çš„æ¬Šé‡åŠ è¼‰å®ƒã€‚</li><li>å¾é€™å…©å¥è©±ä¸­æ§‹å»ºä¸€å€‹åºåˆ—ï¼Œä½¿ç”¨æ­£ç¢ºçš„ç‰¹å®šæ–¼æ¨¡å‹çš„åˆ†éš”ç¬¦æ¨™è¨˜é¡å‹idå’Œæ³¨æ„åŠ›æ©ç¢¼(encode()å’Œencode_plus()è™•ç†é€™å€‹å•é¡Œ)</li><li>å°‡é€™å€‹åºåˆ—å‚³éåˆ°æ¨¡å‹ä¸­ï¼Œä»¥ä¾¿å°‡å…¶åˆ†é¡åˆ°å…©å€‹å¯ç”¨çš„é¡ä¸­çš„ä¸€å€‹ï¼š0(ä¸æ˜¯è§£é‡‹)å’Œ1(æ˜¯è§£é‡‹)</li><li>è¨ˆç®—çµæœçš„softmaxç²å–é¡çš„æ¦‚ç‡</li><li>æ‰“å°çµæœ</li></ul><p>Pytorchä»£ç¢¼</p><pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassificationimport torchtokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased-finetuned-mrpc&#34;)model = AutoModelForSequenceClassification.from_pretrained(&#34;bert-base-cased-finetuned-mrpc&#34;)classes = [&#34;not paraphrase&#34;, &#34;is paraphrase&#34;]sequence_0 = &#34;The company HuggingFace is based in New York City&#34;sequence_1 = &#34;Apples are especially bad for your health&#34;sequence_2 = &#34;HuggingFace&#39;s headquarters are situated in Manhattan&#34;paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, return_tensors=&#34;pt&#34;)not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=&#34;pt&#34;)paraphrase_classification_logits = model(**paraphrase)[0]not_paraphrase_classification_logits = model(**not_paraphrase)[0]paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]print(&#34;Should be paraphrase&#34;)for i in range(len(classes)):    print(f&#34;{classes[i]}: {round(paraphrase_results[i] * 100)}%&#34;)print(&#34;\nShould not be paraphrase&#34;)for i in range(len(classes)):    print(f&#34;{classes[i]}: {round(not_paraphrase_results[i] * 100)}%&#34;)</code></pre><p>TensorFlowä»£ç¢¼</p><pre><code>from transformers import AutoTokenizer, TFAutoModelForSequenceClassificationimport tensorflow as tftokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased-finetuned-mrpc&#34;)model = TFAutoModelForSequenceClassification.from_pretrained(&#34;bert-base-cased-finetuned-mrpc&#34;)classes = [&#34;not paraphrase&#34;, &#34;is paraphrase&#34;]sequence_0 = &#34;The company HuggingFace is based in New York City&#34;sequence_1 = &#34;Apples are especially bad for your health&#34;sequence_2 = &#34;HuggingFace&#39;s headquarters are situated in Manhattan&#34;paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, return_tensors=&#34;tf&#34;)not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=&#34;tf&#34;)paraphrase_classification_logits = model(paraphrase)[0]not_paraphrase_classification_logits = model(not_paraphrase)[0]paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]print(&#34;Should be paraphrase&#34;)for i in range(len(classes)):    print(f&#34;{classes[i]}: {round(paraphrase_results[i] * 100)}%&#34;)print(&#34;\nShould not be paraphrase&#34;)for i in range(len(classes)):    print(f&#34;{classes[i]}: {round(not_paraphrase_results[i] * 100)}%&#34;)</code></pre><p>é€™å°‡è¼¸å‡ºä»¥ä¸‹çµæœï¼š</p><pre><code>Should be paraphrasenot paraphrase: 10%is paraphrase: 90%Should not be paraphrasenot paraphrase: 94%is paraphrase: 6%</code></pre><h1 class=pgc-h-arrow-right>æŠ½å–å¼å•ç­”</h1><p>æŠ½å–å¼å•ç­”æ˜¯å¾çµ¦å®šå•é¡Œçš„æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆçš„ä»»å‹™ã€‚å•ç­”æ•¸æ“šé›†çš„ä¸€å€‹ä¾‹å­æ˜¯SQuADæ•¸æ“šé›†ï¼Œå®ƒå®Œå…¨åŸºæ–¼è©²ä»»å‹™ã€‚å¦‚æœä½ æƒ³åœ¨åœ˜éšŠä»»å‹™ä¸­å¾®èª¿æ¨¡å‹ï¼Œå¯ä»¥åˆ©ç”¨run_SQuAD.pyã€‚</p><p>ä¸‹é¢æ˜¯ä¸€å€‹ä½¿ç”¨ç®¡é“é€²è¡Œå•ç­”çš„ç¤ºä¾‹ï¼šå¾çµ¦å®šå•é¡Œçš„æ–‡æœ¬ä¸­æå–ç­”æ¡ˆã€‚å®ƒåˆ©ç”¨äº†ä¸€å€‹å°éšŠçš„å¾®èª¿æ¨¡å‹ã€‚</p><pre><code>from transformers import pipelinenlp = pipeline(&#34;question-answering&#34;)context = r&#34;&#34;&#34;Extractive Question Answering is the task of extracting an answer from a text given a question. An example of aquestion answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tunea model on a SQuAD task, you may leverage the `run_squad.py`.&#34;&#34;&#34;print(nlp(question=&#34;What is extractive question answering?&#34;, context=context))print(nlp(question=&#34;What is a good example of a question answering dataset?&#34;, context=context))</code></pre><p>é€™å°‡è¿”å›å¾æ–‡æœ¬ä¸­æå–çš„ç­”æ¡ˆï¼Œä¸€å€‹ç½®ä¿¡åº¦ï¼Œä»¥åŠâ€œé–‹å§‹â€å’Œâ€œçµæŸâ€å€¼ï¼Œé€™äº›å€¼æ˜¯æå–çš„ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­çš„ä½ç½®ã€‚</p><pre><code>{&#39;score&#39;: 0.622232091629833, &#39;start&#39;: 34, &#39;end&#39;: 96, &#39;answer&#39;: &#39;the task of extracting an answer from a text given a question.&#39;}{&#39;score&#39;: 0.5115299158662765, &#39;start&#39;: 147, &#39;end&#39;: 161, &#39;answer&#39;: &#39;SQuAD dataset,&#39;}</code></pre><p>ä¸‹é¢æ˜¯ä¸€å€‹ä½¿ç”¨æ¨¡å‹å’ŒTokenizerå›ç­”å•é¡Œçš„ç¤ºä¾‹ã€‚è©²éç¨‹å¦‚ä¸‹ï¼š</p><ul><li>å¾checkpointåç¨±å¯¦ä¾‹åŒ–ä¸€å€‹tokenizerå’Œä¸€å€‹æ¨¡å‹ã€‚è©²æ¨¡å‹è¢«è­˜åˆ¥ç‚ºä¸€å€‹BERTæ¨¡å‹ï¼Œä¸¦ç”¨å­˜å„²åœ¨checkpointä¸­çš„æ¬Šé‡åŠ è¼‰å®ƒã€‚</li><li>å®šç¾©ä¸€æ®µæ–‡æœ¬å’Œå¹¾å€‹å•é¡Œã€‚</li><li>éæ­·å•é¡Œä¸¦æ ¹æ“šæ–‡æœ¬å’Œç•¶å‰å•é¡Œæ§‹å»ºä¸€å€‹åºåˆ—ï¼Œä½¿ç”¨æ­£ç¢ºçš„æ¨¡å‹ç‰¹å®šåˆ†éš”ç¬¦æ¨™è¨˜é¡å‹idå’Œæ³¨æ„åŠ›æ©ç¢¼å°‡æ­¤åºåˆ—å‚³éåˆ°æ¨¡å‹ä¸­ã€‚é€™å°‡è¼¸å‡ºæ•´å€‹åºåˆ—æ¨™è¨˜(å•é¡Œå’Œæ–‡æœ¬)çš„é–‹å§‹ä½ç½®å’ŒçµæŸä½ç½®çš„ä¸€ç³»åˆ—åˆ†æ•¸ã€‚</li><li>è¨ˆç®—çµæœçš„softmaxä»¥ç²å¾—å¾æ¨™è¨˜çš„é–‹å§‹ä½ç½®å’Œåœæ­¢ä½ç½®å°æ‡‰çš„æ¦‚ç‡</li><li>å°‡é€™äº›æ¨™è¨˜è½‰æ›ç‚ºå­—ç¬¦ä¸²ã€‚</li><li>æ‰“å°çµæœ</li></ul><p>Pytorchä»£ç¢¼</p><pre><code>from transformers import AutoTokenizer, AutoModelForQuestionAnsweringimport torchtokenizer = AutoTokenizer.from_pretrained(&#34;bert-large-uncased-whole-word-masking-finetuned-squad&#34;)model = AutoModelForQuestionAnswering.from_pretrained(&#34;bert-large-uncased-whole-word-masking-finetuned-squad&#34;)text = r&#34;&#34;&#34; Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purposearchitectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and NaturalLanguage Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability betweenTensorFlow 2.0 and PyTorch.&#34;&#34;&#34;questions = [    &#34;How many pretrained models are available in Transformers?&#34;,    &#34;What does Transformers provide?&#34;,    &#34;Transformers provides interoperability between which frameworks?&#34;,]for question in questions:    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=&#34;pt&#34;)    input_ids = inputs[&#34;input_ids&#34;].tolist()[0]    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)    answer_start_scores, answer_end_scores = model(**inputs)    answer_start = torch.argmax(        answer_start_scores    )  # Get the most likely beginning of answer with the argmax of the score    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))    print(f&#34;Question: {question}&#34;)    print(f&#34;Answer: {answer}\n&#34;)</code></pre><p>TensorFlowä»£ç¢¼</p><pre><code>from transformers import AutoTokenizer, TFAutoModelForQuestionAnsweringimport tensorflow as tftokenizer = AutoTokenizer.from_pretrained(&#34;bert-large-uncased-whole-word-masking-finetuned-squad&#34;)model = TFAutoModelForQuestionAnswering.from_pretrained(&#34;bert-large-uncased-whole-word-masking-finetuned-squad&#34;)text = r&#34;&#34;&#34; Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purposearchitectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and NaturalLanguage Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability betweenTensorFlow 2.0 and PyTorch.&#34;&#34;&#34;questions = [    &#34;How many pretrained models are available in Transformers?&#34;,    &#34;What does Transformers provide?&#34;,    &#34;Transformers provides interoperability between which frameworks?&#34;,]for question in questions:    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=&#34;tf&#34;)    input_ids = inputs[&#34;input_ids&#34;].numpy()[0]    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)    answer_start_scores, answer_end_scores = model(inputs)    answer_start = tf.argmax(        answer_start_scores, axis=1    ).numpy()[0]  # Get the most likely beginning of answer with the argmax of the score    answer_end = (        tf.argmax(answer_end_scores, axis=1) + 1    ).numpy()[0]  # Get the most likely end of answer with the argmax of the score    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))    print(f&#34;Question: {question}&#34;)    print(f&#34;Answer: {answer}\n&#34;)</code></pre><p>é€™å°‡è¼¸å‡ºé æ¸¬ç­”æ¡ˆå¾Œçš„å•é¡Œï¼š</p><pre><code>Question: How many pretrained models are available in Transformers?Answer: over 32 +Question: What does Transformers provide?Answer: general - purpose architecturesQuestion: Transformers provides interoperability between which frameworks?Answer: tensorflow 2 . 0 and pytorch</code></pre><h1 class=pgc-h-arrow-right>èªè¨€å»ºæ¨¡</h1><p>èªè¨€å»ºæ¨¡æ˜¯å°‡ä¸€å€‹æ¨¡å‹èˆ‡ä¸€å€‹ç‰¹å®šé ˜åŸŸçš„èªæ–™åº«ç›¸åŒ¹é…çš„ä»»å‹™ã€‚æ‰€æœ‰æµè¡Œçš„åŸºæ–¼transformerçš„æ¨¡å‹éƒ½æ˜¯ä½¿ç”¨èªè¨€å»ºæ¨¡çš„è®Šé«”ä¾†è¨“ç·´çš„ï¼Œä¾‹å¦‚æ©ç¢¼èªè¨€å»ºæ¨¡çš„BERTã€å› æœèªè¨€å»ºæ¨¡çš„GPT-2ã€‚</p><p>èªè¨€å»ºæ¨¡åœ¨é è¨“ç·´ä¹‹å¤–ä¹Ÿå¾ˆæœ‰ç”¨ï¼Œä¾‹å¦‚å°‡æ¨¡å‹åˆ†ä½ˆè½‰æ›ç‚ºç‰¹å®šé ˜åŸŸï¼šä½¿ç”¨åœ¨éå¸¸å¤§çš„èªæ–™åº«ä¸Šè¨“ç·´çš„èªè¨€æ¨¡å‹ï¼Œç„¶å¾Œå°‡å…¶å¾®èª¿åˆ°æ–°èæ•¸æ“šé›†æˆ–ç§‘å­¸è«–æ–‡ä¸Šï¼Œä¾‹å¦‚LysandreJik/arxiv nlp(https://huggingface.co/lysandre/arxiv-nlp)ã€‚</p><h1 class=pgc-h-arrow-right>æ©ç¢¼èªè¨€å»ºæ¨¡</h1><p>æ©ç¢¼èªè¨€å»ºæ¨¡æ˜¯ç”¨æ©ç¢¼æ¨™è¨˜å°åºåˆ—ä¸­çš„æ¨™è¨˜é€²è¡Œæ©ç¢¼ï¼Œä¸¦æç¤ºæ¨¡å‹ç”¨é©ç•¶çš„æ¨™è¨˜å¡«å……è©²æ©ç¢¼çš„ä»»å‹™ã€‚é€™å…è¨±æ¨¡å‹åŒæ™‚è™•ç†å³ä¸Šä¸‹æ–‡(æ©ç¢¼å³å´çš„æ¨™è¨˜)å’Œå·¦ä¸Šä¸‹æ–‡(æ©ç¢¼å·¦å´çš„æ¨™è¨˜)ã€‚é€™æ¨£çš„è¨“ç·´ç‚ºéœ€è¦é›™å‘èƒŒæ™¯çš„ä¸‹æ¸¸ä»»å‹™(å¦‚SQuAD)å¥ å®šäº†å …å¯¦çš„åŸºç¤ã€‚</p><p>ä¸‹é¢æ˜¯ä½¿ç”¨ç®¡é“ä¾†æ›¿æ›åºåˆ—ä¸­çš„æ©ç¢¼çš„ç¤ºä¾‹ï¼š</p><pre><code>from transformers import pipelinenlp = pipeline(&#34;fill-mask&#34;)print(nlp(f&#34;HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.&#34;))</code></pre><p>é€™å°‡åœ¨Tokenizerè©å½™è¡¨ä¸­è¼¸å‡ºå¡«å……äº†æ©ç¢¼çš„åºåˆ—ã€ç½®ä¿¡åº¦å¾—åˆ†ä»¥åŠæ¨™è¨˜idï¼š</p><pre><code>[    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a tool that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.15627853572368622, &#39;token&#39;: 3944},    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a framework that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.11690319329500198, &#39;token&#39;: 7208},    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a library that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.058063216507434845, &#39;token&#39;: 5560},    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a database that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.04211743175983429, &#39;token&#39;: 8503},    {&#39;sequence&#39;: &#39;&lt;s&gt; HuggingFace is creating a prototype that the community uses to solve NLP tasks.&lt;/s&gt;&#39;, &#39;score&#39;: 0.024718601256608963, &#39;token&#39;: 17715}]</code></pre><p>ä¸‹é¢æ˜¯ä¸€å€‹ä½¿ç”¨æ¨¡å‹å’ŒTokenizeré€²è¡Œæ©ç¢¼èªè¨€å»ºæ¨¡çš„ç¤ºä¾‹ã€‚è©²éç¨‹å¦‚ä¸‹ï¼š</p><ul><li>å¾checkpointåç¨±å¯¦ä¾‹åŒ–ä¸€å€‹tokenizerå’Œä¸€å€‹æ¨¡å‹ã€‚è©²æ¨¡å‹è¢«è­˜åˆ¥ç‚ºä¸€å€‹DistilBERTæ¨¡å‹ï¼Œä¸¦ç”¨å­˜å„²åœ¨checkpointä¸­çš„æ¬Šé‡åŠ è¼‰å®ƒã€‚</li><li>å®šç¾©ä¸€å€‹å¸¶æ©ç¢¼æ¨™è¨˜çš„åºåˆ—ï¼Œä¸ä½¿ç”¨å–®è©è€Œæ˜¯é¸æ“‡tokenizer.mask_tokené€²è¡Œæ”¾ç½®(é€²è¡Œæ©ç¢¼)ã€‚</li><li>å°‡è©²åºåˆ—ç·¨ç¢¼ç‚ºidï¼Œä¸¦åœ¨è©²idåˆ—è¡¨ä¸­æ‰¾åˆ°æ©ç¢¼æ¨™è¨˜çš„ä½ç½®ã€‚</li><li>åœ¨æ©ç¢¼æ¨™è¨˜çš„ç´¢å¼•è™•æª¢ç´¢é æ¸¬ï¼šæ­¤å¼µé‡èˆ‡è©å½™è¡¨çš„å¤§å°ç›¸åŒï¼Œå€¼æ˜¯æ¯å€‹æ¨™è¨˜çš„åˆ†æ•¸ã€‚æ¨¡å‹å°ä»–èªç‚ºåœ¨é€™ç¨®æƒ…æ³ä¸‹å¯èƒ½å‡ºç¾çš„æ¨™è¨˜æœƒçµ¦å‡ºæ›´é«˜çš„åˆ†æ•¸ã€‚</li><li>ä½¿ç”¨PyTorch topkæˆ–TensorFlow top_kæ–¹æ³•æª¢ç´¢å‰5å€‹æ¨™è¨˜ã€‚</li><li>ç”¨é æ¸¬çš„æ¨™è¨˜æ›¿æ›æ©ç¢¼æ¨™è¨˜ä¸¦æ‰“å°çµæœ</li></ul><p>Pytorchä»£ç¢¼</p><pre><code>from transformers import AutoModelWithLMHead, AutoTokenizerimport torchtokenizer = AutoTokenizer.from_pretrained(&#34;distilbert-base-cased&#34;)model = AutoModelWithLMHead.from_pretrained(&#34;distilbert-base-cased&#34;)sequence = f&#34;Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.&#34;input = tokenizer.encode(sequence, return_tensors=&#34;pt&#34;)mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]token_logits = model(input)[0]mask_token_logits = token_logits[0, mask_token_index, :]top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()for token in top_5_tokens:    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))</code></pre><p>TensorFlowä»£ç¢¼</p><pre><code>from transformers import TFAutoModelWithLMHead, AutoTokenizerimport tensorflow as tftokenizer = AutoTokenizer.from_pretrained(&#34;distilbert-base-cased&#34;)model = TFAutoModelWithLMHead.from_pretrained(&#34;distilbert-base-cased&#34;)sequence = f&#34;Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.&#34;input = tokenizer.encode(sequence, return_tensors=&#34;tf&#34;)mask_token_index = tf.where(input == tokenizer.mask_token_id)[0, 1]token_logits = model(input)[0]mask_token_logits = token_logits[0, mask_token_index, :]top_5_tokens = tf.math.top_k(mask_token_logits, 5).indices.numpy()for token in top_5_tokens:    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))</code></pre><p>é€™å°‡æ‰“å°äº”å€‹åºåˆ—ï¼Œå…¶ä¸­å‰äº”å€‹æ¨™è¨˜ç”±æ¨¡å‹é æ¸¬ï¼š</p><pre><code>Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.</code></pre><h1 class=pgc-h-arrow-right>å› æœèªè¨€å»ºæ¨¡</h1><p>å› æœèªè¨€å»ºæ¨¡æ˜¯æ ¹æ“šä¸€ç³»åˆ—çš„æ¨™è¨˜ä¾†é æ¸¬æ¨™è¨˜çš„ä»»å‹™ã€‚åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæ¨¡å‹åªé—œæ³¨å·¦é‚Šçš„ä¸Šä¸‹æ–‡(æ©ç¢¼å·¦é‚Šçš„æ¨™è¨˜)ã€‚é€™æ¨£çš„è¨“ç·´å°æ–¼ç”Ÿæˆä»»å‹™ä¾†èªªæ˜¯æœ‰ä½œç”¨çš„ã€‚</p><p>ç›®å‰é‚„æ²’æœ‰é€²è¡Œå› æœèªè¨€å»ºæ¨¡/ç”Ÿæˆçš„ç®¡é“ã€‚ ä¸‹é¢æ˜¯ä¸€å€‹ä½¿ç”¨Tokenizerå’Œæ¨¡å‹çš„ç¤ºä¾‹ã€‚åˆ©ç”¨generate()æ–¹æ³•æŒ‰ç…§PyTorchä¸­çš„åˆå§‹åºåˆ—ç”Ÿæˆæ¨™è¨˜ï¼Œä¸¦åœ¨TensorFlowä¸­å‰µå»ºä¸€å€‹ç°¡å–®çš„å¾ªç’°ã€‚</p><p>Pytorchä»£ç¢¼</p><pre><code>from transformers import AutoModelWithLMHead, AutoTokenizertokenizer = AutoTokenizer.from_pretrained(&#34;gpt2&#34;)model = AutoModelWithLMHead.from_pretrained(&#34;gpt2&#34;)sequence = f&#34;Hugging Face is based in DUMBO, New York City, and is&#34;input = tokenizer.encode(sequence, return_tensors=&#34;pt&#34;)generated = model.generate(input, max_length=50)resulting_string = tokenizer.decode(generated.tolist()[0])print(resulting_string)</code></pre><p>TensorFlowä»£ç¢¼</p><pre><code>from transformers import TFAutoModelWithLMHead, AutoTokenizerimport tensorflow as tftokenizer = AutoTokenizer.from_pretrained(&#34;gpt2&#34;)model = TFAutoModelWithLMHead.from_pretrained(&#34;gpt2&#34;)sequence = f&#34;Hugging Face is based in DUMBO, New York City, and is&#34;generated = tokenizer.encode(sequence)for i in range(50):    predictions = model(tf.constant([generated]))[0]    token = tf.argmax(predictions[0], axis=1)[-1].numpy()    generated += [token]resulting_string = tokenizer.decode(generated)print(resulting_string)</code></pre><p>é€™å°‡å¾åŸå§‹åºåˆ—è¼¸å‡º(å¸Œæœ›)çš„å°æ‡‰å­—ç¬¦ä¸²ï¼Œä½¿ç”¨top_p/tok_kåˆ†ä½ˆç²å–generate()æ¡æ¨£çš„çµæœï¼š</p><pre><code>Hugging Face is based in DUMBO, New York City, and is a live-action TV series based on the novel by JohnCarpenter, and its producers, David Kustlin and Steve Pichar. The film is directed by!</code></pre><h1 class=pgc-h-arrow-right>å‘½åå¯¦é«”è­˜åˆ¥</h1><p>å‘½åå¯¦é«”è­˜åˆ¥(NER)æ˜¯æ ¹æ“šé¡åˆ¥å°æ¨™è¨˜é€²è¡Œåˆ†é¡çš„ä»»å‹™ï¼Œä¾‹å¦‚å°‡æ¨™è¨˜æ¨™è­˜ç‚ºå€‹äººã€çµ„ç¹”æˆ–ä½ç½®ã€‚å‘½åå¯¦é«”è­˜åˆ¥æ•¸æ“šé›†çš„ä¸€å€‹ä¾‹å­æ˜¯CoNLL-2003æ•¸æ“šé›†ï¼Œå®ƒå®Œå…¨åŸºæ–¼è©²ä»»å‹™ã€‚å¦‚æœä½ æƒ³å°NERä»»å‹™çš„æ¨¡å‹é€²è¡Œå¾®èª¿ï¼Œå¯ä»¥åˆ©ç”¨ner/run_ner.py(PyTorch)ã€ner/run_pl_ner.py(åˆ©ç”¨PyTorch lightning)æˆ–ner/run_tf_ner.py(TensorFlow)è…³æœ¬ã€‚</p><p>ä¸‹é¢æ˜¯ä¸€å€‹ä½¿ç”¨ç®¡é“é€²è¡Œå‘½åå¯¦é«”è­˜åˆ¥çš„ç¤ºä¾‹ï¼Œè©¦åœ–å°‡æ¨™è¨˜æ¨™è­˜ç‚ºå±¬æ–¼9å€‹é¡ä¹‹ä¸€ï¼š</p><ul><li>O, ä¸æ˜¯å‘½åå¯¦é«”</li><li>B-MIS, ä¸€å€‹é›œé …å¯¦é«”çš„é–‹é ­</li><li>I-MIS, é›œé …å¯¦é«”</li><li>B-PER, ä¸€å€‹äººåçš„é–‹é ­</li><li>I-PER, äººå</li><li>B-ORG, ä¸€å€‹çµ„ç¹”çš„é–‹é ­</li><li>I-ORG, çµ„ç¹”</li><li>B-LOC, ä¸€å€‹åœ°é»çš„é–‹é ­</li><li>I-LOC, åœ°é»</li></ul><p>å®ƒåˆ©ç”¨CoNLL-2003ä¸Šä¸€å€‹ç¶“éå¾®èª¿çš„æ¨¡å‹ï¼Œç”±dbmdzçš„@stefan-ité€²è¡Œäº†å¾®èª¿ã€‚</p><pre><code>from transformers import pipelinenlp = pipeline(&#34;ner&#34;)sequence = &#34;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&#34; \           &#34;close to the Manhattan Bridge which is visible from the window.&#34;print(nlp(sequence))</code></pre><p>é€™å°‡è¼¸å‡ºä¸Šé¢å®šç¾©çš„9å€‹é¡ä¸­æ¨™è­˜ç‚ºå¯¦é«”çš„æ‰€æœ‰å–®è©çš„åˆ—è¡¨ã€‚ä»¥ä¸‹æ˜¯é æœŸçµæœï¼š</p><pre><code>[    {&#39;word&#39;: &#39;Hu&#39;, &#39;score&#39;: 0.9995632767677307, &#39;entity&#39;: &#39;I-ORG&#39;},    {&#39;word&#39;: &#39;##gging&#39;, &#39;score&#39;: 0.9915938973426819, &#39;entity&#39;: &#39;I-ORG&#39;},    {&#39;word&#39;: &#39;Face&#39;, &#39;score&#39;: 0.9982671737670898, &#39;entity&#39;: &#39;I-ORG&#39;},    {&#39;word&#39;: &#39;Inc&#39;, &#39;score&#39;: 0.9994403719902039, &#39;entity&#39;: &#39;I-ORG&#39;},    {&#39;word&#39;: &#39;New&#39;, &#39;score&#39;: 0.9994346499443054, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;York&#39;, &#39;score&#39;: 0.9993270635604858, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;City&#39;, &#39;score&#39;: 0.9993864893913269, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;D&#39;, &#39;score&#39;: 0.9825621843338013, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;##UM&#39;, &#39;score&#39;: 0.936983048915863, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;##BO&#39;, &#39;score&#39;: 0.8987102508544922, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;Manhattan&#39;, &#39;score&#39;: 0.9758241176605225, &#39;entity&#39;: &#39;I-LOC&#39;},    {&#39;word&#39;: &#39;Bridge&#39;, &#39;score&#39;: 0.990249514579773, &#39;entity&#39;: &#39;I-LOC&#39;}]</code></pre><p>æ³¨æ„â€œHugging Faceâ€æ˜¯å¦‚ä½•è¢«ç¢ºå®šç‚ºä¸€å€‹çµ„ç¹”ï¼Œâ€œNew York Cityâ€ï¼Œâ€œDUMBOâ€å’Œâ€œManhattan Bridgeâ€æ˜¯å¦‚ä½•è¢«ç¢ºå®šç‚ºåœ°é»çš„ã€‚</p><p>ä¸‹é¢æ˜¯ä¸€å€‹ä½¿ç”¨æ¨¡å‹å’ŒTokenizeré€²è¡Œå‘½åå¯¦é«”è­˜åˆ¥çš„ç¤ºä¾‹ã€‚ è©²éç¨‹å¦‚ä¸‹ï¼š</p><ul><li>å¾checkpointåç¨±å¯¦ä¾‹åŒ–ä¸€å€‹tokenizerå’Œä¸€å€‹æ¨¡å‹ã€‚è©²æ¨¡å‹è¢«è­˜åˆ¥ç‚ºä¸€å€‹BERTæ¨¡å‹ï¼Œä¸¦ç”¨å­˜å„²åœ¨checkpointä¸­çš„æ¬Šé‡åŠ è¼‰å®ƒã€‚</li><li>å®šç¾©ç”¨æ–¼è¨“ç·´æ¨¡å‹çš„æ¨™ç±¤åˆ—è¡¨ã€‚</li><li>å®šç¾©ä¸€å€‹åŒ…å«å·²çŸ¥å¯¦é«”çš„åºåˆ—ï¼Œä¾‹å¦‚â€œHugging Faceâ€ä½œç‚ºä¸€å€‹çµ„ç¹”ï¼Œâ€œNew York Cityâ€ä½œç‚ºä¸€å€‹ä½ç½®ã€‚</li><li>å°‡å–®è©æ‹†åˆ†ç‚ºæ¨™è¨˜ï¼Œä»¥ä¾¿å®ƒå€‘å¯ä»¥æ˜ å°„åˆ°é æ¸¬ã€‚æˆ‘å€‘ä½¿ç”¨ä¸€å€‹å°æŠ€å·§ï¼Œé¦–å…ˆå°åºåˆ—é€²è¡Œå®Œå…¨çš„ç·¨ç¢¼å’Œè§£ç¢¼ï¼Œé€™æ¨£å°±ç•™ä¸‹äº†ä¸€å€‹åŒ…å«ç‰¹æ®Šæ¨™è¨˜çš„å­—ç¬¦ä¸²ã€‚</li><li>å°‡è©²åºåˆ—ç·¨ç¢¼ç‚ºid(è‡ªå‹•æ·»åŠ ç‰¹æ®Šæ¨™è¨˜)ã€‚</li><li>é€šéå°‡è¼¸å…¥å‚³éåˆ°æ¨¡å‹ä¸¦ç²å¾—ç¬¬ä¸€å€‹è¼¸å‡ºä¾†æª¢ç´¢é æ¸¬ã€‚é€™å°‡å°è‡´æ¯å€‹æ¨™è¨˜åœ¨9å€‹å¯èƒ½çš„é¡ä¸Šåˆ†ä½ˆã€‚æˆ‘å€‘ä½¿ç”¨argmaxä¾†æª¢ç´¢æ¯å€‹æ¨™è¨˜æœ€å¯èƒ½çš„é¡ã€‚</li><li>å°‡æ¯å€‹æ¨™è¨˜åŠå…¶é æ¸¬åˆ°ä¸€èµ·ä¸¦æ‰“å°å‡ºä¾†ã€‚</li></ul><p>Pytorchä»£ç¢¼</p><pre><code>from transformers import AutoModelForTokenClassification, AutoTokenizerimport torchmodel = AutoModelForTokenClassification.from_pretrained(&#34;dbmdz/bert-large-cased-finetuned-conll03-english&#34;)tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased&#34;)label_list = [    &#34;O&#34;,       # ä¸æ˜¯å‘½åå¯¦é«”    &#34;B-MISC&#34;,  # ä¸€å€‹é›œé …å¯¦é«”çš„é–‹é ­    &#34;I-MISC&#34;,  # é›œé …    &#34;B-PER&#34;,   # ä¸€å€‹äººåçš„é–‹é ­    &#34;I-PER&#34;,   # äººå    &#34;B-ORG&#34;,   # ä¸€å€‹çµ„ç¹”çš„é–‹é ­    &#34;I-ORG&#34;,   # çµ„ç¹”    &#34;B-LOC&#34;,   # ä¸€å€‹åœ°é»çš„é–‹é ­    &#34;I-LOC&#34;    # åœ°é»]sequence = &#34;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&#34; \           &#34;close to the Manhattan Bridge.&#34;# Bit of a hack to get the tokens with the special tokenstokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))inputs = tokenizer.encode(sequence, return_tensors=&#34;pt&#34;)outputs = model(inputs)[0]predictions = torch.argmax(outputs, dim=2)print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].tolist())])</code></pre><p>TensorFlowä»£ç¢¼</p><pre><code>from transformers import TFAutoModelForTokenClassification, AutoTokenizerimport tensorflow as tfmodel = TFAutoModelForTokenClassification.from_pretrained(&#34;dbmdz/bert-large-cased-finetuned-conll03-english&#34;)tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased&#34;)label_list = [    &#34;O&#34;,       # ä¸æ˜¯å‘½åå¯¦é«”    &#34;B-MISC&#34;,  # ä¸€å€‹é›œé …å¯¦é«”çš„é–‹é ­    &#34;I-MISC&#34;,  # é›œé …    &#34;B-PER&#34;,   # ä¸€å€‹äººåçš„é–‹é ­    &#34;I-PER&#34;,   # äººå    &#34;B-ORG&#34;,   # ä¸€å€‹çµ„ç¹”çš„é–‹é ­    &#34;I-ORG&#34;,   # çµ„ç¹”    &#34;B-LOC&#34;,   # ä¸€å€‹åœ°é»çš„é–‹é ­    &#34;I-LOC&#34;    # åœ°é»]sequence = &#34;Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very&#34; \           &#34;close to the Manhattan Bridge.&#34;#ç”¨ç‰¹æ®Šçš„æ¨™è¨˜ä¾†ç²å–æ¨™è¨˜çš„ä¸€é»æŠ€å·§tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))inputs = tokenizer.encode(sequence, return_tensors=&#34;tf&#34;)outputs = model(inputs)[0]predictions = tf.argmax(outputs, axis=2)print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())])</code></pre><p>é€™å°‡è¼¸å‡ºæ˜ å°„åˆ°å…¶é æ¸¬çš„æ¯å€‹æ¨™è¨˜çš„åˆ—è¡¨ã€‚èˆ‡ç®¡é“ä¸åŒçš„æ˜¯ï¼Œé€™è£¡æ¯å€‹æ¨™è¨˜éƒ½æœ‰ä¸€å€‹é æ¸¬ï¼Œå› ç‚ºæˆ‘å€‘æ²’æœ‰åˆªé™¤â€œOâ€é¡ï¼Œé€™æ„å‘³è‘—åœ¨è©²æ¨™è¨˜ä¸Šæ‰¾ä¸åˆ°ç‰¹å®šçš„å¯¦é«”ã€‚ä»¥ä¸‹æ•¸çµ„æ‡‰ç‚ºè¼¸å‡ºï¼š</p><pre><code>[(&#39;[CLS]&#39;, &#39;O&#39;), (&#39;Hu&#39;, &#39;I-ORG&#39;), (&#39;##gging&#39;, &#39;I-ORG&#39;), (&#39;Face&#39;, &#39;I-ORG&#39;), (&#39;Inc&#39;, &#39;I-ORG&#39;), (&#39;.&#39;, &#39;O&#39;), (&#39;is&#39;, &#39;O&#39;), (&#39;a&#39;, &#39;O&#39;), (&#39;company&#39;, &#39;O&#39;), (&#39;based&#39;, &#39;O&#39;), (&#39;in&#39;, &#39;O&#39;), (&#39;New&#39;, &#39;I-LOC&#39;), (&#39;York&#39;, &#39;I-LOC&#39;), (&#39;City&#39;, &#39;I-LOC&#39;), (&#39;.&#39;, &#39;O&#39;), (&#39;Its&#39;, &#39;O&#39;), (&#39;headquarters&#39;, &#39;O&#39;), (&#39;are&#39;, &#39;O&#39;), (&#39;in&#39;, &#39;O&#39;), (&#39;D&#39;, &#39;I-LOC&#39;), (&#39;##UM&#39;, &#39;I-LOC&#39;), (&#39;##BO&#39;, &#39;I-LOC&#39;), (&#39;,&#39;, &#39;O&#39;), (&#39;therefore&#39;, &#39;O&#39;), (&#39;very&#39;, &#39;O&#39;), (&#39;##c&#39;, &#39;O&#39;), (&#39;##lose&#39;, &#39;O&#39;), (&#39;to&#39;, &#39;O&#39;), (&#39;the&#39;, &#39;O&#39;), (&#39;Manhattan&#39;, &#39;I-LOC&#39;), (&#39;Bridge&#39;, &#39;I-LOC&#39;), (&#39;.&#39;, &#39;O&#39;), (&#39;[SEP]&#39;, &#39;O&#39;)]</code></pre></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>Transformers</a></li><li><a>ç”¨ä¾‹</a></li><li><a>ä¸­å¸¸</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=æœç´¢>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>ğŸ”</button></form></section><section class=widget><h3 class=widget-title>æœ€æ–°æ–‡ç«  âš¡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/95bc1c3b.html alt=æ–½å·¥ä¸­å¸¸è¦‹8ç¨®é‹¼ç­‹åˆ†é¡ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ee33c2f5893849fbb7ed9dd98f7cba72 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/95bc1c3b.html title=æ–½å·¥ä¸­å¸¸è¦‹8ç¨®é‹¼ç­‹åˆ†é¡>æ–½å·¥ä¸­å¸¸è¦‹8ç¨®é‹¼ç­‹åˆ†é¡</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b15bfb03.html alt=æ–½å·¥ä¸­å¸¸è¦‹çš„é‹¼ç­‹ä½ éœ€è¦çŸ¥é“çš„çŸ¥è­˜ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/e912e61511874e9db8e79597811ebabd style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b15bfb03.html title=æ–½å·¥ä¸­å¸¸è¦‹çš„é‹¼ç­‹ä½ éœ€è¦çŸ¥é“çš„çŸ¥è­˜>æ–½å·¥ä¸­å¸¸è¦‹çš„é‹¼ç­‹ä½ éœ€è¦çŸ¥é“çš„çŸ¥è­˜</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/61d128bc.html alt=å²©åœŸå·¥ç¨‹ä¸­å¸¸è¦‹çš„â€œå•é¡ŒåœŸâ€ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/8803daf3aacd493fb1c4c99d8aebda28 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/61d128bc.html title=å²©åœŸå·¥ç¨‹ä¸­å¸¸è¦‹çš„â€œå•é¡ŒåœŸâ€>å²©åœŸå·¥ç¨‹ä¸­å¸¸è¦‹çš„â€œå•é¡ŒåœŸâ€</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9b8bf240.html alt=æ­£äº¤æ³•â€”â€”æ¸¬è©¦ç”¨ä¾‹è¨­è¨ˆæ–¹æ³• class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/78c1ce95b7c344d0bf5c3d8c5d7463ab style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9b8bf240.html title=æ­£äº¤æ³•â€”â€”æ¸¬è©¦ç”¨ä¾‹è¨­è¨ˆæ–¹æ³•>æ­£äº¤æ³•â€”â€”æ¸¬è©¦ç”¨ä¾‹è¨­è¨ˆæ–¹æ³•</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/81df39fc.html alt=æ¸¬è©¦ç”¨ä¾‹è¨­è¨ˆæ–¹æ³•(äºŒ)â€”â€”æ­£äº¤å¯¦é©—æ³• class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a1f74afae8fe4b7080cb8188d32254b8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/81df39fc.html title=æ¸¬è©¦ç”¨ä¾‹è¨­è¨ˆæ–¹æ³•(äºŒ)â€”â€”æ­£äº¤å¯¦é©—æ³•>æ¸¬è©¦ç”¨ä¾‹è¨­è¨ˆæ–¹æ³•(äºŒ)â€”â€”æ­£äº¤å¯¦é©—æ³•</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f117353f.html alt=è»Ÿä»¶æ¸¬è©¦ç”¨ä¾‹ä¹‹æ­£äº¤è©¦é©—æ³• class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1535955562035888ded133e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f117353f.html title=è»Ÿä»¶æ¸¬è©¦ç”¨ä¾‹ä¹‹æ­£äº¤è©¦é©—æ³•>è»Ÿä»¶æ¸¬è©¦ç”¨ä¾‹ä¹‹æ­£äº¤è©¦é©—æ³•</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1f6fbe66.html alt=ç”Ÿæ´»ä¸­å¸¸è¦‹çš„è‹±æ–‡ç¸®å¯«ï¼Œä½ å­¸æœƒäº†å—ï¼Ÿ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/153416357927380a4b4f9b2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1f6fbe66.html title=ç”Ÿæ´»ä¸­å¸¸è¦‹çš„è‹±æ–‡ç¸®å¯«ï¼Œä½ å­¸æœƒäº†å—ï¼Ÿ>ç”Ÿæ´»ä¸­å¸¸è¦‹çš„è‹±æ–‡ç¸®å¯«ï¼Œä½ å­¸æœƒäº†å—ï¼Ÿ</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ccee16d3.html alt=åŠŸèƒ½æ¸¬è©¦ç”¨ä¾‹è¨­è¨ˆæŠ€è¡“æ–¹æ³•â€”â€”ã€Šå› æœåœ–åˆ†ææ³•ã€‹ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1522490029316e07f49ff30 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ccee16d3.html title=åŠŸèƒ½æ¸¬è©¦ç”¨ä¾‹è¨­è¨ˆæŠ€è¡“æ–¹æ³•â€”â€”ã€Šå› æœåœ–åˆ†ææ³•ã€‹>åŠŸèƒ½æ¸¬è©¦ç”¨ä¾‹è¨­è¨ˆæŠ€è¡“æ–¹æ³•â€”â€”ã€Šå› æœåœ–åˆ†ææ³•ã€‹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b4aca8e2.html alt=ç·¨å¯«æ¸¬è©¦ç”¨ä¾‹çš„å¹¾ç¨®æ–¹æ³• class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7336aa5d97ae46b5a4fd215429afb09b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b4aca8e2.html title=ç·¨å¯«æ¸¬è©¦ç”¨ä¾‹çš„å¹¾ç¨®æ–¹æ³•>ç·¨å¯«æ¸¬è©¦ç”¨ä¾‹çš„å¹¾ç¨®æ–¹æ³•</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/19b46c1c.html alt=4ç¨®è£½é€ æ¥­å¤§æ•¸æ“šåˆ†æç”¨ä¾‹åˆ†äº« class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/79c1667cafcc423b880e472ac32735a6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/19b46c1c.html title=4ç¨®è£½é€ æ¥­å¤§æ•¸æ“šåˆ†æç”¨ä¾‹åˆ†äº«>4ç¨®è£½é€ æ¥­å¤§æ•¸æ“šåˆ†æç”¨ä¾‹åˆ†äº«</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6dff48fd.html alt="CSS å½å…ƒç´ çš„ä¸€äº›ç½•è¦‹ç”¨ä¾‹" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/489c31a395a84b9aaa450264e0e83ec0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6dff48fd.html title="CSS å½å…ƒç´ çš„ä¸€äº›ç½•è¦‹ç”¨ä¾‹">CSS å½å…ƒç´ çš„ä¸€äº›ç½•è¦‹ç”¨ä¾‹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9d573b2b.html alt=Daiç©©å®šå¹£çš„åå¤§ç”¨ä¾‹åŠå…¶å„ªå‹¢ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RivR6n1AU5yy8h style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9d573b2b.html title=Daiç©©å®šå¹£çš„åå¤§ç”¨ä¾‹åŠå…¶å„ªå‹¢>Daiç©©å®šå¹£çš„åå¤§ç”¨ä¾‹åŠå…¶å„ªå‹¢</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/36c77ab8.html alt="UML - å»ºæ¨¡åŸºç¤ - ç”¨ä¾‹é©…å‹•" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/10d1a9db8fa34ac5924bff940d5b2c8f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/36c77ab8.html title="UML - å»ºæ¨¡åŸºç¤ - ç”¨ä¾‹é©…å‹•">UML - å»ºæ¨¡åŸºç¤ - ç”¨ä¾‹é©…å‹•</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4aa66f67.html alt=UMLç”¨ä¾‹åœ–ç¸½çµ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3881c18a31494f37ba1ea7008da33889 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4aa66f67.html title=UMLç”¨ä¾‹åœ–ç¸½çµ>UMLç”¨ä¾‹åœ–ç¸½çµ</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/796dc49a.html alt=â€œç”¨ä¾‹è¦åŠƒâ€åˆ°åº•è©²æ€éº¼åšï¼Ÿ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RatayRtH3ElLfi style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/796dc49a.html title=â€œç”¨ä¾‹è¦åŠƒâ€åˆ°åº•è©²æ€éº¼åšï¼Ÿ>â€œç”¨ä¾‹è¦åŠƒâ€åˆ°åº•è©²æ€éº¼åšï¼Ÿ</a></li><hr></ul></section><section class=widget><h3 class=widget-title>å…¶ä»–</h3><ul class=widget-list><li><a href=TOS.html>ä½¿ç”¨æ¢æ¬¾</a></li><li><a href=CommentPolicy.html>ç•™è¨€æ”¿ç­–</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>è¯çµ¡æˆ‘å€‘</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>æå®¢å¿«è¨Š</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>