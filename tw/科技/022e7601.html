<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制 | 极客快訊</title><meta property="og:title" content="谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/bbee9a1dedfc4193866b16678bf44539"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/022e7601.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/022e7601.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/022e7601.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/022e7601.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/022e7601.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/022e7601.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/022e7601.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/022e7601.html><meta property="article:published_time" content="2020-10-29T21:11:54+08:00"><meta property="article:modified_time" content="2020-10-29T21:11:54+08:00"><meta name=Keywords content><meta name=description content="谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/022e7601.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p style=text-align:justify><p class=pgc-img><img alt=谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bbee9a1dedfc4193866b16678bf44539></p><p style=text-align:justify><br></p><p style=text-align:justify><p style=text-align:justify><strong>【新智元導讀】谷歌、 DeepMind、艾倫圖靈研究院和劍橋大學的科學家們提出了「Performer」，一種線性擴展的人工智能模型架構，並在蛋白質序列建模等任務中表現良好。它有潛力影響生物序列分析的研究，降低計算成本和計算複雜性，同時減少能源消耗和碳排放。</strong></p><p style=text-align:justify><br></p><p style=text-align:justify>Transformer 模型在很多不同的領域都取得了SOTA，包括自然語言，對話，圖像，甚至音樂。每個 Transformer 體系結構的核心模塊是 Attention 模塊，它為一個輸入序列中的所有位置對計算相似度score。</p><p style=text-align:justify></p><p style=text-align:justify>然而，這種方法在輸入序列的長度較長時效果不佳，需要計算時間呈平方增長來產生所有相似性得分，以及存儲空間的平方增長來構造一個矩陣存儲這些score。</p><p style=text-align:justify></p><p style=text-align:justify>對於需要長距離注意力的應用，目前已經提出了幾種快速且更節省空間的方法，如內存緩存技術，但是一種更常見的方法是依賴於稀疏注意力。</p><p style=text-align:justify></p><p style=text-align:justify>稀疏注意力機制通過從一個序列而不是所有可能的Pair中計算經過選擇的相似性得分來減少注意機制的計算時間和內存需求，從而產生一個稀疏矩陣而不是一個完整的矩陣。</p><p style=text-align:justify></p><p style=text-align:justify>這些稀疏條目可以通過優化的方法找到、學習，甚至隨機化，如Sparse Transformers、Longformers、RoutingTransformers、Reformers和BigBird。</p><p class=pgc-img><img alt=谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/46d764dc664f495c857eeefc3d42ff0e></p><p style=text-align:justify>由於稀疏矩陣也可以用圖形和邊來表示，稀疏化方法也受到圖神經網絡文獻的推動，在圖注意網絡中列出了與注意力的具體關係。這種基於稀疏性的體系結構通常需要額外的層來隱式地產生完全的注意力機制。</p><p style=text-align:justify></p><p style=text-align:justify><strong>不幸的是，稀疏注意力的方法仍然會受到一些限制，如：</strong></p><p style=text-align:justify></p><p style=text-align:justify>(1)需要高效的稀疏矩陣乘法運算，但並非所有加速器都能使用;</p><p style=text-align:justify></p><p style=text-align:justify>(2)通常不能為其表示能力提供嚴格的理論保證;</p><p style=text-align:justify></p><p style=text-align:justify>(3)主要針對 Transformer 模型和生成式預訓練進行優化;</p><p style=text-align:justify></p><p style=text-align:justify>(4)它們通常堆疊更多的注意力層以補償稀疏表示，使其難以與其他預訓練模型一起使用，因此需要重新訓練和顯著的內存消耗。</p><p style=text-align:justify></p><p style=text-align:justify>除了這些缺點，稀疏注意力機制往往仍然不足以解決所有的正常注意力機制的問題，如指針網絡（Pointer Network）。同時也存在一些不能稀疏化的操作，比如常用的softmax操作，它使注意機制中的相似度得分歸一化，在工業規模的推薦系統中得到了廣泛的應用。</p><p style=text-align:justify></p><p style=text-align:justify>為了解決這些問題，Google AI的研究人員引入了「Performer」，這是一個具有注意力線性擴展機制的Transformer架構，可以使模型在處理更長序列的同時實現更快的訓練，這是對於特定的圖像數據集如 ImageNet64和文本數據集如 PG-19所必需的。</p><p style=text-align:justify></p><p style=text-align:justify><strong>Performer使用了一個有效的(線性的)廣義注意力框架，它是一種允許基於不同的相似性度量(Kernel)的注意力機制。</strong></p><p style=text-align:justify></p><p style=text-align:justify><strong>廣義注意力機制</strong></p><p style=text-align:justify></p><p style=text-align:justify>在原有的注意力機制中，query和key分別對應於矩陣的行和列，再進行相乘並通過softmax形成一個注意力矩陣，並存儲下來相似性score。</p><p style=text-align:justify></p><p style=text-align:justify>請注意，在這種方法中，不能將query-key傳遞到非線性 softmax 操作之後再將其分解回原來的key和query，但是可以將注意力矩陣分解為原始query和key的隨機非線性函數的乘積，也就是所謂的隨機特徵（random features），這樣就可以更有效地對相似性信息進行編碼。</p><p class=pgc-img><img alt=谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/f649968ca4634724a3b1c03eafc40a3c></p><p style=text-align:justify><strong>FAVOR+: Fast Attention via Matrix Associativity</strong></p><p style=text-align:justify></p><p style=text-align:justify>上面描述的那種矩陣分解，使得可以使用線性而不是二次的複雜度來存儲隱式注意力矩陣，同時也可以通過這種分解得到一個線性時間的注意力機制。</p><p style=text-align:justify></p><p style=text-align:justify>原有的注意力機制是將注意力矩陣乘以輸入的value值來得到最終結果，而注意力矩陣分解後，可以重新排列矩陣乘法來逼近常規注意機制的結果，而無需顯式構造二次的注意力矩陣。</p><p class=pgc-img><img alt=谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/998d5455b16b45919f09866abd41ce07></p><p style=text-align:justify><p style=text-align:justify>上述分析與所謂的雙向注意力有關，即沒有過去和未來概念的「非因果注意力」。</p><p style=text-align:justify></p><p style=text-align:justify>對於單向(因果)注意力，即Mask掉不參與輸入序列後面計算的其他token，只使用前面的token參與計算，只存儲運行矩陣計算的結果，而不是存儲一個顯式的下三角注意力矩陣。</p><p class=pgc-img><img alt=谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d9dd7e188b3147899b7ed7716bb8dd4f></p><p style=text-align:justify><br></p><p style=text-align:justify><strong>性能</strong></p><p style=text-align:justify></p><p style=text-align:justify>我們首先對Performer的空間和時間複雜度進行基準測試，結果表明，注意力加速和內存減少幾乎是最優的，也就是說，結果非常接近於在模型中根本不使用注意力機制。</p><p class=pgc-img><img alt=谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2208f8d90b9f470b85bbeb314002cdb1></p><p style=text-align:justify>研究人員又進一步展示了 Performer，使用無偏 softmax 逼近，向後兼容經過一點微調的預訓練Transformer模型，可以通過提高推斷速度降低成本，而不需要完全重新訓練已有的模型。</p><p class=pgc-img><img alt=谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/c6b1e96e0b6d4eebbe634b27ea5b49fc></p><p style=text-align:justify><strong>案例：蛋白質序列建模</strong></p><p style=text-align:justify></p><p style=text-align:justify>蛋白質是具有複雜三維結構和特定功能的大分子，對生命來說至關重要。與單詞一樣，蛋白質被指定為線性序列，其中每個字符是20個氨基酸構建塊中的一個。</p><p style=text-align:justify></p><p style=text-align:justify>將 Transformers 應用於大型未標記的蛋白質序列產生的模型可用於對摺疊的功能性大分子進行準確的預測。</p><p style=text-align:justify></p><p style=text-align:justify>Performer-ReLU (使用基於 relu 的注意力，這是一個不同於 softmax 的廣義注意力)在蛋白質序列數據建模方面有很強的表現，而 Performer-Softmax 與 Transformer 的性能相匹配，正如理論所預測的結果那樣。</p><p class=pgc-img><img alt=谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b408a58edb774f418b1f861e02d3ff7c></p><p style=text-align:justify>下面，我們可視化一個蛋白質Performer模型，使用基於 relu 的近似注意力機制進行訓練，使用 Performer 來估計氨基酸之間的相似性，從序列比對中分析進化替換模式得到的替換矩陣中恢復類似的結構。</p><p class=pgc-img><img alt=谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/df4f6e2035d744339040b5d0e0b493bd></p><p style=text-align:justify>更一般地說，我們發現局部和全局注意力機制與用蛋白質數據訓練的Transformer模型一致。Dense Attention的近似Performer有可能捕捉跨越多個蛋白質序列的全局相互作用。</p><p class=pgc-img><img alt=谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5dca4419489747a8b8d5cffb2460f346></p><p style=text-align:justify>作為概念的驗證，對長串聯蛋白質序列進行模型訓練，會使得常規 Transformer 模型的內存過載，但 Performer模型的內存不會過載，因為它的空間利用很高效。</p><p style=text-align:justify></p><p style=text-align:justify><strong>結論</strong></p><p style=text-align:justify></p><p style=text-align:justify>Google AI的這項工作有助於改進基於非稀疏的方法和基於Kernel的Transformer，這種方法也可以與其他技術互操作，研究人員甚至還將 FAVOR 與Reformer的代碼集成在一起。同時研究人員還提供了論文、 Performer的代碼和蛋白質語言模型的代碼鏈接。</p><p style=text-align:justify></p><p style=text-align:justify>Google AI的研究人員相信，他們對於Performer的研究開闢了一種關於Attention、Transformer架構甚至Kernel的全新的思維方式，對於進一步的改進有巨大的啟示作用。</p><p style=text-align:justify></p><p style=text-align:justify><p style=text-align:justify><br></p><p class=pgc-img><img alt=谷歌聯手DeepMind提出Performer：用新方式重新思考注意力機制 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7ee8943aeb034b0fbb6a5d987a2e99f6></p><p style=text-align:justify><br></p></p></p></p></p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>聯手</a></li><li><a>DeepMind</a></li><li><a>Performer</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/b11edaf9.html alt="聯手HERE，高德進軍地圖海外市場 | CES 2020" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RmzgE048umzR6f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b11edaf9.html title="聯手HERE，高德進軍地圖海外市場 | CES 2020">聯手HERE，高德進軍地圖海外市場 | CES 2020</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f7df70d5.html alt="嶗山區金家嶺街道：黨群聯手戮力同心 打好疫情阻擊戰" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f7df70d5.html title="嶗山區金家嶺街道：黨群聯手戮力同心 打好疫情阻擊戰">嶗山區金家嶺街道：黨群聯手戮力同心 打好疫情阻擊戰</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9bde3cab.html alt=大華樂橙佈局民用智能安防！聯手阿里、華為推智能雲門鎖 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/1540389038739b86f8bf536 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9bde3cab.html title=大華樂橙佈局民用智能安防！聯手阿里、華為推智能雲門鎖>大華樂橙佈局民用智能安防！聯手阿里、華為推智能雲門鎖</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1fb22a23.html alt="布加迪與弗勞恩霍夫聯手 利用3D打印設備製造鈦金屬製動鉗" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/65b40013401a0e17d98b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1fb22a23.html title="布加迪與弗勞恩霍夫聯手 利用3D打印設備製造鈦金屬製動鉗">布加迪與弗勞恩霍夫聯手 利用3D打印設備製造鈦金屬製動鉗</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/568587ac.html alt=《重案六組》第五部來了，六組老人再度聯手推出《天下無詐》！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7c3e2dfa396a4a738a1f730f22001887 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/568587ac.html title=《重案六組》第五部來了，六組老人再度聯手推出《天下無詐》！>《重案六組》第五部來了，六組老人再度聯手推出《天下無詐》！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6fb83afc.html alt=全球民間最大教育組織聯手百年童書協會：這些書告訴孩子創造性思維如何改變世界 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RT8JhxnF4hpOvY style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6fb83afc.html title=全球民間最大教育組織聯手百年童書協會：這些書告訴孩子創造性思維如何改變世界>全球民間最大教育組織聯手百年童書協會：這些書告訴孩子創造性思維如何改變世界</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6806f9ec.html alt="四強聯手 長三角支持長鑫12英寸存儲器基地項目建設" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ea10bb0377f94b6f8bff872f7b2f08f4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6806f9ec.html title="四強聯手 長三角支持長鑫12英寸存儲器基地項目建設">四強聯手 長三角支持長鑫12英寸存儲器基地項目建設</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/e3c49030.html alt=京東聯手電信推權益卡，19元月租再送一年愛奇藝會員+無門檻京卷 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/0c32306fe7834295a16b3e33223c2bc9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/e3c49030.html title=京東聯手電信推權益卡，19元月租再送一年愛奇藝會員+無門檻京卷>京東聯手電信推權益卡，19元月租再送一年愛奇藝會員+無門檻京卷</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/b8b6c889.html alt="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/S1YPO2YkDdit style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/b8b6c889.html title="帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020">帝國理工聯手谷歌提出抽象文本摘要最佳模型 | ICML 2020</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/1b40fc7f.html alt="“鐵齒鋼牙”聯手邁向科創板 湖南株洲產業鏈多點開花" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/S3SqCmHA9ZPtSN style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/1b40fc7f.html title="“鐵齒鋼牙”聯手邁向科創板 湖南株洲產業鏈多點開花">“鐵齒鋼牙”聯手邁向科創板 湖南株洲產業鏈多點開花</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/71f894ef.html alt=新創公司聯手IMEC用GAA晶體管打造最小SRAM class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/15276208161253817d88699 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/71f894ef.html title=新創公司聯手IMEC用GAA晶體管打造最小SRAM>新創公司聯手IMEC用GAA晶體管打造最小SRAM</a></li><hr><li><a href=../../tw/%E9%81%8A%E6%88%B2/206d9dee.html alt=精彩超一龍西提猜！泰拳天王聯手太極實戰第一人決戰峨眉之巔 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/471f00043af81800441a style=border-radius:25px></a>
<a href=../../tw/%E9%81%8A%E6%88%B2/206d9dee.html title=精彩超一龍西提猜！泰拳天王聯手太極實戰第一人決戰峨眉之巔>精彩超一龍西提猜！泰拳天王聯手太極實戰第一人決戰峨眉之巔</a></li><hr><li><a href=../../tw/%E9%81%8A%E6%88%B2/21afab05.html alt=物理學家和考古學家、歷史學家聯手合作，揭開了吉薩金字塔的祕密 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/436700033219b69bcef5 style=border-radius:25px></a>
<a href=../../tw/%E9%81%8A%E6%88%B2/21afab05.html title=物理學家和考古學家、歷史學家聯手合作，揭開了吉薩金字塔的祕密>物理學家和考古學家、歷史學家聯手合作，揭開了吉薩金字塔的祕密</a></li><hr><li><a href=../../tw/%E9%81%8A%E6%88%B2/3492717a.html alt=聯手打造“天師符”起承文化創新服務模式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/a4469b2ba41c49839f8ef4dbb5f82c90 style=border-radius:25px></a>
<a href=../../tw/%E9%81%8A%E6%88%B2/3492717a.html title=聯手打造“天師符”起承文化創新服務模式>聯手打造“天師符”起承文化創新服務模式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/616e57b3.html alt=4月11日鋼廠聯手暴漲！再漲150！漲到你哭！（附報價） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/26e30725-d157-4e8f-8d88-08261efdd69d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/616e57b3.html title=4月11日鋼廠聯手暴漲！再漲150！漲到你哭！（附報價）>4月11日鋼廠聯手暴漲！再漲150！漲到你哭！（附報價）</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>