<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述 | 极客快訊</title><meta property="og:title" content="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/4b5d8378a8e746f9bd96bb031b90e2f1"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/03a3050.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/03a3050.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/03a3050.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/03a3050.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/03a3050.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/03a3050.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/03a3050.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/03a3050.html><meta property="article:published_time" content="2020-10-29T21:05:26+08:00"><meta property="article:modified_time" content="2020-10-29T21:05:26+08:00"><meta name=Keywords content><meta name=description content="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/03a3050.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><blockquote>圖神經網絡（GNN）熱度持續上升，之前我們曾介紹了清華兩篇綜述論文，參見：深度學習時代的圖模型，清華髮文綜述圖網絡，和清華大學圖神經網絡綜述：模型與應用。最近，IEEE Fellow、Senior Member 和 Member Zonghan Wu 等人又貢獻了一篇圖神經網絡綜述文章。這篇文章介紹了 GNN 的背景知識、發展歷史、分類與框架、應用等，詳細介紹了各種模型與方法，包括公式、模型圖示、算法等，希望對大家有所幫助。</blockquote><p class=ql-align-center><strong>引言</strong></p><p class=ql-align-justify>深度網絡的最新進展推進了模式識別和數據挖掘領域的研究。目標檢測、機器翻譯、語音識別等許多機器學習任務曾高度依賴手工特徵工程來提取信息特徵集合，但多種端到端深度學習方式（即卷積神經網絡、長短期記憶網絡和自編碼器）改變了這種狀況。深度學習在多個領域的成功主要歸功於計算資源的快速發展（如 GPU）、大量訓練數據的收集，還有深度學習從歐幾里得數據（如圖像、文本和視頻）中提取潛在表徵的有效性。例如 CNN 可以利用平移不變性、局部連通性和圖像數據語意合成性，從而提取出與整個數據集共享的局部有意義的特徵，用於各種圖像分析任務。</p><p class=ql-align-justify>儘管深度學習已經在歐幾里得數據中取得了很大的成功，但從非歐幾里得域生成的數據已經取得更廣泛的應用，它們需要有效分析。例如，在電子商務領域，一個基於圖的學習系統能夠利用用戶和產品之間的交互以實現高度精準的推薦。在化學領域，分子被建模為圖，新藥研發需要測定其生物活性。在論文引用網絡中，論文之間通過引用關係互相連接，需要將它們分成不同的類別。</p><p class=ql-align-justify>圖數據的複雜性對現有機器學習算法提出了重大挑戰，因為圖數據是不規則的。每張圖大小不同、節點無序，一張圖中的每個節點都有不同數目的鄰近節點，使得一些在圖像中容易計算的重要運算（如卷積）不能再直接應用於圖。此外，現有機器學習算法的核心假設是實例彼此獨立。然而，圖數據中的每個實例都與周圍的其它實例相關，含有一些複雜的連接信息，用於捕獲數據之間的依賴關係，包括引用、朋友關係和相互作用。</p><p class=ql-align-justify>最近，越來越多的研究開始將深度學習方法應用到圖數據領域。受到深度學習領域進展的驅動，研究人員在設計圖神經網絡的架構時借鑑了卷積網絡、循環網絡和深度自編碼器的思想。為了應對圖數據的複雜性，重要運算的泛化和定義在過去幾年中迅速發展。例如，圖 1 展示了受標準 2D 卷積啟發得到的圖卷積。本文旨在對這些方法進行全面概述，受眾包括想要進入這一快速發展領域的研究人員和想要對比圖神經網絡算法的專家。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4b5d8378a8e746f9bd96bb031b90e2f1><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><em>圖 1：2D 卷積 vs. 圖卷積</em></p><p class=ql-align-justify><strong>圖神經網絡簡史</strong></p><p class=ql-align-justify>圖神經網絡的概念首先由 Gori 等人（2005）[16] 提出，並由 Scarselli 等人（2009）[17] 進一步闡明。這些早期的研究以迭代的方式通過循環神經架構傳播鄰近信息來學習目標節點的表示，直到達到穩定的固定點。該過程所需計算量龐大，而近來也有許多研究致力於解決這個難題。在本文中，圖神經網絡代表的是所有用於圖數據的深度學習方法。</p><p class=ql-align-justify>受到卷積網絡在計算機視覺領域所獲巨大成功的激勵，近來出現了很多為圖數據重新定義卷積概念的方法。這些方法屬於圖卷積網絡（GCN）的範疇。Bruna 等人（2013）提出了關於圖卷積網絡的第一項重要研究，他們基於譜圖論（spectral graph theory）開發了一種圖卷積的變體。自此，基於譜的圖卷積網絡不斷改進、拓展、進階。由於譜方法通常同時處理整個圖，並且難以並行或擴展到大圖上，基於空間的圖卷積網絡開始快速發展。這些方法通過聚集近鄰節點的信息，直接在圖結構上執行卷積。結合採樣策略，計算可以在一個批量的節點而不是整個圖中執行，這種做法有望提高效率。</p><p class=ql-align-justify>除了圖卷積網絡，近幾年還開發出了很多替代的圖神經網絡。這些方法包括圖注意力網絡（GAT）、圖自編碼器、圖生成網絡以及圖時空網絡。關於這些方法的分類細節詳見第三章。</p><p class=ql-align-justify>圖神經網絡相關研究。Bronstein 等人用符號幾何深度學習概述了非歐幾里得領域的深度學習方法，包括圖和流形。雖然這是對圖卷積網絡的第一次回顧，但這一項研究遺漏了幾個基於空間的重要方法，包括 [15], [19], [24], [26], [27], [28]，這些方法更新了最新的基準。此外，這項調查沒有囊括很多新開發的架構，這些架構的重要性不亞於圖卷積網絡。</p><p class=ql-align-justify>對於另一項研究，Battaglia 等人 [29] 將圖網絡定位為從關係數據中學習的構建塊，並在統一的框架下回顧了部分圖神經網絡。然而，他們整體的框架是高度抽象的，失去了每種方法在原論文中的見解。Lee 等人 [30] 對圖注意力模型（一種圖神經網絡）進行了部分調查。最近，Zhang 等人 [31] 提出了一項關於圖深度學習的最新調查，卻忽略了對圖生成網絡和圖時空網絡的研究。總之，現有的研究沒有一個對圖神經網絡進行全面的回顧，只覆蓋了部分圖卷積神經網絡且檢查的研究有限，因此遺漏了圖神經網絡替代方法的最新進展，如圖生成網絡和圖時空網絡。</p><p class=ql-align-justify>圖神經網絡 vs. 網絡嵌入。對圖神經網絡的研究與圖嵌入或網絡嵌入緊密相關，這也是數據挖掘和機器學習社區日益關注的一個話題 [32] [33] [34] [35], [36], [37]。網絡嵌入旨在通過保留網絡拓撲架構和節點內容信息，將網絡頂點表示到低維向量空間中，以使任何後續的圖分析任務（如分類、聚類和推薦）都可以通過使用簡單的現成學習機算法（如用於分類的支持向量機）輕鬆執行。許多網絡嵌入算法都是無監督算法，它們大致可分為三組 [32]，即矩陣分解 [38], [39]、隨機遊走 [40] 和深度學習方法。用於網絡嵌入的深度學習方法同時還屬於圖神經網絡，包括基於圖自編碼器的算法（如 DNGR [41] 和 SDNE [42]）和具有無監督訓練的圖卷積神經網絡（如 GraphSage [24]）。圖 2 描述了本文中網絡嵌入和圖神經網絡的區別。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4195e94ad303461baed333486c5948b5><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><em>圖 2：網絡嵌入 vs 圖神經網絡。</em></p><p class=ql-align-justify><strong>本文作出的貢獻如下</strong>：</p><ul><li class=ql-align-justify>新的分類體系：考慮到深度學習在圖數據上的研究與日俱增，我們提出了圖神經網絡（GNN）的新分類體系。在這種分類體系下，GNN 被分成了 5 個類別：圖卷積網絡、圖注意力網絡、圖自編碼器、圖生成網絡和圖時空網絡。我們確定了圖神經網絡和網絡嵌入之間的區別，並在不同的圖神經網絡架構之間建立了聯繫。</li><li class=ql-align-justify>全面的概述：這個綜述提供了在圖數據上的現代深度學習技術的全面概述。對每一種類型的圖神經網絡，我們提供了表徵算法的細節描述，並做了必要的對比和對應算法的總結。</li><li class=ql-align-justify>豐富的資源：這篇綜述提供了圖神經網絡的豐富資源，其中包括當前最佳算法、基準數據集、開源代碼和實踐應用。這篇綜述可以作為理解、使用和開發不同實際應用的深度學習方法的實踐指南。</li><li class=ql-align-justify>未來方向：這篇綜述還強調了已有算法的當前限制，指出了這個快速發展領域未來的可能方向。</li></ul><p class=ql-align-justify><br></p><p class=ql-align-justify><strong>論文：A Comprehensive Survey on Graph Neural Networks</strong></p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fad50f5d8ef84cb6a22f582196d9c254><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p>論文鏈接：https://arxiv.org/pdf/1901.00596v1.pdf</p><p class=ql-align-justify><br></p><p class=ql-align-justify><strong>摘要</strong>：近年來，從圖像分類到視頻處理再到語音識別和自然語言處理，深度學習已經變革了多項機器學習任務。這些任務中的數據通常表示在歐幾里得空間中。然而，越來越多的應用使用非歐幾里得域生成的數據，並將它們表示為具有複雜關係和相互依賴關係的圖。雖然圖數據的複雜性對現有機器學習算法提出了重大挑戰，但最近許多研究開始將深度學習方法擴展到圖數據。</p><p class=ql-align-justify>本文綜述了數據挖掘和機器學習領域中的圖神經網絡（GNN），並按照新的方法對圖神經網絡的最新進展進行了分類。在關注圖卷積網絡的同時，他們還回顧了最近開發的其他架構，例如圖注意力網絡、圖自編碼器，圖生成網絡以及圖時空網絡等。我們還進一步討論了圖神經網絡在多個領域的應用並總結了不同學習任務現有算法的開源代碼及基準。最後，我們提出了這一快速發展領域的研究方向。</p><p class=ql-align-center><strong>2. 定義</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>在這一節，我們提供基礎圖概念的定義。為了方便查詢，我們在表 1 總結了常用的符號。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bbf0629fc3d74ad68e5c96f86a93d0bb><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><em>表 1：常用符號。</em></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c2b7d14f7ac54900bc3f9d490b490027><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/656c4c61afd4461a8c27bce1804e736b><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eef8882e00574722afca607634880de6><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/01c3b03191744dc586daec8f81b631cb><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-center><strong>3. 分類與框架</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>這一部分內容給出了圖神經網絡的分類方法。我們考慮到了所有能與神經架構組合成圖神經網絡的可微圖模型，把圖神經網絡最終分類為：圖卷積網絡、圖注意力網絡、圖自編碼器、圖生成網絡和圖時空網絡。這些網絡中，圖卷積網絡在捕捉架構依存關係上扮演著核心角色。如下圖 3 所示，屬於其他類別的方法部分使用圖卷積網絡作為基礎。表 2 總結了每個類別的代表性方法。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5f2905edab034c408915d68883d2d8d2><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><em>圖 3：圖神經網絡分類</em></p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a7ddaf6f849d40e784155c54081fc40e><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><em>表 2：圖神經網絡代表性論文</em></p><p class=ql-align-justify>下圖 4 展示了圖卷積網絡節點表徵學習的過程。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/814d4f4ad57341bcbbc4a661ec74eed2><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><em>圖 4：有多層 GCN 層的圖卷積網絡變體。通過從鄰域聚合特徵信息，一個 GCN 層把每個節點的隱藏表徵進行壓縮。在特徵聚合之後，非線性置換被應用到生成的輸出上。通過多層堆疊，每個節點的最終隱藏表徵從後續鄰域獲得信息。</em></p><p class=ql-align-justify>下圖 5 展示了多個建立在 GCN 上的圖神經網絡模型。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/812f3521a5ea452eb3a0f613bfada942><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><em>圖 5：建立在 GCN 上的不同圖神經網絡模型。</em></p><p class=ql-align-justify>下圖展示了圖卷積網絡和圖注意力網絡在聚合鄰近節點信息方面的區別。</p><p class=ql-align-justify></p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4fb4628a831d4aab8c74e26ea362bcf9><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p class=ql-align-justify><strong>3.2 框架</strong></p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/99b0a644d3164048aeafb0658285cfbd><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><em>表 3：圖卷積網絡的總結。Node-level 輸出與節點回歸和分類任務相關，Edge-level 輸出與邊分類和鏈接預測任務相關，Graph-level 輸出與圖分類任務相關。</em></p><p class=ql-align-justify><strong>端到端訓練框架</strong>。圖卷積網絡可以以（半）監督或純無監督的方式在端到端學習框架中訓練，依賴於學習任務和可用的標籤信息。</p><ul><li class=ql-align-justify>節點級分類的半監督學習。給定部分節點被標記的單個網絡，圖卷積網絡可以學習到一個魯棒的模型，高效識別未標記節點的類別標籤 [14]。為此，可以通過堆疊一系列的圖卷積層和 softmax 層來建立端到端框架進行多類別分類。</li><li class=ql-align-justify>圖級分類的監督學習。給定一個圖數據集，圖級分類旨在預測整個圖的類別標籤 [55], [56], [74], [75]。這一任務的端到端學習可以利用一個結合了圖卷積層和池化步驟的框架實現 [55], [56]。</li><li class=ql-align-justify>圖嵌入的無監督學習。如果圖中無可用類別標籤，我們可以在一個端到端框架中以完全無監督的方式學習圖嵌入。這些算法通過兩種方式利用邊級（edge-level）信息。一種簡單的方法是採用自編碼器框架，其中編碼器使用圖卷積層將圖嵌進潛在表徵中，然後使用解碼器重構圖結構 [59], [61]。另一種方法是利用負採樣方法，採樣一部分節點對作為負對（negative pair），而圖中已有的節點作為正對（positive pair）。然後在卷積層之後應用 logistic 迴歸層，以用於端到端學習 [24]。</li></ul><p class=ql-align-justify><br></p><p class=ql-align-center><strong>4. 圖卷積網絡</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>這一章概覽圖卷積網絡（GCN），這是很多複雜圖神經網絡模型的基礎。GCN 方法分為兩類，分別基於譜和空間。基於譜的方法通過從圖信號處理的角度引入濾波器來定義圖卷積，其中圖卷積運算被解釋為從圖信號中去除噪聲 [76]。基於空間的方法將圖卷積表徵為聚合來自近鄰的特徵信息。雖然 GCN 在節點級別上運行，但是圖池化模塊可以與 GCN 層交替，將圖粗粒化為高級子結構。如圖 5a 所示，這種架構設計可用於提取圖級表徵、執行圖分類任務。下文會分別介紹、基於空間的 GCN 和圖池化模塊。</p><p class=ql-align-justify>基於譜的 GCN 部分介紹了其背景、方法等，這些方法包括 Spectral CNN、Chebyshev Spectral CNN (ChebNet)、First order of ChebNet (1stChebNet) 和 Adaptive Graph Convolution Network (AGCN)。</p><p class=ql-align-justify>基於空間的 GCN 分為兩類：Recurrent-based Spatial GCN 和 Composition Based Spatial GCN。前者包括圖神經網絡（Graph Neural Networks，GNN）、門控圖神經網絡（Gated Graph Neural Networks，GGNN）和 Stochastic Steady-state Embedding (SSE)。後者涉及了：Message Passing Neural Networks (MPNN)、GraphSage。此外，這部分還介紹了這兩大類之外的空間 GCN 變體，包括 Diffusion Convolution Neural Networks (DCNN)、PATCHY-SAN、Large-scale Graph Convolution Networks (LGCN)、Mixture Model Network (MoNet)。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/864873f327ba493fb7c636b7163577a2><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1a69b1651e5c4be5ad3a27f3f4728fcf><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><em>SSE 算法。</em></p><p class=ql-align-justify>這一章還從效率、通用性和靈活性方面，對比了基於譜的 GCN 和基於空間的 GCN，認為基於空間的 GCN 更具優勢，也因此吸引了更多研究興趣。</p><p class=ql-align-center><strong>5 圖卷積網絡之外的模型</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>這部分概述了圖卷積網絡之外的其他圖神經網絡，包括圖注意力神經網絡、圖自編碼器、圖生成模型和圖時空網絡。下表總結了每個類別下的主要方法。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b39e64632cca4c6b9e1469a3bafcb001><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><em>表 4：圖卷積網絡之外的其他圖神經網絡概覽。該表根據網絡的輸入、輸出、目標任務和是否基於 GCN 總結了每種網絡下的多種方法。輸入列代表每種方法適合分佈式圖 (A)、有向圖 (D) 還是時空圖 (S)。</em></p><p class=ql-align-justify><strong>5.1 圖注意力網絡</strong></p><p class=ql-align-justify>注意力機制幾乎成為序列任務中的標配。它的價值在於能夠聚焦於對象最重要的部分。該機制被證明在多項任務中有用，如機器翻譯和自然語言理解。由於注意力機制的模型容量越來越大，圖神經網絡在聚合信息、集成多個模型的輸出、生成重要性導向的隨機遊走時，可以從注意力機制中獲益良多。</p><p class=ql-align-justify>這部分介紹了圖注意力網絡的多種方法，包括圖注意力網絡（Graph Attention Network，GAT）、門控注意力網絡（Gated Attention Network，GAAN）、圖注意力模型（Graph Attention Model，GAM）、注意力遊走（Attention Walks）。</p><p class=ql-align-justify>注意力機制對圖神經網絡的貢獻有三部分，即在聚合特徵信息時向不同近鄰分配注意力權重、根據注意力權重集成多個模型，以及使用注意力權重引導隨機遊走。儘管我們把 GAT 和 GAAN 分類為圖注意力網絡的兩種方法，但是它們都可以作為基於空間的卷積網絡。二者的優勢是它們可以適應性地學習近鄰的重要性權重（如圖 6 所示）。但是，由於我們必須計算每對近鄰之間的注意力權重，因此計算成本和內存消耗會快速增長。</p><p class=ql-align-justify><strong>5.2 圖自編碼器</strong></p><p class=ql-align-justify>圖自編碼器是一類網絡嵌入方法，旨在通過神經網絡架構將網絡頂點表徵到低維向量空間。典型的解決方案是使用多層感知機作為編碼器來獲取節點嵌入，解碼器重建節點的近鄰統計，如正逐點互信息（positive pointwise mutual information，PPMI）或一階、二階接近度（proximities）[42]。最近，研究人員嘗試在設計圖自編碼器時用 GCN 作為編碼器、結合 GCN 和 GAN，或者結合 LSTM 和 GAN。</p><p class=ql-align-justify>這部分介紹了基於 GCN 的自編碼器和其他變體。基於 GCN 的自編碼器部分介紹了：圖自編碼器（Graph Auto-encoder，GAE）、對抗正則化圖自編碼器（Adversarially Regularized Graph Autoencoder，ARGA）。其他變體包括：具備對抗正則化自編碼器的網絡表徵（Network Representations with Adversarially Regularized Autoencoders，NetRA）、用於圖表徵的深度神經網絡（Deep Neural Networks for Graph Representations，DNGR）、結構化深度網絡嵌入（Structural Deep Network Embedding，SDNE）、深度遞歸網絡嵌入（Deep Recursive Network Embedding，DRNE）。</p><p class=ql-align-justify>DNGR 和 SDNE 僅基於拓撲結構學習節點嵌入，而 GAE、ARGA、NetRA 和 DRNE 需要基於拓撲信息和節點內容特徵學習節點嵌入。圖自編碼器的一個挑戰是鄰接矩陣的稀疏性，會導致解碼器正條目（positive entry）的數量遠遠少於負條目。為了解決這個問題，DNGR 重建了一個較稠密的矩陣——PPMI 矩陣，SDNE 對鄰接矩陣的零條目進行懲罰，GAE 重新調整鄰接矩陣中項的權重，NetRA 將圖線性化為序列。</p><p class=ql-align-justify><strong>5.3 圖生成網絡</strong></p><p class=ql-align-justify>圖生成網絡的目標是基於一組可觀察圖來生成圖。其中的很多方法都是領域特定的。例如，在分子圖生成方面，一些研究將分子圖的表徵建模為字符串 SMILES [94], [95], [96], [97]。在自然語言處理中，生成語義圖或知識圖通常需要一個給定的句子 [98], [99]。最近，研究人員又提出了多個通用方法。一些研究將生成過程看成節點或邊的形成 [64], [65]，而另一些則使用生成對抗訓練 [66], [67]。該領域的方法要麼使用 GCN 作為構造塊，要麼使用不同的架構。</p><p class=ql-align-justify>這部分介紹了基於 GCN 的圖生成網絡和其他圖生成網絡。前者包括：分子生成對抗網絡（Molecular Generative Adversarial Networks，MolGAN）和深度圖生成模型（Deep Generative Models of Graphs，DGMG）；後者涉及 GraphRNN（通過兩級循環神經網絡使用深度圖生成模型）和 NetGAN（結合 LSTM 和 Wasserstein GAN 從基於隨機遊走的方法中生成圖）。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/079f72f47ab34fe1b492e04a07ae2159><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><em>圖 9：MolGAN 框架圖示。</em></p><p class=ql-align-justify><strong>5.4 圖時空網絡</strong></p><p class=ql-align-justify>圖時空網絡同時捕捉時空圖的時間和空間依賴。時空圖具備全局圖結構，每個節點的輸入隨著時間而改變。例如在交通網絡中，使用每個傳感器作為節點來連續記錄某條道路的交通流動速度，其中交通網絡的邊由傳感器對之間的距離決定。圖時空網絡的目標是預測未來節點值或標籤，或預測時空圖標籤。近期研究探索了僅使用 GCN、結合 GCN 和 RNN 或 CNN，以及專用於圖結構的循環架構。</p><p class=ql-align-justify>這部分介紹了基於 GCN 的圖時空網絡和其他圖時空網絡。前者包括：Diffusion Convolutional Recurrent Neural Network (DCRNN)、CNN-GCN、時空 GCN（Spatial Temporal GCN，ST-GCN）。其他方法有 Structural-RNN，一種循環結構化框架。</p><p class=ql-align-justify>DCRNN 的優勢是能夠處理長期依賴，因為它具備循環網絡架構。儘管 CNN-GCN 比 DCRNN 簡單一些，但 CNN-GCN 能夠更高效地處理時空圖，這要歸功於 1D CNN 的快速實現。時空 GCN 將時間流作為圖的邊，這導致鄰接矩陣的大小呈平方增長。一方面，它增加了圖卷積層的計算成本。另一方面，要捕捉長期依賴，圖卷積層必須多次堆疊。StructuralRNN 在同一個語義組內共享相同的 RNN，從而改善了模型效率，但是 StructuralRNN 需要人類先驗知識來分割語義組。</p><p class=ql-align-center><strong>6 應用</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>圖神經網絡應用廣泛。下面將首先介紹在文獻中頻繁使用的基準數據集。接著將報告各種方法在四種常用數據集上的基準性能，並列出可用的圖神經網絡開源實現。最後，我們將介紹圖神經網絡在各個領域的實際應用案例。</p><p class=ql-align-justify><strong>6.1 數據集</strong></p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/93bab63f457e4bb0a9cf70752309e152><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><em>表 5：常用數據集總結。</em></p><p class=ql-align-justify><strong>6.2 基準和開源實現</strong></p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/52cbfa3f584c4768bfef8663db1cee7d><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><em>表 6：各種方法在四種最常用數據集上的基準性能。以上列出的方法都使用相同的訓練、驗證和測試數據集進行評估。</em></p><p class=ql-align-center><br></p><div class=pgc-img><img alt="圖神經網絡概述第三彈：來自IEEE Fellow的GNN綜述" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/84e69fec22234eed9161dfc39befc0ef><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><em>表 7：開源實現概覽。</em></p><p class=ql-align-justify><strong>6.3 實際應用案例</strong></p><p class=ql-align-justify>本文按領域介紹了 GNN 的應用，包括計算機視覺、推薦系統、交通、化學等。</p><p class=ql-align-center><br></p><p class=ql-align-center><strong>7 未來方向</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify><strong>加深網絡</strong>。深度學習的成功在於深度神經架構。例如在圖像分類中，模型 ResNet 具有 152 層。但在圖網絡中，實證研究表明，隨著網絡層數增加，模型性能急劇下降 [147]。根據論文 [147]，這是由於圖卷積的影響，因為它本質上推動相鄰節點的表示更加接近彼此，所以理論上，通過無限次卷積，所有節點的表示將收斂到一個點。這導致了一個問題：加深網絡是否仍然是學習圖結構數據的好策略？</p><p class=ql-align-justify><strong>感受野</strong>。節點的感受野是指一組節點，包括中心節點和其近鄰節點。節點的近鄰（節點）數量遵循冪律分佈。有些節點可能只有一個近鄰，而有些節點卻有數千個近鄰。儘管採用了採樣策略 [24], [26], [27]，但如何選擇節點的代表性感受野仍然有待探索。</p><p class=ql-align-justify><strong>可擴展性</strong>。大部分圖神經網絡並不能很好地擴展到大型圖上。主要原因是當堆疊一個圖卷積的多層時，節點的最終狀態涉及其大量近鄰節點的隱藏狀態，導致反向傳播變得非常複雜。雖然有些方法試圖通過快速採樣和子圖訓練來提升模型效率 [24], [27]，但它們仍無法擴展到大型圖的深度架構上。</p><p class=ql-align-justify><strong>動態性和異質性</strong>。大多數當前的圖神經網絡都處理靜態同質圖。一方面，假設圖架構是固定的。另一方面，假設圖的節點和邊來自同一個來源。然而，這兩個假設在很多情況下是不現實的。在社交網絡中，一個新人可能會隨時加入，而之前就存在的人也可能退出該社交網絡。在推薦系統中，產品可能具有不同的類型，而其輸出形式也可能不同，也許是文本，也許是圖像。因此，應當開發新方法來處理動態和異質圖結構。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>圖神經</a></li><li><a>網絡</a></li><li><a>IEEE</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/ea1bb612.html alt=光纜——未來網絡主導 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/e75c1afe12354a93bad8495ad1057693 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ea1bb612.html title=光纜——未來網絡主導>光纜——未來網絡主導</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/bdc59733.html alt="網絡詞名場面是什麼意思 名場面是什麼梗" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bdc59733.html title="網絡詞名場面是什麼意思 名場面是什麼梗">網絡詞名場面是什麼意思 名場面是什麼梗</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a002ca18.html alt=王一博那句年度網絡流行語「不愧是我」的：正版英文翻譯 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/26add5cdc08e4214800b25e21b623eb1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a002ca18.html title=王一博那句年度網絡流行語「不愧是我」的：正版英文翻譯>王一博那句年度網絡流行語「不愧是我」的：正版英文翻譯</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html alt=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/65c4000bda98898dcdbb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html title=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼>谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8dce33e7.html alt=理解生成對抗網絡，一步一步推理得到GANs（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/bee194d6fbec4d6f82e82998def3f7a3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8dce33e7.html title=理解生成對抗網絡，一步一步推理得到GANs（一）>理解生成對抗網絡，一步一步推理得到GANs（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1fe1c2dd.html alt=瞭解生成對抗網絡（GAN） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/634604de44ad4d17931ccc0bcf3e46ef style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1fe1c2dd.html title=瞭解生成對抗網絡（GAN）>瞭解生成對抗網絡（GAN）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2a9a3956.html alt="100 個網絡基礎知識普及，看完成半個網絡高手！" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/955ea722-be77-4b2d-b87a-64a8a04c8ea8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2a9a3956.html title="100 個網絡基礎知識普及，看完成半個網絡高手！">100 個網絡基礎知識普及，看完成半個網絡高手！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/78c87167.html alt=奧迪A7車載電路與網絡連接之網絡連接 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d34213e3b03545bd8e87ab074b54ca0f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/78c87167.html title=奧迪A7車載電路與網絡連接之網絡連接>奧迪A7車載電路與網絡連接之網絡連接</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8a15db0b.html alt=網絡上共享跨平臺的點對點文件 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/666bf87a9e16425ea1c9adc4f2c10acd style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8a15db0b.html title=網絡上共享跨平臺的點對點文件>網絡上共享跨平臺的點對點文件</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2b6ecb90.html alt=網絡基礎知識術語你知道哪些？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2b6ecb90.html title=網絡基礎知識術語你知道哪些？>網絡基礎知識術語你知道哪些？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6474ccee.html alt=點對點網絡的基本知識分享 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8678a34470a144ad88fc0487f0d8da91 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6474ccee.html title=點對點網絡的基本知識分享>點對點網絡的基本知識分享</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a61e29b6.html alt="幫助信息網絡犯罪活動罪 | 廈門刑事律師--辦案資料" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a61e29b6.html title="幫助信息網絡犯罪活動罪 | 廈門刑事律師--辦案資料">幫助信息網絡犯罪活動罪 | 廈門刑事律師--辦案資料</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ded83afe.html alt=博士論文摘要｜馬下平：“陸態網絡”並址站歸心基線精密解算及GNSS基準站數據處理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/53350006726e50ef72f9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ded83afe.html title=博士論文摘要｜馬下平：“陸態網絡”並址站歸心基線精密解算及GNSS基準站數據處理>博士論文摘要｜馬下平：“陸態網絡”並址站歸心基線精密解算及GNSS基準站數據處理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/65c66d61.html alt=網絡分析儀校準很頭疼，三大要點得記住！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/41dd3f31e88f48688e75d64978b36acf style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/65c66d61.html title=網絡分析儀校準很頭疼，三大要點得記住！>網絡分析儀校準很頭疼，三大要點得記住！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2bc1496a.html alt=為了更好的深度神經網絡視覺，只需添加反饋（循環） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/17fccfd7096d44eeb3921bbd0dc29a13 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2bc1496a.html title=為了更好的深度神經網絡視覺，只需添加反饋（循環）>為了更好的深度神經網絡視覺，只需添加反饋（循環）</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>