<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>圖像配準的前世今生：從人工設計特徵到深度學習 | 极客快訊</title><meta property="og:title" content="圖像配準的前世今生：從人工設計特徵到深度學習 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/aa2bad10159143038cdd088ab470c611"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/03a3c47.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/03a3c47.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/03a3c47.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/03a3c47.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/03a3c47.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/03a3c47.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/03a3c47.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/03a3c47.html><meta property="article:published_time" content="2020-10-29T20:50:55+08:00"><meta property="article:modified_time" content="2020-10-29T20:50:55+08:00"><meta name=Keywords content><meta name=description content="圖像配準的前世今生：從人工設計特徵到深度學習"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/03a3c47.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>圖像配準的前世今生：從人工設計特徵到深度學習</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p class=ql-align-center>選自Medium</p><p><strong>作者：Emma Kamoun</strong></p><p><strong>機器之心編譯</strong></p><p><strong>參與：Nurhachu Null，Geek AI</strong></p><blockquote>作為計算機視覺的重要研究課題，圖像配準經歷了從傳統方法走向深度學習的重要革命。本文將回顧圖像配準技術的前世今生，為讀者提供一個該領域的宏觀視野。</blockquote><p>圖像配準是計算機視覺領域的一個基礎步驟。在本文深入探討深度學習之前，我們先展示一下 OpenCV 中基於特徵的方法。</p><p><strong>什麼是圖像配準？</strong></p><p>圖像配準就是將同一個場景的不同圖像轉換到同樣的座標系統中的過程。這些圖像可以是不同時間拍攝的（多時間配準），可以是不同傳感器拍攝的（多模配準），可以是不同視角拍攝的。這些圖像之間的空間關係可能是剛體的（平移和旋轉）、仿射的（例如錯切），也有可能是單應性的，或者是複雜的大型形變模型。</p><div class=pgc-img><img alt=圖像配準的前世今生：從人工設計特徵到深度學習 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/aa2bad10159143038cdd088ab470c611><p class=pgc-img-caption></p></div><p>圖像配準有很廣泛的應用：只要我們面對的任務需要比較相同場景的多張圖片，它就是必不可少的。它在醫學影像、衛星圖像分析以及光流領域都是很常用的。</p><div class=pgc-img><img alt=圖像配準的前世今生：從人工設計特徵到深度學習 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0a3fce3e17834973a1c6cbb5104c031b><p class=pgc-img-caption></p></div><p><em>經過配準後的 CT 掃描和核磁共振圖像</em></p><p>在本文中，我們會重點關注在一張參考圖像和一張待配準圖像之間進行圖像配準的不同方法。我們不會選擇迭代式的/基於強度的方法，因為它們並不像本文中提到的方法這樣常用。</p><p><strong>傳統的基於特徵的方法</strong></p><p>自本世紀初以來，圖像配準主要使用傳統的基於特徵的方法。這些方法基於三個步驟：關鍵點檢測和特徵描述，特徵匹配，圖像變形。簡而言之，我們在兩幅圖像中選擇興趣點，將參考圖像中的每個興趣點和它在待配準圖像中的對應點關聯起來，然後對待批准圖像進行變換，這樣兩幅圖像就得以對齊。</p><div class=pgc-img><img alt=圖像配準的前世今生：從人工設計特徵到深度學習 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d18f649e05cf424daddd16be870223d0><p class=pgc-img-caption></p></div><p><em>對一個圖像對通過單應性變換進行關聯的基於特徵的方法</em></p><p><strong>關鍵點檢測和特徵描述</strong></p><p>關鍵點就是感興趣的點。它定義了一幅圖像中重要並且有特點的地方（如角，邊等）。每個關鍵點都由一個描述子（包含關鍵點本質特點的特徵向量）表徵。描述子應該對圖像變換（如位置變換、縮放變換、亮度變換等）是魯棒的。很多算法都要執行關鍵點檢測和特徵描述：</p><ul><li class=ql-align-justify>SIFT（Scale-invariant feature transform，尺度不變的特徵變換）是用於關鍵點檢測的原始算法，但是它並不能免費地被用於商業用途。SIFT 特徵描述子對均衡的縮放，方向、亮度變化是保持不變的，對仿射形變也是部分不變的。</li><li class=ql-align-justify>SURF（Speeded Up Robust Features，加速魯棒特徵）是受到 SIFT 深刻啟發設計的檢測器和描述子。與 SIFT 相比，它的運行速度要快好幾倍。當然，它也是受專利保護的。</li><li class=ql-align-justify>ORB（定向的 FAST 和旋轉的 BRIEF）是基於 FAST（Features from Accelerated Segment Test）關鍵點檢測器和 BRIEF（Binary robust independent elementary features）描述子的組合的快速二值描述子，具有旋轉不變性和對噪聲的魯棒性。它是由 OpenCV Lab 開發的高效、免費的 SIFT 替代方案。</li><li class=ql-align-justify>AKAZE(Accelerated-KAZE) 是 KAZE 的加速版本。它為非線性尺度空間提出了一種快速多尺度的特徵檢測和描述方法。它對於縮放和旋轉也是具有不變性的，可以免費使用。</li></ul><p>這些算法在 OenCV 中都得到了實現，易於使用。在下面的例子中，我們使用了 AKAZE 的 OpenCV 實現。其它算法的代碼大致也是一樣的：只需修改一下算法的名字即可。</p><pre>import numpy as npimport cv2 as cvimg = cv.imread('image.jpg')gray= cv.cvtColor(img, cv.COLOR_BGR2GRAY)akaze = cv.AKAZE_create()kp, descriptor = akaze.detectAndCompute(gray, None)img=cv.drawKeypoints(gray, kp, img)cv.imwrite('keypoints.jpg', img)</pre><div class=pgc-img><img alt=圖像配準的前世今生：從人工設計特徵到深度學習 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3b1e93c88c614f239c416e6902e33048><p class=pgc-img-caption></p></div><p><em>圖像的關鍵點</em></p><p>更多關於特徵檢測和描述的細節，請參閱下面的 OpenCV 教程：https://docs.opencv.org/3.4/d7/d66/tutorial_feature_detection.html</p><p><strong>特徵匹配</strong></p><p>當組成一個圖像對的兩張圖的關鍵點都被識別出來以後，我們需要將它們關聯（或稱「匹配」）起來，兩張圖像中對應的關鍵點在現實中是同一個點。一個可以實現該功能的函數是「BFMatcher.knnMatch()」。這個匹配器（matcher）會衡量每一對關鍵點的描述子之間的距離，然後返回與每個關鍵點距離最小的 k 個最佳匹配結果。</p><p>接下來，我們應用比例濾波器來保持正確的匹配。事實上，為了實現可靠的匹配，配對的關鍵點應該比距離最近的錯誤匹配點更接近。</p><pre>import numpy as npimport cv2 as cvimport matplotlib.pyplot as pltimg1 = cv.imread('image1.jpg', cv.IMREAD_GRAYSCALE) # referenceImageimg2 = cv.imread('image2.jpg', cv.IMREAD_GRAYSCALE) # sensedImage# Initiate AKAZE detectorakaze = cv.AKAZE_create()# Find the keypoints and descriptors with SIFTkp1, des1 = akaze.detectAndCompute(img1, None)kp2, des2 = akaze.detectAndCompute(img2, None)# BFMatcher with default paramsbf = cv.BFMatcher()matches = bf.knnMatch(des1, des2, k=2)# Apply ratio testgood_matches = []for m,n in matches:  if m.distance &lt; 0.75*n.distance:  good_matches.append([m])  # Draw matches img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good_matches,None,flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS) cv.imwrite('matches.jpg', img3)</pre><div class=pgc-img><img alt=圖像配準的前世今生：從人工設計特徵到深度學習 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ef3a89c6f43347248a2265668f5191c9><p class=pgc-img-caption></p></div><p><em>配對的關鍵點</em></p><p>請參閱下面的文檔來了解 OpenCV 中實現的其他特徵匹配方法：https://docs.opencv.org/trunk/dc/dc3/tutorial_py_matcher.html</p><p><strong>圖像變形</strong></p><p>在匹配到至少 4 對關鍵點之後，我們就可以將一幅圖像相對於另一幅圖像進行轉換。這個過程被稱作圖像變形（image warping）。空間中同一平面的任意兩幅圖像都是通過單應性變換關聯起來的。單應性變換是具有 8 個參數的幾何變換，通過一個 3×3 的矩陣表徵。它們代表著對一幅圖像整體所做的任何變形（與局部形變不同）。因此，為了得到變換後的待配準圖像，我們計算了單應矩陣，並將它應用在了待配準圖像上。</p><p>為了保證最優的變形，我們使用了 RANSAC 算法來檢測輪廓，並且在進行最終的單應性變換之前將輪廓刪除。該過程直接內置於 OpenCV 的「findHomography()」函數中。目前也有一些 RANSAC 的替代方案，例如 LMED（Least-Median robust method，最小中值魯棒方法）。</p><pre># Select good matched keypointsref_matched_kpts = np.float32([kp1[m[0].queryIdx].pt for m in good_matches]).reshape(-1,1,2)sensed_matched_kpts = np.float32([kp2[m[0].trainIdx].pt for m in good_matches]).reshape(-1,1,2)# Compute homographyH, status = cv.findHomography(ref_matched_kpts, sensed_matched_kpts, cv.RANSAC,5.0)# Warp imagewarped_image = cv.warpPerspective(img1, H, (img1.shape[1]+img2.shape[1], img1.shape[0])) cv.imwrite('warped.jpg', warped_image)</pre><div class=pgc-img><img alt=圖像配準的前世今生：從人工設計特徵到深度學習 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/08f9c0b1f30a4de1bbb2555d97de27f9><p class=pgc-img-caption></p></div><p><em>變形後的待配準圖像</em></p><p>關於以上三個步驟的更多細節，請參閱 OpenCV 官方發佈的系列教程（https://docs.opencv.org/3.1.0/db/d27/tutorial_py_table_of_contents_feature2d.html）。</p><p><strong>深度學習方法</strong></p><p>目前大多數的圖像配準研究都涉及到深度學習的使用。過去幾年裡，深度學習方案在計算機視覺任務中（如圖像分類、目標檢測和分割）達到了最先進的性能。當然，圖像配準也沒有理由拒絕深度學習。</p><p>（二級）特徵提取</p><p>深度學習在圖像配準中使用的第一種方式就是將其用於特徵提取。在卷積神經網絡（CNN）中，連續的層能夠成功地捕獲到越來越複雜的圖像特徵，學習到特定任務的特徵。自 2014 年以來，研究者們就開始將這些神經網絡應用於特徵提取步驟，代替使用 SIFT 或者其它類似的算法。</p><ul><li class=ql-align-justify>2014 年，Dosovitskiy 等人提出了通用特徵學習方法「Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks」（https://arxiv.org/abs/1406.6909），僅使用無標籤的數據來訓練卷積神經網絡。這些特徵的泛化屬性使得它們對變換是魯棒的。這些特徵（或稱描述子），在匹配任務中要優於 SIFT 描述子。</li><li class=ql-align-justify>2018 年，Yang 等人基於同樣的思想研發了一種非剛性配準方法「Multi-temporal Remote Sensing Image Registration Using Deep Convolutional Features」（https://ieeexplore.ieee.org/document/8404075）。他們使用預訓練的 VGG 網絡層來生成能夠同時保持卷積信息和定位能力的特徵描述子。這些描述子似乎也要優於和 SIFT 類的描述子，尤其是在 SIFT 包含很多輪廓或者不能匹配到足夠數目的特徵點的情況下。</li></ul><p class=ql-align-justify><br></p><div class=pgc-img><img alt=圖像配準的前世今生：從人工設計特徵到深度學習 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9216879cdefa4b0fba280b7474319967><p class=pgc-img-caption></p></div><p><em>SIFT 和基於深度學習的非剛性配準方法描述子的實驗結果</em></p><p>論文「Multi-temporal Remote Sensing Image Registration Using Deep Convolutional Features」的代碼鏈接如下：https://github.com/yzhq97/cnn-registration。</p><p>儘管我們在 15 分鐘內就在自己的圖像上完成了對這個配准算法的測試，但是它幾乎要比我們在本文前面在 SIFT 類的方法上的實現慢了近 70 倍。</p><p><strong>單應性學習</strong></p><p>研究者們並沒有侷限於神經網絡在特徵提取上的使用，而是嘗試使用神經網絡來直接學習幾何變換，從而對齊兩張圖像。</p><p>1. 監督學習</p><p>2016 年，DeTone 等人發表了論文「深度單應性估計」（https://arxiv.org/pdf/1606.03798.pdf），提出了「Regression HomographyNet」網絡，這是一個類似於 VGG 的網絡，能夠學習到將兩幅圖像關聯起來的單應性變換。這個算法彰顯了以端到端的方式，同時學習單應性變換以及卷積神經網絡參數的好處：不再需要之前的兩個步驟了！</p><div class=pgc-img><img alt=圖像配準的前世今生：從人工設計特徵到深度學習 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/7c10e5e1872744898a2efe30ba947a25><p class=pgc-img-caption></p></div><p><em>Regression HomographyNet 示意圖</em></p><p>這個網絡產生了 8 個實數作為輸出。它是以有監督的方式進行訓練的，以輸出和真實的單應之間的歐氏距離作為損失函數。</p><div class=pgc-img><img alt=圖像配準的前世今生：從人工設計特徵到深度學習 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/714b1a8c5164444ab360111191889cac><p class=pgc-img-caption></p></div><p><em>有監督的深度單應性估計</em></p><p>與所有的監督方法類似，這個單應性估計方法需要帶標籤的數據對。儘管在人工生成的圖像對上得到這樣的單應變換矩陣是很容易的，但是在真實的數據上卻需要付出高昂的代價。</p><p>2. 監督學習</p><p>考慮到這一點，Nguyen 等人提出了一種無監督的深度圖像單應估計方法（https://arxiv.org/pdf/1709.03966.pdf）。</p><p>他們保持了原來的 CNN，但是使用了新的、適應於無監督方法的損失函數：他們選擇了不需要真實標籤的光度損失（photometric loss）。此外，它還計算了參考圖像和待配準變換圖像之間的相似度。</p><div class=pgc-img><img alt=圖像配準的前世今生：從人工設計特徵到深度學習 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a38dc27bcc9a4f36a5fbbeadd543a6f7><p class=pgc-img-caption></p></div><p><em>L1 光度損失函數</em></p><p>這個方法引入了兩個新的網絡結構：張量直接線性變換（Tensor Direct Linear Transform）和空間變換層。我們在本文中不會詳述這些組成部分的細節，我們只要知道這些是被用來使用 CNN 模型的單應性參數輸出獲得變換後的待配準圖像就行了，我們會用它計算光度損失函數。</p><div class=pgc-img><img alt=圖像配準的前世今生：從人工設計特徵到深度學習 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1c94288f6c344bf9ad0c45903f1937c2><p class=pgc-img-caption></p></div><p><em>無監督深度單應性估計</em></p><p>這篇文章的作者稱，與傳統的基於特徵的方法相比，這種無監督方法以更快的推理速度得到了相當的或者更高的準確率，以及關於光照變化的魯棒性。此外，與監督方法相比，它還具有更高的適應能力和性能。</p><p><strong>其它方法</strong></p><p>1. 強化學習</p><p>作為醫療應用中的配準方法，深度強化學習正日益受到關注。與預定義的優化算法不同，在該方法中，我們使用訓練過的智能體（agent）來進行配準。</p><div class=pgc-img><img alt=圖像配準的前世今生：從人工設計特徵到深度學習 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/5d804bceab9b4730a4a6ec1c24a402a0><p class=pgc-img-caption></p></div><p><em>基於強化學習的配準流程示意圖</em></p><ul><li class=ql-align-justify>2016 年，Liao 等人首次在圖像配準中使用強化學習。他們的方法（https://arxiv.org/pdf/1611.10336.pdf）是以用於端到端訓練的貪婪監督算法為基礎的。其目標是通過找到最佳的動作序列來對齊圖像。雖然這種方法優於一些目前最先進的模型，但是它僅僅被用於剛性變換。</li><li class=ql-align-justify>強化學習也被用在更加複雜的變換中。在論文「Robust non-rigid registration through agent-based action learning」（https://hal.inria.fr/hal-01569447/document）中，Krebs 等人使用了人工智能體來優化形變模型的參數。這個方法在前列腺核磁共振圖像的受試者間配準上進行了測試，在 2D 和 3D 圖像上都展現出了良好的效果。</li></ul><p>2. 複雜變換</p><p>目前有很大一部分關於圖像配準的研究關注於醫療圖像領域。通常，由於受試者的局部形變（如呼吸變化、解剖學變化等），兩幅醫療圖像之間的變換不能簡單地通過單應矩陣來描述。所以需要更復雜的變換模型，例如可以用位移矢量場表示的微分同胚。</p><div class=pgc-img><img alt=圖像配準的前世今生：從人工設計特徵到深度學習 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c424dce7c0f8453dbbeefee02794174b><p class=pgc-img-caption></p></div><p><em>心臟核磁共振圖像上的變形網格和位移矢量場的示例</em></p><p>研究者們已試圖使用神經網絡來估計這些擁有很多參數的大規模形變模型。</p><ul><li class=ql-align-justify>第一個例子就是上文提及的 Krebs 等人的強化學習方法。</li><li class=ql-align-justify>2017 年，De Vos 等人提出了 DIRNet。這是一個使用 CNN 來預測控制點的網格的神經網絡，這些控制點能夠被用來生成根據參考圖像來對待配準圖像進行變形的位移矢量場。</li></ul><div class=pgc-img><img alt=圖像配準的前世今生：從人工設計特徵到深度學習 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9f32baae1ace4dcda47129e33e50e2da><p class=pgc-img-caption></p></div><p><em>DIRNet 示意圖（使用 MNIST 數據集中的兩張圖像作為輸入）</em></p><p>Quicksilver 配準解決了一個類似的問題。Quicksilver 使用深度「編碼器-解碼器」網絡直接在圖像外觀上預測圖塊級（patch-wise）的形變。</p><p><em>原文地址：</em></p><p><em>https://medium.com/m/global-identity?redirectUrl=https%3A%2F%2Fblog.sicara.com%2Fimage-registration-sift-deep-learning-3c794d794b7a</em></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>圖像</a></li><li><a>配準</a></li><li><a>設計</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/403377d.html alt=MATLAB圖像配準各方法介紹和對比 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a544d694126c47618bb1ca91efc79195 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/403377d.html title=MATLAB圖像配準各方法介紹和對比>MATLAB圖像配準各方法介紹和對比</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a97f673.html alt=全面梳理：圖像配準綜述 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/fd6f1d9ada674606946642c4cb01ce8b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a97f673.html title=全面梳理：圖像配準綜述>全面梳理：圖像配準綜述</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1ef1595.html alt=圖像配準SIFT class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a76d41267463482bade6f89246503944 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1ef1595.html title=圖像配準SIFT>圖像配準SIFT</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/612f314.html alt=圖像配準傳統算法總結 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/590f704b5a5640ef8229e9b2350fbf9a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/612f314.html title=圖像配準傳統算法總結>圖像配準傳統算法總結</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e4eb034.html alt=圖像配準數據集合集-整理方案（1024初稿） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/962ba0f782ff4988b06968d6aff4e4dc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e4eb034.html title=圖像配準數據集合集-整理方案（1024初稿）>圖像配準數據集合集-整理方案（1024初稿）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6ba9fca.html alt=圖像配準：從SIFT到深度學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/442c425619b84460b306bf5f15583d2c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6ba9fca.html title=圖像配準：從SIFT到深度學習>圖像配準：從SIFT到深度學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9bd6b239.html alt=鋼構人福利——鋼結構設計經典問題解讀 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a72bd60ea48d471b8ce03ebf0ce15869 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9bd6b239.html title=鋼構人福利——鋼結構設計經典問題解讀>鋼構人福利——鋼結構設計經典問題解讀</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6bbe6ec4.html alt=鋼管混凝土結構設計原理及在強震區橋樑結構研究成果達國際領先 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/153569494612546362474a9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6bbe6ec4.html title=鋼管混凝土結構設計原理及在強震區橋樑結構研究成果達國際領先>鋼管混凝土結構設計原理及在強震區橋樑結構研究成果達國際領先</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6d006e47.html alt=鋼結構設計基礎知識問答 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/63b48928-d7cd-4346-b80f-4f77015517c1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6d006e47.html title=鋼結構設計基礎知識問答>鋼結構設計基礎知識問答</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0dc64d99.html alt=38個鋼結構設計問題，都很常見 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/f1181ce8-cf01-4194-b9c2-362f43894ddb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0dc64d99.html title=38個鋼結構設計問題，都很常見>38個鋼結構設計問題，都很常見</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b6fd930e.html alt=步步解析橋樑設計計算，不可錯過的一篇乾貨 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d88dd927b4f2474fbe4ea261c2397ba8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b6fd930e.html title=步步解析橋樑設計計算，不可錯過的一篇乾貨>步步解析橋樑設計計算，不可錯過的一篇乾貨</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/97b90000.html alt=結構設計、校對、審核三字經，速速收藏 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/97b90000.html title=結構設計、校對、審核三字經，速速收藏>結構設計、校對、審核三字經，速速收藏</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fd0540fa.html alt=解析橋樑設計計算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/fa4d0bcf2a204a2fac9b37978b1b0713 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fd0540fa.html title=解析橋樑設計計算>解析橋樑設計計算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/83530974.html alt=解析橋樑設計計算，不可錯過的一篇乾貨 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/af9ca7f120da4e23b0ca589375e05dd2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/83530974.html title=解析橋樑設計計算，不可錯過的一篇乾貨>解析橋樑設計計算，不可錯過的一篇乾貨</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e8b9b44c.html alt=設計師解析橋樑設計計算，錯過了就沒有機會 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/e01645937bda4ae7a7f148666ff89117 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e8b9b44c.html title=設計師解析橋樑設計計算，錯過了就沒有機會>設計師解析橋樑設計計算，錯過了就沒有機會</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>