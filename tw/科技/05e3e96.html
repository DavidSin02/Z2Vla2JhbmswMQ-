<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>NLP的文本分析與特徵工程 | 极客快訊</title><meta property="og:title" content="NLP的文本分析與特徵工程 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/1abdc7297e4f49eca02c3a3feb27aa72"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/05e3e96.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/05e3e96.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/05e3e96.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/05e3e96.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/05e3e96.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/05e3e96.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/05e3e96.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/05e3e96.html><meta property="article:published_time" content="2020-10-29T21:07:50+08:00"><meta property="article:modified_time" content="2020-10-29T21:07:50+08:00"><meta name=Keywords content><meta name=description content="NLP的文本分析與特徵工程"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/05e3e96.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>NLP的文本分析與特徵工程</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><h1 class=pgc-h-arrow-right>摘要</h1><p>在本文中，我將使用NLP和Python解釋如何為機器學習模型分析文本數據和提取特徵。</p><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1abdc7297e4f49eca02c3a3feb27aa72><p class=pgc-img-caption></p></div><p>自然語言處理（NLP）是人工智能的一個研究領域，它研究計算機與人類語言之間的相互作用，特別是如何對計算機進行編程以處理和分析大量自然語言數據。</p><p>NLP常用於文本數據的分類。文本分類是根據文本數據的內容對其進行分類的問題。文本分類最重要的部分是特徵工程：從原始文本數據為機器學習模型創建特徵的過程。</p><p>在本文中，我將解釋不同的方法來分析文本並提取可用於構建分類模型的特徵。我將介紹一些有用的Python代碼。</p><p>這些代碼可以很容易地應用於其他類似的情況（只需複製、粘貼、運行），並且我加上了註釋，以便你可以理解示例（鏈接到下面的完整代碼）。</p><p>https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/deep_learning_natural_language_processing/text_classification_example.ipynb</p><p>我將使用“新聞類別數據集”（以下鏈接），其中向你提供從赫芬頓郵報獲得的2012年至2018年的新聞標題，並要求你使用正確的類別對其進行分類。</p><p>https://www.kaggle.com/rmisra/news-category-dataset</p><p>特別是，我將通過：</p><ul><li>環境設置：導入包並讀取數據。</li><li>語言檢測：瞭解哪些自然語言數據在其中。</li><li>文本預處理：文本清理和轉換。</li><li>長度分析：用不同的指標來衡量。</li><li>情緒分析：判斷一篇文章是正面的還是負面的。</li><li>命名實體識別：帶有預定義類別（如人名、組織、位置）的標識文本。</li><li>詞頻：找出最重要的n個字母。</li><li>詞向量：把一個字轉換成向量。</li><li>主題模型：從語料庫中提取主題。</li></ul><hr><h1 class=pgc-h-arrow-right>環境設置</h1><p>首先，我需要導入以下庫。</p><pre><code>## 數據import pandas as pdimport collectionsimport json## 繪圖import matplotlib.pyplot as pltimport seaborn as snsimport wordcloud## 文本處理import reimport nltk## 語言檢測import langdetect ## 情感分析from textblob import TextBlob## 命名實體識別import spacy## 詞頻from sklearn import feature_extraction, manifold## word embeddingimport gensim.downloader as gensim_api## 主題模型import gensim</code></pre><p>數據集包含在一個json文件中，因此我將首先使用json包將其讀入字典列表，然後將其轉換為pandas數據幀。</p><pre><code>lst_dics = []with open('data.json', mode='r', errors='ignore') as json_file:    for dic in json_file:        lst_dics.append( json.loads(dic) )## 打印第一個 lst_dics[0]</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b858e40b07c3410e93edecee5510aad1><p class=pgc-img-caption></p></div><p>原始數據集包含30多個類別，但在本教程中，我將使用3個類別的子集：娛樂、政治和技術(Entertainment, Politics, Tech)。</p><pre><code>## 創建dtfdtf = pd.DataFrame(lst_dics)## 篩選類別dtf = dtf[ dtf["category"].isin(['ENTERTAINMENT','POLITICS','TECH']) ][["category","headline"]]## 重命名列dtf = dtf.rename(columns={"category":"y", "headline":"text"})## 打印5個隨機行dtf.sample(5)</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/96c01237a1d54395a4be9985ea6d8400><p class=pgc-img-caption></p></div><p>為了理解數據集的組成，我將通過用條形圖顯示標籤頻率來研究單變量分佈（僅一個變量的概率分佈）。</p><pre><code>x = "y"fig, ax = plt.subplots()fig.suptitle(x, fontsize=12)dtf[x].reset_index().groupby(x).count().sort_values(by=        "index").plot(kind="barh", legend=False,         ax=ax).grid(axis='x')plt.show()</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3b0e67e970a14ed39a68891291880996><p class=pgc-img-caption></p></div><p>數據集是不平衡的：與其他數據集相比，科技新聞的比例確實很小。這可能是建模過程中的一個問題，對數據集重新採樣可能很有用。</p><p>現在已經設置好了，我將從清理數據開始，然後從原始文本中提取不同的細節，並將它們作為數據幀的新列添加。這些新信息可以作為分類模型的潛在特徵。</p><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8ff370e984d1487289c2dba8d5d4536b><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>語言檢測</h1><p>首先，我想確保我使用的是同一種語言，並且使用langdetect包，這非常簡單。為了舉例說明，我將在數據集的第一個新聞標題上使用它：</p><pre><code>txt = dtf["text"].iloc[0]print(txt, " --&gt; ", langdetect.detect(txt))</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0acbf5a5ec5641b3b315aef19a5a436b><p class=pgc-img-caption></p></div><p>我們為整個數據集添加一個包含語言信息的列：</p><pre><code>dtf['lang'] = dtf["text"].apply(lambda x: langdetect.detect(x) if                                  x.strip() != "" else "")dtf.head()</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d7ee78b8b3d04440bc44713f915a3dcb><p class=pgc-img-caption></p></div><p>數據幀現在有一個新列。使用之前的相同代碼，我可以看到有多少種不同的語言：</p><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/5265568347ac48a2a6f72f1b06cbaf1b><p class=pgc-img-caption></p></div><p>即使有不同的語言，英語也是主要的語言。所以我要用英語過濾新聞。</p><pre><code>dtf = dtf[dtf["lang"]=="en"]</code></pre><h1 class=pgc-h-arrow-right>文本預處理</h1><p>數據預處理是準備原始數據以使其適合機器學習模型的階段。對於NLP，這包括文本清理、刪除停用詞、詞幹還原。</p><p>文本清理步驟因數據類型和所需任務而異。通常，在文本被標識化之前，字符串被轉換為小寫，標點符號被刪除。<strong>標識化</strong>（<strong>Tokenization</strong>）是將字符串拆分為字符串列表（或“標識”）的過程。</p><p>再以第一條新聞標題為例：</p><pre><code>print("--- original ---")print(txt)print("--- cleaning ---")txt = re.sub(r'[^\w\s]', '', str(txt).lower().strip())print(txt)print("--- tokenization ---")txt = txt.split()print(txt)</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cea451b3c3de4516a9d60152abf402f9><p class=pgc-img-caption></p></div><p>我們要保留列表中的所有標識嗎？我們沒有。事實上，我們想刪除所有不提供額外信息的單詞。</p><p>在這個例子中，最重要的詞是“song”，因為它可以將任何分類模型指向正確的方向。相比之下，像“and”、“for”、“the”這樣的詞並不有用，因為它們可能出現在數據集中幾乎所有的觀察中。</p><p>這些是停用詞的例子。停用詞通常指的是語言中最常見的單詞，但是我們沒有一個通用的停用詞列表。</p><p>我們可以使用NLTK（自然語言工具包）為英語詞彙表創建一個通用停用詞列表，它是一套用於符號和統計自然語言處理的庫和程序。</p><pre><code>lst_stopwords = nltk.corpus.stopwords.words("english")lst_stopwords</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f185e1512fae4c35a813207b672067b8><p class=pgc-img-caption></p></div><p>讓我們從第一個新聞標題中刪除這些停用詞：</p><pre><code>print("--- remove stopwords ---")txt = [word for word in txt if word not in lst_stopwords]print(txt)</code></pre><p>我們需要非常小心的停用詞，因為如果你刪除了錯誤的標識，你可能會失去重要的信息。例如，刪除了“Will”一詞，我們丟失了此人是Will Smith的信息。</p><p>考慮到這一點，在刪除停用詞之前對原始文本進行一些手動修改是很有用的（例如，將“Will Smith”替換為“Will_Smith”）。</p><p>既然我們有了所有有用的標識，就可以應用word轉換了。詞幹化（<strong>Stemming</strong>）和引理化（<strong>Lemmatization</strong>）都產生了單詞的詞根形式。</p><p>他們的區別在於詞幹可能不是一個實際的單詞，而引理是一個實際的語言單詞（詞幹通常更快）。這些算法都是由NLTK提供的。</p><p>繼續示例：</p><pre><code>print("--- stemming ---")ps = nltk.stem.porter.PorterStemmer()print([ps.stem(word) for word in txt])print("--- lemmatisation ---")lem = nltk.stem.wordnet.WordNetLemmatizer()print([lem.lemmatize(word) for word in txt])</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/71175b8fa3164958836075bcd7fd9265><p class=pgc-img-caption></p></div><p>如你所見，有些單詞已經改變了：“joins”變成了它的根形式“join”，就像“cups”。另一方面，“official”只隨著詞幹的變化而變化，詞幹“offici”不是一個詞，而是通過去掉後綴“-al”而產生的。</p><p>我將把所有這些預處理步驟放在一個函數中，並將其應用於整個數據集。</p><pre><code>'''預處理.:parameter    :param text: string - 包含文本的列的名稱    :param lst_stopwords: list - 要刪除的停用詞列表    :param flg_stemm: bool - 是否應用詞幹    :param flg_lemm: bool - 是否應用引理化:return    cleaned text'''def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):    ## 清洗（轉換為小寫並刪除標點和字符，然後刪除）    text = re.sub(r'[^\w\s]', '', str(text).lower().strip())    ## 標識化（從字符串轉換為列表）    lst_text = text.split()    ## 刪除停用詞    if lst_stopwords is not None:        lst_text = [word for word in lst_text if word not in                     lst_stopwords]    ## 詞幹化    if flg_stemm == True:        ps = nltk.stem.porter.PorterStemmer()        lst_text = [ps.stem(word) for word in lst_text]    ## 引理化    if flg_lemm == True:        lem = nltk.stem.wordnet.WordNetLemmatizer()        lst_text = [lem.lemmatize(word) for word in lst_text]    ## 從列表返回到字符串    text = " ".join(lst_text)    return text</code></pre><p>請注意，你不應該同時應用詞幹和引理化。在這裡我將使用後者。</p><pre><code>dtf["text_clean"] = dtf["text"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords))</code></pre><p>和以前一樣，我創建了一個新的列：</p><pre><code>dtf.head()</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c81607f48ea74b66b7a4fd93b325be41><p class=pgc-img-caption></p></div><pre><code>print(dtf["text"].iloc[0], " --&gt; ", dtf["text_clean"].iloc[0])</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/36bf9643fa0c43c286f48d40cec4ab1f><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>長度分析</h1><p>查看文本的長度很重要，因為這是一個簡單的計算，可以提供很多信息。</p><p>例如，也許我們足夠幸運地發現，一個類別系統地比另一個類別長，而長度只是構建模型所需的唯一特徵。不幸的是，由於新聞標題的長度相似，情況並非如此，但值得一試。</p><p>文本數據有幾種長度度量。我將舉幾個例子：</p><ul><li><strong>字數</strong>：統計文本中的標識數（用空格分隔）</li><li><strong>字符數</strong>：將每個標識的字符數相加</li><li><strong>句子計數</strong>：計算句子的數量（用句點分隔）</li><li><strong>平均字長</strong>：字長之和除以字數（字數/字數）</li><li><strong>平均句子長度</strong>：句子長度之和除以句子數（字數/句子數）</li></ul><pre><code>dtf['word_count'] = dtf["text"].apply(lambda x: len(str(x).split(" ")))dtf['char_count'] = dtf["text"].apply(lambda x: sum(len(word) for word in str(x).split(" ")))dtf['sentence_count'] = dtf["text"].apply(lambda x: len(str(x).split(".")))dtf['avg_word_length'] = dtf['char_count'] / dtf['word_count']dtf['avg_sentence_lenght'] = dtf['word_count'] / dtf['sentence_count']dtf.head()</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1df29b5fc7c745dd8a9bb7c4a356cd63><p class=pgc-img-caption></p></div><p>讓我們看看例子：</p><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ec535bd18cdb4d03bd08f7ff627b49ad><p class=pgc-img-caption></p></div><p>這些新變量相對於目標的分佈情況如何？為了回答這個問題，我將研究二元分佈（兩個變量如何一起影響）。</p><p>首先，我將整個觀察結果分成3個樣本（政治、娛樂、科技），然後比較樣本的直方圖和密度。如果分佈不同，那麼變量是可預測的，因為這三組有不同的模式。</p><p>例如，讓我們看看字符數是否與目標變量相關：</p><pre><code>x, y = "char_count", "y"fig, ax = plt.subplots(nrows=1, ncols=2)fig.suptitle(x, fontsize=12)for i in dtf[y].unique():    sns.distplot(dtf[dtf[y]==i][x], hist=True, kde=False,                  bins=10, hist_kws={"alpha":0.8},                  axlabel="histogram", ax=ax[0])    sns.distplot(dtf[dtf[y]==i][x], hist=False, kde=True,                  kde_kws={"shade":True}, axlabel="density",                    ax=ax[1])ax[0].grid(True)ax[0].legend(dtf[y].unique())ax[1].grid(True)plt.show()</code></pre><p>這三個類別具有相似的長度分佈。在這裡，密度圖非常有用，因為樣本有不同的大小。</p><h1 class=pgc-h-arrow-right>情感分析</h1><p>情感分析是通過數字或類來表達文本數據的主觀情感。由於自然語言的模糊性，計算情感是自然語言處理中最困難的任務之一。</p><p>例如，短語“This is so bad that it’s good”有不止一種解釋。一個模型可以給“好”這個詞分配一個積極的信號，給“壞”這個詞分配一個消極的信號，從而產生一種中性的情緒。這是因為上下文未知。</p><p>最好的方法是訓練你自己的情緒模型，使之適合你的數據。當沒有足夠的時間或數據時，可以使用預訓練好的模型，比如Textblob和Vader。</p><ul><li>Textblob建立在NLTK的基礎上，是最流行的一種，它可以給單詞賦予極性，並作為一個平均值來估計整個文本的情緒。</li><li>另一方面，Vader（Valence-aware dictionary and mootion reasoner）是一個基於規則的模型，尤其適用於社交媒體數據。</li></ul><p>我將使用Textblob添加一個情感特徵：</p><pre><code>dtf["sentiment"] = dtf[column].apply(lambda x:                    TextBlob(x).sentiment.polarity)dtf.head()</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6f295c1e9e7a44b28e5300a91843c877><p class=pgc-img-caption></p></div><pre><code>print(dtf["text"].iloc[0], " --&gt; ", dtf["sentiment"].iloc[0])</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f107e91b733545a29f78d1ecd7c4f2c0><p class=pgc-img-caption></p></div><p>分類和情緒之間有規律嗎？</p><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/dcaf080fe85c42e0b4524cf4c022a20b><p class=pgc-img-caption></p></div><p>大多數的頭條新聞都是中性的，除了政治新聞偏向於負面，科技新聞偏向於正面。</p><h1 class=pgc-h-arrow-right>命名實體識別</h1><p>命名實體識別（Named entity recognition，NER）是用預定義的類別（如人名、組織、位置、時間表達式、數量等）提取非結構化文本中的命名實體的過程。</p><p>訓練一個NER模型是非常耗時的，因為它需要一個非常豐富的數據集。幸運的是有人已經為我們做了這項工作。最好的開源NER工具之一是SpaCy。它提供了不同的NLP模型，這些模型能夠識別多種類型的實體。</p><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/28b06f0dc8f740ed8da3850a4978fb92><p class=pgc-img-caption></p></div><p>我將在我們通常的標題（未經預處理的原始文本）中使用SpaCy模型en_core_web_lg（網絡數據上訓練的英語的大型模型）,給出一個例子：</p><pre><code>## 調用ner = spacy.load("en_core_web_lg")## 打標籤txt = dtf["text"].iloc[0]doc = ner(txt)## 展示結果spacy.displacy.render(doc, style="ent")</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7000ab429fe24e418734f4291f4f809d><p class=pgc-img-caption></p></div><p>這很酷，但是我們怎麼能把它變成有用的特徵呢？這就是我要做的：</p><ul><li>對數據集中的每個文本觀察運行NER模型，就像我在前面的示例中所做的那樣。</li><li>對於每個新聞標題，我將把所有被認可的實體以及同一實體出現在文本中的次數放入一個新的列（稱為“tags”）。在這個例子中：{ (‘Will Smith’, ‘PERSON’):1, (‘Diplo’, ‘PERSON’):1, (‘Nicky Jam’, ‘PERSON’):1, (“The 2018 World Cup’s”, ‘EVENT’):1 }</li><li>然後，我將為每個標識類別（Person、Org、Event，…）創建一個新列，並計算每個標識類別找到的實體數。在上面的例子中，特徵將是tags_PERSON = 3tags_EVENT = 1</li></ul><pre><code>## 標識文本並將標識導出到列表中dtf["tags"] = dtf["text"].apply(lambda x: [(tag.text, tag.label_)                                 for tag in ner(x).ents] )## utils函數計算列表元素def utils_lst_count(lst):    dic_counter = collections.Counter()    for x in lst:        dic_counter[x] += 1    dic_counter = collections.OrderedDict(                      sorted(dic_counter.items(),                      key=lambda x: x[1], reverse=True))    lst_count = [ {key:value} for key,value in dic_counter.items() ]    return lst_count## 計數dtf["tags"] = dtf["tags"].apply(lambda x: utils_lst_count(x))## utils函數為每個標識類別創建新列def utils_ner_features(lst_dics_tuples, tag):    if len(lst_dics_tuples) &gt; 0:        tag_type = []        for dic_tuples in lst_dics_tuples:            for tuple in dic_tuples:                type, n = tuple[1], dic_tuples[tuple]                tag_type = tag_type + [type]*n                dic_counter = collections.Counter()                for x in tag_type:                    dic_counter[x] += 1        return dic_counter[tag]    else:        return 0## 提取特徵tags_set = []for lst in dtf["tags"].tolist():     for dic in lst:          for k in dic.keys():              tags_set.append(k[1])tags_set = list(set(tags_set))for feature in tags_set:     dtf["tags_"+feature] = dtf["tags"].apply(lambda x:                              utils_ner_features(x, feature))## 結果dtf.head()</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/25c3cf9252d448b7acd686a213ae864f><p class=pgc-img-caption></p></div><p>現在我們可以在標識類型分佈上有一個視圖。以組織標籤（公司和組織）為例：</p><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/40e14c92321c41a79d31ddae06565d29><p class=pgc-img-caption></p></div><p>為了更深入地分析，我們需要使用在前面的代碼中創建的列“tags”。讓我們為標題類別之一繪製最常用的標識：</p><pre><code>y = "ENTERTAINMENT"tags_list = dtf[dtf["y"]==y]["tags"].sum()map_lst = list(map(lambda x: list(x.keys())[0], tags_list))dtf_tags = pd.DataFrame(map_lst, columns=['tag','type'])dtf_tags["count"] = 1dtf_tags = dtf_tags.groupby(['type',                  'tag']).count().reset_index().sort_values("count",                  ascending=False)fig, ax = plt.subplots()fig.suptitle("Top frequent tags", fontsize=12)sns.barplot(x="count", y="tag", hue="type",             data=dtf_tags.iloc[:top,:], dodge=False, ax=ax)ax.grid(axis="x")plt.show()</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ac6e359689d94ab1ab40798e9fcafe7a><p class=pgc-img-caption></p></div><p>接著介紹NER的另一個有用的應用程序：你還記得我們從“Will Smith”的名稱中刪除了“Will”這個單詞的停用詞嗎？解決這個問題的一個有趣的方法是將“Will Smith”替換為“Will_Smith”，這樣它就不會受到停用詞刪除的影響。</p><p>遍歷數據集中的所有文本來更改名稱是不可能的，所以讓我們使用SpaCy。如我們所知，SpaCy可以識別一個人名，因此我們可以使用它來檢測姓名，然後修改字符串。</p><pre><code>## 預測txt = dtf["text"].iloc[0]entities = ner(txt).ents## 打標籤tagged_txt = txtfor tag in entities:    tagged_txt = re.sub(tag.text, "_".join(tag.text.split()),                         tagged_txt) ## 結果print(tagged_txt)</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/03f46611a5de4880a1c6f7f2b80733a3><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>詞頻</h1><p>到目前為止，我們已經看到了如何通過分析和處理整個文本來進行特徵工程。</p><p>現在，我們將通過計算n-grams頻率來研究單個單詞的重要性。n-gram是給定文本樣本中n個項的連續序列。當n-gram的大小為1時，稱為unigram（大小為2是一個bigram）。</p><p>例如，短語“I like this article”可以分解為：</p><ul><li>4個unigram： “I”, “like”, “this”, “article”</li><li>3個bigrams：“I like”, “like this”, “this article”</li></ul><p>我將以政治新聞為例說明如何計算unigram和bigrams頻率。</p><pre><code>y = "POLITICS"corpus = dtf[dtf["y"]==y]["text_clean"]lst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=" "))fig, ax = plt.subplots(nrows=1, ncols=2)fig.suptitle("Most frequent words", fontsize=15)## unigramsdic_words_freq = nltk.FreqDist(lst_tokens)dtf_uni = pd.DataFrame(dic_words_freq.most_common(),                        columns=["Word","Freq"])dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(                  kind="barh", title="Unigrams", ax=ax[0],                   legend=False).grid(axis='x')ax[0].set(ylabel=None)## bigramsdic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))dtf_bi = pd.DataFrame(dic_words_freq.most_common(),                       columns=["Word","Freq"])dtf_bi["Word"] = dtf_bi["Word"].apply(lambda x: " ".join(                   string for string in x) )dtf_bi.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(                  kind="barh", title="Bigrams", ax=ax[1],                  legend=False).grid(axis='x')ax[1].set(ylabel=None)plt.show()</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2cabce0d2a264bb5834de67e154dd033><p class=pgc-img-caption></p></div><p>如果有n個字母只出現在一個類別中（即政治新聞中的“Republican”），那麼這些就可能成為新的特徵。一種更為費力的方法是對整個語料庫進行向量化，並使用所有的單詞作為特徵（單詞包方法）。</p><p>現在我將向你展示如何在數據幀中添加單詞頻率作為特徵。我們只需要Scikit learn中的CountVectorizer，它是Python中最流行的機器學習庫之一。</p><p>CountVectorizer將文本文檔集合轉換為計數矩陣。我將用3個n-grams來舉例：“box office”（經常出現在娛樂圈）、“republican”（經常出現在政界）、“apple”（經常出現在科技界）。</p><pre><code>lst_words = ["box office", "republican", "apple"]## 計數lst_grams = [len(word.split(" ")) for word in lst_words]vectorizer = feature_extraction.text.CountVectorizer(                 vocabulary=lst_words,                  ngram_range=(min(lst_grams),max(lst_grams)))dtf_X = pd.DataFrame(vectorizer.fit_transform(dtf["text_clean"]).todense(), columns=lst_words)## 將新特徵添加為列dtf = pd.concat([dtf, dtf_X.set_index(dtf.index)], axis=1)dtf.head()</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/16a70d8994a14924872bb5bc5edabc1f><p class=pgc-img-caption></p></div><p>可視化相同信息的一個很好的方法是使用word cloud，其中每個標識的頻率用字體大小和顏色顯示。</p><pre><code>wc = wordcloud.WordCloud(background_color='black', max_words=100,                          max_font_size=35)wc = wc.generate(str(corpus))fig = plt.figure(num=1)plt.axis('off')plt.imshow(wc, cmap=None)plt.show()</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1abdc7297e4f49eca02c3a3feb27aa72><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>詞向量</h1><p>最近，NLP領域開發了新的語言模型，這些模型依賴於神經網絡結構，而不是更傳統的n-gram模型。這些新技術是一套語言建模和特徵學習技術，將單詞轉換為實數向量，因此稱為詞嵌入。</p><p>詞嵌入模型通過構建所選單詞前後出現的標識的概率分佈，將特定單詞映射到向量。這些模型很快變得流行，因為一旦你有了實數而不是字符串，你就可以執行計算了。例如，要查找相同上下文的單詞，可以簡單地計算向量距離。</p><p>有幾個Python庫可以使用這種模型。SpaCy是其中之一，但由於我們已經使用過它，我將談論另一個著名的包：Gensim。</p><p>它是使用現代統計機器學習的用於無監督主題模型和自然語言處理的開源庫。使用Gensim，我將加載一個預訓練的GloVe模型。</p><p>GloVe是一種無監督學習算法，用於獲取300個單詞的向量表示。</p><pre><code>nlp = gensim_api.load("glove-wiki-gigaword-300")</code></pre><p>我們可以使用此對象將單詞映射到向量：</p><pre><code>word = "love"nlp[word]</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2947c6a0ca54411fbcf1fdebf87517b5><p class=pgc-img-caption></p></div><pre><code>nlp[word].shape</code></pre><p>現在讓我們來看看什麼是最接近的詞向量，換句話說，就是大多數出現在相似上下文中的詞。</p><p>為了在二維空間中繪製向量圖，我需要將維數從300降到2。我將使用Scikit learn中的t-分佈隨機鄰居嵌入來實現這一點。</p><p>t-SNE是一種可視化高維數據的工具，它將數據點之間的相似性轉換為聯合概率。</p><pre><code>## 找到最近的向量labels, X, x, y = [], [], [], []for t in nlp.most_similar(word, topn=20):    X.append(nlp[t[0]])    labels.append(t[0])## 降維pca = manifold.TSNE(perplexity=40, n_components=2, init='pca')new_values = pca.fit_transform(X)for value in new_values:    x.append(value[0])    y.append(value[1])## 繪圖fig = plt.figure()for i in range(len(x)):    plt.scatter(x[i], y[i], c="black")    plt.annotate(labels[i], xy=(x[i],y[i]), xytext=(5,2),                textcoords='offset points', ha='right', va='bottom')## 添加中心plt.scatter(x=0, y=0, c="red")plt.annotate(word, xy=(0,0), xytext=(5,2), textcoords='offset              points', ha='right', va='bottom')</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/508deb596dfa41f588710eb3621f9efd><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>主題模型</h1><p>Genism包專門用於主題模型。主題模型是一種用於發現文檔集合中出現的抽象“主題”的統計模型。</p><p>我將展示如何使用LDA（潛Dirichlet分佈）提取主題：它是一個生成統計模型，它允許由未觀察到的組解釋觀察結果集，解釋為什麼數據的某些部分是相似的。</p><p>基本上，文檔被表示為潛在主題上的隨機混合，每個主題的特徵是在單詞上的分佈。</p><p>讓我們看看我們可以從科技新聞中提取哪些主題。我需要指定模型必須簇的主題數，我將嘗試使用3：</p><pre><code>y = "TECH"corpus = dtf[dtf["y"]==y]["text_clean"]## 預處理語料庫lst_corpus = []for string in corpus:    lst_words = string.split()    lst_grams = [" ".join(lst_words[i:i + 2]) for i in range(0,                      len(lst_words), 2)]    lst_corpus.append(lst_grams)## 將單詞映射到idid2word = gensim.corpora.Dictionary(lst_corpus)## 創建詞典 word:freqdic_corpus = [id2word.doc2bow(word) for word in lst_corpus] ## 訓練LDAlda_model = gensim.models.ldamodel.LdaModel(corpus=dic_corpus, id2word=id2word, num_topics=3, random_state=123, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)## 輸出lst_dics = []for i in range(0,3):    lst_tuples = lda_model.get_topic_terms(i)    for tupla in lst_tuples:        lst_dics.append({"topic":i, "id":tupla[0],                          "word":id2word[tupla[0]],                          "weight":tupla[1]})dtf_topics = pd.DataFrame(lst_dics,                          columns=['topic','id','word','weight'])## plotfig, ax = plt.subplots()sns.barplot(y="word", x="weight", hue="topic", data=dtf_topics, dodge=False, ax=ax).set_title('Main Topics')ax.set(ylabel="", xlabel="Word Importance")plt.show()</code></pre><div class=pgc-img><img alt=NLP的文本分析與特徵工程 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a7740c6a8a64464997ae985d0796b263><p class=pgc-img-caption></p></div><p>試圖僅用3個主題捕捉6年的內容可能有點困難，但正如我們所看到的，關於蘋果公司的一切都以同一個主題結束。</p><h1 class=pgc-h-arrow-right>結論</h1><p>本文是演示如何使用NLP分析文本數據併為機器學習模型提取特徵的教程。</p><p>我演示瞭如何檢測數據所使用的語言，以及如何預處理和清除文本。然後我解釋了長度的不同度量，用Textblob進行了情緒分析，並使用SpaCy進行命名實體識別。最後，我解釋了Scikit學習的傳統詞頻方法與Gensim的現代語言模型之間的區別。</p><p>現在，你已經瞭解了開始處理文本數據的所有NLP基礎知識。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>特徵</a></li><li><a>NLP</a></li><li><a>文本</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/929d155d.html alt=耳模的基本特徵 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/929d155d.html title=耳模的基本特徵>耳模的基本特徵</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6d8adef2.html alt=樑配筋特徵，板、柱鋼筋下料分析，剪力牆筋構造，鋼筋知識順口溜 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/41066b7c174f47f38ee89f4113601791 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6d8adef2.html title=樑配筋特徵，板、柱鋼筋下料分析，剪力牆筋構造，鋼筋知識順口溜>樑配筋特徵，板、柱鋼筋下料分析，剪力牆筋構造，鋼筋知識順口溜</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/06217c6d.html alt=江恩：價格頂點的9個數字特徵 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/06217c6d.html title=江恩：價格頂點的9個數字特徵>江恩：價格頂點的9個數字特徵</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8aefbc6c.html alt=被動陸緣遠端帶基底性質和變形特徵研究取得新進展 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/S8HpHQZ2E0ffWa style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8aefbc6c.html title=被動陸緣遠端帶基底性質和變形特徵研究取得新進展>被動陸緣遠端帶基底性質和變形特徵研究取得新進展</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2543d3f6.html alt=SolidWorks模型分享：滑輪組（2016版全特徵） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/03b0b432951c4537b463c46582c8b1ac style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2543d3f6.html title=SolidWorks模型分享：滑輪組（2016版全特徵）>SolidWorks模型分享：滑輪組（2016版全特徵）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8755a182.html alt=多名市民突然被停機，只因“打太多”？運營商：符合騷擾電話特徵 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/e8236005a7c84145b8bcd6d95e6b8ca0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8755a182.html title=多名市民突然被停機，只因“打太多”？運營商：符合騷擾電話特徵>多名市民突然被停機，只因“打太多”？運營商：符合騷擾電話特徵</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/eb2bc83c.html alt=廈門常用植物圖集-四季秋海棠，主要介紹特徵，習性，花期等知識 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/72475fabe6ca4ee08a1873852537b6f9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/eb2bc83c.html title=廈門常用植物圖集-四季秋海棠，主要介紹特徵，習性，花期等知識>廈門常用植物圖集-四季秋海棠，主要介紹特徵，習性，花期等知識</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/33d04d4d.html alt=2020年各大頂會NLP、ML優質論文分類整理分享 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/fb47112700b049aa88994c8949ec9403 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/33d04d4d.html title=2020年各大頂會NLP、ML優質論文分類整理分享>2020年各大頂會NLP、ML優質論文分類整理分享</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b2a9a652.html alt=如果一個人具有這3個特徵，說明他骨子裡很幼稚，易被人控制 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2dfd8144162d4e409c22df0eb0b6f6f4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b2a9a652.html title=如果一個人具有這3個特徵，說明他骨子裡很幼稚，易被人控制>如果一個人具有這3個特徵，說明他骨子裡很幼稚，易被人控制</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/caedaf62.html alt=心理學家：如果你有這3個特徵，說明你骨子裡很幼稚，易被人控制 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/6718a489818e4e6390f92eb4b680b280 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/caedaf62.html title=心理學家：如果你有這3個特徵，說明你骨子裡很幼稚，易被人控制>心理學家：如果你有這3個特徵，說明你骨子裡很幼稚，易被人控制</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e9476969.html alt=心理學家：如果你有這些特徵，說明你骨子裡很幼稚，易被他人控制 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/218f30ce2d164bd1aaeff331859cde2f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e9476969.html title=心理學家：如果你有這些特徵，說明你骨子裡很幼稚，易被他人控制>心理學家：如果你有這些特徵，說明你骨子裡很幼稚，易被他人控制</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b767dfe9.html alt=野兔的特徵特性及防治新技術 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/95d9367c151e468eb06ae651690fade8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b767dfe9.html title=野兔的特徵特性及防治新技術>野兔的特徵特性及防治新技術</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5e37e483.html alt=基於Transformer模型的中文文本自動校對研究 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RnA5iHG5yimfrr style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5e37e483.html title=基於Transformer模型的中文文本自動校對研究>基於Transformer模型的中文文本自動校對研究</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cdd645da.html alt=50幅美麗逼真的眼睛繪畫作品，眼睛是臉部美麗的特徵之一 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/f4fb42b263414d24a4bb4958074d1f47 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cdd645da.html title=50幅美麗逼真的眼睛繪畫作品，眼睛是臉部美麗的特徵之一>50幅美麗逼真的眼睛繪畫作品，眼睛是臉部美麗的特徵之一</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ca2e5982.html alt=1-6年級特徵分析，幾年級開始學編程最好？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/779189e97c984486abcea0e1a7f24041 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ca2e5982.html title=1-6年級特徵分析，幾年級開始學編程最好？>1-6年級特徵分析，幾年級開始學編程最好？</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>