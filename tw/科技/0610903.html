<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>深度學習中的優化器對比 | 极客快訊</title><meta property="og:title" content="深度學習中的優化器對比 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/e6c889ca1bde4454985ccf697dd54117"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0610903.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0610903.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0610903.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0610903.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0610903.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0610903.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0610903.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0610903.html><meta property="article:published_time" content="2020-10-29T21:01:05+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:05+08:00"><meta name=Keywords content><meta name=description content="深度學習中的優化器對比"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/0610903.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>深度學習中的優化器對比</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p class=pgc-end-literature>介紹了神經網絡訓練過程中的常見優化策略，並進行了分析和對比，包括梯度下降、小批量梯度下降、動量梯度下降、RMSProp、Adam 以及最新的RAdam和Ranger</p><h1 class=pgc-h-arrow-right>批量梯度下降法BGD</h1><p>假設訓練樣本總數為n，樣本為 {(x1,y1),(x2,y2),,,,(xn,yn)}，模型參數θ， ，損失函數為fuhao J(θ)，在第i對樣本 (xi,yi)上損失函數關於參數的梯度為 ▽J(θ,xi,yi), 學習率為α，，則使用BGD更新參數為：</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e6c889ca1bde4454985ccf697dd54117><p class=pgc-img-caption></p></div><p>由上式可以看出，每進行一次參數更新，需要計算整個數據樣本集，因此導致批量梯度下降法的速度會比較慢，尤其是數據集非常大的情況下，收斂速度就會非常慢，但是由於每次的下降方向為總體平均梯度，它得到的會是一個全局最優解。</p><h1 class=pgc-h-arrow-right>SGD</h1><p>隨機梯度下降法，不像BGD每一次參數更新，需要計算整個數據樣本集的梯度，而是每次參數更新時，僅僅選取一個樣本(xi,yi) 計算其梯度，參數更新公式為：</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/37a922b335674ffa830fd2904c954270><p class=pgc-img-caption></p></div><p>可以看到BGD和SGD是兩個極端，SGD由於每次參數更新僅僅需要計算一個樣本的梯度，訓練速度很快，即使在樣本量很大的情況下，可能只需要其中一部分樣本就能迭代到最優解，由於每次迭代並不是都向著整體最優化方向，導致梯度下降的波動非常大，更容易從一個局部最優跳到另一個局部最優，準確度下降。</p><h1 class=pgc-h-arrow-right>Momentum</h1><h2 class=pgc-h-arrow-right>動量優化法</h2><p>動量優化方法引入物理學中的動量思想，<strong>加速梯度下降</strong>，有Momentum和Nesterov兩種算法。當我們將一個小球從山上滾下來，沒有阻力時，它的動量會越來越大，但是如果遇到了阻力，速度就會變小，動量優化法就是借鑑此思想，使得梯度方向在不變的維度上，參數更新變快，梯度有所改變時，更新參數變慢，這樣就能夠加快收斂並且減少動盪。</p><h2 class=pgc-h-arrow-right>Momentum</h2><p>momentum算法思想：參數更新時在一定程度上保留之前更新的方向，同時又利用當前batch的梯度微調最終的更新方向，簡言之就是通過積累之前的動量來加速當前的梯度。假設 mt表示t時刻的動量，</p><p>μ 表示動量因子，通常取值0.9或者近似值，在SGD的基礎上增加動量，則參數更新公式如下：</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/76d46a8f05e847769f4a8ea49d96d078><p class=pgc-img-caption></p></div><p>在梯度方向改變時，momentum能夠降低參數更新速度，從而減少震盪；在梯度方向相同時，momentum可以加速參數更新， 從而加速收斂。總而言之，momentum能夠加速SGD收斂，抑制震盪。</p><h1 class=pgc-h-arrow-right>自適應學習率優化算法</h1><p>在機器學習中，學習率是一個非常重要的超參數，但是學習率是非常難確定的，雖然可以通過多次訓練來確定合適的學習率，但是一般也不太確定多少次訓練能夠得到最優的學習率，玄學事件，對人為的經驗要求比較高，所以是否存在一些策略自適應地調節學習率的大小，從而提高訓練速度。 目前的自適應學習率優化算法主要有：AdaGrad算法，RMSProp算法，Adam算法以及AdaDelta算法。</p><h1 class=pgc-h-arrow-right>AdaGradfuhao</h1><p>定義參數：全局學習率δ，一般會選擇 δ=0.01 ; 一個極小的常量 ε ，通常取值10e-8,目的是為了分母為0; 梯度加速變量(gradient accumulation variable) r。</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/10c4cdd95f784ce1935357aec66d0777><p class=pgc-img-caption></p></div><p>從上式可以看出，梯度加速變量r為t時刻前梯度的平方和</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6467d743ee2e4eb58c1cf9c835401cc6><p class=pgc-img-caption></p></div><p>, 那麼參數更新量</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ea6be94a2c874a748acdb44cc9fac33b><p class=pgc-img-caption></p></div><p>，將</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/01b32742246d4fb7ad64774680e8492a><p class=pgc-img-caption></p></div><p>看成一個約束項regularizer. 在前期，梯度累計平方和比較小，也就是r相對較小，則約束項較大，這樣就能夠放大梯度, 參數更新量變大; 隨著迭代次數增多，梯度累計平方和也越來越大，即r也相對較大，則約束項變小，這樣能夠縮小梯度，參數更新量變小。</p><p><strong>缺點：</strong></p><ul><li>仍需要手工設置一個全局學習率δ , 如果δ 設置過大的話，會使regularizer過於敏感，對梯度的調節太大</li><li>中後期，分母上梯度累加的平方和會越來越大，使得參數更新量趨近於0，使得訓練提前結束，無法學習</li></ul><h1 class=pgc-h-arrow-right>Adadelta</h1><p>Adagrad會累加之前所有的梯度平方，而Adadelta只累加固定大小的項，並且也不直接存儲這些項，僅僅是近似計算對應的平均值</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/e6e6a1d638d4470b8d7666bd71bb85ff><p class=pgc-img-caption></p></div><p>從上式中可以看出，Adadelta其實還是依賴於全局學習率ŋ，但是作者做了一定處理，經過近似牛頓迭代法之後</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/34d109049a324dc0bbc02220aac8352b><p class=pgc-img-caption></p></div><p>此時可以看出Adadelta已經不依賴全局learning rate了。</p><p><strong>特點：</strong></p><ul><li>訓練初中期，加速效果不錯，很快。</li><li>訓練後期，反覆在局部最小值附近抖動。</li></ul><h1 class=pgc-h-arrow-right>RMSprop</h1><p>RMSProp算法修改了AdaGrad的梯度平方和累加為指數加權的移動平均，使得其在非凸設定下效果更好。設定參數：全局初始率 δ , 默認設為0.001; decay rate ρ,默認設置為0.9,一個極小的常量 ε ，通常為10e-6</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5cb1b7814d2846e199d4e0e2971b2d9d><p class=pgc-img-caption></p></div><p><strong>特點：</strong></p><ul><li>其實RMSprop依然依賴於全局學習率 δ</li><li>RMSprop算是Adagrad的一種發展，和Adadelta的變體，效果趨於二者之間</li><li>適合處理非平穩目標——對於RNN效果很好</li></ul><h1 class=pgc-h-arrow-right>Adam: Adaptive Moment Estimation</h1><p>Adam中動量直接併入了梯度一階矩（指數加權）的估計。其次，相比於缺少修正因子導致二階矩估計可能在訓練初期具有很高偏置的RMSProp，Adam包括偏置修正，修正從原點初始化的一階矩（動量項）和（非中心的）二階矩估計。 默認參數值設定為： β₁=0.9，β₂=0.999，ε =10-8</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/171f3c5f4d044b08a60c622528b1676b><p class=pgc-img-caption></p></div><p>其中， mt， nt分別是對梯度的一階矩估計和二階矩估計；</p><p>m^t ，n^t 是對 mt，nt 的偏差校正，這樣可以近似為對期望的無偏估計</p><p><strong>特點：</strong></p><ul><li>Adam梯度經過偏置校正後，每一次迭代學習率都有一個固定範圍，使得參數比較平穩。</li><li>結合了Adagrad善於處理稀疏梯度和RMSprop善於處理非平穩目標的優點</li><li>為不同的參數計算不同的自適應學習率</li><li>也適用於大多非凸優化問題——適用於大數據集和高維空間。</li></ul><h1 class=pgc-h-arrow-right>算法的表現</h1><p>下圖是各個算法在等高線的表現，它們都從相同的點出發，走不同的路線達到最小值點。可以看到，Adagrad，Adadelta和RMSprop在正確的方向上很快地轉移方向，並且快速地收斂，然而Momentum和NAG先被領到一個偏遠的地方，然後才確定正確的方向，NAG比momentum率先更正方向。SGD則是緩緩地朝著最小值點前進。</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a085268bdb1e46e69c2826ab4f91e0dc><p class=pgc-img-caption></p></div><p><br></p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5dd74dd150b84d08a6f618579615199b><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>RAdam</h1><p><br></p><p>新的優化器：Rectified Adam（RAdam），相比adam，可以穩定提高準確率.</p><p>這是經典Adam優化器的一個新變種，Adam用到的warmup是一種方差衰減器，但所需的warmup程度是未知的，而且數據集之間是不同的，因此，用數學算法來作為一種動態方差衰減器，即構建了一個整流器項，這允許自適應動量作為一個潛在的方差的函數緩慢但穩定地得到充分表達。完整模型：</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6f32824146b64a1eab5417679530b5aa><p class=pgc-img-caption></p></div><p><br>RAdam根據方差的潛在散度動態地打開或關閉自適應學習率。實際上，它提供了不需要可調參數的動態warmup。</p><p><br></p><h1 class=pgc-h-arrow-right>Ranger</h1><p>新的state of the art優化器：Ranger</p><p>Ranger是RAdam 與 LookAhead 的互補</p><p>RAdam 可以說是優化器在開始訓練時的最佳基礎。RAdam 利用動態整流器根據方差調整 Adam 的自適應動量，並有效提供能夠根據當前數據集定製的自動預熱機制，能夠確保訓練以紮實的基礎順利邁出第一步。</p><p>LookAhead 則受到深度神經網絡損失面的最新理解進展啟發，能夠在整個訓練期間提供健壯且穩定的突破。</p><p>引用 LookAhead 團隊的說法——LookAhead“減少了對廣泛超參數調整的需求”，同時實現了“以最小計算開銷確保不同深度學習任務實現更快收斂速度。”</p><p>因此，二者都在深度學習優化的不同方面帶來了突破，而且這種組合具有高度協同性，有望為大家的深度學習結果提供兩項最佳改進。如此一來，通過將兩項最新突破 (RAdam + LookAhead) 加以結合，Ranger 的整合成果有望為深度學習帶來新的發展驅動力，幫助我們進一步追求更穩定且強大的優化方法。</p><p>Hinton 等人曾表示：“我們憑經驗證明，Lookahead 能夠顯著提高 SGD 與 Adam 的性能，包括在 ImageNet、CIFAR-10/100、神經機器翻譯以及 Penn Treebank 上的默認超參數設置場景之下。”</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e9a12e6525c048839bb149ac07c4dc78><p class=pgc-img-caption></p></div><p>Lookahead：探索損失面的輔助系統，帶來更快且更穩定的探索與收斂效果</p><div class=pgc-img><img alt=深度學習中的優化器對比 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/34f2c2c9a56146c788ad0b43d9744202><p class=pgc-img-caption></p></div><p><br></p><p>轉載：</p><p class=pgc-end-literature><br></p><p class=pgc-end-literature>優化算法Optimizer比較和總結https://zhuanlan.zhihu.com/p/55150256</p><p class=pgc-end-literature>https://zhuanlan.zhihu.com/p/22252270</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>化器</a></li><li><a>學習</a></li><li><a>深度</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/5a7c0dad.html alt=深度學習中的線性代數 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/dfic-imagehandler/677561e3-e0ec-4693-bd88-0cd182b21a17 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5a7c0dad.html title=深度學習中的線性代數>深度學習中的線性代數</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/31ece80.html alt="深度 | 劉群：基於深度學習的自然語言處理，邊界在哪裡？" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/RaimUrx1Qpfnvn style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/31ece80.html title="深度 | 劉群：基於深度學習的自然語言處理，邊界在哪裡？">深度 | 劉群：基於深度學習的自然語言處理，邊界在哪裡？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c5e8471.html alt=深度學習與醫學圖像分析 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/2490e2dab8ed41e59acad9c5a23348aa style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c5e8471.html title=深度學習與醫學圖像分析>深度學習與醫學圖像分析</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2a4fc7b.html alt=深度強化學習-深度Q網絡（DQN）介紹 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RXfpIwbEJgChY1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2a4fc7b.html title=深度強化學習-深度Q網絡（DQN）介紹>深度強化學習-深度Q網絡（DQN）介紹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html alt=直流鍋爐給水控制學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/eba10edcc8d14d9f8cde6fd5b212d90e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html title=直流鍋爐給水控制學習>直流鍋爐給水控制學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html alt=HTMLCSS學習筆記（六）——元素類型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb5988349894ce9bf568c6418f85b7d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html title=HTMLCSS學習筆記（六）——元素類型>HTMLCSS學習筆記（六）——元素類型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html alt="web前端（從零開始），每天更新學習筆記 HTML5元素分類" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/46d70004fcd55e1ddad3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html title="web前端（從零開始），每天更新學習筆記 HTML5元素分類">web前端（從零開始），每天更新學習筆記 HTML5元素分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html alt="MySQL 學習筆記" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html title="MySQL 學習筆記">MySQL 學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html alt=深入學習MySQL事務：ACID特性的實現原理「轉」 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/cdc702d66d6943499997d11e931425eb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html title=深入學習MySQL事務：ACID特性的實現原理「轉」>深入學習MySQL事務：ACID特性的實現原理「轉」</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html alt=如何學習模擬IC設計？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html title=如何學習模擬IC設計？>如何學習模擬IC設計？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html alt=小猿圈python學習-三大特性之多態 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ad0e8e3777854337abeb7c779ad79a04 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html title=小猿圈python學習-三大特性之多態>小猿圈python學習-三大特性之多態</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/03a295fc.html alt=地理學習5——地球的運動（地球的公轉及其地理意義） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7b2b74c871eb40beb8ee143627d29611 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/03a295fc.html title=地理學習5——地球的運動（地球的公轉及其地理意義）>地理學習5——地球的運動（地球的公轉及其地理意義）</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>