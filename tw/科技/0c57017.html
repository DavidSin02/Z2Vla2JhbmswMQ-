<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>Service Mesh 在百度網盤數萬後端的落地實踐 | 极客快訊</title><meta property="og:title" content="Service Mesh 在百度網盤數萬後端的落地實踐 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p9.pstatp.com/large/pgc-image/1dc24790352b4786aa3285c1fb3be147"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0c57017.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0c57017.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0c57017.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0c57017.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0c57017.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0c57017.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0c57017.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0c57017.html><meta property="article:published_time" content="2020-10-29T20:52:48+08:00"><meta property="article:modified_time" content="2020-10-29T20:52:48+08:00"><meta name=Keywords content><meta name=description content="Service Mesh 在百度網盤數萬後端的落地實踐"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/0c57017.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>Service Mesh 在百度網盤數萬後端的落地實踐</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1dc24790352b4786aa3285c1fb3be147><p class=pgc-img-caption></p></div><p>作者 | 李鴻斌</p><p>1 背景</p><p>起初，在網盤快速發展期，為了快速上線，採用了服務單體化 + 主幹開發模式進行研發，隨著用戶規模爆發式的增長以及產品形態的豐富，單體化的不足就體現出來了，於是架構上採用了微服務架構，開始對業務邏輯進行拆分部署。</p><p>服務拆分之後，也引入了新的問題，具體如下：</p><ul class=list-paddingleft-2><li>請求路由：服務部署從物理機向虛擬化方式遷移中，有大量的切流量操作，需要相關的上游都進行升級上線修改，效率低下；</li><li>故障管理：單實例異常、服務級別異常、機房故障異常、網絡異常等，嚴重缺失或者不完善，同時配套的故障定位也沒有，服務穩定性不足；</li><li>流量轉發：不同的服務採用了不同的框架，甚至裸框架，策略不完善，導致負載不均衡；</li><li>研發效率：相同的功能點，需要在不同的語言框架上實現一次，浪費人力，同時升級週期比較長，收斂效率低</li></ul><p>2 解決方案 - UFC</p><p>2.1 UFC 發展史</p><p>為了解決這個問題，從 2015 年底開始思考解決方案，確定瞭解決問題的核心在於管控請求流量，在 2016 年開始自研網絡流量轉發中間件 - UFC(Unified Flow Control)，業務通過同機部署的 agent 進行服務通信，相關的發展史如下：</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0e86c294e5c34d53a18f05f4ee94e4ae><p class=pgc-img-caption></p></div><p>2.2 UFC 和 Service Mesh 的關係</p><p>後來在調研業界相關技術的時候，發現了 istio(業界 Service Mesh 的典型代表)，從而發現了 Service Mesh 的存在，而它的定義是在 2016.9 由 Buoyant 公司的 CEO William Morgan 提出的：</p><blockquote><p>Service Mesh 是一個專門用來處理服務和服務之間通信的基礎設施。它複雜確保在一個由複雜服務構成的拓撲的現代雲應用中，請求能夠被穩定的傳遞。在實踐中，Service Mesh 通常通過一系列輕量級的代理來進行實現。這些代理和應用同機部署，而應用不需要感知到代理的存在。</p></blockquote><blockquote><p>A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application. In practice, the service mesh is typically implemented as an array of lightweight network proxies that are deployed alongside application code, without the application needing to be aware.</p></blockquote><p>從定義上，我們不難發現 UFC 和 Service Mesh 的設計理念是一致的，都是基於 sidecar 模式，有著控制面板和數據面板，UFC 是 Service Mesh 的一種實現。感慨的說，歷史的發展潮流是一致的，不管你在哪裡。</p><p>目前 UFC 應用於網盤過千個服務上，涉及虛擬化實例數量超過 20W，千億 PV，機器規模 10W+(網盤 + 其它產品線機器)，10 個 IDC，從已知的實例規模上看，是國內最大的 Service Mesh 的實踐落地。</p><p>2.3 基於 Service Mesh 之上的服務治理</p><p>百度網盤的實踐落地並不只侷限於 Service Mesh，首先是構建了從點延伸到線的 Service Mesh 進行服務通信管控，然後是在 UFC 這個 Service Mesh 的基礎之上，站在全局視角對服務進行治理，保障服務的穩定性，相關能力等發展如下：</p><ul class=list-paddingleft-2><li>點：關注與下游進行通信，不關注上游的情況；</li><li>線：加入上游的標識，基於上游做異構的通信策略；</li><li>面：多條線組成面，站著全局視角進行服務治理；</li></ul><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b6ad2a7ddba046e3ba27aacc795ada3d><p class=pgc-img-caption></p></div><p>2.4 本文概要</p><p>本文將會先介紹 UFC(如何實現一個 Service Mesh)，然後是基於 UFC 做服務治理 (基於 Service Mesh 的實踐應用)</p><p>3 UFC 外部視角簡介</p><p>3.1 用戶使用視角</p><p>只需要做兩個事情：</p><ol start=1><li>服務註冊：需要先確保自己的服務 (上游) 和要訪問的服務 (下游) 已經註冊過了，沒註冊過，則需要服務的 owner 進行服務註冊；</li><li>服務通信：用戶通過單機 agent 進行服務通信訪問下游，下圖顯示了用戶從直接訪問下游變成了通過同機 agent 訪問下游</li></ol><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8c1305cc2c8b4250930a2e11708329a4><p class=pgc-img-caption></p></div><p>3.1.1 服務註冊</p><p>UFC 為每個註冊的服務分配一個 service_name，service_name 是這個服務的唯一標識。同時需要對這個 service_name 配置它的相關配置：比如 destination 來源、負載均衡、熔斷、超時重試等策略</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ccb59143880e4752a4365e428854abaa><p class=pgc-img-caption></p></div><p>3.1.2 服務通信</p><p>訪問下游的時候，只需要訪問本機固定端口，帶上下游服務的 service_name、自身標記、trace 等信息，例如下面就是一個發請求的 demo 例子：</p><ul class="code-snippet__line-index code-snippet__js"><li><br></li><li><br></li><li><br></li><li><br></li></ul><pre>curl “http://127.0.0.1:8888/URI”-H “`x-ufc-service-name`=to_service_name”–H “`x-ufc-self-service-name`=from_service_name”-H “`x-ufc-trace-xxxx` = xxxxx”</pre><p>3.2 UFC 能力視角</p><p>介紹 UFC 基於點和線視角的相關能力，從服務聲明、請求路由、流量轉發、故障處理、故障定位、安全等維度和 istio 做了一個比較。而 istio 是大部分是基於下游這個點進行相關通信能力設計，線視角能力很少 (比如權限認證等)</p><p>總結來說，istio 是能力全面，但是具體實現上策略比較簡單，而 UFC 是更貼近實際的業務場景 (具體的可以看後面的介紹內容)</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0669378d49734efa8751702fbb786c6d><p class=pgc-img-caption></p></div><p>PS: 有些能力 (比如流量複製 / 權限管理)UFC 沒有，並代表百度網盤沒有這方面的能力，只是因為歷史原因，通過其它的方案解決了。但是故障注入這塊，的確是 UFC 需要考慮的，有了這塊的能力，混沌工程也更容易的落地。</p><p>4 UFC 內部視角簡介</p><p>主要是介紹架構和相關具體能力的實現設計初衷</p><p>4.1 架構設計</p><p>整個架構和 Service Mesh 一樣，都是採用了同機 Sidecar 進行了流量的轉發 + 中心化控制。</p><p>4.1.1 核心流程圖</p><p>UFC 組件</p><ul class=list-paddingleft-2><li>Service-Mgr: 服務 (實例) 管理，提供服務的增刪改查 (存儲到 db + cache)。定期從 naming 服務拉取 destination 列表，寫入 cache(多個 idc cache)。此外還會和 paas 進行協作 (比如通知 paas 遷移異常的實例)；</li><li>Agent：每臺機器部署一個，四個功能：1）通信代理：為服務提供通信代理 2）配置同步：從同機房的 meta 模塊同步配置 3) 上報：異常和統計數據 4）系統異常監控：接收中心的監控檢測，包括 agent 存活和配置同步時效性；</li><li>Meta：提供服務配置元信息的查詢，多個 IDC 地域部署；</li><li>Monitor：worker 模式，定期發起監控系統和業務是否存在異常，系統層面監控單機 agent 存活率 / 配置同步等，業務層面監控異常實例 / 服務等；</li><li>Metrics：聚會後的數據，數據來源來 agent 和 monitor；</li></ul><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/29acf157e4f24dba8178c770e3d6affb><p class=pgc-img-caption></p></div><p>4.1.2 相關設計說明</p><p><strong>4.1.2.1 高可用</strong></p><p><strong>服務單點</strong></p><p>Agent: 單機 agent 肯定會掛斷，解決思路是基於如何避免請求受損出發點，1) 基於 backup 集群臨時訪問：業務使用 sdk 進行訪問的時候，當訪問本機 agent 失敗之後，會通過訪問遠端的 backup agent 集群進行訪問下游 2) 基於探活屏蔽 + 修復：Monitor 會檢測單機 agent 的存活性，當單機 agent 異常的時候，Monitor 會通知 meta 服務屏蔽單機上的實例，避免上游訪問該機器，同時進行 agent 修復；</p><p>Meta: 每個 IDC 獨立部署，另外 IDC 內部通過多實例 + 無狀態設計解決單點問題；</p><p>Monitor: 每個 IDC 獨立部署，另外 IDC 內部通過多實例 + 分佈式鎖互斥設計解決單點問題；</p><p>Metrics: 多實例，前面有一層 proxy，將 service 進行一致性 hash 進行服務拆分，單機統計聚合數據；</p><p>Service-Mgr: 多實例 + 無狀態設計解決單點問題；</p><p>IDC cache: 當某個 IDC 的 cache 異常之後，通過 Meta 切換讀取 backup IDC 的 cache 解決</p><p><strong>機房鏈路故障：</strong></p><p>會導致配置更新異常，解決方案是通過接入層將 IDC 流量切空，不在 UFC 這層考慮解決問題</p><p><strong>業務實例異常：</strong></p><p>除了通過 UFC 自身的故障屏蔽解決之外，還會聯動 paas 將長期處於異常的實例通過遷移恢復，實現異常處理的閉環</p><p><strong>4.1.2.2 擴展性</strong></p><p>paas 擴展性：很容易融入不同的 paas 平臺，只需要兼容不同的 naming 系統即可。</p><p>業務協議：支持任意業務協議，通過 bypass 旁路式解決 (後面的能力會進行相關介紹)。</p><p><strong>4.1.2.3 監控</strong></p><p>系統：各種系統的異常點，比如單機 agent 的存活和配置同步情況，中心配置定時器同步。</p><p>業務：服務單點異常、服務實例存活率、服務 sla 等業務指標儀表盤監控。</p><p><strong>4.1.2.4 配置同步</strong></p><p>單機上的配置是全量的，通過版本號進行增量同步 (全局的版本號 + 服務級別的版本號)，同時會通過 monitor 監控配置是否同步成功</p><p><strong>4.1.2.5 具體實現</strong></p><p>數據面板：基於 OpenResty 開發。從理論角度：nginx 轉發性能本身就好，同時在 2016 年的時候 Go 的 gc 問題還比較嚴重，從實際壓測：OpenResty 要優於 Go。</p><p>控制面板：基於 Go 開發，比較好實現併發編程。</p><p><strong>4.1.2.6 實時 debug</strong></p><p>對於一個實時運行的系統，如何實時獲取相關的 stat 數據，用於問題定位呢？比如想獲取當前封禁後端列表之類的。</p><p>為了滿足需求，增加了一個獲取數據的接口，獲取內存裡指定字段的數據，以 ^ 為分隔符。比如以下 demo 就獲取了一個服務的動態數據，包括了請求次數，更新時間，失敗次數等信息。</p><pre>request:curl -v -H "token:token" 'http://10.10.11.11:8888/bypass?command=query&amp;ar=request_info^pcs-upload'response：{"request_info": {"5xx_cnt": 2,"mtime": 1573456024,"request_cnt": 6,"success_cnt": 4}}</pre><p>4.2 功能設計</p><p>主要介紹 UFC 基於點和線的相關能力細節以及設計出發點</p><p>4.2.1 請求路由</p><p>提供相關的匹配能力，根據匹配條件進行後續相關的操作，比如設置後端的權重等進行多版本灰度測試、流量遷移等功能。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ca5c8fedfd7f4f81b31cdda9640d655e><p class=pgc-img-caption></p></div><p><strong>4.2.1.1 基於請求匹配</strong></p><p>基於請求特徵做路由匹配，例如 http request header: headers[“x-http-flow-control”]，可以按隨機比例 或者 等價 進行匹配</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ca5c8fedfd7f4f81b31cdda9640d655e><p class=pgc-img-caption></p></div><p>類似於 istio 的路由規則</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ca5c8fedfd7f4f81b31cdda9640d655e><p class=pgc-img-caption></p></div><p><strong>4.2.1.2 基於上游匹配</strong></p><p>基於上游匹配是從線視角出發，UFC 的線視角如下：</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ca5c8fedfd7f4f81b31cdda9640d655e><p class=pgc-img-caption></p></div><p>為啥要做基於上游匹配，出發點如下：</p><p><strong>流量等級：</strong></p><p>不同的上游服務重要性也不一樣，產生請求的重要性也不一樣，於是基於上游的標誌設置異構的策略就很有必要。比如網盤的用戶文件列表展示服務和多端列表同步服務一樣會訪問數據庫，顯然前者比後者重要，用戶直接感知，需要區別對待。而如果是重試請求，這裡面的重要性又不一樣了，為了簡化策略配置，目前在將請求權重化，根據若干個請求因素的計算出請求權重，然後根據請求權重執行不同的策略。</p><p>具體的實踐落地：請求權重可以用於細粒度的降級策略。</p><p><strong>機房策略：</strong></p><p>上游訪問下游，默認是不關注上游請求的來源機房，也不關注下游 endpoint 的機房歸屬，流量在機房之間是混連的，那麼當時候，要切空機房流量異常麻煩。基於上游請求的機房的標識設置機房路由規則就很有必要了。</p><p>具體的實踐落地：可以定製機房轉發策略，比如流量在同一個物理 或者 邏輯機房內轉發，詳見下文流量轉發裡的基於上游轉發。</p><p>4.2.2 流量轉發</p><p>從協議、負載均衡、基於上游轉發、傳輸策略四個方面介紹</p><p><strong>4.2.2.1 協議</strong></p><p>Istio 只支持 http/tcp/grpc 協議，而業務使用的協議肯定不止這些協議，比如 redis/mysql 等，如果都支持這些協議以及後續的其它協議，那麼兼容的研發成本將非常的高，在用戶需求和研發成本之間，UFC 找到了折衷的解決方案實現了支持任意協議，具體上是以穿透式和旁路式進行落地的。</p><p>穿透式請求: 適用場景為 http 協議控制流，和 istio 的使用方式一樣</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/335c9271332940ae851ad3ac87f81d9e><p class=pgc-img-caption></p></div><p>旁路式請求: 加強版的 DNS，適用場景為 http 協議數據流 or 非 http 協議，業務先從 UFC 拿到一個後端地址，然後業務自己通過這個後端地址訪問後端，最後需要回調 UFC 告知 UFC 這次通信的結果，方便 UFC 更新相關數據 (比如該後端的訪問成功率等，決策是否需要封禁之類的)。通過旁路式解決了任意協議的場景，同時對後端流量進行了管控。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6947037f827640b89d236743497edb8f><p class=pgc-img-caption></p></div><p><strong>4.2.2.2 負載均衡</strong></p><p>採用了比較常見的負載均衡策略，比如輪詢、隨機、一致性 hash(根據業務定義的字段進行 hash)</p><p><strong>4.2.2.3 基於上游轉發</strong></p><p>主要的應用在於上下游 IDC 流量路由上，比如設置完下面的路由表之後，就實現了同 (邏輯 / 物理) 機房的流量轉發：</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8826f32bbd5248d68cb305f1f9932aa2><p class=pgc-img-caption></p></div><p><strong>4.2.2.4 傳輸策略</strong></p><p>主要是從連接策略、超時策略，重試策略介紹的</p><p><strong>連接策略：</strong></p><p>上游：短連接、長連接</p><p>下游：短連接、長連接</p><p><strong>超時策略:</strong></p><p>基本功能：設置與下游的連接 / 讀寫超時，可以被上游請求裡的指定參數覆蓋超時時間。</p><p>自動切換：兩套超時策略配置，結合故障處理自動切換。常規下 T1 超時策略 (客戶端可覆蓋)，雪崩下 T2 超時策略。當雪崩的時候自動切換到 T2 超時策略。這樣既可以照顧到常規情況下的請求長尾，又避免了雪崩等場景下下游服務拖跨上游訪問。</p><p><strong>重試策略：</strong></p><p>什麼場景下重試：需要分清楚重試的邊界。從功能角度看，Service Mesh 的功能是保障服務通信成功，所以 UFC 只在與後端通信失敗場景 (連接 / 寫 / 讀失敗) 下才重試，對於業務語義層面的錯誤，由業務發起重試，UFC 不介入重試邏輯。另外，從性能的角度出發，也不該把業務層面的重試下沉到 Service Mesh 這個基礎組件上來，因為涉及到對返回的 body 做反序列化，這個會影響其它的請求，增加請求耗時。</p><p>基本功能：設置與下游交互失敗的重試次數。</p><p>自動切換：兩套重試策略，結合故障處理自動切換。常規下重試，雪崩下不重試，當雪崩的時候自動切換到不重試。這樣既可以照顧到常規情況下少量的異常，又避免了雪崩等場景下加劇下游的雪崩，減少無意義的重試請求。</p><p>4.2.3 故障處理</p><p>從故障的範圍影響上分成：</p><p>單點故障：少量後端異常，比如進程因為 oom 掛掉 或者 因為機器異常導致進程異常。</p><p>服務級別故障：當多個單點故障之後，就引發了服務級別的故障，比如服務雪崩。另外業務自身異常 (比如全部返回 5xx) 不在處理範疇內。</p><p>從故障處理流程上分成：</p><p>發現故障: 怎麼定位故障的發生，包括單點 / 服務級別故障。</p><p>處理故障：怎麼處理使得單點 / 服務故障能夠恢復。</p><p><strong>4.2.3.1 單點故障</strong></p><p>故障處理具體發現故障和解決故障兩方面的內容，難點在於發現單點故障的準確性，是否存在漏判。</p><p><strong>發現故障</strong></p><p>基於請求：</p><p>單次異常：連接異常 (連接超時 / 拒絕 /Reset 等)、讀寫異常 (讀寫超時等)</p><p>判斷標準：N 次請求裡 M 次請求異常</p><p>基於業務：</p><p>業務維度：http status code、耗時等</p><p>判斷標準：單個後端和平均值 (算平均值的時候需要排除掉比較對象的數據) 做比較，看哪個後端嚴重偏離平均值，比如狀態碼區間比例之類的。這裡沒有采用單次異常判斷是因為有些場景下，通過單次請求是無法判斷為異常的，比如一個後端返回 404 的 http status code，這個是符合預期的？沒人知道，因為常規情況下業務也會返回 404，而在 LNMP 架構裡，如果單個實例磁盤問題導致 PHP 文件丟失，Nginx 找不到 PHP 文件，也會返回 404，這是一個混沌的狀態。UFC 的解決方案，就是和平均值進行比較，看是否偏離平均值。以 下圖後端狀態碼區間比例統計為例子，後端 2 4xx 的 http code 達到了 100%，而其它的後端這個比例也只有不到 5%，那麼大概率後端 2 就存在問題。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a99dd2b822cf43d09d1d15b23c81a6ec><p class=pgc-img-caption></p></div><p><strong>解決故障</strong></p><p>處理：初始封禁 T 時間</p><p>封禁收斂：後端連續封禁時間翻倍，解決異常後端被反覆解封、封禁的場景</p><p>封禁閉環：長時間封禁的後端，通知 paas 進行異常實例的遷移封禁 backup 機制：達到最大封禁比例，通知人工介入處理</p><p>恢復：當封禁時間到了之後，檢測端口是否可用，可用則解封</p><p><strong>4.2.3.2 服務故障</strong></p><p>主要是針對雪崩過載的場景下如何儘量保障服務可用，也是從發現故障和解決故障進行介紹的，難點在於如何減少故障帶來的業務流量損失。</p><p><strong>發現故障</strong></p><p>基於通信成功率：下游服務 N 次請求裡成功率低於 X%</p><p>基於業務語義成功率: UFC 沒采用的原因是不能越俎代庖做自己能力之外的事情，Service Mesh 核心在於流量控制，它的能力是流量控制來避免下游過載，防止雪崩，例如業務代碼 bug 導致的成功率下降，Service Mesh 也沒有辦法。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/007c38d4ce59472ba220fec732398a25><p class=pgc-img-caption></p></div><p><strong>解決故障</strong></p><p>PS: 動態熔斷的思想是借鑑了網絡，當雪崩過載的時候，相當於發生了請求的擁塞，和網絡擁塞是一樣的特徵行為，網絡鏈路都帶寬相當於服務的容量</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/abf10195e20a4dd0ba5a625c9902e832><p class=pgc-img-caption></p></div><p>4.2.4 儀表盤</p><p>儀表盤基於數據源進行分析得到以下內容：</p><ul class=list-paddingleft-2><li>分佈式服務的日誌追蹤；</li><li>業務度量數據的收集 / 分析 / 展示。</li></ul><p><strong>4.2.4.1 數據源</strong></p><p>有兩方面的數據源：</p><p>服務通信 access 日誌：通過日誌傳輸進行聚合分析。</p><p>何時由上游 A 對下游 B 進行訪問，請求通過了後端 X1/X2 進行訪問，重試了 N 次，耗時為 T1/T2 等，狀態碼為 S1/S2 等，</p><p>異常行為：業務層面比如服務熔斷等，系統層面比如封禁計數器異常等，由 agent 實時上報到中心做聚合分析</p><p><strong>4.2.4.2 distributed traces</strong></p><p>首先基於服務通信 access 日誌做鏈路分析，然後將鏈路信息存入 Elasticsearch 方便檢索，比如下圖就是一個鏈路檢索</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/26dfe9c070f449b0af045102ad368942><p class=pgc-img-caption></p></div><p><strong>4.2.4.3 metrics</strong></p><p>從點、線、面視角上進行數據的聚合分析。</p><p><strong>點視角：後端維度</strong></p><p><strong>服務後端異常監控：</strong></p><p><strong>單實例異常觸發封禁：</strong> 觸發封禁的後端，長時間封禁走 paas 遷移流程。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/108b1a900e53448682f93c1092917387><p class=pgc-img-caption></p></div><p><br></p><p><strong>服務觸發熔斷：</strong> 觸發服務動態熔斷降級。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/643b19fa168e4ec8a886f4bf593c5a3b><p class=pgc-img-caption></p></div><p><strong>線視角：上下游維度</strong></p><p><strong>服務下游監控：</strong> 服務訪問所有下游的概貌，支持按 http 狀態碼和 idc 做過濾，同時支持環比 (昨天 / 一週前)。下面以視頻轉碼為例子，展示對若干個下游的訪問概貌。</p><p><strong>請求數:</strong> 可以根據曲線分析是否存在異常。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3ada73a8185e4ebab8044305f35ddd96><p class=pgc-img-caption></p></div><p><strong>請求失敗數：</strong> 根據失敗數量可以算出上游對下游的請求 sla</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/e943a39ef00c4357bc2902b451aee0e5><p class=pgc-img-caption></p></div><p><strong>請求耗時:</strong> 可以比較昨天 / 上週的耗時數據，看是否存在異常</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5803c75754d4494db86985a3c1d5086e><p class=pgc-img-caption></p></div><p><strong>服務上游監控：</strong> 服務被所有上游訪問的概貌，可以按 http 狀態碼和 idc 粒度進行過濾，同時支持環比 (昨天 / 一週前)。下面以一個異步化服務為例子，被一堆的上游訪問，統計這些上游的訪問概貌。</p><p><strong>請求數:</strong> 在定位服務的請求數突增，可以很容易識別出是哪個上游導致的。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7e873ae688764ecb94be70a1e687c0a8><p class=pgc-img-caption></p></div><p><strong>請求失敗數：</strong> 如果一些上游訪問失敗數比較高，可以聯繫業務進行分析定位。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/052c85ad0e2943b79ffd430d8c327051><p class=pgc-img-caption></p></div><p><strong>請求耗時:</strong> 可以比較昨天 / 上週的耗時數據，看是否存在異常</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/01670e37774d43a182066146d24d4b1f><p class=pgc-img-caption></p></div><p><strong>面視角：全局視角</strong></p><p><strong>核心功能鏈路 SLA 監控：</strong> 當鏈路 sla 降低的時候，可以很快定位到是哪個鏈路分支出現的異常。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1ee8517ec5c5421a93fb3126b7dfe020><p class=pgc-img-caption></p></div><p><br></p><p><strong>耗時維度：</strong> 全部服務裡，耗時增加 top 10 業務，快速知曉業務概貌。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/323b79bb438847a2823e9d0cbe99a0ae><p class=pgc-img-caption></p></div><p><strong>業務失敗率維度:</strong> 全部服務裡，失敗數最多的 top 10 業務，快速知曉業務概貌。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b2a4aa0e277f43438cddc0e0a4ef2309><p class=pgc-img-caption></p></div><p><br></p><p><strong>機器維度：</strong> 全部機器裡，請求 5xx 失敗最高的機器 top 10，快速知曉機器異常。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e5c305796cbd4b089ff94f23e56d86d5><p class=pgc-img-caption></p></div><p>5 服務治理</p><p>5.1 服務治理的定義</p><p>如前文所述，服務治理是建立在 Service Mesh 基礎能力之上的，站在全局視角統籌規劃，以保障服務穩定性為出發點。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d3814e693874868b2f3e85ee0c62518><p class=pgc-img-caption></p></div><p>5.2 具體實踐</p><p>服務治理的意義：如下圖所示，百度網盤全局拓撲異常複雜，靠傳統的人工套路去保障服務穩定性，效率和覆蓋面都有很大的缺陷，基於流量控制的 service mesh 進行服務治理才是王道，具體從故障預防、故障定位和故障處理出發。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4370eaa8d7ea42cf878c484f7550adf1><p class=pgc-img-caption></p></div><p>5.2.1 故障預防</p><p>故障預防從以下三個維度進行：</p><p>流量隔離：如何規避低優先級的請求流量影響高優先級的請求，避免喧賓奪主；</p><p>容量管理：如何保障服務容量滿足實際需求，避免容量不足導致雪崩；</p><p>無效請求: 如何減少無效的請求，避免服務陷入雪崩的危機之中</p><p><strong>5.2.1.1 流量隔離</strong></p><p>具體落地中又分成機房流量隔離以及在離線流量隔離:</p><p><strong>機房流量隔離：</strong> 統一網盤所有服務邏輯 IDC 映射關係，UFC 自動識別上游所在機房，將請求轉發到下游服務相應機房，從入口到請求終端，以邏輯 IDC 機房維度進行了流量隔離。當發生機房故障的時候，入口處切流量即可解決機房故障。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/40b64ed425a1499bbb0729a06497948a><p class=pgc-img-caption></p></div><p><strong>在離線流量隔離：</strong> 定義網盤所有服務的等級，包括在離線標記，UFC 根據全局流量拓撲，可以發現是否存在在離線混連情況，避免低優先級的請求流量影響高優先級的請求，導致喧賓奪主。比如下圖中，Online-a 和 Offline-a 都訪問 Online-b 服務，這樣 Offline-a 有可能引發 Online-b 服務異常，而從影響 Online-a 與 Online-b 的請求，間接影響用戶請求。發現這種在離線混連的情況，需要對服務進行拆分，Online-b/Onine-c 各自拆分成 Onine 和 Offline 兩套服務，進行在離線流量的隔離。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3b24da16cee447e5b916d8d667bd9e73><p class=pgc-img-caption></p></div><p><strong>5.2.1.2 容量管理</strong></p><p>具體落地中又分成容量評估以及容量壓測，前置是根據鏈路拓撲做評估，後者是通過壓測實踐驗證評估的準確性:</p><p><strong>容量評估：</strong> 通過鏈路上的 qps，分析出每個服務需要增加的 qps，進一步推算出需要擴容多少實例 (PS: 需要解決環路的問題)</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/57761c3654f34ed689fb300616ef4e51><p class=pgc-img-caption></p></div><p><strong>容量壓測:</strong> 通過以線上流量以機房切流量逐漸加壓的方式來壓測，期間監控服務 sla，低於某個閾值之後，自動停止壓測</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e970c2b6491d4c4c9927c9d47c118aa2><p class=pgc-img-caption></p></div><p><strong>5.2.1.3 無效請求</strong></p><p><strong>無效請求產生的背景：</strong> 當 client 斷開連接之後，server 還在繼續訪問其它的後端，進行無效的請求。比如下圖中，client 以 300ms 的超時時間訪問 server，server 在訪問 A 和 B 之後，已經用掉了 300ms，這個時候 client 已經斷開了和 server 的連接，但是 server 卻繼續訪問 C 和 D，進行無效的請求。當這種無效請求在整個鏈路蔓延開，client 又在大量的重試的時候，就是雪崩降臨的時刻。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c38ba8f90e3d4839b2eb231289f6594f><p class=pgc-img-caption></p></div><p><strong>解決方案 - 基本思路：</strong> 基於鏈路的智能超時，設置整個鏈路的超時時間，當發現超時時間已經到了，就不再訪問其它的下游服務，避免無效請求。上游需要傳遞給下游執行超時時間 (採用相對時間，避免機器之間的時鐘不同步)，用於下游判斷執行時間是否已經到了。</p><p><strong>解決方案 - 基於業務：</strong> 不需要使用 service mesh，業務自己維護當前的超時時間，業務改造的成本比較大。</p><p><strong>解決方案 - 基於 service mesh：</strong>UFC 以唯一 ID 映射到一個請求到所有後端交互的鏈路上，UFC 自動維護剩餘的請求耗時，實現對業務近似 0 侵入 (PS: 還未用於生產環境)。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9e0fa0eabb9343efaf1d598941a9da2b><p class=pgc-img-caption></p></div><p>5.2.2 故障定位</p><p>定位的思路為：發現異常 –> 收集系統異常點 / 異常時間點相關變動 –> 定位原因。</p><p>除了基於 service mesh 採集系統的異常點，還需要聯動其它的系統監控項，比如服務容量。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0636000b14f34250bb580e64f73c6b5e><p class=pgc-img-caption></p></div><p>5.2.3 故障處理</p><p>根據故障的影響面可以分成局部 / 全局雪崩故障。</p><p><strong>5.2.3.1 局部故障</strong></p><p>前置條件：</p><p>部署：服務部署在多個邏輯機房；</p><p>上線：服務分級發佈，單點 -> 單機房 -> 全機房。</p><p>場景：通過機房切流量快速解決機房硬件故障、小流量上線引發故障、後端異常封禁失敗等局部異常</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/55f221403ceb480abc43dfe0504b9a34><p class=pgc-img-caption></p></div><p><strong>5.2.3.2 全局雪崩故障</strong></p><p>前置條件：</p><p>等級：需要定義服務 / 流量的等級；</p><p>統一降級標記：統一所有服務的降級標記。</p><p>場景：通過全局動態熔斷 + 異構降級 + 降級閉環策略解決服務引起的雪崩問題，儘量保障服務的可用度，具備自愈能力</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/f9b0479be8814bcb9bdf8763ff6f6025><p class=pgc-img-caption></p></div><p>6 總結與展望</p><p>在業務拆分面對眾多問題的背景之下，百度網盤從 2015 年底開始思考解決方案，確定瞭解決問題的核心在於管控請求流量，在 2016 年開始自研網絡流量轉發中間件 - UFC(Unified Flow Control)，業務通過同機部署的 agent 進行服務通信。後來通過調研借鑑業界技術的時候發現 UFC 和 Service Mesh 的設計理念是一致的，都是基於 sidecar 模式，有著控制面板和數據面板，UFC 是 Service Mesh 的一種實現。經過多年的線上實踐驗證，UFC 這個 Service Mesh 實現了動態熔斷 + 異構降級 + 降級閉環等故障處理、結合故障 / 上游進行流量轉發等創造性設計，滿足業務的實際場景需求。但是百度網盤的實踐落地並不只侷限於 Service Mesh，首先是構建了從點延伸到線的 Service Mesh 進行服務通信管控，然後是在 UFC 這個 Service Mesh 的基礎之上，站在全局視角對服務進行治理，保障服務的穩定性。</p><p>未來，UFC 將會加入故障注入等能力，同時基於該能力落地混沌工程，而這只是服務治理中預防的一部分。服務治理的目標是自愈，為了完成這個目標，還需要更加努力。</p><div class=pgc-img><img alt="Service Mesh 在百度網盤數萬後端的落地實踐" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/acb26408e7c04521922d3224911e5589><p class=pgc-img-caption></p></div><p>作者介紹</p><p>李鴻斌，百度資深研發工程師。2011 年畢業後加入百度的基礎架構部門，參與了百度分佈式雲存儲的設計與開發，支撐了百度所有業務線的對象存儲。2012 年至今，作為核心研發人員參與了網盤從 0 到 1 到 N 的基礎架構演進，先後參與或負責對象存儲 / 個人文件存儲 / runtime 基礎環境 / 資源管理 / 微服務架構，設計並實現了計算混佈於萬級別在線存儲機器的資源模型以及服務資源調度，節省下了數萬臺服務器的計算資源，產品線資源利用率公司第一，同時從 2016 年至今主導了自研的 Service Mesh- UFC（Unified Flow Control），實現了從點到線到面的服務治理保障了網盤服務的高可用，近期專注於邊緣計算方向，此外業餘時間反哺社區，php-src/hhvm/beego 等源碼貢獻者。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>Service</a></li><li><a>Mesh</a></li><li><a>網盤</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/cf01fec1.html alt=百度網盤被曝用戶隱私文件被洩露，在進行文件分享的時候要注意了 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1411a0e6edbc40248153068475a98adb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cf01fec1.html title=百度網盤被曝用戶隱私文件被洩露，在進行文件分享的時候要注意了>百度網盤被曝用戶隱私文件被洩露，在進行文件分享的時候要注意了</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6dffb3b7.html alt="百度網盤Hash/MD5衝突？Win10 2004鏡像隨機損壞" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/c3bfd2c438ea4f99b7a9c2816ded2240 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6dffb3b7.html title="百度網盤Hash/MD5衝突？Win10 2004鏡像隨機損壞">百度網盤Hash/MD5衝突？Win10 2004鏡像隨機損壞</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0dcf6b7a.html alt=百度網盤是怎麼佔你家帶寬，嫖你家電費的？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2461dcdc3ba24941966c32113e8e88df style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0dcf6b7a.html title=百度網盤是怎麼佔你家帶寬，嫖你家電費的？>百度網盤是怎麼佔你家帶寬，嫖你家電費的？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/62b96e03.html alt=Service層需要接口嗎？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/62b96e03.html title=Service層需要接口嗎？>Service層需要接口嗎？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5d1585c0.html alt=討論：Service層需要接口嗎？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/S7QqYwT3jhkulD style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5d1585c0.html title=討論：Service層需要接口嗎？>討論：Service層需要接口嗎？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7c1f0b40.html alt=百度網盤私密文件變成公共資源？官方敲黑板 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/050d2e5e8d28478580314eae63821fc6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7c1f0b40.html title=百度網盤私密文件變成公共資源？官方敲黑板>百度網盤私密文件變成公共資源？官方敲黑板</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/38135bbf.html alt="Best Service 電子合成音源合集" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a11ae92b95e24349b7191e7a142b6612 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/38135bbf.html title="Best Service 電子合成音源合集">Best Service 電子合成音源合集</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8157f84.html alt=跟網盤說再見，手把手教你用旁路盒子自建私有網盤 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a002473a9caf4202a4077f350b98840f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8157f84.html title=跟網盤說再見，手把手教你用旁路盒子自建私有網盤>跟網盤說再見，手把手教你用旁路盒子自建私有網盤</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1870339.html alt=測試經理網盤爆出面試題！「內附答案」 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d8554089a4294bf0af339e8fd058facf style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1870339.html title=測試經理網盤爆出面試題！「內附答案」>測試經理網盤爆出面試題！「內附答案」</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/af77060.html alt="告別百度網盤和微信，不要錯過這 10 款傳輸神器" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/85da16a085f8439b81ba1e3703d61ca2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/af77060.html title="告別百度網盤和微信，不要錯過這 10 款傳輸神器">告別百度網盤和微信，不要錯過這 10 款傳輸神器</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/065a9ec.html alt=雲都科技MIMO低頻Mesh無人機陸空數據鏈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RA1W1QU7YtYGjC style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/065a9ec.html title=雲都科技MIMO低頻Mesh無人機陸空數據鏈>雲都科技MIMO低頻Mesh無人機陸空數據鏈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0963694.html alt="基於 MOSN 和 Istio Service Mesh 的服務治理實踐" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d2f7664f8e5647a9835274cded4e9983 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0963694.html title="基於 MOSN 和 Istio Service Mesh 的服務治理實踐">基於 MOSN 和 Istio Service Mesh 的服務治理實踐</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/61253e6.html alt=有電腦的注意了！百度網盤正在用你電腦“挖礦”，你知道嗎？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/61253e6.html title=有電腦的注意了！百度網盤正在用你電腦“挖礦”，你知道嗎？>有電腦的注意了！百度網盤正在用你電腦“挖礦”，你知道嗎？</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>