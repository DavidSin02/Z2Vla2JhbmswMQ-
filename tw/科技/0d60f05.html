<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>長文本表示學習概述 | 极客快訊</title><meta property="og:title" content="長文本表示學習概述 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/4f776fbdeeb7495a8abff3797803b42e"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0d60f05.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0d60f05.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0d60f05.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0d60f05.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0d60f05.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0d60f05.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/0d60f05.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/0d60f05.html><meta property="article:published_time" content="2020-10-29T21:08:29+08:00"><meta property="article:modified_time" content="2020-10-29T21:08:29+08:00"><meta name=Keywords content><meta name=description content="長文本表示學習概述"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/0d60f05.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>長文本表示學習概述</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p>摘要: "如果你願意一層一層的剝開我的心·································那麼你會坐牢的我跟你說"。自然語言就是這麼神奇，句子中的長距離特徵對於理解語義也非常關鍵，本文基於Tranformer、RNN、CNN、TCN分別概述近期學界對長句表示學習的諸多方法。</p><h1>1.長文本表示學習挑戰</h1><p>NLP任務的特點和圖像有極大的不同，上圖展示了一個例子，NLP的輸入往往是一句話或者一篇文章，所以它有幾個特點：首先，輸入是個一維線性序列，這個好理解；其次，輸入是不定長的，有的長有的短，而這點其實對於模型處理起來也會增加一些小麻煩；再者，單詞或者子句的相對位置關係很重要，兩個單詞位置互換可能導致完全不同的意思。如果你聽到我對你說：“你欠我那一千萬不用還了”和“我欠你那一千萬不用還了”，你聽到後分別是什麼心情？兩者區別瞭解一下。最後，句子中的長距離特徵對於理解語義也非常關鍵，徵抽取器能否具備長距離特徵捕獲能力這一點對於解決NLP任務來說也是很關鍵的。針對長文本表示學習，現階段主要有Transformer、RNN、CNN、TCN四種流派，下面分別概述。</p><h1>2.學界研究</h1><p><strong>2.1Transformer 流派</strong></p><p>近期基於Tranformer的模型在NLP諸多領域取得了不錯的效果，針對長句表示學習，谷歌和OpenAI分別提出了相應的改進方案。</p><p><strong>2.1.1 Transformer-XL</strong></p><p>以往的 Transformer 網絡由於受到上下文長度固定的限制，學習長期以來關係的潛力有限。谷歌的Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context提出的新神經架構 Transformer-XL 可以在不引起時間混亂的前提下，可以超越固定長度去學習依賴性，同時還能解決上下文碎片化問題。具體地說，此方法在 Transformer 架構中引入了循環機制。在訓練過程中，為之前的片段計算的隱藏狀態序列是固定的，將其緩存起來，並在模型處理後面的新片段時作為擴展上下文重複使用，如下圖所示:</p><div class=pgc-img><img alt=長文本表示學習概述 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4f776fbdeeb7495a8abff3797803b42e><p class=pgc-img-caption></p></div><p>Transformer-XL 學習到的依賴性比 RNN 學習到的長 80%，比標準 Transformer 學到的長 450%，無論在長序列還是短序列中都得到了更好的結果，而且在評估時比標準 Transformer 快 1800+ 倍。</p><p><strong>2.1.2 Sparse Transformer</strong></p><p>OpenAl 的論文Generating Long Sequences with Sparse Transformers提出了一種適用於文本、圖像和語音的稀疏Transformer，將先前基於注意力機制的算法處理序列的長度提高了三十倍。OpenAI的研究人員在最新的論文中為注意力矩陣引入了多種稀疏的分解方式，通過將完全注意力計算分解為多個更快的注意力操作，通過稀疏組合來進行稠密注意力操作，在不犧牲性能的情況下大幅降低了對於內存和算力的需求。</p><p>稀疏Transformer將先前Transformer的平方複雜度O(N^2)降低為O(NN^1/2),通過一些額外的改進使得自注意力機制可以直接用於長程的語音、文本和圖像數據。</p><div class=pgc-img><img alt=長文本表示學習概述 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7ee5029e9bc0490caa01f4f9531d964a><p class=pgc-img-caption></p></div><p>上圖中間是第一種步進注意力的版本，可以粗略的理解為每一個位置需要注意它所在的行和列；另一種固定注意力的方式則嘗試著從固定的列和元素中進行處理，這種方式對於非二維結構的數據有著很好的效果。</p><p><strong>2.1.3 小結</strong></p><p>儘管新興的Tranformer在NLP各個領域都取得了不錯的效果，針對長文本表示學習也有諸多優化技術，然而Tranformer本質上無法編碼位置（只能基於位置編碼）, Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures對Transformer進行了深入的實證分析，在建模長距離主謂一致任務上的實驗結果並沒有表明，Transformer 在這方面優於 RNN, Transformer 是強大的語義特徵提取器，但是在某些NLP任務上RNN仍然有一席空間。</p><p><strong>2.2 RNN流派</strong></p><p>因為RNN的結構天然適配解決NLP的問題，NLP的輸入往往是個不定長的線性序列句子，而RNN本身結構就是個可以接納不定長輸入的由前向後進行信息線性傳導的網絡結構，而在LSTM引入三個門後，對於捕獲長距離特徵也是非常有效的。然而對於長句表示學習，RNN的並行計算能力極差。因為T時刻的計算依賴T-1時刻的隱層計算結果，而T-1時刻的計算依賴T-2時刻的隱層計算結果……..這樣就形成了所謂的序列依賴關係。自從深度學習在NLP領域火爆以來，出現了很多針對RNN方法。</p><p><strong>2.2.1 稀疏注意力回溯</strong></p><p>本方法出自Bengio組的論文Sparse Attentive Backtracking （SAB）: Temporal Credit Assignment Through Reminding。先舉個例子，當你在高速公路上開車時，你聽到了一種異常的爆炸聲。但是仍然不以為意，直到你停下來加油時發現輪胎癟了。然後你突然想起了在開車時聽到的爆炸聲。這樣的回想能夠幫助你確定爆胎的原因，並可能導致突觸變化。而由於這種變化，在以後開車時聽到這種爆炸聲，你的處理方式可能會不一樣。</p><p>​ 在稀疏性限制最大的條件下（不利用過去的經驗），SAB 將退化為使用常規的靜態神經網絡。在稀疏性限制最小的條件下（利用過去所有的經驗），SAB 將退化為完全使用自注意力機制。為了達到目的，他們通過特定種類的增強 LSTM 模型探究前面二者之間的差距。</p><div class=pgc-img><img alt=長文本表示學習概述 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/28ceb798375a41818a9f030b46a304dc><p class=pgc-img-caption></p></div><p>SAB分為兩個階段：</p><ul><li>在前饋傳播過程中，管理一個內存單元，並在每個時間步中最多選擇過去記憶中的一個稀疏子集。這個過程稱之為稀疏檢索。</li><li>在反向傳播過程中，將梯度僅僅傳播到記憶的稀疏子集及其局部環境中。這個過程稱之為稀疏回放。</li></ul><p><strong>2.2.2 QRNN</strong></p><p>​論文Quasi-Recurrent Neural NetworksQuasi 提出QRNN，其結合了RNN和CNN的特性。在使用卷積結構替代循環結構上，QRNN 沒有純 CNN 模型（e.g. WaveNet）那麼激進，依然保留了一些循環結構。</p><div class=pgc-img><img alt=長文本表示學習概述 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a531a484b86743b08c34480757948270><p class=pgc-img-caption></p></div><p><strong>2.2.3 IndRNN</strong></p><p>論文Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN, 提出了 IndRNN，亮點在於：1) 將 RNN 層內神經元解耦，使它們相互獨立，提高神經元的可解釋性。2) 有序列表能夠使用 Relu 等非飽和激活函數，解決層內和層間梯度消失/爆炸問題，同時模型也具有魯棒性。3) 有序列表比 LSTM 能處理更長的序列信息。</p><p>​對比原始的RNN隱層計算：</p><div class=pgc-img><img alt=長文本表示學習概述 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0d3a8095c90a440d80e9971d67a05ae4><p class=pgc-img-caption></p></div><p>​IndRNN 引入了 Relu 作為激活函數，並且將層內的神經元獨立開來。對 RNN 的式子稍加改進，就變成了 IndRNN：</p><div class=pgc-img><img alt=長文本表示學習概述 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0bf4fb743eb34c32943d5c4c5bc40532><p class=pgc-img-caption></p></div><p><strong>2.2.4 SRU</strong></p><p>​論文Simple Recurrent Units for Highly Parallelizable Recurrence中提出的SRU方法，它最本質的改進是把隱層之間的神經元依賴由全連接改成了哈達馬乘積，這樣T時刻隱層單元本來對T-1時刻所有隱層單元的依賴，改成了只是對T-1時刻對應單元的依賴，於是可以在隱層單元之間進行並行計算，但是收集信息仍然是按照時間序列來進行的。所以其並行性是在隱層單元之間發生的，而不是在不同時間步之間發生的。SRU的測試速度為：在文本分類上和原始TextCNN的速度相當。</p><p><strong>2.2.5 SRNN</strong></p><p>​ 論文Sliced Recurrent Neural Networks提出打斷隱層之間的連接，比如每隔2個時間步打斷一次，並通過層深來建立遠距離特徵之間的聯繫。SRNN速度比GRU模型快5到15倍。</p><div class=pgc-img><img alt=長文本表示學習概述 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0a093cb7046b4ce6a8e98949819e60f0><p class=pgc-img-caption></p></div><p><strong>2.2.6小結</strong></p><p>​ 對於原生的RNN，目前很多實驗已經證明效果比起Transformer有較大差距，只有少數NLP任務RNN仍佔有一席之地。然而RNN並行計算能力受限制太嚴重，RNN的發展目前處於進退維谷的階段，也許未來的發展方向是RNN和其他模塊如Transformer相結合，提高並行性。</p><p><strong>2.4 CNN流派</strong></p><p>​最早將CNN引入NLP的是Kim在2014年做的工作。CNN捕獲到的特徵關鍵在於卷積核覆蓋的那個滑動窗口。大小為k的滑動窗口輕輕的穿過句子的一個個單詞，捕獲到的是單詞的k-gram片段信息，這些k-gram片段就是CNN捕獲到的特徵，k的大小決定了能捕獲多遠距離的特徵，對於長句而言，暴力的取很大的k是無法解決長句依賴的特徵提取問題的，針對長句表示學習挑戰，針對CNN有以下兩個方向的改進。</p><p><strong>2.4.1 Dilated CNN</strong></p><p>​空洞卷積主要通過跳躍的方式實現擴大感受野，在論文An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling中，作者利用Dilated CNN拓展單層卷積層的輸入覆蓋長度，利用全卷積層堆疊層深，使用Skip Connection輔助優化，引入Casual CNN讓網絡結構看不到T時間步後的數據。</p><div class=pgc-img><img alt=長文本表示學習概述 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0a093cb7046b4ce6a8e98949819e60f0><p class=pgc-img-caption></p></div><p>不過TCN的論文有兩個明顯問題：一個問題是任務除了語言模型外都不是典型的NLP任務，而是合成數據任務，所以論文結論很難直接說就適合NLP領域；另外一點，它用來進行效果比較的對比方法，沒有用當時效果很好的模型來對比，比較基準低。所以TCN的模型效果說服力不太夠。不過本人認為精調過的TCN對長句表示學習還是會優於傳統的CNN。</p><p><strong>2.4.2 Deep CNN</strong></p><p>​針對CNN改進的第二個方向是加深層數。第一層卷積層，假設滑動窗口大小k是3，如果再往上疊一層卷積層，假設滑動窗口大小也是3，但是第二層窗口覆蓋的是第一層窗口的輸出特徵，所以它其實能覆蓋輸入的距離達到了5。如果繼續往上疊加捲積層，可以繼續增大卷積核覆蓋輸入的長度。</p><p>​在論文Do Convolutional Networks need to be Deep for Text Classification出現之前，卷積神經網絡進行文本分類都是用的很淺層的CNN，基本是一個詞嵌入層、一個卷積一個池化然後兩個全連接層，文章利用了29個卷積層，提升文本分類的準去率。</p><div class=pgc-img><img alt=長文本表示學習概述 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b56015fd57eb473689b062943b772eee><p class=pgc-img-caption></p></div><p><strong>2.4.2 小結</strong></p><p>​以上是兩種典型的解決CNN遠距離特徵捕獲能力的方案，Dilated CNN偏技巧一些，而且疊加捲積層時超參如何設置有些學問，因為連續跳接可能會錯過一些特徵組合，所以需要精心調節參數搭配，保證所有可能組合都被覆蓋到。相對而言，把CNN作深是主流發展方向，似乎BERT中疊加了n層的Transformer也是做深的一個例子。總的來說，CNN本身具有並行優勢，在NLP也佔有一席之地，不過近期不斷被Transformer蠶食。</p><h1>3 總結</h1><p>1.不同的句子表徵方法都有不同的特色，在具體業務問題上需要根據具體問題選擇模型。</p><p>2.Tranformer和做深，高度並行是未來的一個趨勢。</p><p>3.集合各種模型的優勢如Transformer+CNN+RNN也是未來的一個趨勢。</p><p>4.模擬人思考的方式進行模型的改進也是一個趨勢，如Sparse Attentive Backtracking</p><p>5.RNN不管怎麼改進並行化還是很差，實際使用先用CNN或Transformer。</p><p>參考文獻:</p><p>1.Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding</p><p>2.Generating Long Sequences with Sparse Transformers</p><p>3.Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</p><p>4.Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures</p><p>5.Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN</p><p>6.Simple Recurrent Units for Highly Parallelizable Recurrence</p><p>7.Sliced Recurrent Neural Networks</p><p>8.Quasi-Recurrent Neural Networks</p><p>9.An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</p><p>10.Do Convolutional Networks need to be Deep for Text Classification ?</p><p>作者：zhangningyu1690</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>學習</a></li><li><a>文本</a></li><li><a>概述</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/8fcf8e6.html alt=深度學習之文本摘要自動生成 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/de33e624639749bcab7ee814be3b56fc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8fcf8e6.html title=深度學習之文本摘要自動生成>深度學習之文本摘要自動生成</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html alt=直流鍋爐給水控制學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/eba10edcc8d14d9f8cde6fd5b212d90e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html title=直流鍋爐給水控制學習>直流鍋爐給水控制學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html alt=HTMLCSS學習筆記（六）——元素類型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb5988349894ce9bf568c6418f85b7d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html title=HTMLCSS學習筆記（六）——元素類型>HTMLCSS學習筆記（六）——元素類型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html alt="web前端（從零開始），每天更新學習筆記 HTML5元素分類" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/46d70004fcd55e1ddad3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html title="web前端（從零開始），每天更新學習筆記 HTML5元素分類">web前端（從零開始），每天更新學習筆記 HTML5元素分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/00145145.html alt=福建省“平潭縣”基本概述 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/e272987dca8b435a8dc733a7d49ef2e6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/00145145.html title=福建省“平潭縣”基本概述>福建省“平潭縣”基本概述</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html alt="MySQL 學習筆記" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html title="MySQL 學習筆記">MySQL 學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html alt=深入學習MySQL事務：ACID特性的實現原理「轉」 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/cdc702d66d6943499997d11e931425eb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html title=深入學習MySQL事務：ACID特性的實現原理「轉」>深入學習MySQL事務：ACID特性的實現原理「轉」</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/61de4f95.html alt=蒙特卡羅方法概述 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/776e8b236f774a14841c556abaae8272 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/61de4f95.html title=蒙特卡羅方法概述>蒙特卡羅方法概述</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html alt=如何學習模擬IC設計？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html title=如何學習模擬IC設計？>如何學習模擬IC設計？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html alt=小猿圈python學習-三大特性之多態 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ad0e8e3777854337abeb7c779ad79a04 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html title=小猿圈python學習-三大特性之多態>小猿圈python學習-三大特性之多態</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/03a295fc.html alt=地理學習5——地球的運動（地球的公轉及其地理意義） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7b2b74c871eb40beb8ee143627d29611 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/03a295fc.html title=地理學習5——地球的運動（地球的公轉及其地理意義）>地理學習5——地球的運動（地球的公轉及其地理意義）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ebad378f.html alt=繼續學習打卡，還真心學不會了，努力，堅持 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/f36d6d47a06840aaaf78138853b9d9d1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ebad378f.html title=繼續學習打卡，還真心學不會了，努力，堅持>繼續學習打卡，還真心學不會了，努力，堅持</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>