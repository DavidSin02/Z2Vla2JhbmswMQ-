<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>TensorFlowå¯¦ç¾æ™‚é–“åºåˆ—é æ¸¬ | æå®¢å¿«è¨Š</title><meta property="og:title" content="TensorFlowå¯¦ç¾æ™‚é–“åºåˆ—é æ¸¬ - æå®¢å¿«è¨Š"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/27d989e04ac245cf9dc645bd7b60f356"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/12b78f12.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/12b78f12.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/12b78f12.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/12b78f12.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/12b78f12.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/12b78f12.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/12b78f12.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/12b78f12.html><meta property="article:published_time" content="2020-11-14T21:03:07+08:00"><meta property="article:modified_time" content="2020-11-14T21:03:07+08:00"><meta name=Keywords content><meta name=description content="TensorFlowå¯¦ç¾æ™‚é–“åºåˆ—é æ¸¬"><meta name=author content="æå®¢å¿«è¨Š"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/12b78f12.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>ğŸ¤“ æå®¢å¿«è¨Š Geek Bank</a></h1><p class=description>ç‚ºä½ å¸¶ä¾†æœ€å…¨çš„ç§‘æŠ€çŸ¥è­˜ ğŸ§¡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>çŒœä½ å–œæ­¡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=ç§‘æŠ€>ç§‘æŠ€</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=éŠæˆ²>éŠæˆ²</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=ç§‘å­¸>ç§‘å­¸</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>TensorFlowå¯¦ç¾æ™‚é–“åºåˆ—é æ¸¬</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>ç§‘æŠ€</a></span></div><div class=post-content><p>å¸¸å¸¸æœƒç¢°åˆ°å„ç¨®å„æ¨£æ™‚é–“åºåˆ—é æ¸¬å•é¡Œï¼Œå¦‚å•†å ´äººæµé‡çš„é æ¸¬ã€å•†å“åƒ¹æ ¼çš„é æ¸¬ã€è‚¡åƒ¹çš„é æ¸¬ï¼Œç­‰ç­‰ã€‚TensorFlowæ–°å¼•å…¥äº†ä¸€å€‹TensorFlow Time Seriesåº«ï¼ˆä»¥ä¸‹ç°¡ç¨±ç‚ºTFTSï¼‰ï¼Œå®ƒå¯ä»¥å¹«åŠ©åœ¨TensorFlowä¸­å¿«é€Ÿæ­å»ºé«˜æ€§èƒ½çš„æ™‚é–“åºåˆ—é æ¸¬ç³»çµ±ï¼Œä¸¦æä¾›åŒ…æ‹¬ARã€LSTMåœ¨å…§çš„å¤šå€‹æ¨¡å‹ã€‚</p><h2 class=pgc-h-arrow-right>æ™‚é–“åºåˆ—å•é¡Œ</h2><p>ä¸€èˆ¬è€Œè¨€ï¼Œæ™‚é–“åºåˆ—æ•¸æ“šæŠ½è±¡ç‚ºå…©éƒ¨åˆ†ï¼šè§€å¯Ÿçš„æ™‚é–“é»å’Œè§€å¯Ÿçš„å€¼ ï¼ˆä»¥å•†å“åƒ¹æ ¼ç‚ºä¾‹ï¼ŒæŸå¹´ä¸€æœˆçš„åƒ¹æ ¼ç‚º120å…ƒï¼ŒäºŒæœˆçš„åƒ¹æ ¼ç‚º130å…ƒï¼Œä¸‰æœˆçš„åƒ¹æ ¼ç‚º135å…ƒï¼Œå››æœˆçš„åƒ¹æ ¼ç‚º132å…ƒã€‚é‚£éº¼è§€å¯Ÿçš„æ™‚é–“é»å¯ä»¥çœ‹ä½œæ˜¯1,2,3,4ï¼Œè€Œåœ¨å„æ™‚é–“é»ä¸Šè§€å¯Ÿåˆ°çš„æ•¸æ“šçš„å€¼ç‚º120,130,135,132ï¼‰ ã€‚è§€å¯Ÿçš„æ™‚é–“é»å¯ä»¥ä¸é€£çºŒï¼Œæ¯”å¦‚äºŒæœˆçš„æ•¸æ“šæœ‰ç¼ºå¤±ï¼Œé‚£éº¼å¯¦éš›çš„è§€å¯Ÿæ™‚é–“é»ç‚º1,3,4ï¼Œå°æ‡‰çš„æ•¸æ“šç‚º120,135,132ã€‚ æ‰€è¬‚æ™‚é–“åºåˆ—é æ¸¬ï¼Œæ˜¯æŒ‡é æ¸¬æŸäº›æœªä¾†çš„æ™‚é–“é»ä¸Šï¼ˆå¦‚5,6ï¼‰æ•¸æ“šçš„å€¼æ‡‰è©²æ˜¯å¤šå°‘ ã€‚</p><p>TFTSåº«æŒ‰ç…§æ™‚é–“é»+è§€å¯Ÿå€¼çš„æ–¹å¼å°æ™‚é–“åºåˆ—å•é¡Œé€²è¡ŒæŠ½è±¡åŒ…è£ã€‚è§€å¯Ÿçš„ <strong>æ™‚é–“é»ç”¨â€œtimesâ€è¡¨ç¤º</strong> ï¼Œå°æ‡‰çš„ <strong>å€¼ç”¨â€œvaluesâ€è¡¨ç¤º</strong> ã€‚åœ¨è¨“ç·´æ¨¡å‹æ™‚ï¼Œè¼¸å…¥æ•¸æ“šéœ€è¦åŒæ™‚å…·æœ‰timeså’Œvalueså…©å€‹å­—æ®µï¼›åœ¨é æ¸¬æ™‚ï¼Œéœ€è¦çµ¦å®šä¸€äº›åˆå§‹çš„æ•¸å€¼ï¼Œä»¥åŠéœ€è¦é æ¸¬çš„æ™‚é–“é»timesã€‚</p><h2 class=pgc-h-arrow-right>è®€å…¥æ™‚é–“åºåˆ—æ•¸æ“š</h2><p>åœ¨è¨“ç·´æ¨¡å‹ä¹‹å‰ï¼Œéœ€è¦å°‡æ™‚é–“åºåˆ—æ•¸æ“šè®€å…¥æˆTensorçš„å½¢å¼ã€‚ <strong>TFTS</strong> åº«ä¸­æä¾›äº†å…©å€‹æ–¹ä¾¿çš„è®€å–å™¨</p><ul><li><strong>NumpyReader</strong> ï¼šç”¨æ–¼å¾Numpyæ•¸çµ„ä¸­è®€å…¥æ•¸æ“š</li><li><strong>CSVReader</strong> ï¼šç”¨æ–¼å¾CSVæ–‡ä»¶ä¸­è®€å…¥æ•¸æ“š</li></ul><h2 class=pgc-h-arrow-right>å¾Numpyæ•¸çµ„ä¸­è®€å–æ™‚é–“åºåˆ—</h2><p>å°å…¥éœ€è¦çš„åŒ…åŠå‡½æ•¸</p><pre><code>import numpy as npimport matplotlibmatplotlib.use('agg')import matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.timeseries.python.timeseries import NumpyReader</code></pre><p>æ¥è‘—ï¼Œåˆ©ç”¨np.sinç”Ÿæˆä¸€å€‹å¯¦é©—ç”¨çš„æ™‚é–“åºåˆ—æ•¸æ“šã€‚è©²æ™‚é–“åºåˆ—æ•¸æ“šå¯¦éš›ä¸Šæ˜¯åœ¨æ­£å¼¦æ›²ç·šä¸ŠåŠ å…¥äº†ä¸Šå‡çš„è¶¨å‹¢å’Œä¸€äº›éš¨æ©Ÿçš„å™ªè²ï¼š</p><pre><code>x = np.array(range(1000))noise = np.random.uniform(-0.2, 0.2, 1000)y = np.sin(np.pi * x / 100) + x / 200. + noiseplt.plot(x, y)plt.savefig('timeseries_y.jpg')</code></pre><div class=pgc-img><img alt=TensorFlowå¯¦ç¾æ™‚é–“åºåˆ—é æ¸¬ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/27d989e04ac245cf9dc645bd7b60f356><p class=pgc-img-caption></p></div><p>æ©«åº§æ¨™å°æ‡‰è®Šé‡â€œxâ€ï¼Œç¸±åº§æ¨™å°æ‡‰è®Šé‡â€œyâ€ï¼Œå®ƒå€‘åˆ†åˆ¥å°æ‡‰ä¹‹å‰æåˆ°éçš„â€œè§€å¯Ÿçš„æ™‚é–“é»â€å’Œâ€œè§€å¯Ÿåˆ°çš„å€¼â€ã€‚TFTSè®€å…¥xå’Œyçš„æ–¹å¼éå¸¸ç°¡å–®ï¼Œè«‹çœ‹ä¸‹é¢çš„ä»£ç¢¼ï¼š</p><pre><code>data = {    tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,    tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,}reader = NumpyReader(data)</code></pre><p>é¦–å…ˆæŠŠxå’Œyè®ŠæˆPythonä¸­çš„å­—å…¸ï¼ˆè®Šé‡dataï¼‰ã€‚ä¸Šé¢çš„å®šç¾©ç›´æ¥å¯«æˆâ€œ <strong>data={â€˜times':x, â€˜values':y}</strong> â€ä¹Ÿæ˜¯å¯ä»¥çš„ã€‚å¯«æˆæ¯”è¼ƒè¤‡é›œçš„å½¢å¼æ˜¯ç‚ºäº†å’Œæºç¢¼ä¸­çš„å¯«æ³•ä¿æŒä¸€è‡´ã€‚</p><p>å¾—åˆ°çš„readeræœ‰ä¸€å€‹read_full()æ–¹æ³•ï¼Œå®ƒçš„è¿”å›å€¼æ˜¯æ™‚é–“åºåˆ—å°æ‡‰çš„Tensorï¼Œå¯ä»¥ç”¨ä¸‹é¢çš„ä»£ç¢¼é€²è¡Œè©¦é©—ï¼š</p><pre><code>with tf.Session() as sess:    full_data = reader.read_full()    # èª¿ç”¨read_fullæ–¹æ³•æœƒç”Ÿæˆè®€å–éšŠåˆ—    # è¦ç”¨tf.train.start_queue_runnerså•Ÿå‹•éšŠåˆ—æ‰èƒ½æ­£å¸¸é€²è¡Œè®€å–    coord = tf.train.Coordinator()    threads = tf.train.start_queue_runners(sess=sess, coord=coord)    print(sess.run(full_data))    coord.request_stop()</code></pre><p>åœ¨è¨“ç·´æ™‚ï¼Œé€šå¸¸ä¸æœƒä½¿ç”¨æ•´å€‹æ•¸æ“šé›†é€²è¡Œè¨“ç·´ï¼Œè€Œæ˜¯æ¡ç”¨batchçš„å½¢å¼ã€‚å¾readerå‡ºç™¼ï¼Œå»ºç«‹batchæ•¸æ“šçš„æ–¹æ³•ä¹Ÿå¾ˆç°¡å–®ï¼š</p><pre><code>train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(    reader, batch_size=2, window_size=10)</code></pre><p>tf.contrib.timeseries.RandomWindowInputFnæœƒåœ¨readerçš„æ‰€æœ‰æ•¸æ“šä¸­ï¼Œéš¨æ©Ÿé¸å–çª—å£é•·åº¦ç‚ºwindow_sizeçš„åºåˆ—ï¼Œå¹·åŒ…è£æˆbatch_sizeå¤§å°çš„batchæ•¸æ“šã€‚æ›å¥è©±èªªï¼Œä¸€å€‹batchå…§å…±æœ‰batch_sizeå€‹åºåˆ—ï¼Œæ¯å€‹åºåˆ—çš„é•·åº¦ç‚ºwindow_sizeã€‚</p><p>ä»¥batch_size=2, window_size=10ç‚ºä¾‹ï¼Œå¯ä»¥æ‰“å°å‡ºä¸€å€‹batchçš„æ•¸æ“šï¼š</p><pre><code>with tf.Session() as sess:    batch_data = train_input_fn.create_batch()    coord = tf.train.Coordinator()    threads = tf.train.start_queue_runners(sess=sess, coord=coord)    one_batch = sess.run(batch_data[0])    coord.request_stop()print('one_batch_data:', one_batch)# one_batch_data: {'times': array([[11, 12, 13, 14, 15, 16, 17, 18, 19, 20],#        [21, 22, 23, 24, 25, 26, 27, 28, 29, 30]]), 'values': array([[[0.33901882],#         [0.29966548],#         [0.64006627],#         [0.35204604],#         [0.66049626],#         [0.57470108],#         [0.68309054],#         [0.46613038],#         [0.60309193],#         [0.84166497]],# #        [[0.77312242],#         [0.82185951],#         [0.71022706],#         [0.63987861],#         [0.7011966 ],#         [0.84051192],#         [1.05796465],#         [0.92981324],#         [1.0542786 ],#         [0.89828743]]])}</code></pre><p>åŸå…ˆçš„æ•¸æ“šé•·åº¦ç‚º1000çš„æ™‚é–“åºåˆ—ï¼ˆx=np.array(range(1000))ï¼‰ï¼Œä½¿ç”¨tf.contrib.timeseries.RandomWindowInputFnï¼Œä¸¦æŒ‡å®šwindow_size=10, batch_size=2çš„åŠŸèƒ½æ˜¯åœ¨é€™é•·åº¦ç‚º1000çš„æ™‚é–“åºåˆ—ä¸­ï¼Œéš¨æ©Ÿé¸å–é•·åº¦ç‚º10çš„åºåˆ—ï¼Œä¸¦åœ¨æ¯å€‹batchè£¡åŒ…å«å…©å€‹é€™æ¨£çš„åºåˆ—ã€‚é€™ä¹Ÿå¯ä»¥å¾æ‰“å°å‡ºçš„æ•¸æ“šä¸­çœ‹å‡ºä¾†ã€‚</p><p>ä½¿ç”¨tf.contrib.timeseries.RandomWindowInputFnè¿”å›çš„train_input_fnå¯ä»¥é€²è¡Œè¨“ç·´äº†ã€‚é€™æ˜¯åœ¨TFTSä¸­è®€å…¥Numpyæ•¸çµ„æ™‚é–“åºåˆ—çš„åŸºæœ¬æ–¹å¼ã€‚ä¸‹é¢ä»‹ç´¹å¦‚ä½•è®€å…¥CSVæ ¼å¼çš„æ•¸æ“šã€‚</p><h2 class=pgc-h-arrow-right>å¾CSVæ–‡ä»¶ä¸­è®€å–æ™‚é–“åºåˆ—</h2><p>æœ‰æ™‚ï¼Œæ™‚é–“åºåˆ—æ•¸æ“šæ˜¯å­˜åœ¨CSVæ–‡ä»¶ä¸­çš„ã€‚ç•¶ç„¶å¯ä»¥å°‡å…¶å…ˆè®€å…¥ç‚ºNumpyæ•¸çµ„ï¼Œå†ä½¿ç”¨ä¹‹å‰çš„æ–¹æ³•è™•ç†ã€‚æ›´æ–¹ä¾¿çš„åšæ³•æ˜¯ä½¿ç”¨tf.contrib.timeseries.CSVReaderè®€å…¥ã€‚æ•¸æ“šæ–‡ä»¶ <strong>period_trend.csv</strong></p><p>å‡è¨­CSVæ–‡ä»¶çš„æ™‚é–“åºåˆ—æ•¸æ“šçš„å½¢å¼ç‚ºï¼š</p><pre><code>1,-0.66566037142,-0.11643803593,0.73986264884,0.73686330295,0.2289480898...</code></pre><p>CSVæ–‡ä»¶çš„ç¬¬ä¸€åˆ—ç‚ºæ™‚é–“é»ï¼Œç¬¬äºŒåˆ—ç‚ºè©²æ™‚é–“é»ä¸Šè§€å¯Ÿåˆ°çš„å€¼ã€‚å°‡å…¶è®€å…¥çš„æ–¹æ³•ç‚ºï¼š</p><pre><code>import tensorflow as tfcsv_file_name = './period_trend.csv'reader = tf.contrib.timeseries.CSVReader(csv_file_name)</code></pre><p>å¯¦éš›è®€å…¥çš„ä»£ç¢¼åªæœ‰ä¸€è¡Œï¼Œç›´æ¥ä½¿ç”¨å‡½æ•¸tf.contrib.timeseries.CSVReaderå¾—åˆ°äº†readerã€‚å°‡readerä¸­æ‰€æœ‰æ•¸æ“šæ‰“å°å‡ºä¾†çš„æ–¹æ³•å’Œä¹‹å‰æ˜¯ä¸€æ¨£çš„ï¼š</p><pre><code>with tf.Session() as sess:    data = reader.read_full()    coord = tf.train.Coordinator()    threads = tf.train.start_queue_runners(sess=sess, coord=coord)    print(sess.run(data))    coord.request_stop()</code></pre><p>å¾readerå‡ºç™¼ï¼Œå»ºç«‹batchæ•¸æ“šçš„train_input_fnçš„æ–¹æ³•ä¹Ÿå®Œå…¨ç›¸åŒï¼š</p><pre><code>train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(reader, batch_size=4, window_size=16)</code></pre><p>æœ€å¾Œï¼Œå¯ä»¥æ‰“å°å‡ºå…©å€‹batchçš„æ•¸æ“šé€²è¡Œæ¸¬è©¦ï¼š</p><pre><code>with tf.Session() as sess:    data = train_input_fn.create_batch()    coord = tf.train.Coordinator()    threads = tf.train.start_queue_runners(sess=sess, coord=coord)    batch1 = sess.run(data[0])    batch2 = sess.run(data[0])    coord.request_stop()print('batch1:', batch1)print('batch2:', batch2)</code></pre><p>ä»¥ä¸Šæ˜¯TFTSåº«ä¸­æ•¸æ“šçš„è®€å–æ–¹å¼ã€‚ç¸½çš„ä¾†èªªï¼Œ å¾Numpyæ•¸çµ„æˆ–è€…CSVæ–‡ä»¶å‡ºç™¼æ§‹é€ ä¸€å€‹readerï¼Œå†åˆ©ç”¨readerç”Ÿæˆbatchæ•¸æ“šã€‚æœ€å¾Œå¾—åˆ°çš„Tensorç‚ºtrain_input_fnï¼Œé€™å€‹train_input_fnæœƒè¢«ç•¶ä½œè¨“ç·´æ™‚çš„è¼¸å…¥ ã€‚</p><h2 class=pgc-h-arrow-right>ä½¿ç”¨ARæ¨¡å‹é æ¸¬æ™‚é–“åºåˆ—</h2><h2 class=pgc-h-arrow-right>ARæ¨¡å‹çš„è¨“ç·´</h2><p>è‡ªè¿´æ­¸æ¨¡å‹ï¼ˆAutoregressive modelï¼Œç°¡ç¨±ç‚ºARæ¨¡å‹ï¼‰æ˜¯çµ±è¨ˆå­¸ä¸Šè™•ç†æ™‚é–“åºåˆ—æ¨¡å‹çš„åŸºæœ¬æ–¹æ³•ä¹‹ä¸€ã€‚TFTSä¸­å·²ç¶“å¯¦ç¾äº†ä¸€å€‹è‡ªè¿´æ­¸æ¨¡å‹ï¼Œæˆ‘å€‘åªéœ€è¦å°å…¶é€²è¡Œèª¿ç”¨å³å¯ä½¿ç”¨ã€‚</p><p>æˆ‘å€‘å…ˆå®šç¾©å‡ºä¸€å€‹train_input_fn</p><pre><code>x = np.array(range(1000))noise = np.random.uniform(-0.2, 0.2, 1000)y = np.sin(np.pi * x / 100) + x / 200. + noiseplt.plot(x, y)plt.savefig('timeseries_y.jpg')data = {    tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,    tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,}reader = NumpyReader(data)train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(    reader, batch_size=16, window_size=40)</code></pre><p>ä½¿ç”¨çš„æ™‚é–“åºåˆ—æ•¸æ“šå¦‚åœ–æ‰€ç¤ºã€‚</p><div class=pgc-img><img alt=TensorFlowå¯¦ç¾æ™‚é–“åºåˆ—é æ¸¬ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c61ac017d78444ed87facc63b0119a1f><p class=pgc-img-caption></p></div><p>å®šç¾©ARæ¨¡å‹ï¼š</p><pre><code>ar = tf.contrib.timeseries.ARRegressor(    periodicities=200, input_window_size=30, output_window_size=10,    num_features=1,    loss=tf.contrib.timeseries.ARModel.NORMAL_LIKELIHOOD_LOSS)</code></pre><p>åƒæ•¸ï¼š</p><ul><li><strong>periodicities</strong> ï¼šåºåˆ—çš„è¦å¾‹æ€§é€±æœŸã€‚åœ¨å®šç¾©æ•¸æ“šæ™‚ä½¿ç”¨çš„èªå¥æ˜¯â€œy=np.sin(np.pi * x /100)+x /200.+noiseâ€ï¼Œå› æ­¤é€±æœŸç‚º200</li><li><strong>input_window_size</strong> ï¼šæ¨¡å‹æ¯æ¬¡è¼¸å…¥çš„å€¼</li><li><strong>output_window_size</strong> ï¼šæ¨¡å‹æ¯æ¬¡è¼¸å‡ºçš„å€¼</li><li><strong>num_features</strong> ï¼šè¡¨ç¤ºåœ¨ä¸€å€‹æ™‚é–“é»ä¸Šè§€å¯Ÿåˆ°çš„æ•¸çš„ç¶­åº¦ã€‚é€™è£¡æ¯ä¸€æ­¥éƒ½æ˜¯ä¸€å€‹å–®ç¨çš„å€¼ï¼Œæ‰€ä»¥num_features=1</li><li><strong>loss</strong> ï¼šæŒ‡å®šæ¡å–å“ªä¸€ç¨®æå¤±ï¼ŒNORMAL_LIKELIHOOD_LOSS æˆ– SQUARED_LOSS</li><li><strong>model_dir</strong> ï¼šæ¨¡å‹è¨“ç·´å¥½å¾Œä¿å­˜çš„åœ°å€ï¼Œå¦‚æœä¸æŒ‡å®šçš„è©±ï¼Œæœƒéš¨æ©Ÿåˆ†é…ä¸€å€‹è‡¨æ™‚åœ°å€</li></ul><p>input_window_sizeå’Œoutput_window_sizeåŠ èµ·ä¾†å¿…é ˆç­‰æ–¼train_input_fnä¸­ç¸½çš„window_sizeã€‚ç¸½çš„window_sizeç‚º40, input_window_sizeç‚º30,output_window_sizeç‚º10ï¼›ä¹Ÿæ˜¯èªªï¼Œä¸€å€‹batchå…§æ¯å€‹åºåˆ—çš„é•·åº¦ç‚º40ï¼Œå…¶ä¸­å‰30å€‹æ•¸è¢«ç•¶ä½œæ¨¡å‹çš„è¼¸å…¥å€¼ï¼Œå¾Œé¢10å€‹æ•¸ç‚ºé€™äº›è¼¸å…¥å°æ‡‰çš„ç›®æ¨™è¼¸å‡ºå€¼ã€‚</p><p>ä½¿ç”¨è®Šé‡arçš„trainæ–¹æ³•å¯ä»¥ç›´æ¥é€²è¡Œè¨“ç·´ï¼š</p><pre><code>ar.train(input_fn=train_input_fn, steps=6000)</code></pre><h2 class=pgc-h-arrow-right>ARæ¨¡å‹çš„é©—è­‰å’Œé æ¸¬</h2><p>TFTSä¸­é©—è­‰ï¼ˆevaluationï¼‰çš„å«ç¾©æ˜¯ï¼šä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹åœ¨åŸå…ˆçš„è¨“ç·´é›†ä¸Šé€²è¡Œè¨ˆç®—ï¼Œç”±æ­¤å¯ä»¥è§€å¯Ÿåˆ°æ¨¡å‹çš„æ“¬åˆæ•ˆæœï¼Œå°æ‡‰çš„ç¨‹åºæ®µæ˜¯ï¼š</p><pre><code>evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)# keys of evaluation: ['covariance', 'loss', 'mean', 'observed', 'start_tuple', 'times', 'global_step']evaluation = ar.evaluate(input_fn=evaluation_input_fn, steps=1)</code></pre><p>å¦‚æœæƒ³è¦æ˜ç™½é€™è£¡çš„é‚è¼¯ï¼Œé¦–å…ˆè¦ç†è§£ä¹‹å‰å®šç¾©çš„ARæ¨¡å‹ï¼šå®ƒæ¯æ¬¡éƒ½æ¥æ”¶ä¸€å€‹é•·åº¦ç‚º30çš„è¼¸å…¥è§€æ¸¬åºåˆ—ï¼Œä¸¦è¼¸å‡ºé•·åº¦ç‚º10çš„é æ¸¬åºåˆ—ã€‚æ•´å€‹è¨“ç·´é›†æ˜¯ä¸€å€‹é•·åº¦ç‚º1000çš„åºåˆ—ï¼Œå‰30å€‹æ•¸é¦–å…ˆè¢«ç•¶ä½œâ€œåˆå§‹è§€æ¸¬åºåˆ—â€è¼¸å…¥åˆ°æ¨¡å‹ä¸­ï¼Œç”±æ­¤å¯ä»¥è¨ˆç®—å‡ºä¸‹é¢10æ­¥çš„é æ¸¬å€¼ã€‚æ¥è‘—åˆæœƒå–30å€‹æ•¸é€²è¡Œé æ¸¬ï¼Œ é€™30å€‹æ•¸ä¸­æœ‰10å€‹æ•¸æ˜¯å‰ä¸€æ­¥çš„é æ¸¬å€¼ ï¼Œæ–°å¾—åˆ°çš„é æ¸¬å€¼åˆæœƒè®Šæˆä¸‹ä¸€æ­¥çš„è¼¸å…¥ï¼Œä¾æ­¤é¡æ¨ã€‚</p><p>æœ€çµ‚å¾—åˆ°970å€‹é æ¸¬å€¼ï¼ˆ970=1000-30ï¼Œå› ç‚ºå‰30å€‹æ•¸æ˜¯æ²’è¾¦æ³•é€²è¡Œé æ¸¬çš„ï¼‰ã€‚970å€‹é æ¸¬å€¼è¢«è¨˜éŒ„åœ¨ <strong>evaluation[â€˜mean']</strong> ä¸­ã€‚evaluationé‚„æœ‰å…¶ä»–å¹¾å€‹éµå€¼ï¼Œå¦‚ <strong>evaluation[â€˜times']</strong> è¡¨ç¤ºevaluation[â€˜mean']å°æ‡‰çš„æ™‚é–“é»ï¼Œ <strong>evaluation[â€˜loss']</strong> è¡¨ç¤ºç¸½çš„æå¤±ç­‰ç­‰ã€‚</p><p>evaluation[â€˜start_tuple']æœƒè¢«ç”¨æ–¼ä¹‹å¾Œçš„é æ¸¬ä¸­ï¼Œå®ƒç›¸ç•¶æ–¼æœ€å¾Œ30æ­¥çš„è¼¸å‡ºå€¼å’Œå°æ‡‰çš„æ™‚é–“é»ã€‚ä»¥æ­¤ç‚ºèµ·é»ï¼Œå¯ä»¥å°1000æ­¥ä»¥å¾Œçš„å€¼é€²è¡Œé æ¸¬ï¼Œå°æ‡‰çš„ä»£ç¢¼ç‚ºï¼š</p><pre><code>(predictions,) = tuple(ar.predict(    input_fn=tf.contrib.timeseries.predict_continuation_input_fn(        evaluation, steps=250)))</code></pre><p>é€™è£¡çš„ä»£ç¢¼åœ¨1000æ­¥ä¹‹å¾Œåˆå‘å¾Œé æ¸¬äº†250å€‹æ™‚é–“é»ã€‚å°æ‡‰çš„å€¼ä¿å­˜åœ¨predictions[â€˜mean']ä¸­ã€‚å¯ä»¥æŠŠè§€æ¸¬åˆ°çš„å€¼ã€æ¨¡å‹æ“¬åˆçš„å€¼ã€é æ¸¬å€¼ç”¨ä¸‹é¢çš„ä»£ç¢¼ç•«å‡ºä¾†ï¼š</p><pre><code>plt.figure(figsize=(15, 5))plt.plot(data['times'].reshape(-1), data['values'].reshape(-1), label='origin')plt.plot(evaluation['times'].reshape(-1), evaluation['mean'].reshape(-1), label='evaluation')plt.plot(predictions['times'].reshape(-1), predictions['mean'].reshape(-1), label='prediction')plt.xlabel('time_step')plt.ylabel('values')plt.legend(loc=4)plt.savefig('predict_result.jpg')</code></pre><div class=pgc-img><img alt=TensorFlowå¯¦ç¾æ™‚é–“åºåˆ—é æ¸¬ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f67442c9ddd748568983486f0009d1cb><p class=pgc-img-caption></p></div><p>å‰1000æ­¥æ¨¡å‹åŸå§‹è§€æ¸¬å€¼çš„æ›²ç·šå’Œæ¨¡å‹æ“¬åˆå€¼éå¸¸æ¥è¿‘ï¼Œèªªæ˜æ¨¡å‹æ“¬åˆå¾—å·²ç¶“æ¯”è¼ƒå¥½äº†ï¼Œ1000æ­¥ä¹‹å¾Œçš„é æ¸¬ä¹Ÿåˆæƒ…åˆç†ã€‚</p><pre><code># coding: utf-8from __future__ import print_functionimport numpy as npimport matplotlibmatplotlib.use('agg')import matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.timeseries.python.timeseries import  NumpyReaderdef main(_):    x = np.array(range(1000))    noise = np.random.uniform(-0.2, 0.2, 1000)    y = np.sin(np.pi * x / 100) + x / 200. + noise    plt.plot(x, y)    plt.savefig('timeseries_y.jpg')    data = {        tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,        tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,    }    reader = NumpyReader(data)    train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(        reader, batch_size=16, window_size=40)    ar = tf.contrib.timeseries.ARRegressor(        periodicities=200, input_window_size=30, output_window_size=10,        num_features=1,        loss=tf.contrib.timeseries.ARModel.NORMAL_LIKELIHOOD_LOSS)    ar.train(input_fn=train_input_fn, steps=6000)    evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)    # keys of evaluation: ['covariance', 'loss', 'mean', 'observed', 'start_tuple', 'times', 'global_step']    evaluation = ar.evaluate(input_fn=evaluation_input_fn, steps=1)    (predictions,) = tuple(ar.predict(        input_fn=tf.contrib.timeseries.predict_continuation_input_fn(            evaluation, steps=250)))    plt.figure(figsize=(15, 5))    plt.plot(data['times'].reshape(-1), data['values'].reshape(-1), label='origin')    plt.plot(evaluation['times'].reshape(-1), evaluation['mean'].reshape(-1), label='evaluation')    plt.plot(predictions['times'].reshape(-1), predictions['mean'].reshape(-1), label='prediction')    plt.xlabel('time_step')    plt.ylabel('values')    plt.legend(loc=4)    plt.savefig('predict_result.jpg')if __name__ == '__main__':    tf.logging.set_verbosity(tf.logging.INFO)    tf.app.run()ç¤ºä¾‹å®Œæ•´ä»£ç¢¼</code></pre><h2 class=pgc-h-arrow-right>ä½¿ç”¨LSTMæ¨¡å‹é æ¸¬æ™‚é–“åºåˆ—</h2><p>ç‚ºäº†ä½¿ç”¨LSTMæ¨¡å‹ï¼Œéœ€è¦å…ˆä½¿ç”¨TFTSåº«å°å…¶é€²è¡Œå®šç¾©ã€‚</p><h2 class=pgc-h-arrow-right>å–®è®Šé‡æ™‚é–“åºåˆ—é æ¸¬</h2><p>åŒæ¨£ï¼Œç”¨å‡½æ•¸åŠ å™ªè²çš„æ–¹æ³•æ¨¡æ“¬ç”Ÿæˆæ™‚é–“åºåˆ—æ•¸æ“šï¼š</p><pre><code>x = np.array(range(1000))noise = np.random.uniform(-0.2, 0.2, 1000)y = np.sin(np.pi * x / 50) + np.cos(np.pi * x / 50) + np.sin(np.pi * x / 25) + noisedata = {    tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,    tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,}reader = NumpyReader(data)train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(    reader, batch_size=4, window_size=100)</code></pre><p>å¾—åˆ°yå’Œxå¾Œï¼Œä½¿ç”¨NumpyReaderè®€å…¥ç‚ºTensorå½¢å¼ï¼Œæ¥è‘—ç”¨tf.contrib.timeseries.RandomWindowInputFnå°‡å…¶è®Šç‚ºbatchè¨“ç·´æ•¸æ“šã€‚ä¸€å€‹batchä¸­æœ‰4å€‹éš¨æ©Ÿé¸å–çš„åºåˆ—ï¼Œæ¯å€‹åºåˆ—çš„é•·åº¦ç‚º100ã€‚</p><p>æ¥ä¸‹ä¾†å®šç¾©ä¸€å€‹LSTMæ¨¡å‹ï¼š</p><pre><code>estimator = ts_estimators.TimeSeriesRegressor(        model=_LSTMModel(num_features=1, num_units=128),        optimizer=tf.train.AdamOptimizer(0.001))</code></pre><p>num_features=1è¡¨ç¤ºå–®è®Šé‡æ™‚é–“åºåˆ—ï¼Œå³æ¯å€‹æ™‚é–“é»ä¸Šè§€å¯Ÿåˆ°çš„é‡åªæ˜¯ä¸€å€‹å–®ç¨çš„æ•¸å€¼ï¼Œnum_units=128è¡¨ç¤ºä½¿ç”¨éš±å±¤ç‚º128å¤§å°çš„LSTMæ¨¡å‹ã€‚</p><p>è¨“ç·´ã€é©—è­‰å’Œé æ¸¬çš„æ–¹æ³•éƒ½å’Œä¹‹å‰é¡ä¼¼ã€‚åœ¨è¨“ç·´æ™‚ï¼Œåœ¨å·²æœ‰çš„1000æ­¥çš„è§€å¯Ÿé‡çš„åŸºç¤ä¸Šå‘å¾Œé æ¸¬200æ­¥ï¼š</p><pre><code>estimator.train(input_fn=train_input_fn, steps=2000)ã€€ã€€ã€€ã€€# è¨“ç·´æ¨¡å‹evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)ã€€ã€€# æ¸¬è©¦æ•¸æ“ševaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)ã€€ã€€# å¾—åˆ°è©•ä¼°å¾Œçš„æ•¸æ“š# è©•ä¼°å¾Œé æ¸¬200æ­¥æ•¸æ“š(predictions,) = tuple(estimator.predict(    input_fn=tf.contrib.timeseries.predict_continuation_input_fn(        evaluation, steps=200)))</code></pre><p>å°‡é©—è­‰ã€é æ¸¬çš„çµæœå–å‡ºä¸¦ç•«æˆç¤ºæ„åœ–ï¼Œç•«å‡ºçš„åœ–åƒæœƒä¿å­˜æˆâ€œpredict_result.jpgâ€æ–‡ä»¶ï¼š</p><pre><code>observed_times = evaluation["times"][0]observed = evaluation["observed"][0, :, :]evaluated_times = evaluation["times"][0]evaluated = evaluation["mean"][0]predicted_times = predictions['times']predicted = predictions["mean"]plt.figure(figsize=(15, 5))plt.axvline(999, linestyle="dotted", linewidth=4, color='r')observed_lines = plt.plot(observed_times, observed, label="observation", color="k")evaluated_lines = plt.plot(evaluated_times, evaluated, label="evaluation", color="g")predicted_lines = plt.plot(predicted_times, predicted, label="prediction", color="r")plt.legend(handles=[observed_lines[0], evaluated_lines[0], predicted_lines[0]],           loc="upper left")plt.savefig('predict_result.jpg')</code></pre><p>é æ¸¬æ•ˆæœå¦‚åœ–15-4æ‰€ç¤ºï¼Œæ©«åº§æ¨™ç‚ºæ™‚é–“è»¸ï¼Œå‰1000æ­¥æ˜¯è¨“ç·´æ•¸æ“šï¼Œ1000~1200æ­¥æ˜¯æ¨¡å‹é æ¸¬çš„å€¼ã€‚</p><div class=pgc-img><img alt=TensorFlowå¯¦ç¾æ™‚é–“åºåˆ—é æ¸¬ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4e65f29cb64440e2ae522107ff38c84d><p class=pgc-img-caption></p></div><pre><code>import numpy as npimport tensorflow as tffrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimatorsfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_modelfrom tensorflow.contrib.timeseries.python.timeseries import NumpyReaderimport matplotlibmatplotlib.use("agg")import matplotlib.pyplot as pltclass _LSTMModel(ts_model.SequentialTimeSeriesModel):    """A time series model-building example using an RNNCell."""    def __init__(self, num_units, num_features, dtype=tf.float32):        """Initialize/configure the model object.        Note that we do not start graph building here. Rather, this object is a        configurable factory for TensorFlow graphs which are run by an Estimator.        Args:          num_units: The number of units in the model's LSTMCell.          num_features: The dimensionality of the time series (features per            timestep).          dtype: The floating point data type to use.        """        super(_LSTMModel, self).__init__(            # Pre-register the metrics we'll be outputting (just a mean here).            train_output_names=["mean"],            predict_output_names=["mean"],            num_features=num_features,            dtype=dtype)        self._num_units = num_units        # Filled in by initialize_graph()        self._lstm_cell = None        self._lstm_cell_run = None        self._predict_from_lstm_output = None    def initialize_graph(self, input_statistics):        """Save templates for components, which can then be used repeatedly.        This method is called every time a new graph is created. It's safe to start        adding ops to the current default graph here, but the graph should be        constructed from scratch.        Args:          input_statistics: A math_utils.InputStatistics object.        """        super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)        self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)        # Create templates so we don't have to worry about variable reuse.        self._lstm_cell_run = tf.make_template(            name_="lstm_cell",            func_=self._lstm_cell,            create_scope_now_=True)        # Transforms LSTM output into mean predictions.        self._predict_from_lstm_output = tf.make_template(            name_="predict_from_lstm_output",            func_=lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),            create_scope_now_=True)    def get_start_state(self):        """Return initial state for the time series model."""        return (            # Keeps track of the time associated with this state for error checking.            tf.zeros([], dtype=tf.int64),            # The previous observation or prediction.            tf.zeros([self.num_features], dtype=self.dtype),            # The state of the RNNCell (batch dimension removed since this parent            # class will broadcast).            [tf.squeeze(state_element, axis=0)             for state_element             in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])    def _transform(self, data):        """Normalize data based on input statistics to encourage stable training."""        mean, variance = self._input_statistics.overall_feature_moments        return (data - mean) / variance    def _de_transform(self, data):        """Transform data back to the input scale."""        mean, variance = self._input_statistics.overall_feature_moments        return data * variance + mean    def _filtering_step(self, current_times, current_values, state, predictions):        """Update model state based on observations.        Note that we don't do much here aside from computing a loss. In this case        it's easier to update the RNN state in _prediction_step, since that covers        running the RNN both on observations (from this method) and our own        predictions. This distinction can be important for probabilistic models,        where repeatedly predicting without filtering should lead to low-confidence        predictions.        Args:          current_times: A [batch size] integer Tensor.          current_values: A [batch size, self.num_features] floating point Tensor            with new observations.          state: The model's state tuple.          predictions: The output of the previous `_prediction_step`.        Returns:          A tuple of new state and a predictions dictionary updated to include a          loss (note that we could also return other measures of goodness of fit,          although only "loss" will be optimized).        """        state_from_time, prediction, lstm_state = state        with tf.control_dependencies(                [tf.assert_equal(current_times, state_from_time)]):            transformed_values = self._transform(current_values)            # Use mean squared error across features for the loss.            predictions["loss"] = tf.reduce_mean(                (prediction - transformed_values) ** 2, axis=-1)            # Keep track of the new observation in model state. It won't be run            # through the LSTM until the next _imputation_step.            new_state_tuple = (current_times, transformed_values, lstm_state)        return (new_state_tuple, predictions)    def _prediction_step(self, current_times, state):        """Advance the RNN state using a previous observation or prediction."""        _, previous_observation_or_prediction, lstm_state = state        lstm_output, new_lstm_state = self._lstm_cell_run(            inputs=previous_observation_or_prediction, state=lstm_state)        next_prediction = self._predict_from_lstm_output(lstm_output)        new_state_tuple = (current_times, next_prediction, new_lstm_state)        return new_state_tuple, {"mean": self._de_transform(next_prediction)}    def _imputation_step(self, current_times, state):        """Advance model state across a gap."""        # Does not do anything special if we're jumping across a gap. More advanced        # models, especially probabilistic ones, would want a special case that        # depends on the gap size.        return state    def _exogenous_input_step(            self, current_times, current_exogenous_regressors, state):        """Update model state based on exogenous regressors."""        raise NotImplementedError(            "Exogenous inputs are not implemented for this example.")if __name__ == '__main__':    tf.logging.set_verbosity(tf.logging.INFO)    x = np.array(range(1000))    noise = np.random.uniform(-0.2, 0.2, 1000)    y = np.sin(np.pi * x / 50) + np.cos(np.pi * x / 50) + np.sin(np.pi * x / 25) + noise    data = {        tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,        tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,    }    reader = NumpyReader(data)    train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(        reader, batch_size=4, window_size=100)    estimator = ts_estimators.TimeSeriesRegressor(        model=_LSTMModel(num_features=1, num_units=128),        optimizer=tf.train.AdamOptimizer(0.001))    estimator.train(input_fn=train_input_fn, steps=2000)    evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)    evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)    # Predict starting after the evaluation    (predictions,) = tuple(estimator.predict(        input_fn=tf.contrib.timeseries.predict_continuation_input_fn(            evaluation, steps=200)))    observed_times = evaluation["times"][0]    observed = evaluation["observed"][0, :, :]    evaluated_times = evaluation["times"][0]    evaluated = evaluation["mean"][0]    predicted_times = predictions['times']    predicted = predictions["mean"]    plt.figure(figsize=(15, 5))    plt.axvline(999, linestyle="dotted", linewidth=4, color='r')    observed_lines = plt.plot(observed_times, observed, label="observation", color="k")    evaluated_lines = plt.plot(evaluated_times, evaluated, label="evaluation", color="g")    predicted_lines = plt.plot(predicted_times, predicted, label="prediction", color="r")    plt.legend(handles=[observed_lines[0], evaluated_lines[0], predicted_lines[0]],               loc="upper left")    plt.savefig('predict_result.jpg')LSTMå–®è®Šé‡å®Œæ•´ä»£ç¢¼</code></pre><h2 class=pgc-h-arrow-right>å¤šè®Šé‡æ™‚é–“åºåˆ—é æ¸¬</h2><p>æ‰€è¬‚å¤šè®Šé‡æ™‚é–“åºåˆ—ï¼Œæ˜¯æŒ‡åœ¨æ¯å€‹æ™‚é–“é»ä¸Šçš„è§€æ¸¬é‡æœ‰å¤šå€‹å€¼ã€‚åœ¨ <strong>multivariate_periods.csv</strong> æ–‡ä»¶ä¸­ï¼Œä¿å­˜äº†ä¸€å€‹å¤šè®Šé‡æ™‚é–“åºåˆ—çš„æ•¸æ“šï¼š</p><pre><code>0    0.926906299771    1.99107237682    2.56546245685    3.07914768197    4.048390578671    0.108010001864    1.41645361423    2.1686839775    2.94963962176    4.12635033032    -0.800567600028    1.0172132907    1.96434754116    2.99885333086    4.043004858643    0.0607042871898    0.719540073421    1.9765012584    2.89265588817    4.0951014426...99    0.987764008058    1.85581989607    2.84685706149    2.94760204892    6.0212151724</code></pre><p>é€™å€‹CSVæ–‡ä»¶çš„ç¬¬ä¸€åˆ—æ˜¯è§€å¯Ÿæ™‚é–“é»ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œæ¯ä¸€è¡Œé‚„æœ‰5å€‹æ•¸ï¼Œè¡¨ç¤ºåœ¨é€™å€‹æ™‚é–“é»ä¸Šè§€å¯Ÿåˆ°çš„æ•¸æ“šã€‚æ›å¥è©±èªªï¼Œæ™‚é–“åºåˆ—ä¸Šæ¯ä¸€æ­¥éƒ½æ˜¯ä¸€å€‹5ç¶­çš„å‘é‡ã€‚</p><p>ä½¿ç”¨TFTSè®€å…¥è©²CSVæ–‡ä»¶çš„æ–¹æ³•ç‚ºï¼š</p><pre><code>csv_file_name = path.join("./data/multivariate_periods.csv")reader = tf.contrib.timeseries.CSVReader(    csv_file_name,    column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)                  + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 5))train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(    reader, batch_size=4, window_size=32)</code></pre><p>èˆ‡ä¹‹å‰çš„è®€å…¥ç›¸æ¯”ï¼Œå”¯ä¸€çš„å€åˆ¥æ˜¯column_namesåƒæ•¸ã€‚å®ƒå‘Šè¨´TFTSåœ¨CSVæ–‡ä»¶ä¸­ï¼Œå“ªäº›åˆ—è¡¨ç¤ºæ™‚é–“ï¼Œå“ªäº›åˆ—è¡¨ç¤ºè§€æ¸¬é‡ã€‚</p><p>æ¥ä¸‹ä¾†å®šç¾©LSTMæ¨¡å‹ï¼š</p><pre><code>estimator = ts_estimators.TimeSeriesRegressor(    model=_LSTMModel(num_features=5, num_units=128),    optimizer=tf.train.AdamOptimizer(0.001))</code></pre><p>å€åˆ¥åœ¨æ–¼ä½¿ç”¨num_features=5è€Œä¸æ˜¯1ï¼ŒåŸå› åœ¨æ–¼æ¯å€‹æ™‚é–“é»ä¸Šçš„è§€æ¸¬é‡æ˜¯ä¸€å€‹5ç¶­å‘é‡ã€‚</p><p>è¨“ç·´ã€é©—è­‰ã€é æ¸¬åŠç•«åœ–çš„ä»£ç¢¼èˆ‡ä¹‹å‰æ¯”è¼ƒé¡ä¼¼ï¼Œæœ€å¾Œçš„é‹è¡Œçµæœåœ–æ‰€ç¤º</p><div class=pgc-img><img alt=TensorFlowå¯¦ç¾æ™‚é–“åºåˆ—é æ¸¬ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7de9c244d52f43d09e0bc5652d2ea00d><p class=pgc-img-caption></p></div><p>ä½¿ç”¨LSTMé æ¸¬å¤šè®Šé‡æ™‚é–“åºåˆ—</p><p>å‰100æ­¥æ˜¯è¨“ç·´æ•¸æ“šï¼Œä¸€æ¢ç·šä»£è¡¨è§€æ¸¬é‡åœ¨ä¸€å€‹ç¶­åº¦ä¸Šçš„å–å€¼ã€‚100æ­¥ä¹‹å¾Œç‚ºé æ¸¬å€¼ã€‚</p><pre><code>from os import pathimport tensorflow as tffrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimatorsfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_modelimport matplotlibmatplotlib.use("agg")import matplotlib.pyplot as pltclass _LSTMModel(ts_model.SequentialTimeSeriesModel):    """A time series model-building example using an RNNCell."""    def __init__(self, num_units, num_features, dtype=tf.float32):        """Initialize/configure the model object.        Note that we do not start graph building here. Rather, this object is a        configurable factory for TensorFlow graphs which are run by an Estimator.        Args:          num_units: The number of units in the model's LSTMCell.          num_features: The dimensionality of the time series (features per            timestep).          dtype: The floating point data type to use.        """        super(_LSTMModel, self).__init__(            # Pre-register the metrics we'll be outputting (just a mean here).            train_output_names=["mean"],            predict_output_names=["mean"],            num_features=num_features,            dtype=dtype)        self._num_units = num_units        # Filled in by initialize_graph()        self._lstm_cell = None        self._lstm_cell_run = None        self._predict_from_lstm_output = None    def initialize_graph(self, input_statistics):        """Save templates for components, which can then be used repeatedly.        This method is called every time a new graph is created. It's safe to start        adding ops to the current default graph here, but the graph should be        constructed from scratch.        Args:          input_statistics: A math_utils.InputStatistics object.        """        super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)        self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)        # Create templates so we don't have to worry about variable reuse.        self._lstm_cell_run = tf.make_template(            name_="lstm_cell",            func_=self._lstm_cell,            create_scope_now_=True)        # Transforms LSTM output into mean predictions.        self._predict_from_lstm_output = tf.make_template(            name_="predict_from_lstm_output",            func_=lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),            create_scope_now_=True)    def get_start_state(self):        """Return initial state for the time series model."""        return (            # Keeps track of the time associated with this state for error checking.            tf.zeros([], dtype=tf.int64),            # The previous observation or prediction.            tf.zeros([self.num_features], dtype=self.dtype),            # The state of the RNNCell (batch dimension removed since this parent            # class will broadcast).            [tf.squeeze(state_element, axis=0)             for state_element             in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])    def _transform(self, data):        """Normalize data based on input statistics to encourage stable training."""        mean, variance = self._input_statistics.overall_feature_moments        return (data - mean) / variance    def _de_transform(self, data):        """Transform data back to the input scale."""        mean, variance = self._input_statistics.overall_feature_moments        return data * variance + mean    def _filtering_step(self, current_times, current_values, state, predictions):        """Update model state based on observations.        Note that we don't do much here aside from computing a loss. In this case        it's easier to update the RNN state in _prediction_step, since that covers        running the RNN both on observations (from this method) and our own        predictions. This distinction can be important for probabilistic models,        where repeatedly predicting without filtering should lead to low-confidence        predictions.        Args:          current_times: A [batch size] integer Tensor.          current_values: A [batch size, self.num_features] floating point Tensor            with new observations.          state: The model's state tuple.          predictions: The output of the previous `_prediction_step`.        Returns:          A tuple of new state and a predictions dictionary updated to include a          loss (note that we could also return other measures of goodness of fit,          although only "loss" will be optimized).        """        state_from_time, prediction, lstm_state = state        with tf.control_dependencies(                [tf.assert_equal(current_times, state_from_time)]):            transformed_values = self._transform(current_values)            # Use mean squared error across features for the loss.            predictions["loss"] = tf.reduce_mean(                (prediction - transformed_values) ** 2, axis=-1)            # Keep track of the new observation in model state. It won't be run            # through the LSTM until the next _imputation_step.            new_state_tuple = (current_times, transformed_values, lstm_state)        return (new_state_tuple, predictions)    def _prediction_step(self, current_times, state):        """Advance the RNN state using a previous observation or prediction."""        _, previous_observation_or_prediction, lstm_state = state        lstm_output, new_lstm_state = self._lstm_cell_run(            inputs=previous_observation_or_prediction, state=lstm_state)        next_prediction = self._predict_from_lstm_output(lstm_output)        new_state_tuple = (current_times, next_prediction, new_lstm_state)        return new_state_tuple, {"mean": self._de_transform(next_prediction)}    def _imputation_step(self, current_times, state):        """Advance model state across a gap."""        # Does not do anything special if we're jumping across a gap. More advanced        # models, especially probabilistic ones, would want a special case that        # depends on the gap size.        return state    def _exogenous_input_step(            self, current_times, current_exogenous_regressors, state):        """Update model state based on exogenous regressors."""        raise NotImplementedError(            "Exogenous inputs are not implemented for this example.")if __name__ == '__main__':    tf.logging.set_verbosity(tf.logging.INFO)    csv_file_name = path.join("./data/multivariate_periods.csv")    reader = tf.contrib.timeseries.CSVReader(        csv_file_name,        column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)                      + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 5))    train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(        reader, batch_size=4, window_size=32)    estimator = ts_estimators.TimeSeriesRegressor(        model=_LSTMModel(num_features=5, num_units=128),        optimizer=tf.train.AdamOptimizer(0.001))    estimator.train(input_fn=train_input_fn, steps=200)    evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)    evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)    # Predict starting after the evaluation    (predictions,) = tuple(estimator.predict(        input_fn=tf.contrib.timeseries.predict_continuation_input_fn(            evaluation, steps=100)))    observed_times = evaluation["times"][0]    observed = evaluation["observed"][0, :, :]    evaluated_times = evaluation["times"][0]    evaluated = evaluation["mean"][0]    predicted_times = predictions['times']    predicted = predictions["mean"]    plt.figure(figsize=(15, 5))    plt.axvline(99, linestyle="dotted", linewidth=4, color='r')    observed_lines = plt.plot(observed_times, observed, label="observation", color="k")    evaluated_lines = plt.plot(evaluated_times, evaluated, label="evaluation", color="g")    predicted_lines = plt.plot(predicted_times, predicted, label="prediction", color="r")    plt.legend(handles=[observed_lines[0], evaluated_lines[0], predicted_lines[0]],               loc="upper left")    plt.savefig('predict_result.jpg')LSTMå¤šè®Šé‡å®Œæ•´ä»£ç¢¼</code></pre></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>TensorFlow</a></li><li><a>å¯¦ç¾</a></li><li><a>æ™‚é–“</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=æœç´¢>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>ğŸ”</button></form></section><section class=widget><h3 class=widget-title>æœ€æ–°æ–‡ç«  âš¡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/27b407d4.html alt=Excelå¯¦ç¾æ—¥æœŸæ™‚é–“å¿«é€Ÿåˆ†é›¢çš„5ç¨®æ–¹æ³•ï¼Œå¿«ä¾†äº†è§£ä¸‹å§ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/32121720077f4b4198c2b485be3dbb37 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/27b407d4.html title=Excelå¯¦ç¾æ—¥æœŸæ™‚é–“å¿«é€Ÿåˆ†é›¢çš„5ç¨®æ–¹æ³•ï¼Œå¿«ä¾†äº†è§£ä¸‹å§>Excelå¯¦ç¾æ—¥æœŸæ™‚é–“å¿«é€Ÿåˆ†é›¢çš„5ç¨®æ–¹æ³•ï¼Œå¿«ä¾†äº†è§£ä¸‹å§</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ac68602d.html alt=è·å ´ä¸Šå¯¦ç¾æœ‰æ•ˆæ™‚é–“ç®¡ç†ï¼Œæé«˜å·¥ä½œæ•ˆç‡çš„æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/5e720001933040e62bb0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ac68602d.html title=è·å ´ä¸Šå¯¦ç¾æœ‰æ•ˆæ™‚é–“ç®¡ç†ï¼Œæé«˜å·¥ä½œæ•ˆç‡çš„æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ>è·å ´ä¸Šå¯¦ç¾æœ‰æ•ˆæ™‚é–“ç®¡ç†ï¼Œæé«˜å·¥ä½œæ•ˆç‡çš„æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/35379e4.html alt=ç”¨æ™‚é–“ç¹¼é›»å™¨å¯¦ç¾è² è¼‰é–“éš”é‹è¡Œçš„å¹¾ç¨®é›»æ°£æ§åˆ¶é›»è·¯ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1529732986004f0abad7973 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/35379e4.html title=ç”¨æ™‚é–“ç¹¼é›»å™¨å¯¦ç¾è² è¼‰é–“éš”é‹è¡Œçš„å¹¾ç¨®é›»æ°£æ§åˆ¶é›»è·¯>ç”¨æ™‚é–“ç¹¼é›»å™¨å¯¦ç¾è² è¼‰é–“éš”é‹è¡Œçš„å¹¾ç¨®é›»æ°£æ§åˆ¶é›»è·¯</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0c39d52.html alt=å¯¦ç¾é‡å­å®‰å…¨æ™‚é–“å‚³éå¥ å®šæœªä¾†å°èˆªåŸºç¤ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0c39d52.html title=å¯¦ç¾é‡å­å®‰å…¨æ™‚é–“å‚³éå¥ å®šæœªä¾†å°èˆªåŸºç¤>å¯¦ç¾é‡å­å®‰å…¨æ™‚é–“å‚³éå¥ å®šæœªä¾†å°èˆªåŸºç¤</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/14feb44.html alt=æ™‚é–“â€œå€’æµâ€é¦–æ¬¡åœ¨é‡å­è¨ˆç®—æ©Ÿä¸Šå¯¦ç¾ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RL1WntOB3RtvPD style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/14feb44.html title=æ™‚é–“â€œå€’æµâ€é¦–æ¬¡åœ¨é‡å­è¨ˆç®—æ©Ÿä¸Šå¯¦ç¾>æ™‚é–“â€œå€’æµâ€é¦–æ¬¡åœ¨é‡å­è¨ˆç®—æ©Ÿä¸Šå¯¦ç¾</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d25f7fd.html alt=é‡å­å¯ä»¥å¯¦ç¾â€œæ™‚é–“â€åè½‰äº†ï¼Ÿ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d3c72cb8d28a4bc3aff3d29c36cd18ec style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d25f7fd.html title=é‡å­å¯ä»¥å¯¦ç¾â€œæ™‚é–“â€åè½‰äº†ï¼Ÿ>é‡å­å¯ä»¥å¯¦ç¾â€œæ™‚é–“â€åè½‰äº†ï¼Ÿ</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f09ac34c.html alt=å½©è‰²é›»å­æ›¸åœ¨å»£å·ç‡å…ˆå¯¦ç¾é‡ç”¢ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RkPMb9G6tipobr style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f09ac34c.html title=å½©è‰²é›»å­æ›¸åœ¨å»£å·ç‡å…ˆå¯¦ç¾é‡ç”¢>å½©è‰²é›»å­æ›¸åœ¨å»£å·ç‡å…ˆå¯¦ç¾é‡ç”¢</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/65709773.html alt=å¹¹æµ·å¸¶æ³¡è»Ÿå¤ªæµªè²»æ™‚é–“äº†ï¼Œé€™å€‹å°å¦™æ‹›è™•ç†å¾Œï¼Œä¸‰åˆ†é˜å°±è®Šè»Ÿ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/66b80001068b5f6c9893 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/65709773.html title=å¹¹æµ·å¸¶æ³¡è»Ÿå¤ªæµªè²»æ™‚é–“äº†ï¼Œé€™å€‹å°å¦™æ‹›è™•ç†å¾Œï¼Œä¸‰åˆ†é˜å°±è®Šè»Ÿ>å¹¹æµ·å¸¶æ³¡è»Ÿå¤ªæµªè²»æ™‚é–“äº†ï¼Œé€™å€‹å°å¦™æ‹›è™•ç†å¾Œï¼Œä¸‰åˆ†é˜å°±è®Šè»Ÿ</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2d12804e.html alt=[ç©è½‰MySQLä¹‹ä¹]MySQLå¯¦ç¾ACIDä¹‹åŸå­æ€§ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb044d821f74107a3fd9119fc34c642 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2d12804e.html title=[ç©è½‰MySQLä¹‹ä¹]MySQLå¯¦ç¾ACIDä¹‹åŸå­æ€§>[ç©è½‰MySQLä¹‹ä¹]MySQLå¯¦ç¾ACIDä¹‹åŸå­æ€§</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fb2bc471.html alt="ã€Œè­¯ã€ Spring çš„åˆ†ä½ˆå¼äº‹å‹™å¯¦ç¾â€”ä½¿ç”¨å’Œä¸ä½¿ç”¨ XAâ€”ç¬¬äºŒéƒ¨åˆ†" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fb2bc471.html title="ã€Œè­¯ã€ Spring çš„åˆ†ä½ˆå¼äº‹å‹™å¯¦ç¾â€”ä½¿ç”¨å’Œä¸ä½¿ç”¨ XAâ€”ç¬¬äºŒéƒ¨åˆ†">ã€Œè­¯ã€ Spring çš„åˆ†ä½ˆå¼äº‹å‹™å¯¦ç¾â€”ä½¿ç”¨å’Œä¸ä½¿ç”¨ XAâ€”ç¬¬äºŒéƒ¨åˆ†</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e41fd8de.html alt="æ’«é †å„é …é˜²æ±›å·¥ä½œå¯¦ç¾â€œå…­åˆ°ä½â€ ç¢ºä¿å…¨å¸‚å®‰å…¨åº¦æ±›" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e41fd8de.html title="æ’«é †å„é …é˜²æ±›å·¥ä½œå¯¦ç¾â€œå…­åˆ°ä½â€ ç¢ºä¿å…¨å¸‚å®‰å…¨åº¦æ±›">æ’«é †å„é …é˜²æ±›å·¥ä½œå¯¦ç¾â€œå…­åˆ°ä½â€ ç¢ºä¿å…¨å¸‚å®‰å…¨åº¦æ±›</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f22ee5ad.html alt="Redis è¨­è¨ˆèˆ‡å¯¦ç¾ : Lua è…³æœ¬" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f22ee5ad.html title="Redis è¨­è¨ˆèˆ‡å¯¦ç¾ : Lua è…³æœ¬">Redis è¨­è¨ˆèˆ‡å¯¦ç¾ : Lua è…³æœ¬</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ebbbc375.html alt=æ™‚é–“ç¹¼é›»å™¨æ¸¬è©¦å„€çš„ä½¿ç”¨æ–¹æ³• class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1205bf787f1f4de6a5f1e73e7737887b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ebbbc375.html title=æ™‚é–“ç¹¼é›»å™¨æ¸¬è©¦å„€çš„ä½¿ç”¨æ–¹æ³•>æ™‚é–“ç¹¼é›»å™¨æ¸¬è©¦å„€çš„ä½¿ç”¨æ–¹æ³•</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/56e2a065.html alt=é€™ä½å¤§å”åœ¨éš¨æ©Ÿçš„å½©ç¥¨ä¸Šå¯¦ç¾äº†90%çš„ä¸­çç‡ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/50ab0003166decded7e4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/56e2a065.html title=é€™ä½å¤§å”åœ¨éš¨æ©Ÿçš„å½©ç¥¨ä¸Šå¯¦ç¾äº†90%çš„ä¸­çç‡>é€™ä½å¤§å”åœ¨éš¨æ©Ÿçš„å½©ç¥¨ä¸Šå¯¦ç¾äº†90%çš„ä¸­çç‡</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cf633068.html alt="Java å¤šæ…‹çš„å¯¦ç¾æ©Ÿåˆ¶ï¼Œçœ‹äº†éƒ½èªªå¥½" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9d3b0e55813d46b4982ae7d9b81d1802 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cf633068.html title="Java å¤šæ…‹çš„å¯¦ç¾æ©Ÿåˆ¶ï¼Œçœ‹äº†éƒ½èªªå¥½">Java å¤šæ…‹çš„å¯¦ç¾æ©Ÿåˆ¶ï¼Œçœ‹äº†éƒ½èªªå¥½</a></li><hr></ul></section><section class=widget><h3 class=widget-title>å…¶ä»–</h3><ul class=widget-list><li><a href=TOS.html>ä½¿ç”¨æ¢æ¬¾</a></li><li><a href=CommentPolicy.html>ç•™è¨€æ”¿ç­–</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>è¯çµ¡æˆ‘å€‘</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>æå®¢å¿«è¨Š</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>