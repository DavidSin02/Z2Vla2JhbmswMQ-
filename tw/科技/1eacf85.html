<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習較常用到的數學工具：概率與統計 | 极客快訊</title><meta property="og:title" content="機器學習較常用到的數學工具：概率與統計 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/b9b3561ea7eb4594a3068ce80c9d91d9"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/1eacf85.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/1eacf85.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/1eacf85.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/1eacf85.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/1eacf85.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/1eacf85.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/1eacf85.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/1eacf85.html><meta property="article:published_time" content="2020-10-29T21:01:05+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:05+08:00"><meta name=Keywords content><meta name=description content="機器學習較常用到的數學工具：概率與統計"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/1eacf85.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習較常用到的數學工具：概率與統計</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>概率與統計及相關概念是整個機器學習的基礎。其與空間幾何、線性代數一起構成了深度學習的理論基石。很多機器學習的理論描述都是基於概率的。而概率本身也是理工學科的基礎性工具，廣泛地應用於工程的各個領域。掌握好概率論是深入學習機器學習的基礎，可以幫助我們進行相關公式的推演以及系統的描述。</p><p style=text-align:start>這種概率化描述系統的過程比我們前面確定性的描述過程（空間曲面）更加抽象與難以理解。很多機器學習工作者在這裡會遇到學習過程中的一個很大的障礙—很多機器學習系統是通過概率來進行描述的，這種不確定性通常與我們的直覺相悖。</p><p style=text-align:start>本書在編寫過程中儘量使用兩種方式來描述系統，即函數式描述以及概率式描述，方便對概率論不甚瞭解的讀者閱讀本書。如果讀者對概率概念較為熟悉，則可跳過本章進行後續學習。本章將對概率與統計領域的基本概念進行闡述，需要著重理解什麼是建模以及最大似然估計。</p><h1 class=pgc-h-arrow-right>2.1　概率基礎概念</h1><p style=text-align:start>機器學習非常依賴於概率以及相關的數學工具。因此在深度學習中與概率相關的概念的出現頻率非常高。我們習慣了使用確定性的思維來描述事物，這種確定性的思維在一定程度上類似於函數。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b9b3561ea7eb4594a3068ce80c9d91d9><p class=pgc-img-caption></p></div><p>　　(2.1)</p><p>也就是說，我們給定一個</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fa30bf01ff1d47acb21132ead6085bd2><p class=pgc-img-caption></p></div><p>就會有一個確定的</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4ca0e90f02534feb82a920aa071abe1f><p class=pgc-img-caption></p></div><p>。但這種描述方式存在缺陷。因為環境本身可能存在噪聲，這使我們給定</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fa30bf01ff1d47acb21132ead6085bd2><p class=pgc-img-caption></p></div><p>的時候輸出的值與</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4ca0e90f02534feb82a920aa071abe1f><p class=pgc-img-caption></p></div><p>之間可能出現偏差，或者由於我們的模型本身複雜度不足以描述數據導致</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fa30bf01ff1d47acb21132ead6085bd2><p class=pgc-img-caption></p></div><p>與</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4ca0e90f02534feb82a920aa071abe1f><p class=pgc-img-caption></p></div><p>之間可能出現偏差。回顧第1章所列舉的超定方程的例子。我們在用直線擬合4個數據點的過程中由於直線形式較為簡單，因此並不能完美地穿過所有的數據點，而只能近似。這種近似就是模型本身複雜度不足所引起的偏差。因此為了描述真實世界，引入概率是必要的。列舉一個簡單的例子：拋硬幣。這是一個簡單的隨機事件，隨機事件就是在重複試驗中有規律地出現的事件。拋硬幣只有兩種情況{正面，反面}，這種由全體樣本點組成的集合，稱為樣本空間，可以用大寫字母表示。由於在拋硬幣的過程中彼此之間並不影響，出現正面和反面的概率均是0.5，因此我們稱樣本是獨立同分布的（Independent and Identically Distributed, IID）。</p><blockquote><p>概率與頻率</p><p>拋硬幣過程中假設做了1 000次試驗，出現了501次正面，那麼此時出現正面的頻率就是。而概率就是樣本無窮大時的頻率，代表了隨機事件的特徵。通常用<em>p</em>來表示概率—</p></blockquote><p style=text-align:start>在這個過程中我們並未獲取任何知識，因為拋硬幣試驗本身就是一個等概率分佈。從另外一個角度來講，我們更加深入地研究了拋硬幣的過程，獲取了更多的特徵，這裡的特徵指的是我們觀測到拋硬幣試驗中拋硬幣的高度、使用力氣的大小、風速等一系列觀測參數。此時我們再計算硬幣正面的概率就是在這些條件下所得到的概率。</p><p style=text-align:center><em>p</em>(正面|高度, 力度, 風速,⋯)　　(2.2)</p><p style=text-align:start>實際上式 (2.2) 就是一個條件概率，它代表了在我們觀察到外界的情況下對拋硬幣事件的預測。更加通用的條件概率書寫形式如下。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c54fb61497a846a2a0d880b845820112><p class=pgc-img-caption></p></div><p>　　(2.3)</p><p style=text-align:start>如果此時概率依然是0.5，那麼代表我們實際上沒有獲取任何知識。如果我們通過一系列統計將預測硬幣概率為0.9，那麼代表我們是可以通過外界的觀察而對隨機事件進行有效預測的。這就是說我們從數據中發現了可用的知識，這是一個典型的機器學習過程。機器學習就是通過對觀測數據進行分析，從而獲取有用的知識。</p><p style=text-align:start>如果拋硬幣試驗的樣本空間是離散的，則只有兩種情況。而對於其他情況，比如說某一電視機第一次損壞的時間，這個時間是連續的，這種稱為連續型隨機變量。離散型隨機變量與連續型隨機變量對應於機器學習的兩個基本問題—分類問題與迴歸問題。連續型隨機變量的概率僅在積分條件下有意義。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4277f334ba234f60be786ae68cde74a5><p class=pgc-img-caption></p></div><p>　　(2.4)</p><p>對於電視機損壞的問題而言，這代表從<em>a</em>時刻開始到<em>b</em>時刻之間損壞的概率。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ddda62cf97bb447eb1fb158dfe752ef8><p class=pgc-img-caption></p></div><p>稱為概率密度函數。概率密度函數符合下面的約束條件。</p><p>（1）</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f57df298c8444058886756c57a7a5939><p class=pgc-img-caption></p></div><p>，概率不存在負值。</p><p>（2）</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/4b30e25f28b2405d8fba0729e5e549ae><p class=pgc-img-caption></p></div><p>，所有可能情況之和為1。離散類型隨機變量需將積分改為求和。</p><p style=text-align:start>如果有多個隨機變量，則概率可以寫為如下形式。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fd61fe093cd24f59aaf14f8bacbea593><p class=pgc-img-caption></p></div><p>　　(2.5)</p><p>此時稱為聯合概率分佈，其代表了</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/7b2043aa3ee3499d9417d561d16b2dee><p class=pgc-img-caption></p></div><p>同時發生的概率。舉一個簡單的例子，我們有兩枚硬幣A和B，硬幣是不均勻的，A出現正面概率是0.6，B出現正面概率是0.7。那麼可以將兩枚硬幣的聯合概率寫成如表2.1所示的形式。</p><p style=text-align:center><strong>表2.1　兩枚硬幣試驗中條件概率與邊緣概率</strong></p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ff7e21609402405092f569763952619c><p class=pgc-img-caption></p></div><p>由表2.1可以看到，對於獨立試驗而言，其概率是直接相乘的。拋硬幣A、B這種獨立試驗假設也是樸素貝葉斯算法的基本假設。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/681303a1c833472592c104fd66cd6a53><p class=pgc-img-caption></p></div><p>　　(2.6)</p><p style=text-align:start>其中涉及了新的概念—邊緣概率。邊緣概率就是根據概率的聯合分佈獲取某一隨機變量的分佈。其形式如下。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/89bd421e40ec4fd39b715b4c46c2799f><p class=pgc-img-caption></p></div><p>　　(2.7)</p><p style=text-align:start>對於條件概率和聯合概率有如下公式。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b1a2be7972634fe18882bbed6dfa232f><p class=pgc-img-caption></p></div><p>　　(2.8)</p><p style=text-align:start>從另一個角度來講，條件概率給定了某些事件的依賴關係，比如溼度過高會直接導致下雨，而下雨又與降溫有直接關係，但溫度降低和溼度之間沒有明顯的依賴關係。這種依賴關係可以通過圖形化的方式來展示，如圖2.1所示。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5cedf135f4da4bc1bcfb3ced074835c6><p class=pgc-img-caption></p></div><p style=text-align:center>圖2.1　概率的有向圖模型</p><p style=text-align:start>如果溼度、下雨、降溫三者之間沒有明顯的關係，也就是獨立的事件，那麼可以用式 (2.6)進行聯合概率的分解。但三者之間顯然不是獨立的。下雨在溼度確定的情況下是獨立的，而降溫則是在下雨的條件下是獨立的。因此，概率分解方式應該為<em>p</em>(溼度，下雨，降溫)=<em>p</em>(溼度)<em>p</em>(下雨|溼度)<em>p</em>(降溫|下雨)。這種概率分解可以簡化建模。</p><p style=text-align:start>這稱為概率圖模型，它代表了隨機變量的依賴關係。假設對於聯合概率分佈某些變量存在依賴關係，則其可以寫為如下形式。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5b0306b0f4294865bf2b0d08c79fda52><p class=pgc-img-caption></p></div><p>　　(2.9)</p><p style=text-align:start>此時概率有向圖的形式如圖2.2所示。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0173571d7c5d4d049b257deb2f1ec1dd><p class=pgc-img-caption></p></div><p style=text-align:center>圖2.2　概率有向圖模型</p><p style=text-align:start>如果概率之間並無依賴關係，則可以用無向圖來表示，如圖2.3所示。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cd9900382a554fd6aa98d25e293c70f4><p class=pgc-img-caption></p></div><p style=text-align:center>圖2.3　無向圖表示的概率模型</p><p style=text-align:start>此時概率可以分解為如下形式。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/51fec3ea741349669f337b472ecd061e><p class=pgc-img-caption></p></div><p>　　(2.10)</p><p style=text-align:start>這稱為概率無向圖模型或馬爾科夫隨機場，其中<em>Z</em>是歸一化常數。式 (2.10) 的分解依據為最大子團的分解。任意兩個節點間均有線連接，而加入任意新節點均無法滿足前面的條件，則稱這種結構為最大子團。兩種圖模型均可以用來表示聯合概率分解。這種圖示對於表示來說是清晰直觀的。</p><h1 class=pgc-h-arrow-right>2.2　隨機變量數字特徵</h1><p style=text-align:start>對於隨機變量本身，我們很難用確定性的公式來描述，因此可以藉助隨機變量的數字特徵來描述變量內在特徵。在機器學習中我們所關注的隨機變量的數字特徵主要有隨機變量的數學期望、方差、標準差、協方差等。這其中最簡單也是最重要的就是期望。期望的公式形式如下。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/27b11f9c16bf4239b865ff8d43e5ad50><p class=pgc-img-caption></p></div><p>　　(2.11)</p><p style=text-align:start>式 (2.11) 列舉了兩種隨機變量的表示形式，一種是離散型隨機變量，另一種是連續型隨機變量。一般認為積分就是特殊形式的求和，因此兩個公式並無本質區別。但這裡需要說明的一點是，期望（Expected Value）與均值（Arithmetic Mean）是不同的。期望描繪的是數據的真實情況，是概率學內容；均值僅是對樣本數據進行的統計，屬於統計學範疇。在樣本數量較多的情況下，由大數定理可以知道均值和樣本相等。一般認為，樣本均值是對期望的無偏估計。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c2e323d22dfb4ea68e98ac328fc39328><p class=pgc-img-caption></p></div><p>　　(2.12)</p><p style=text-align:start>僅有數據均值是沒有用的。對於樣本本身分佈而言還需要統計分佈的離散程度，這種離散程度稱為方差。方差（Variance）概念的產生就是為了描述變量的離散程度，其表達方式如下。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7dc5ba847e80468da0b82b1d35bea1b7><p class=pgc-img-caption></p></div><p>　　(2.13)</p><p style=text-align:start>標準差是在方差的基礎上開根號，其與方差可以一起用來描述數據的分佈情況。為了說明問題，我們繪製圖像來展示數據分佈的描述方式，如圖2.4所示。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/2b13595f79dd408fa6ce96ea124d6407><p class=pgc-img-caption></p></div><p style=text-align:center>圖2.4　數據的不同方差圖示</p><p style=text-align:start>如果數據方差越大，則數據分佈越分散。從統計條形圖上可以看到，紅色統計圖數據標準差較大，因此看起來分佈更加分散。</p><blockquote><p>條形圖</p><p>條形圖用來描述數據的分佈情況，數據x軸表示隨機變量取值，縱軸表示在某一區間樣本數量的多少。可以對條形圖概率進行歸一化，在大樣本情況下歸一化條形圖描繪了樣本的概率分佈。</p></blockquote><p style=text-align:start>前面兩個統計數據均值以及方差均是描繪了一維數據的特徵。如果樣本本身有兩個屬性，則可以通過協方差（Covariance）來描述數據兩個屬性之間的線性相關性。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/973313a2906346e5b51626d72d52a2c1><p class=pgc-img-caption></p></div><p>　　(2.14)</p><p>這裡</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7232172e7d4d452fb45fe955652a2805><p class=pgc-img-caption></p></div><p>、</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bbb42dcc83784b94977cad91e2c3111f><p class=pgc-img-caption></p></div><p>代表數據點的兩個屬性，如果將數據存儲為二維矩陣，那麼每一行代表一個樣本，每一列代表數據的某一屬性。此時</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7232172e7d4d452fb45fe955652a2805><p class=pgc-img-caption></p></div><p>、</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bbb42dcc83784b94977cad91e2c3111f><p class=pgc-img-caption></p></div><p>就是二維矩陣的列向量。如果對式 (2.14) 使用方差進行歸一化，我們得到的就是兩列數據之間的皮爾遜相關係數(Pearson Correlation Coefficient)。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d4994018163b46c9b021402d5069de67><p class=pgc-img-caption></p></div><p>　　(2.15)</p><p style=text-align:start>這種歸一化表示是有益的，我們可以通過直接觀察相關係數的取值來衡量兩列之間的相關性，如圖2.5所示。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/89af8a849dbf439592a395bb81e46c45><p class=pgc-img-caption></p></div><p style=text-align:center>圖2.5　不同分佈數據的線性相關性</p><p style=text-align:start>由圖2.5可知，如果兩個列屬性之間線性相關性較強，則其相關係數接近於1或者−1，否則接近於0。</p><blockquote><p>線性相關性</p><p>這裡的線性相關性與第1章中的向量的相關性是類似的。如果兩個向量（列向量）具有很強的線性相關性，則代表一個向量可以由另一個向量來表示：。從另一個角度來講，數據內部存在冗餘，我們僅需要存儲即可表徵。</p></blockquote><p>對於多維向量而言，其組織形式為矩陣：</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d2d965d8b43b40138486547a018187ce><p class=pgc-img-caption></p></div><p>，其中</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7232172e7d4d452fb45fe955652a2805><p class=pgc-img-caption></p></div><p>代表列向量，矩陣的每一行代表一個樣本，每一列</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/983ebe7d05cc45f4819caa4f4246c8fb><p class=pgc-img-caption></p></div><p>代表樣本的元素。對於這種數據，需要將協方差變為協方差矩陣（Convariance Matrix）來描述線性相關性。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b2eca5cd9f8a44d68d1094e49d9e9491><p class=pgc-img-caption></p></div><p>　　(2.16)</p><h1 class=pgc-h-arrow-right><strong>衍生算法：主成分分析（PCA）算法</strong></h1><p style=text-align:start>它用來衡量樣本各列之間的線性相關性，如果各列之間的線性相關性比較強，則意味著其中一列可以用另一列來表示。這種儘量減少各列數據的數據相關性的算法就稱為PCA算法。PCA算法可以有效地減少數據冗餘，通常用於數據預處理過程。PCA算法的基本思想的公式化描述如下。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/63233fe926bb4dbbb4b13b84f507e4bd><p class=pgc-img-caption></p></div><p>　　(2.17)</p><p>假設</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2f758667c84341938f8f2a42a7870e81><p class=pgc-img-caption></p></div><p>中每一列元素均值為0，那麼式 (2.17) 就能成立。由此現在的一個問題就是找到一個合適的變換矩陣</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1db77b81904e415a851f913da59d6a06><p class=pgc-img-caption></p></div><p>使得</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0026588bc94c4d5c871fd08423b429a9><p class=pgc-img-caption></p></div><p>能夠變換為</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/00e123217e3c4c8ab12534cb39031402><p class=pgc-img-caption></p></div><p>的形式。也就是變換後使各列之間的線性相關性最小，這樣協方差矩陣可以對角化，在此假設對</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0026588bc94c4d5c871fd08423b429a9><p class=pgc-img-caption></p></div><p>進行變換的方式為線性變換。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2ec64490260e451dbb3ad01527e0ef08><p class=pgc-img-caption></p></div><p>　　(2.18)</p><p>要使變換後矩陣</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/00e123217e3c4c8ab12534cb39031402><p class=pgc-img-caption></p></div><p>可以用式 (2.17) 的形式對角化，可以對矩陣</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e8fc139ee5de48fc8529b6c432a5af63><p class=pgc-img-caption></p></div><p>其進行特徵值分解。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d72568dc0a2e4ddba0282ba0dc48b1cd><p class=pgc-img-caption></p></div><p>　　(2.19)</p><p>此時僅需要使</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2b1aef70a83544318b79dc55f170a217><p class=pgc-img-caption></p></div><p>，那麼變換後形式如下。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/32be84c2c6a04148a1e68cdd05d21a2e><p class=pgc-img-caption></p></div><p>　　(2.20)</p><p>式 (2.18) 中的變換矩陣</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1db77b81904e415a851f913da59d6a06><p class=pgc-img-caption></p></div><p>就是式 (2.19)中的</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/82737fc5922a431baf817c867f36cd6b><p class=pgc-img-caption></p></div><p>。PCA算法實際上與矩陣的奇異值分解有很大程度的相似性，或者底層算法可以通用。</p><h1 class=pgc-h-arrow-right>2.3　信息熵</h1><p style=text-align:start>信息論是概率與統計的衍生內容。很多時候我們需要對系統的混亂程度進行衡量，通常而言這是難以量化的，在熱力學中引入了熵的概念。在物理學中系統總是趨於向熵增大的方向發展，也就是從有用的機械能到內能的轉換，這種轉換在孤立系統中是不可逆的。以一個形象的例子來說：兩種顏色的沙子，在混合前是有規律的。而在將其混合後整個系統的混亂程度變得很高，如果要將兩種顏色分開，需要人為挑選，這個過程需要做功。同樣地，機器學習過程也是如此，在開始過程中系統輸出是無規律的，我們需要進行訓練使整個系統可以進行某種預測。為了衡量系統本身的複雜度，在信息論中引入了與熱力學熵類似的信息熵。在瞭解信息熵之前，我們需要定義自信息。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a13d3441a1924e9285f396fab28261bc><p class=pgc-img-caption></p></div><p>　　(2.21)</p><p>自信息在信息學中是以2為底的</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/73b4841e4dd8453a99c970a5d1b46b5e><p class=pgc-img-caption></p></div><p>，單位是bit。這個概念比較容易理解，如果某一概率特別小的事件發生了，那麼說明它帶來了足夠多的有用信息。對於某一變量而言，我們通常並不關心它的具體取值，而只是關注它的分佈形式。對自信息取均值，就得到了信息熵，也稱香農熵（Shannon Entropy），其可以用來衡量系統的混亂程度。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2af4e644bb394c06989571097f932636><p class=pgc-img-caption></p></div><p>　　(2.22)</p><p>這裡如果</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/114269cbc60d461baed09be191554dff><p class=pgc-img-caption></p></div><p>或1，那麼</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3351facba0a04781b9e4e265a0f05d3e><p class=pgc-img-caption></p></div><p>，式 (2.21) 對於離散變量就可以寫成求和形式。</p><p style=text-align:start>這裡以拋硬幣來舉例。如果在拋硬幣的過程中，我們得到正反面的概率均為0.5，前面說到這種情況是無法學到任何知識的，這種知識量化就是信息熵，計算公式如下。</p><p style=text-align:center><em>H</em>(硬幣) = - <em>p</em>(正面)log( <em>p</em>(正面) ) - <em>p</em>(反面)log{( <em>p</em>(反面) ) =}1　　(2.23)</p><p style=text-align:start>前面說到，log以2為底，單位是bit。此時，對於硬幣而言，用1bit信息就可以表示狀態0或者1。如果通過某種方式，我們預測得知正面概率變為了1，則計算可得以下結果。</p><p style=text-align:center><em>H</em>(硬幣) = - <em>p</em>(正面)log( <em>p</em>(正面) ) - <em>p</em>(反面)log{( <em>p</em>(反面) ) =}0　　(2.24)</p><p style=text-align:start>此時信息熵變小了，也就是系統混亂程度變小了。前面講過我們可以通過一定條件預測出拋硬幣的結果。此時我們從系統中學到了有用的知識，從而使系統混亂程度降低。以硬幣正面概率作為變量，以熵作為函數，如圖2.6所示。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cf239dfdbce046979d734b2bd1db4db9><p class=pgc-img-caption></p></div><p style=text-align:center>圖2.6　預測硬幣正反概率所對應的信息熵</p><p style=text-align:start>因此，機器學習過程就是從數據中尋找規律從而使系統熵變小的過程。</p><p style=text-align:start>在機器學習中衡量兩個分佈相似度的概念是交叉熵（Cross Entropy）。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e14bc54c8f4c481ca50088bfce0bd54a><p class=pgc-img-caption></p></div><p>　　(2.25)</p><p style=text-align:start>這也是機器學習中常用的損失函數之一（損失函數我們放到後面詳細描述）。相比傳統的點的距離的損失函數，交叉熵在計算梯度的過程中通常更加有效。因此，交叉熵在處理多分類問題時是更加合理的選擇。</p><h1 class=pgc-h-arrow-right>Softmax</h1><p>在機器學習中很多理論是基於概率的，但是在理論推演過程或者實現過程中它通常用函數</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ec10798a157c46c7a2df550ed4484971><p class=pgc-img-caption></p></div><p>來表示，其中</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7232172e7d4d452fb45fe955652a2805><p class=pgc-img-caption></p></div><p>是輸入樣本，</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/60c4fd885b21414a99fadc717155803e><p class=pgc-img-caption></p></div><p>是模型。在這個過程中，需要在概率與函數輸出之間進行轉換，其常用的形式如下。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a159019410274c45bb38f27c2dc4f79e><p class=pgc-img-caption></p></div><p>　　(2.26)</p><p>這裡將模型輸出轉換為概率形式，</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/fdc501856f124128a0f1782bdfbaf9da><p class=pgc-img-caption></p></div><p>是歸一化常數，m為自定義常數通常為1。這個過程稱為Softmax。對於多分類問題，我們給定的數據標籤為<em>d</em>，它是一個多維向量，每一個維度上都保存了可能屬於某一類的概率。例如，對於年齡層劃分［青年,中年,老年］的問題，數據標籤可能為</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/161cd83210c9432c8bd948366e5d88b7><p class=pgc-img-caption></p></div><p>，表示這個人屬於老年的概率為100%，這是因為我們在標註數據時可以確定這個人是老年人。這種編碼方式稱為one-hot編碼。</p><p>這種編碼方式是對應機器學習問題而產生的，因為如果用1、2、3來表示不同的年齡階段，則可能難以訓練。而預測輸出</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/6530f1b4341f4158810bc20742d91c0b><p class=pgc-img-caption></p></div><p>，顯然並非概率的表示，因為概率表示不會存在負數，同時滿足約束之和為1。</p><p>對於第一個問題，我們可以通過e指數的方式解決。將</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bbb42dcc83784b94977cad91e2c3111f><p class=pgc-img-caption></p></div><p>變為</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9b04d657090e489a9fa048ebabaffadd><p class=pgc-img-caption></p></div><p>，之後再進行歸一化</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7cc04a62be594d67af258ef856597f37><p class=pgc-img-caption></p></div><p>，整個過程稱為Softmax。它實際上解決的問題是將函數轉換為概率表示，也是基於能量的模型。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6cfb0a15fa334c0fb4819c8bb60eb899><p class=pgc-img-caption></p></div><p>　　(2.27)</p><p>之後就可以用Softmax的結果與原有的標籤</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/49657ac005944ad0867679d5a385e40b><p class=pgc-img-caption></p></div><p>計算交叉熵來作為損失函數。</p><h1 class=pgc-h-arrow-right>2.4　概率模型下的線性變換</h1><p style=text-align:start>機器學習模型均可以通過概率的方式進行描述。對於上面所述的一系列變換過程，均可以描繪為概率生成模型。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/9ffc51a19fab488596e802d3e77dda14><p class=pgc-img-caption></p></div><p>　　(2.28)</p><p>假設</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f56341742ca6431dbc3935cfaca4be11><p class=pgc-img-caption></p></div><p>是從某種分佈</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ed73c816df1740e0b100f94432bc78f0><p class=pgc-img-caption></p></div><p>中抽取的向量，對於線性因子來講，其生成過程如下。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2038f80d0b0842c688f69ced79f5111a><p class=pgc-img-caption></p></div><p>　　(2.29)</p><p>式 (2.29) 中</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6756e7fb9a1842d9a515a19c9cb438bb><p class=pgc-img-caption></p></div><p>為噪聲，那麼它所描述的生成過程類似於如下公式。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1d3b935cbcd54e96a1e51c8ceb76cf3a><p class=pgc-img-caption></p></div><p>　　(2.30)</p><p style=text-align:start>式 (2.30) 描述的是線性變換的過程。因此很多線性變換可以描述成上述形式，這個過程如圖2.7所示。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cb53e761ed254f8ca305dff00b1ea9ea><p class=pgc-img-caption></p></div><p style=text-align:center>圖2.7　線性因子圖示</p><p>對於 PCA 算法來講，式 (2.28) 中的</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f56341742ca6431dbc3935cfaca4be11><p class=pgc-img-caption></p></div><p>是線性獨立的。它的產生方式也是式 (2.30) 所描述的線性方式，而分佈是單位分佈。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c751ff13d96c49c792715fcdfbd1b2de><p class=pgc-img-caption></p></div><p>　　(2.31)</p><p>假設</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f56341742ca6431dbc3935cfaca4be11><p class=pgc-img-caption></p></div><p>的協方差矩陣為單位矩陣，則由其產生的</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b5b2df12ac3540eb86123d8a13a650f5><p class=pgc-img-caption></p></div><p>如下。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/59d744d7cea3496180f6f650d852dbc7><p class=pgc-img-caption></p></div><p>　　(2.32)</p><h1 class=pgc-h-arrow-right>2.5　最大似然估計與最大後驗估計</h1><p>這裡依然以拋硬幣試驗為例。記錄</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/10eb7960a77d433d93386a96151d214f><p class=pgc-img-caption></p></div><p>次隨機試驗</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7957a4ccb4f64aacb49c007296c5eade><p class=pgc-img-caption></p></div><p>，硬幣正面</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/39c8769ab5ff4699894634f6d1845107><p class=pgc-img-caption></p></div><p>，出現正面的概率為</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ce7cec94fd3249c782b62f744e9fdffe><p class=pgc-img-caption></p></div><p>，從頻率學派的觀點來看，概率</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ce7cec94fd3249c782b62f744e9fdffe><p class=pgc-img-caption></p></div><p>應該是僅依賴於現有試驗結果的，也就是通過數據取得的。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f68624a0191e4074b99e114838f9e370><p class=pgc-img-caption></p></div><p>　　(2.33)</p><p style=text-align:start>這個概率應該是令式 (2.33) 取最大值的概率。在假設了正面出現的概率後，假設試驗是有順序的同時也是獨立重複的，那麼可以計算出現隨機試驗情況下所得的概率。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4de47a8b96e9451bb1e9ccac8ba2d304><p class=pgc-img-caption></p></div><p>　　(2.34)</p><p>計算上述以</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ce7cec94fd3249c782b62f744e9fdffe><p class=pgc-img-caption></p></div><p>為自變量的函數的最大值。</p><p><br></p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/08923965d54e45f2949a0735513c3f9a><p class=pgc-img-caption></p></div><p>　　</p><p style=text-align:start>令函數為如下形式。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6b36aa5e57064741833a1e87700cf330><p class=pgc-img-caption></p></div><p>　　(2.36)</p><p>式 (2.36) 取得最小值時</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6cd3a0df37dc463c9de1073e655cb16d><p class=pgc-img-caption></p></div><p>，由此認為取得正面的概率為</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dfe3434d0c874c4aa69904c2296fc34c><p class=pgc-img-caption></p></div><p>。求解硬幣取得正面概率的過程稱為最大似然估計（MLE）。</p><p style=text-align:start>貝葉斯學派則認為概率符合一個先驗分佈。這個先驗分佈可以糾正採樣的偏差。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/ed80f394c02c41cbab8a7712ab9a81dd><p class=pgc-img-caption></p></div><p>　　(2.37)</p><blockquote><p>在式 (2.37) 中</p><p>（1）為後驗。</p><p>（2）為似然。</p><p>（3）為先驗。</p><p>（4）先驗概率+數據=後驗概率。</p></blockquote><p>先驗概率是什麼意思？假設為了估計此次投擲硬幣為正面的概率，我先用自己的硬幣做了</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/475b460271cc4d2ebf97f0ca3c794bc0><p class=pgc-img-caption></p></div><p>次實驗，正面出現了</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b982b33b8aef4007b494948c649e2e7f><p class=pgc-img-caption></p></div><p>次，假設此時正面概率為</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7232172e7d4d452fb45fe955652a2805><p class=pgc-img-caption></p></div><p>，那麼出現這種情況的概率如下。</p><p><br></p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b8aad330c7ba4bebbbfcf50a3d2a412b><p class=pgc-img-caption></p></div><p style=text-align:start>式 (2.38) 中constant為歸一化常數，稱為Beta函數。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/24e59bef2be144b9bc1cd653a3d4c603><p class=pgc-img-caption></p></div><p>　　(2.39)</p><p>到此為止，我們都在描述</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7232172e7d4d452fb45fe955652a2805><p class=pgc-img-caption></p></div><p>的分佈，也就是硬幣為正面概率的分佈。可以看到使用我自己的硬幣估計時，取得0.5的概率是最大的，如圖2.8所示。</p><p>這個概率就可以作為我們的先驗分佈，它是概率的概率。以這個先驗分佈，我們去估計其他硬幣的試驗，假設使用另一枚硬幣，拋4次，出現正面為</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/19c102ea7850413ba59bcb7e5743d3f3><p class=pgc-img-caption></p></div><p>次，出現反面為</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b71839d08faa4132af7a40c59ae4e2a0><p class=pgc-img-caption></p></div><p>次，那麼在貝葉斯理論下出現正面的概率如下。</p><p><br></p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e6286533b1b349fd9756a5ff27b78e55><p class=pgc-img-caption></p></div><p><br></p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7f40791fcac644db95ec5e4e2527db62><p class=pgc-img-caption></p></div><p style=text-align:center>圖2.8　硬幣為正面的概率分佈</p><p style=text-align:start>將式 (2.40) 繪製成圖2.9所示的曲線。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/95c82e4036d84194b0894fe5ce4b9daa><p class=pgc-img-caption></p></div><p style=text-align:center>圖2.9　融入先驗概率後計算的正面概率分佈</p><p style=text-align:start>此時硬幣正面概率最大值為0.375。這個數值很重要，我們以最大似然估計預測的硬幣正面概率是0.25。此時是沒有先驗分佈的，如果引入了先驗分佈，則取得的正面概率應該為0.375。而我們拋硬幣的次數僅為4次，因此很大可能出現偏差，先驗概率的引入則可以糾正這種偏差。從另一個角度來看相當於在最大似然估計的基礎上加入了正則化項，這種估計稱為最大後驗估計（MAP）。</p><p>假設神經網絡輸出為</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f56341742ca6431dbc3935cfaca4be11><p class=pgc-img-caption></p></div><p>，通過Softmax處理。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ec962ecb421748a8a3be989d829c9fe6><p class=pgc-img-caption></p></div><p>　　(2.41)</p><p>此時分類問題神經網絡預測過程可以描述為，給定樣本</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7232172e7d4d452fb45fe955652a2805><p class=pgc-img-caption></p></div><p>後輸出屬於某一類的概率</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bbb42dcc83784b94977cad91e2c3111f><p class=pgc-img-caption></p></div><p>，而神經網絡的可訓練參數為</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/75da717f77a34d8991360890e5308710><p class=pgc-img-caption></p></div><p>。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/62c5b52d7bb7400fba9590f6b58c59a1><p class=pgc-img-caption></p></div><p>　　(2.42)</p><p style=text-align:start>神經網絡的訓練過程則可以描述為如下形式。</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4dc36cd064004da08ef5f42b30bce5d9><p class=pgc-img-caption></p></div><p>式 (2.43) 中，</p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/49657ac005944ad0867679d5a385e40b><p class=pgc-img-caption></p></div><p>為one-hot形式的向量。可以看到，最大似然估計與交叉熵作為損失函數具有相同的意義。</p><p><strong>本文截選自《深度學習算法與實踐》</strong></p><div class=pgc-img><img alt=機器學習較常用到的數學工具：概率與統計 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7277469448ef4e038d6b482ecdac090b><p class=pgc-img-caption></p></div><p>本書主要內容分為3個部分。</p><p style=text-align:start>第一部分為深度學習的數學基礎（第1～4章）。這部分內容包括空間幾何與線性代數、概率與統計、函數建模與優化、機器學習庫的使用。其中，前兩個是相對獨立的，因此讀者可以根據自己的基礎進行選擇性閱讀；建模與優化綜合了線性代數、概率與統計的內容。</p><p style=text-align:start>第二部分為深度學習基本組件（第5～9章）。這部分內容包括深度學習模型與全連接網絡、卷積神經網絡、循環神經網絡基礎、循環神經網絡擴展以及深度學習優化。這些基本組件均配有與訓練預測過程基本實現。讀者可以脫離機器學習庫實現深度學習算法，這是理想的學習結果，但並非理想的學習過程。作為初學者應當在實踐中逐步深入地進行學習。</p><p style=text-align:start>第三部分為深度學習中常見的應用場景以及相關模型（第10～12章）。這部分內容包括圖像處理（物體檢測、人臉識別）、自然語言處理（語音識別、自然語言翻譯、語音生成）和非監督學習（對抗生成網絡、圖像去噪、增強學習）。</p><p style=text-align:start>作為入門圖書，本書會從簡單的函數入手描述深度學習（這是編程所必需的），同時介紹深度學習的基本元素與實現（如卷積神經網絡、循環神經網絡）。至於更復雜的理論，僅進行預測過程公式的說明與機器學習庫版本的實現（如注意力機制）。訓練過程可能需要藉助TensorFlow來完成，但這不代表其本身與TensorFlow是綁定的關係。希望看完本書的讀者能夠抽出時間來進行更系統的學習。比如從空間幾何、統計理論開始學習，但作為初學者，不建議過分糾結基礎。如果要真正精通機器學習問題，時間和精力也是必須付出的成本。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>數學</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E5%AD%B8/51c857d6.html alt=機器學習中的數學基礎（2）——理解基、線性組合與向量空間 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/aaaaa2e16c3342b1a25f0e4b1cf8a113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/51c857d6.html title=機器學習中的數學基礎（2）——理解基、線性組合與向量空間>機器學習中的數學基礎（2）——理解基、線性組合與向量空間</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/51f526d0.html alt=機器學習數學-矩陣 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7a0202a71f7b4f39a3d562e87c440dff style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/51f526d0.html title=機器學習數學-矩陣>機器學習數學-矩陣</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/3145788f.html alt=機器學習數學基礎系列｜凸優化——開啟新世界的大門（上） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d52002b7fcb7409abab971ff9de5550c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/3145788f.html title=機器學習數學基礎系列｜凸優化——開啟新世界的大門（上）>機器學習數學基礎系列｜凸優化——開啟新世界的大門（上）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9007382b.html alt=持續學習之：數學分析之數項級數 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1531835169567c7c730159b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9007382b.html title=持續學習之：數學分析之數項級數>持續學習之：數學分析之數項級數</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>