<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>工業大數據：分析算法 | 极客快訊</title><meta property="og:title" content="工業大數據：分析算法 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p9.pstatp.com/large/pgc-image/0fbd61d309e344efad19260bf524595f"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/22fa6b3.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/22fa6b3.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/22fa6b3.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/22fa6b3.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/22fa6b3.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/22fa6b3.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/22fa6b3.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/22fa6b3.html><meta property="article:published_time" content="2020-10-29T21:01:05+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:05+08:00"><meta name=Keywords content><meta name=description content="工業大數據：分析算法"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/22fa6b3.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>工業大數據：分析算法</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>一. 應用背景</p><p>大數據分析模型的研究可以分為3個層次，即描述分析(探索歷史數據並描述發生了什麼)、預測分析(未來的概率和趨勢)和規範分析(對未來的決策給出建議)。工業大數據分析的理論和技術研究仍處於起步階段，主要應用場景如下：</p><p>1. 預測性維護。</p><p>傳統製造業面臨核心設備的維護管理、故障診斷等問題，常規維修存在不確定性，緊急狀況下故障處理的難度及壓力都較大。實時監測、有效記錄設備工況信息，通過大數據平臺建模分析，能夠有效地對設備運行狀態進行評估，針對潛在的健康隱患給出對應的預警。基於全生命週期的數據挖掘，可以對工程機械設備的核心耗損性部件的剩餘壽命進行預測，以便進行提前維護或者更換。</p><p>2. 工藝參數優化。</p><p>基於決策樹等弱分類器的集成學習算法是解決工藝參數優化的常規手段，如GBDT，XGBoost，LightGBM等。另一個方向是深度強化學習(DRL，deep reinforcement learning)，DRL集成了深度學習的感知理解能力及強化學習的決策能力，是未來發展的趨勢。</p><p>此外，產量預測、客戶需求分析和服務類型識別等也是廣泛存在的場景。</p><p>二. 預測性維護算法</p><p>系統中部署傳感器，用於監控和收集系統運行的數據。預測性維護所需要的數據是時間序列數據，包括時間戳、在該時間戳所收集的傳感器讀數以及設備號。預測性維護的目的是，在時間“t”，使用截至到該時間的數據來預測設備在近期是否會發生故障。預測性維護可通過以下兩種方法之一來實現：</p><p>(1). 迴歸方法：預測在下次故障發生之前的剩餘時間，即剩餘使用壽命(RUL)。</p><p>迴歸方法只提供一個布爾值答案，但可以使用較少的數據提供更高的準確性。</p><p>預測精度的衡量標準：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/0fbd61d309e344efad19260bf524595f><p class=pgc-img-caption></p></div><p>使用均方根誤差懲罰實際與預測的RUL之間Loss。也可以採用log的形式：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a731f40467144f6da566aba497ccfc06><p class=pgc-img-caption></p></div><p>模型選擇：</p><p>基於scikit learn和H2O的迴歸算法都適用。如，scikit learn-Random Forest Regression、scikit learn-Linear Regression、scikit learn-Ridge、scikit learn-Lasso及H2O-Random Forest Regression、H2O-Gradient Boosting、H2O-Generalized Linear Modeling、H2O-Deep Learning等。</p><p>算法步驟：</p><p>1. 消除傳感器噪聲。</p><p>傳感器讀數通常會有噪聲，往往採用自動編碼器去除噪聲，以主要維度表示數據。與PCA相似，AutoEncoder使用同一套數據集同時作為網絡的輸入和輸出進行訓練，其中網絡中的參數比數據集中的維數少。由於噪聲的維度比常規數據高得多，這種方式可以在一定程度上去噪。H2O引入相應模塊：</p><pre><code>from h2o.estimators.deeplearning import H2OAutoEncoderEstimator</code></pre><p>2. 特徵工程。</p><p>時間序列的特徵大體上可以分成統計特徵，擬合特徵，週期性特徵，分類特徵等幾大類。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2a512a54076a4da28c0ef60dce3154bb><p class=pgc-img-caption></p></div><p>統計特徵：</p><p>已知的時間序列的最大值、最小值、自相關係數等各種各樣的統計指標。</p><p>擬合特徵：</p><p>基本想法是用一些簡單的時間序列算法去擬合數據，然後使用擬合數據和真實數據來形成必要的特徵。時間序列數據集讀數是自相關的，“t”時刻的預測可能會受到“t”之前的某個時間窗口的影響。因此需要通過移動標準差、移動平均值、自迴歸、奇異值分解和深度學習等方法生成特徵並進行特徵組合。</p><p>移動平均算法：已知時間序列X n = [x 1 ,...,x n ]，可以使用一個窗口值w>=1得到一組光滑後的時間序列。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/c92835d438074e4e98a6f65dd9103f43><p class=pgc-img-caption></p></div><p>實際值與光滑後所得到的值的差值SMA n -x n 就可以作為特徵。</p><p>帶權重的移動平均算法：計算平均值的時候將不同的點帶上不同的數值。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7587831cc73c4874b65363cd46d4b374><p class=pgc-img-caption></p></div><p>指數移動平均算法：在已知時間序列的基礎上進行加權操作，而權重的大小是呈指數衰減的。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ef59ba3b1ee7446e8ee9d68373a36dca><p class=pgc-img-caption></p></div><p>只需要構建一個加權求和，然後計算EWMA n -x n 就可以得到特徵。</p><p>分類特徵：</p><p>可以根據時間序列的走勢，例如週期型、毛刺型、定時任務型來構造出時間序列的分類特徵，用於時間序列形狀的多分類問題。具有窗口大小的概率分佈、移動熵及小波分析特徵都很常見。在時間序列的分類特徵裡面，有一種特徵叫做 <strong>值分佈特徵</strong> 。假設時間序列的值域在[0,1]之內，值分佈特徵計算出一個時間序列X n = [x 1 ,...,x n ]的取值在[0,0.1),[0.1,0.2),...,[0.9,1]這十個桶的個數，進一步得到它們落入這十個桶的概率是多少。這一類特徵可以通過count函數生成。</p><p>3. 超參數優化。</p><p>超參數控制了算法的行為。優化以下超參數：epochs、distribution、activation和hidden layer size。可以在H2O文檔中找到每個參數的詳細說明並通過網格搜索(Grid Search)尋找最佳參數。</p><p>(2). 分類方法：預測在接下來的N步中是否有可能發生故障。</p><p>分類方法需要更多的數據，但可以提供有關何時發生故障的更多信息。這種方法不再提供剩餘的生命週期數，而是要預測機器是否會在接下來的30個週期內故障。我們會將故障視為正(positive，P)，而無故障視為常態(normal，N)。</p><p>預測精度的衡量標準：</p><p>準確率描述了預測正確的測試用例數與所有測試用例數的比例。當數據集中同一類別數據量太多，發生類不平衡時，只考慮準確率將會產生誤導(高準確率，但預測性能差)，精確率和召回率(檢出率)可以避免這個問題。精確率指的是預測為正值且實際也是正值的樣本數和所有被預測為正值的樣本數之間的比例；召回率指的是預測為正值的樣本數和應該被預測為正值的樣本數之間的比例。另外，F1分數是衡量準確率的指標，計算F1分數時需要兼顧精確率和召回率。對於準確率、召回率、精確率和F1分數，它們的值越接近1越好。</p><p>模型選擇：</p><p>深度學習分類模型。</p><p>算法步驟：</p><p>參考上述去噪、特徵工程及超參數優化等方法。</p><p>三. 工藝參數優化算法</p><p>這裡主要介紹集成學習算法及DRL算法。</p><p><strong>集成學習算法：</strong></p><p>(1). 集成學習。</p><p>集成學習分為兩種，Boosting(↓Bias & Variance)和Bagging(↓Variance)。</p><p>1. Boosting給固定樣本加權，串行地生成一系列“weak learners”，再學習樣本及分類器的權重，通過“weak learners”組合構造一個“strong learner”。迴歸算法可以通過殘差來實現：每一輪的訓練集發生變化(標籤變為了殘差)，即下一個模型要基於新訓練集進行學習，學習完畢後，將所有模型簡單疊加，就得到了最終模型。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0c3e505af4e9466698d9ef7e09bff5f5><p class=pgc-img-caption></p></div><p>儘管近年來神經網絡復興並大為流行，boosting算法在訓練樣本量有限、所需訓練時間較短、缺乏調參知識等場景依然有其不可或缺的優勢。</p><p>2. Bagging隨機有放回均勻取樣構造n個訓練集，並行訓練k個模型(決策樹、KNN等)。對於分類問題：由投票表決產生的分類結果；對於迴歸問題，由k個模型預測結果的均值作為最後預測的結果(所有模型的重要性相同)。例如，隨機森林，Random Forest。</p><p>(2). Adaboost。</p><p>Adaptive Boosting, 自適應增強算法，通過訓練多個弱分類器來組合得到一個強分類器，每次迭代會生成一棵 <strong>高Bias、低Variance</strong> 的樹形弱分類器。每一輪的訓練會更關注上一輪被分類器分錯的樣本，為其加大權重，具體做法是：1. 提高上一輪被錯誤分類的樣本的權值，降低被正確分類的樣本的權值；2. 線性加權求和。誤差率小的基學習器擁有較大的權值，誤差率大的基學習器擁有較小的權值。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/98ed7efd90264974906cad3fe639c5cf><p class=pgc-img-caption></p></div><p>sign內的符號決定了所預測的類，絕對值代表分類的確信度。</p><p>代碼實現：</p><pre><code>def adaboost_clf(Y_train, X_train, Y_test, X_test, M=20, weak_clf=DecisionTreeClassifier(max_depth = 1)):    n_train, n_test = len(X_train), len(X_test)    # Initialize weights    w = np.ones(n_train) / n_train    pred_train, pred_test = [np.zeros(n_train), np.zeros(n_test)]    for i in range(M):        # Fit a classifier with the specific weights        weak_clf.fit(X_train, Y_train, sample_weight = w)        pred_train_i = weak_clf.predict(X_train)        pred_test_i = weak_clf.predict(X_test)        # Indicator function        miss = [int(x) for x in (pred_train_i != Y_train)]        print("weak_clf_%02d train acc: %.4f"         % (i + 1, 1 - sum(miss) / n_train))        # Error        err_m = np.dot(w, miss)        # Alpha        alpha_m = 0.5 * np.log((1 - err_m) / float(err_m))        # New weights        miss2 = [x if x==1 else -1 for x in miss] # -1 * y_i * G(x_i): 1 / -1        w = np.multiply(w, np.exp([float(x) * alpha_m for x in miss2]))        w = w / sum(w)        # Add to prediction        pred_train_i = [1 if x == 1 else -1 for x in pred_train_i]        pred_test_i = [1 if x == 1 else -1 for x in pred_test_i]        pred_train = pred_train + np.multiply(alpha_m, pred_train_i)        pred_test = pred_test + np.multiply(alpha_m, pred_test_i)    pred_train = (pred_train &gt; 0) * 1    pred_test = (pred_test &gt; 0) * 1    print("train accuracy: %.4f" % (sum(pred_train == Y_train) / n_train))    print("test accuracy: %.4f" % (sum(pred_test == Y_test) / n_test))</code></pre><p>(3). GBDT。</p><p>Gradient Boosting Decision Tree，梯度提升樹，不同於Adaboost加大誤分樣本權重的策略，每一次的計算是都為了減少上一次的殘差，進而在殘差減少(負梯度)的方向上建立一個新的模型。無論是迴歸、二分類還是多分類問題(設定閾值)，GBDT中的決策樹都是迴歸樹(傳統GBDT使用CART)，因為每次迭代要擬合的是 <strong>梯度值</strong> ，是連續值。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8aec8e0004c441608812fc8aefaa4ee9><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d9653a4c11aa44b9950d20383535e436><p class=pgc-img-caption></p></div><p>模型一共訓練M輪，每輪產生一個弱分類器</p><p>T ( x ; θ m )</p><p>。我們希望損失函數能夠儘可能快的減小，沿著梯度方向下降。每輪迭代的時候，都去擬合損失函數在當前模型下的負梯度(殘差的近似值)，整體上，作為迴歸問題，擬合一個迴歸樹，進而發現多種有區分性的特徵以及特徵組合。</p><p>1. XGBoost。</p><p>全稱是eXtreme Gradient Boosting，是GBDT的改進版，能自動利用cpu的多線程。作為一個樹集成模型，XGBoost將K個樹對應的葉節點的值求和進行預測。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dff1ee3dec7642b9914eb8bcabdecfc5><p class=pgc-img-caption></p></div><p>我們的目標是學習這K個樹，因此，需要最小化帶正則項的目標函數：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/61d36164f34e41629999e043102edd7f><p class=pgc-img-caption></p></div><p>在XGBoost裡，每棵樹(f k )是一個一個往裡面加的：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9d6d5cd238e84ef79486e0815776f134><p class=pgc-img-caption></p></div><p>每加一個都希望效果能提升，即目標函數(就是損失)的值下降，如果經過t輪訓練我們做模型預測，之前的目標函數細化為：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/2258498a502447909e31e4ce7d3995a0><p class=pgc-img-caption></p></div><p>除了損失函數是square loss的情形，還是比較複雜。因此，用泰勒展開(取前三項，二階)近似原來的目標函數，這裡就與GBDT不同了，GBDT利用梯度，即一階導數來近似，精度顯然會差一些。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/8ec06e4087f94604b46626a6963ce313><p class=pgc-img-caption></p></div><p>其中，損失函數l可以是square或者logistic Loss，而第二部分Ω正則項(eg：L1和L2等)針對的是樹的複雜度。XGBoost對於樹的複雜度定義如下：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1495d6b77e6e40cc99ef4245e4eef0d8><p class=pgc-img-caption></p></div><p>我們來圖解一下：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3f65dba34858458f82ccafc5ccb48755><p class=pgc-img-caption></p></div><p>這裡面，樹的複雜度項包含兩個部分，葉子節點個數T以及葉子節點得分L2正則化項w(L2平滑避免過擬合)。我們移除常量，定義為每個葉節點 j 上面樣本集合I j = {i|q(x i ) = j}，並帶入上述複雜度函數到正則項中，得到新的目標函數：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/57732a58d2a24186ad7809f2a0dd22f7><p class=pgc-img-caption></p></div><p>g是一階導數，h是二階導數，T是葉節點個數，w是葉節點的分數。目標包含了T個相互獨立的單變量二次函數，化簡為：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cb8d668e751e4a12825975c02bd7c2b2><p class=pgc-img-caption></p></div><p>求w j 最優解(求導=0)並帶入目標函數，得到最終的目標函數：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d40dbb232d0144149ee6bb7f1fac4c4e><p class=pgc-img-caption></p></div><p>Obj表示在指定的樹結構下，我們在目標上最多減多少，因此又叫結構分數(structure score)，越小代表樹的結構越好。那麼如何尋找出一個最優結構的樹呢？XGBoost原始論文中給出了兩種分裂節點的方法。</p><p>Exact Greedy貪心算法：枚舉所有不同樹結構的貪心法。</p><p>枚舉所有可能的樹結構是不可行的。可以使用貪心算法，從單個葉子開始不斷增加分支(分割)，計算分割後左右兩側的導數和(提高效率)，再計算增益。評估候選分裂方式，尋找最優特徵及對應的值：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1f8684c6d2e140a798b9eecb29def5d6><p class=pgc-img-caption></p></div><p>引入新葉子的懲罰項，控制樹的複雜度，同時也作為閾值(增益大於閾值才分裂)，起到了 <strong>預剪枝</strong> 的作用。論文中的算法：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cc14f40911094dabab900dd3e173bf6a><p class=pgc-img-caption></p></div><p>但在連續特徵上枚舉所有可能的分割方案，計算損耗還是太大。為了更高效，XGBoost先使用 <strong>pre-sorted算法</strong> 對所有數據根據特徵數值進行預排序，然後在每次分割時，按順序訪問數據並根據增益找到每個特徵的最優分割點，最後找到最優特徵及分割點，將數據分裂成左右兩個子節點。</p><p>近似算法： 針對數據量太大的情況。</p><p>當數據無法完全加載進內存或是分佈式的情況下，貪心算法就不是特別有效了。</p><p>近似算法根據特徵分佈的百分位數(weighted quantile sketch算法使候選切分點在數據上均勻分佈)，提出候選切分點，然後將連續特徵映射到被這些候選切分點切分成的"buckets"中，聚集統計值，基於統計值推薦分割點，找到最佳方案。該算法有兩種形式：全局近似和局部近似，差別是，全局近似是在創建樹的初始階段提出所有候選切分點，所有的層都使用相同的proposal尋找切分點；局部近似在每個節點分裂之後重新提出候選切分點，這種改進對更深層的樹更合適。近似算法的流程：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/515736db61fa40278ce936b2cfbf9a6e><p class=pgc-img-caption></p></div><p>此外，稀疏自適應分割策略也可以用於分裂節點：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f5e4d4163e4f49019259b37449da9f45><p class=pgc-img-caption></p></div><p>除了在目標函數中引入正則項，XGBoost還引入了 <strong>縮減(shrinkage)</strong> 和 <strong>列抽樣(column subsampling)</strong> ，進一步防止過擬合。通過在每一步的boosting中引入縮減係數，降低每個樹和葉子對結果的影響；列抽樣是借鑑隨機森林中的思想，有時甚至比行抽樣效果更好，同時能夠加速計算。</p><p>綜上，基於貪心+二次最優化的策略，XGBoost算法流程可以總結為：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d0f0f68e86a84303b2798294a2d3dd72><p class=pgc-img-caption></p></div><p>2. LightGBM。</p><p>LigthGBM由微軟提供，也是對GBDT的高效實現，類似XGBoost，採用損失函數的負梯度作為當前決策樹的殘差近似值，去擬合新的決策樹。LightGBM比XGBoost快，內存佔用率低，準確率也有提升。</p><p>動機：</p><p>面對工業級海量的數據，傳統的boosting算法需要對每一個特徵都要掃描所有的樣本點來選擇最好的切分點，把整個訓練數據裝進內存會限制訓練數據的大小，不裝進內存，反覆讀寫訓練數據又非常耗時。</p><p>算法流程：</p><p>解決上述問題的直接方法就是減少特徵量和數據量而且不影響精確度。LightGBM結合使用了以下兩種算法：</p><p>1. <strong>GOSS</strong> (Gradient-based One-Side Sampling)： 基於梯度的單邊採樣，不使用所用的樣本點來計算梯度，而是對樣本進行採樣來計算梯度。</p><p>GBDT不能應用AdaBoost的樣本權重採樣，但每個數據都有不同的梯度值，丟掉梯度小、被學習得很好的數據，有助於採樣。為了避免這樣做帶來的精確度影響(數據分佈改變了)，引入GOSS算法。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/25047a44fbf9490fabf52a1db0626e77><p class=pgc-img-caption></p></div><p>步驟：</p><p>根據樣本點的梯度的絕對值對它們進行降序排序； --> 保留top a(大梯度數據的採樣率為a)個數據實例作為數據子集A，即前a*100%的樣本； --> 對於剩下的數據的實例隨機選取比例為b(小梯度數據的採樣率為b)的數據子集B，即b*(1-a)*100%個樣本點； --> 將小梯度樣本乘上權重係數(1-a)/b並與大梯度樣本合併來計算信息增益，學習一個新的弱學習器； --> 不斷重複之前的步驟直到達到規定的迭代次數或者收斂為止。</p><p>估計信息增益的公式如下：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9b0ad7d92ab04c6bade07af6d8a7fd0c><p class=pgc-img-caption></p></div><p>當a=0時，GOSS算法退化為隨機採樣算法；當a=1時，GOSS算法變為採取整個樣本的算法。算法在不改變數據分佈的前提減少了計算量、保證了精度，同時增加了基學習器的多樣性，提高了泛化能力。</p><p>2. <strong>EFB</strong> (Exclusive Feature Bundling)：互斥特徵捆綁，不使用所有的特徵來進行掃描獲得最佳切分點，而是將某些特徵捆綁在一起來降低特徵的維度。</p><p>LightGBM不僅進行了數據採樣，也進行了特徵抽樣，EFB將衝突比率(特徵不互斥程度)較小的特徵捆綁起來，使模型的訓練速度進一步提升。將特徵劃分為更小的互斥綁定集群是一個NP-hard問題(圖著色問題)，只能尋求允許小部分衝突的近似解。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/abc1a2777ebc4efc9d2c46a74a784a14><p class=pgc-img-caption></p></div><p>步驟：</p><p>構造一個圖(Graph)，特徵看作節點(node)，特徵之間的衝突值(cos夾角)作為邊(edge)的權值； -> 通過節點(特徵)在圖中的度(degree)來降序排序特徵/更高效的實現算法是將特徵按照非零值的個數進行排序； -> 遍歷每個特徵，並按閾值K(最大沖突數)合併特徵，分配給具有小衝突的現有bundle或創建新bundle。</p><p>不同於XGBoost所使用的pre-sorted排序算法，LightGBM最後一步關於互斥特徵的合併用到了直方圖(Histogram)算法：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6c7ddf4f08a549a29960db1b9dd3c91f><p class=pgc-img-caption></p></div><p>基本思想是把連續的特徵值離散成k個整數，並構造寬度為k的直方圖。遍歷數據時，以離散化後的值為索引累積直方圖統計量，遍歷一次數據後，根據直方圖的離散值，遍歷尋找最優的分割點。使用bin放棄了數據的細節特徵，相似的數據被劃分到相同的桶中，差異就消失了，同時，bin相當於增加了正則化，數量越少懲罰越嚴重，欠擬合風險越高。但對決策樹這樣的弱學習器的正則化，抵消了離散化的分裂點對最終分割精度的影響，有效防止了過擬合。而且，由於離散化，模型效率上有很大提升。</p><p>決策樹增長策略：</p><p>XGBoost採用的是Level-wise迭代方式，不加區分的一次分裂同一層的葉子，效率低下，可能產生不必要的葉結點。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8eb08b66da9642e2a326b40dc6ddfa61><p class=pgc-img-caption></p></div><p>LightGBM通過leaf-wise策略來生長樹。每次從當前所有葉子中，找到分裂增益最大的一個葉子，然後分裂，如此循環。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d24df2c07d8e465a8031b2350dbd432c><p class=pgc-img-caption></p></div><p>相比Level-wise，在分裂次數相同的情況下，leaf-wise可以降低更多的誤差，精度更好。但當樣本量較小的時候，leaf-wise需要引入額外的參數 max_depth 來限制樹的深度，避免過擬合。</p><p>此外LightGBm直接支持類別特徵，支持特徵並行和數據並行。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5d9cee7a44fc4680863ceae9ef6d06e1><p class=pgc-img-caption></p></div><p>3. CatBoost。</p><p>CatBoost(categorical boosting)是俄羅斯的搜索巨頭Yandex在2017年開源的機器學習庫。是一種能夠很好地處理類別型特徵的梯度提升算法。採用的策略在降低過擬合的同時保證所有數據集都可用於學習。</p><p>動機：</p><p>CatBoost算法的設計初衷是為了更好的處理GBDT特徵中的categorical features。類別數較少的特徵，一般利用one-hot編碼方法將特徵轉為數值型用。另一種簡單的方式是基於統計，用標籤的平均值來表示特徵並作為節點分裂的標準(Greedy Target-based Statistics)，但在訓練和測試集數據結構和分佈不同時，容易導致條件偏移問題，造成過擬合。</p><p>算法流程：</p><p>標準的改進Greedy TS的方式是添加先驗分佈項，這樣可以減少噪聲和低頻率數據對於數據分佈的影響。CatBoost給出了一種解決方案：</p><p>1. Ordered TS克服梯度偏差(prediction shift)；</p><p>在GBDT中，構建下一棵樹包含兩步，選擇樹的結構和設置葉子節點的值。基於當前模型中的相同的數據點，葉子節點的值都是被當做梯度或牛頓步長的近似值來計算，這樣會造成有偏的點態梯度估計(梯度在特徵空間的任何域中的分佈與該域中梯度的真實分佈相比發生了偏移)，導致過擬合。CatBoost和標準GDBT算法一樣， <strong>第一階段</strong> ，通過構建新樹來擬合當前模型的梯度，但在每一步，依靠目前已經觀察的樣本集，對所有樣本進行隨機排序，在不同的梯度提升步中使用不同的排列，即使用獨立的數據集，這樣就得到了無偏估計的模型。兩種模式的樹結構：Ordered/Plain。基本預測器是無關決策樹oblivious，將浮點型特徵、統計值及one-hot編碼的特徵二值化並放入向量中，利用二值化的特徵快速評分，計算模型的輸出。由於整個層次上使用相同的分割標準，oblivious樹是平衡的，不容易過擬合。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/db74cd1ab0834ec99b1e740466afdc48><p class=pgc-img-caption></p></div><p>Ordered模型在學習訓練的過程中，對於每個樣本，都單獨構建一個利用該樣本之前的樣本點的梯度估計得到的模型，針對這些模型，估計該樣本的梯度，然後利用新模型重新對樣本打分。在算法中每步迭代 中，都在一個隨機排列 的基礎上構建樹。 <strong>第二階段</strong> ，在所有樹結構都建立好的情況下，對兩種模式均採用標準梯度增強程序計算最終模型的葉子節點值。</p><p>2. 樣本的類別型特徵轉為數值型；</p><p>類別形變量需要使用隨機排列，根據排在該樣本之前的該類別標籤取均值作為節點分裂標準，並加入優先級和優先級的權重係數。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a48f3c6162324245859e2ac50b4b6887><p class=pgc-img-caption></p></div><p>其中P是添加的先驗項，a通常是大於0的權重係數。迴歸問題，一般先驗項可取數據集標籤的均值。對於二分類，先驗項是正例的先驗概率。</p><p>3. 特徵組合；</p><p>特徵組合可以得到一個新的強大的特徵，但受制於特徵數量，不可能考慮所有組合。當前樹構造新的分割點時，CatBoost會採用貪婪的策略考慮組合。對於樹的第一次分割，不考慮任何組合。後面的分割中將所有類別型特徵之間的組合考慮進來，組合後的特徵就會變成數值型的。對於數值型和類別型特徵的組合：樹選擇的所有分割點都被視為具有兩個值的類別型特徵，並且組合方式和類別型特徵一樣。</p><p>(4). 提升樹算法工業應用。</p><p>津南數字製造算法挑戰賽——賽場一：原料企業工藝優化。</p><p>https://tianchi.aliyun.com/competition/entrance/231695/introduction?spm=5176.12281915.0.0.678010bdUCRYjV</p><p>季軍方案：nlceyes</p><p>https://github.com/nlceyes/TianChi-JinNan</p><p>DRL算法：</p><p>(1). 組合優化。</p><p>運用深度強化學習求解組合優化問題是目前非常火熱的一個研究方向，強化學習天生就是做序列決策用的，組合優化問題裡邊的序列決策問題完全也可以用強化學習來直接求解，其難點是怎麼定義state, reward。比較經典的TSP(Traveling Salesman Problem旅行商問題)和VRP(Vehicle Routing Problem車輛路徑問題)主要的思路是encoder + decoder。encoder有很多方法，graph embedding, attention, glimpse, multi-head等，decoder也同樣。在工業領域，例如對於Job shop問題(加工車間調度問題)就是決定以什麼順序在機器上加工工件。阿里菜鳥物流人工智能部，使用深度強化學習方法求解一類新型三維裝箱問題(將若干個長方體物體逐個放入一個箱子中並最小化能夠容納所有物品的箱子的表面積)，相對於已有的啟發式算法，獲得大約5%的效果提升。</p><p>1. Pointer Network：</p><p>Pointer Networks是一種seq2seq模型，由Google Brain和UC Berkeley聯合發表。傳統的seq2seq模型無法解決輸出序列的詞彙表會隨著輸入序列長度的改變而改變的問題。特定情況下，我們希望輸出是輸入集合的子集，而Ptr-net在seq2seq基礎上引入attention機制並做了簡化(傳統attention計算權重後對encoder的state進行加權得到一個向量；Pointer Networks計算權重後選擇概率最大的encoder state作為輸出)，類似編程語言中的指針(每個指針對應輸入序列的一個元素，可以直接操作輸入序列而不需要特意設定輸出詞彙表)，克服了seq2seq模型中“輸出嚴重依賴輸入”的問題。網絡對比如下：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a92cc2e46a57400d85d23f4b09c6746d><p class=pgc-img-caption></p></div><p>seq2seq：</p><p>seq2seq屬於encoder-decoder結構的一種，以LSTM為基本單元，一個RNN作為encoder將輸入序列壓縮成指定長度的語義向量，另一個RNN作為decoder根據語義向量(只作為初始狀態參與運算，與後面的運算無關)生成指定的序列，詳細結構如下：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6a7dcfc61a2148bfaff3ce466c020ffd><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d8139c8a2f5c41729498435fa91d85f1><p class=pgc-img-caption></p></div><p>Attention Mechanism：</p><p>Attention Mechanism可以幫助模型對輸入的X每個部分賦予不同的權重，抽取出更加關鍵及重要的信息，使模型做出更加準確的判斷，同時不會對模型的計算和存儲帶來更大的開銷 。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3f18a0bc1f584320b820d7dd7df9d50f><p class=pgc-img-caption></p></div><p>沒加入Attention機制時，Y1 = f(C)；Y2 = f(C,Y1)；Y3 = f(C,Y1,Y2)，各階段的輸出Yi用的都是同一個中間語義c表示，而C是由輸入的每個元素經過Encoder編碼產生的，即C = F(x1,x2,x3,x4)，輸入序列中的任意單詞對目標Yi的影響力是一樣的。此外，Encoder不論接收多長的序列，最後輸出都是一箇中間語義向量C，語句很長時C的表達能力堪憂。引入Attention的Encoder-Decoder框架，每一個輸出Yi用不同的中間向量Ci表示，反應每個輸入元素不同的影響，即Y1 = f(C1)；Y2 = f(C2,Y1)；Y3 = f(C3,Y1,Y2)。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/34fa4993c89f49ec95824f23b92f0daa><p class=pgc-img-caption></p></div><p>Ci定義為：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/2e324d0e76864a9194efecba1098048d><p class=pgc-img-caption></p></div><p>其中L x 表示輸入source長度，a ij 表示輸出第i個元素時source輸入序列中第j個元素的注意力分配係數，h j 是Source輸入序列中第j個元素的語義編碼。a ij 的概率分佈值計算方法如下：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e2fa010bd74d40629d38881ea4672b64><p class=pgc-img-caption></p></div><p>在時刻i生成Yi，i-1時刻隱藏層節點的輸出值H i-1 是已知的，計算H i-1 和Encoder的 <strong>每個</strong> 元素對應的RNN隱藏層節點狀態h j 的相似性F(h j ,H i-1 )來獲得“對齊”可能性。函數F可以採用向量點積、Cosine相似性或MLP網絡，輸出經過Softmax進行歸一化就得到了注意力分配係數的概率分佈值。綜上，Attention機制的本質思想可以表示為：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/dae23b32153d403996399cc27def28ce><p class=pgc-img-caption></p></div><p>將Source中的構成元素看作一系列&lt;Key,Value>數據對，Target中的某個元素看作Query，通過計算Query和各個Key的相似性得到每個Key的權重係數，再對Value進行加權求和，最終的得到Attention vector。</p><p>Self Attention：</p><p>又稱Intra(內部) Attention。在機器翻譯場景中，可以捕獲同一個句子中單詞之間的一些句法特徵或者語義特徵，更容易捕獲句子中長距離的相互依賴的特徵，同時增加計算的並行性。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ffdd657a88584a53bd58a9ea547d5492><p class=pgc-img-caption></p></div><p>在圖像描述任務中，輸入一張圖片，AI系統輸出一句描述，語義等價地描述圖片所示內容。可以使用Encoder-Decoder框架來解決，Encoder輸入一張圖片，提取特徵後，Decoder使用RNN或LSTM來輸出自然語言句子。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/ac6baf55d89749c89581006e3a8a6e26><p class=pgc-img-caption></p></div><p>這裡引入Attention機制，輸出某個實體單詞時會將注意力聚焦在圖片中相應的區域上，能顯著提升系統輸出效果。</p><p>Pointer Networks：</p><p>傳統Attention機制的公式：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/05ceefda51f04461aeaa06d2e78183f2><p class=pgc-img-caption></p></div><p>首先整合Encoder和Decoder的隱式狀態，再學習分配給輸入序列的權重係數a，最後加權求和得到vector來預測下一個輸出。Ptr-net沒有最後一個公式，直接將softmax結果當成輸出，以指針的形式指向輸入序列中最有可能是輸出的元素。</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/08d42e955d164c419b48a6fe13b3a820><p class=pgc-img-caption></p></div><p>Ptr-net很適合用來直接複製輸入序列中的某些元素給輸出序列，本文提到的組合優化場景剛好符合。</p><p>應用實踐：</p><p>https://github.com/higgsfield/np-hard-deep-reinforcement-learning</p><p>(2). 工藝參數推薦。</p><p>激光機工藝推薦、輔助波形調試、焚燒爐工藝優化等場景，需要逐臺進行調試，現場調試技術複雜，有經驗的技術人員的培養週期長。基於圖嵌入的AI模型自動調參顯著提升效率，節約成本。</p><p>1. GNN(圖神經網絡)：</p><div class=pgc-img><img alt=工業大數據：分析算法 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c449eb40fadb4de1bee42f16cc31569a><p class=pgc-img-caption></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>工業</a></li><li><a>大數據</a></li><li><a>算法</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/8e9361b4.html alt=工業大數據漫談12：實時數據庫與時序數據庫 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d91979d4c9c045cdb2fbe0f2665434a0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8e9361b4.html title=工業大數據漫談12：實時數據庫與時序數據庫>工業大數據漫談12：實時數據庫與時序數據庫</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/988ed64.html alt=把握新基建機遇，用好工業大數據助力武漢產業升級 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/988ed64.html title=把握新基建機遇，用好工業大數據助力武漢產業升級>把握新基建機遇，用好工業大數據助力武漢產業升級</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3d984946.html alt=高溫合金：工業皇冠上的明珠材料，軍民兩用共築需求 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/958662b940ed4fa9b2fa0ab5d3e5dd12 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3d984946.html title=高溫合金：工業皇冠上的明珠材料，軍民兩用共築需求>高溫合金：工業皇冠上的明珠材料，軍民兩用共築需求</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c08f7fcf.html alt=寶信軟件：公司聚焦大數據、人工智能、虛擬製造等七大關鍵技術的研究 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c08f7fcf.html title=寶信軟件：公司聚焦大數據、人工智能、虛擬製造等七大關鍵技術的研究>寶信軟件：公司聚焦大數據、人工智能、虛擬製造等七大關鍵技術的研究</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a667cf12.html alt=工業VR應用於智能製造升級、虛擬展示、虛擬裝配、虛擬製造 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a667cf12.html title=工業VR應用於智能製造升級、虛擬展示、虛擬裝配、虛擬製造>工業VR應用於智能製造升級、虛擬展示、虛擬裝配、虛擬製造</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c84c52b4.html alt="5G時代下的工業互聯網機遇 工廠造“數字雙胞胎”" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c84c52b4.html title="5G時代下的工業互聯網機遇 工廠造“數字雙胞胎”">5G時代下的工業互聯網機遇 工廠造“數字雙胞胎”</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/eabe8477.html alt=工業循環水設備進出水壓差 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/19f1a7a78ff244b384c18bd684d62292 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/eabe8477.html title=工業循環水設備進出水壓差>工業循環水設備進出水壓差</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/00d3b405.html alt=聚隆科技：公司工業機器人減速器產品主要有RV減速器和諧波減速器兩大類 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/00d3b405.html title=聚隆科技：公司工業機器人減速器產品主要有RV減速器和諧波減速器兩大類>聚隆科技：公司工業機器人減速器產品主要有RV減速器和諧波減速器兩大類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4f926c17.html alt=施工組織設計（工業廠房） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/4f7e8577-92ca-4a74-be97-b0c5a531eea1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4f926c17.html title=施工組織設計（工業廠房）>施工組織設計（工業廠房）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ed4abe39.html alt=算法小專欄：散列表（二） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a831970b0ccf4e4cbe591777ebd3f2a3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ed4abe39.html title=算法小專欄：散列表（二）>算法小專欄：散列表（二）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6c016da0.html alt=利用工業以太網連接技術加速向工業4.0過渡 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/9df1e18568e44732872ade38dc2deb40 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6c016da0.html title=利用工業以太網連接技術加速向工業4.0過渡>利用工業以太網連接技術加速向工業4.0過渡</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/69d23a45.html alt=工業控制常用的接口協議大全 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/2fb460305abe4cbaa0210943132dca15 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/69d23a45.html title=工業控制常用的接口協議大全>工業控制常用的接口協議大全</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0831f900.html alt=航空工業洪都曾國平與“小六”的二十餘載情緣 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/Ry6bzK96MDM3jZ style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0831f900.html title=航空工業洪都曾國平與“小六”的二十餘載情緣>航空工業洪都曾國平與“小六”的二十餘載情緣</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/38aea254.html alt=七大查找算法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/15393515221731c57aa8da1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/38aea254.html title=七大查找算法>七大查找算法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/51e4e55b.html alt=掌握算法-散列 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7fe8d19cb78241e999d77102bee7e16c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/51e4e55b.html title=掌握算法-散列>掌握算法-散列</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>