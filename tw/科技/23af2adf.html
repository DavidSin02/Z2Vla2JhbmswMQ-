<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍 | 极客快訊</title><meta property="og:title" content="中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/RcGwzyuCkrlJ0I"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/23af2adf.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/23af2adf.html><meta property="article:published_time" content="2020-11-14T21:02:26+08:00"><meta property="article:modified_time" content="2020-11-14T21:02:26+08:00"><meta name=Keywords content><meta name=description content="中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/23af2adf.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><img alt=中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RcGwzyuCkrlJ0I><p class=pgc-img-caption>（圖片由AI科技大本營付費下載自視覺中國）</p><p>作者 | 徐亮（實在智能算法專家)</p><p>來源 | AINLP（ID：nlpjob）</p><p>谷歌ALBERT論文剛剛出爐一週，中文預訓練ALBERT模型來了，感興趣的同學可以直接嚐鮮試用。</p><p>項目鏈接：</p><p>https://github<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">.com</i>/brightmart/albert_zh</p><p>An Implementation of A Lite Bert For Self-Supervised Learning Language Representations with TensorFlow.</p><p>ALBert is based on Bert, but with some improvements. It achieves state of the art performance on main benchmarks with 30% parameters less.</p><p>For albert_base_zh it only has ten percentage parameters compare of original bert model, and main accuracy is retained.</p><p>Chinese version of ALBERT pre-trained model, including checkpoints both for TensorFlow and PyTorch, will be available.</p><p>海量中文語料上預訓練ALBERT模型：參數更少，效果更好。預訓練小模型也能拿下13項NLP任務，ALBERT三大改造登頂GLUE基準。</p><p><strong>***** 2019-10-02: albert_large_zh *****</strong></p><p>Relased albert_large_zh with only 16% parameters of bert_base(<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">64</i>M)</p><p><strong>***** 2019-10-01: albert_base_zh *****</strong></p><p>Relesed albert_base_zh with only 10% parameters of bert_base, a small model(40M) & training can be very fast.</p><p><strong>***** 2019-09-28: codes and test functions *****</strong></p><p>A<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">dd</i> codes and test functions for three main changes of albert from bert</p><p><strong>模型下載 Download Pre-trained Models of Chinese</strong></p><p>1、albert_large_zh,參數量，層數24，大小為<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">64</i>M</p><pre><p>參數量和模型大小為bert_base的六分之一；在口語化描述相似性數據集LCQMC的測試集上相比bert_base上升0.2個點</p></pre><pre>2、albert_base_zh(小模型體驗版)， 參數量12M，層數12，大小為40M<br></pre><pre><pre><p>參數量為bert_base的十分之一，模型大小也十分之一；在口語化描述相似性數據集LCQMC的測試集上相比bert_base下降約1個點；</p><p>相比未預訓練，albert_base提升14個點</p></pre><br></pre><p>3、albert_xlarge、 albert_xxlarge will coming recently.</p><pre><p>if you want use a albert model with best performance among all pre-trained models, just wait a few days.</p></pre><pre><div><p>ALBERT模型介紹 Introduction of ALBERT</p></div></pre><p>ALBERT模型是BERT的改進版，與最近其他State of the art的模型不同的是，這次是預訓練小模型，效果更好、參數更少。</p><p><strong>它對BERT進行了三個改造 Three main changes of ALBert from Bert：</strong></p><p>1）詞嵌入向量參數的因式分解 Factorized embe<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">dd</i>ing parameterization</p><pre><br><pre><p>O(V * H) to O(V * E + E * H)</p><p>如以ALBert_xxlarge為例，V=30000, H=4096, E=128</p><p>那麼原先參數為V * H= 30000 * 4096 = 1.23億個參數，現在則為V * E + E * H = 30000*128+128*4096 = 384萬 + 52萬 = 436萬，</p><p>詞嵌入相關的參數變化前是變換後的28倍。</p></pre><br></pre><p>2）跨層參數共享 Cross-Layer Parameter Sharing</p><pre><p>參數共享能顯著減少參數。共享可以分為全連接層、注意力層的參數共享；注意力層的參數對效果的減弱影響小一點。</p></pre><pre>3）段落連續性任務 Inter-sentence coherence loss.<br></pre><pre><pre><p>使用段落連續性任務。正例，使用從一個文檔中連續的兩個文本段落；負例，使用從一個文檔中連續的兩個文本段落，但位置調換了。</p><p>避免使用原有的NSP任務，原有的任務包含隱含了預測主題這類過於簡單的任務。</p><p>We maintain that inter-sentence modeling is an important aspect of language understanding, but we propose a loss </p><p>based primarily on coherence. That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic </p><p>prediction and instead focuses on modeling inter-sentence coherence. The SOP loss uses as positive examples the </p><p>same technique as BERT (two consecutive segments from the same document), and as negative examples the same two </p><p>consecutive segments but with their order swapped. This forces the model to learn finer-grained distinctions about</p><p>discourse-level coherence properties. </p></pre><p>其他變化，還有 Other changes：</p><pre><p>1）去掉了dropout Remvoe dropout to enlarge capacity of model.</p><p>最大的模型，訓練了1百萬步後，還是沒有過擬合訓練數據。說明模型的容量還可以更大，就移除了dropout</p><p>（dropout可以認為是隨機的去掉網絡中的一部分，同時使網絡變小一些）</p><p>We also note that, even after training for 1M steps, our largest models still do not overfit to their training data. </p><p>As a result, we decide to remove dropout to further increase our model capacity.</p><p>其他型號的模型，在我們的實現中我們還是會保留原始的dropout的比例，防止模型對訓練數據的過擬合。</p><p>2）為加快訓練速度，使用LAMB做為優化器 Use lAMB as optimizer, to train with big batch size</p><p>使用了大的batch_size來訓練(4096)。LAMB優化器使得我們可以訓練，特別大的批次batch_size，如高達6萬。</p><p>3）使用n-gram(uni-gram,bi-gram, tri-gram）來做遮蔽語言模型 Use n-gram as make language model</p><p>即以不同的概率使用n-gram,uni-gram的概率最大，bi-gram其次，tri-gram概率最小。</p><p>本項目中目前使用的是在中文上做whole word mask，稍後會更新一下與n-gram mask的效果對比。n-gram從spanBERT中來。</p></pre></pre><p></p><h1 toutiao-origin=h2>發佈計劃 Release <i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">Plan</i></h1><p>1、albert_base，參數量12M, 層數12，10月7號</p><p>2、albert_large，參數量18M, 層數24，10月13號</p><p>3、albert_xlarge，參數量59M, 層數24，10月6號</p><p>4、albert_xxlarge，參數量233M, 層數12，10月7號（效果最佳的模型）</p><p><strong>訓練語料/訓練配置 Training Data & Configuration</strong></p><p>30g中文語料，超過100億漢字，包括多個百科、新聞、互動社區。</p><p>預訓練序列長度sequence_length設置為512，批次batch_size為4096，訓練產生了3.5億個訓練數據(instance)；每一個模型默認會訓練125k步，albert_xxlarge將訓練更久。</p><p>作為比較，roberta_zh預訓練產生了2.5億個訓練數據、序列長度為256。由於albert_zh預訓練生成的訓練數據更多、使用的序列長度更長，</p><pre><br><pre><p>我們預計albert_zh會有比roberta_zh更好的性能表現，並且能更好處理較長的文本。</p></pre><br></pre><p>訓練使用TPU v3 Pod，我們使用的是v3-256，它包含32個v3-8。每個v3-8機器，含有128G的顯存。</p><p><strong>模型性能與對比(英文) Performance and Comparision</strong></p><img alt=中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Re42wuX547Yk5y><img alt=中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Re42wv55YS5Qv4><img alt=中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Re42wvUA3u7Dj3><p><strong>中文任務集上效果對比測試 Performance on Chinese datasets</strong></p><ul><li><h2 toutiao-origin=h3><strong>自然語言推斷：XNLI of Chinese Version</strong></h2></li></ul><img alt=中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Re42wvpDiRUbWB><p>注：BERT-wwm-ext來自於這裡；XLNet來自於這裡; RoBERTa-zh-base，指12層RoBERTa中文模型</p><ul><li><p><strong>問題匹配語任務：LCQMC(Sentence Pair Matching)</strong></p></li></ul><img alt=中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Re42x0V17WQ0MB><ul><li><p><strong>語言模型、文本段預測準確性、訓練時間 Mask Language Model Accuarcy & Training Time</strong></p></li></ul><img alt=中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Re42x0r380iJUY><p>注：? 將很快替換</p><p></p><h1 toutiao-origin=h2>模型參數和配置 Configuration of Models</h1><img alt=中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Re42x16DoN9bGp><p><strong>代碼實現和測試 Implementation and Code Testing</strong></p><p>通過運行以下命令測試主要的改進點，包括但不限於詞嵌入向量參數的因式分解、跨層參數共享、段落連續性任務等。</p><pre><p>python <strong class=highlight-text toutiao-origin=span>test_changes</strong><strong class=highlight-text toutiao-origin=span>.py</strong></p></pre><p></p><h1 toutiao-origin=h2>預訓練 Pre-training</h1><ul><li><p><strong>生成特定格式的文件(tfrecords) Generate tfrecords Files</strong></p></li></ul><p>運行以下命令即可。項目自動了一個示例的文本文件(data/news_zh_1.txt)</p><pre><br><pre><p>bash <strong class=highlight-text toutiao-origin=span>create_pretrain_data</strong><strong class=highlight-text toutiao-origin=span>.sh</strong></p></pre><br><p>如果你有很多文本文件，可以通過傳入參數的方式，生成多個特定格式的文件(tfrecords）</p></pre><p></p><h2 toutiao-origin=h4>執行預訓練 pre-training on GPU/TPU</h2><pre><br><pre><p>GPU:</p><p>export BERT_BASE_DIR=albert_config</p><p>nohup python3 run_pretraining.py --input_file=./data/tf*.tfrecord \</p><p>--output_dir=my_new_model_path --do_train=True --do_eval=True --bert_config_file=$BERT_BASE_DIR/albert_config_xxlarge.json \</p><p>--train_batch_size=4096 --max_seq_length=512 --max_predictions_per_seq=76 \</p><p>--num_train_steps=125000 --num_warmup_steps=12500 --learning_rate=0.00<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-4">176</i> \</p><p>--save_checkpoints_steps=2000 --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt &amp;</p><p>TPU, a<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">dd</i> following information:</p><p>--use_tpu=True --tpu_name=grpc://10.240.1.66:8470 --tpu_zone=us-central1-a</p><p>注：如果你從頭開始訓練，可以不指定init_checkpoint；</p><p>如果你從現有的模型基礎上訓練，指定一下BERT_BASE_DIR的路徑，並確保bert_config_file和init_checkpoint兩個參數的值能對應到相應的文件上；</p><p>領域上的預訓練，根據數據的大小，可以不用訓練特別久。</p></pre><br></pre><p></p><h1 toutiao-origin=h2>下游任務 Fine-tuning</h1><p>以使用albert_base做LCQMC任務為例。LCQMC任務是在口語化描述的數據集上做文本的相似性預測。</p><p>下載LCQMC數據集，包含訓練、驗證和測試集，訓練集包含24萬口語化描述的中文句子對，標籤為1或0。1為句子語義相似，0為語義不相似。</p><p>通過運行下列命令做LCQMC數據集上的fine-tuning:</p><pre><br><pre><div><p>1. Clone this project:</p><p>git clone https://github<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">.com</i>/brightmart/albert_zh.git</p><p>2. Fine-tuning by running the following command：</p><p>export BERT_BASE_DIR=./albert_large_zh</p><p>export TEXT_DIR=./lcqmc</p><p>nohup python3 run_classifier.py --task_name=lcqmc_pair --do_train=False --do_eval=true --data_dir=$TEXT_DIR --vocab_file=./albert_config/vocab.txt \</p><p>--bert_config_file=./albert_config/albert_config_large.json --max_seq_length=128 --train_batch_size=<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">64</i> --learning_rate=2e-5 --num_train_epochs=3 \</p><p>--output_dir=albert_large_lcqmc_checkpoints --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt &amp;</p><p>Notice/注：</p><p>you need to download pre-trained chinese albert model, and also download LCQMC dataset </p><p>你需要下載預訓練的模型，並放入到項目當前項目，假設目錄名稱為albert_large_zh; 需要下載LCQMC數據集，並放入到當前項目，</p><p>假設數據集目錄名稱為lcqmc</p></div></pre><br></pre><p></p><h2 toutiao-origin=h4>Reference</h2><p>1、ALBERT: A Lite BERT For Self-Supervised Learning Of Language Representations</p><p>2、BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p><p>3、SpanBERT: Improving Pre-training by Representing and Predicting Spans</p><p>4、RoBERTa: A Robustly Optimized BERT Pretraining Approach</p><p>5、Large Batch Optimization for Deep Learning: Training BERT in 76 minutes(LAMB)</p><p>6、LAMB Optimizer,TensorFlow version</p><p><em>（*本文為 AI科技大本營轉載文章，轉</em><em>載請聯繫原作者）</em></p><p>◆</p><p>◆</p><p>2019 中國大數據技術大會（BDTC）歷經十一載，再度火熱來襲！豪華主席陣容及百位技術專家齊聚，15 場精選專題技術和行業論壇，超強幹貨+技術剖析+行業實踐立體解讀，深入解析熱門技術在行業中的實踐落地。<strong class=highlight-text toutiao-origin=span>【早鳥票】</strong>與<strong class=highlight-text toutiao-origin=span>【特惠學生票】</strong>限時搶購，掃碼瞭解詳情！</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>模型</a></li><li><a>預訓練</a></li><li><a>ALBERT</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/e1360dd4.html alt=「Linux」高併發服務器模型（多進程和多線程實例模型） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/170e1596c32348f39d6ace1f327e45d5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e1360dd4.html title=「Linux」高併發服務器模型（多進程和多線程實例模型）>「Linux」高併發服務器模型（多進程和多線程實例模型）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4f2fe4db.html alt=全等三角形八大模型——邊邊角模型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/66b90004d37cea41ea93 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4f2fe4db.html title=全等三角形八大模型——邊邊角模型>全等三角形八大模型——邊邊角模型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b7d18c2c.html alt=「思維模型」#生物學：6.複製 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3ea4498d170b4e65b4c5c8b113bf9924 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b7d18c2c.html title=「思維模型」#生物學：6.複製>「思維模型」#生物學：6.複製</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/52db9979.html alt="基於 Python 的時序模型——AMIRA模型" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1531801339038572a99eb47 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/52db9979.html title="基於 Python 的時序模型——AMIRA模型">基於 Python 的時序模型——AMIRA模型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/86431765.html alt=時間序列模型怎麼畫？乾貨分享高顏值模型圖軟件 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/24e3eb9f18f746d2ae3a4e98a1fbaa60 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/86431765.html title=時間序列模型怎麼畫？乾貨分享高顏值模型圖軟件>時間序列模型怎麼畫？乾貨分享高顏值模型圖軟件</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a35add78.html alt=RFM模型是什麼？輕鬆製作好看模型圖軟件 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7217744eab464b748da50bb57645caa1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a35add78.html title=RFM模型是什麼？輕鬆製作好看模型圖軟件>RFM模型是什麼？輕鬆製作好看模型圖軟件</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d3abf3a2.html alt="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/Rjm7XFREsRlxK style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d3abf3a2.html title="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能">華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c677e605.html alt=軟件的質量模型（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8aaa2a8120704809af7e859613470b4b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c677e605.html title=軟件的質量模型（一）>軟件的質量模型（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a16de757.html alt=44思維模型：三重心智模型一為什麼聰明人也會做蠢事 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/6476a6326d854588bb6686c54f4242f1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a16de757.html title=44思維模型：三重心智模型一為什麼聰明人也會做蠢事>44思維模型：三重心智模型一為什麼聰明人也會做蠢事</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/6fd71411.html alt=“專業塑造，品質模型”長沙市模型公司優秀企業推薦 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/493a856786464f46a4c4da2226bca508 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/6fd71411.html title=“專業塑造，品質模型”長沙市模型公司優秀企業推薦>“專業塑造，品質模型”長沙市模型公司優秀企業推薦</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/d3d5884b.html alt=商之道：利用飛輪模型來推演現代商業系統模型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3236576184584329aba7ce3bc377fdb4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/d3d5884b.html title=商之道：利用飛輪模型來推演現代商業系統模型>商之道：利用飛輪模型來推演現代商業系統模型</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/31276977.html alt="疾病模型國際研討會在粵召開 跨領域探討疾病模型發展" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2b26db4c9e8141ff9346c9c6e5fc64a4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/31276977.html title="疾病模型國際研討會在粵召開 跨領域探討疾病模型發展">疾病模型國際研討會在粵召開 跨領域探討疾病模型發展</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/979ea406.html alt=現代計算機模型-J.U.C併發系列（1） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/0a2ce00c253f402cbcd3c7a9969d2543 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/979ea406.html title=現代計算機模型-J.U.C併發系列（1）>現代計算機模型-J.U.C併發系列（1）</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/6c823316.html alt=初中必會幾何模型（口訣突破）：手拉手模型（或旋轉型） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/af3d6fa7183a47a98414eb4410772850 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/6c823316.html title=初中必會幾何模型（口訣突破）：手拉手模型（或旋轉型）>初中必會幾何模型（口訣突破）：手拉手模型（或旋轉型）</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/748d2c3e.html alt="34套恐龍模型合集 Dinosaurs with Rig恐龍模型" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/23af0714027345c4b03b00080d9e7408 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/748d2c3e.html title="34套恐龍模型合集 Dinosaurs with Rig恐龍模型">34套恐龍模型合集 Dinosaurs with Rig恐龍模型</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>