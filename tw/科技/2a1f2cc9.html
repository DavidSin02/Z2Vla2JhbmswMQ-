<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>谷歌Transformer再升級—新模型實現性能、速度雙提升 | 极客快訊</title><meta property="og:title" content="谷歌Transformer再升級—新模型實現性能、速度雙提升 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/aa63807597454a2e875c855491eb7d82"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a1f2cc9.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a1f2cc9.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2a1f2cc9.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a1f2cc9.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a1f2cc9.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2a1f2cc9.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2a1f2cc9.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a1f2cc9.html><meta property="article:published_time" content="2020-11-14T21:05:18+08:00"><meta property="article:modified_time" content="2020-11-14T21:05:18+08:00"><meta name=Keywords content><meta name=description content="谷歌Transformer再升級—新模型實現性能、速度雙提升"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/2a1f2cc9.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>谷歌Transformer再升級—新模型實現性能、速度雙提升</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>當我們在翻譯軟件上輸入 “Transformer is a novel neural network architecture based on a self-attention mechanism” 後，計算機就可以迅速將它翻譯為 “Transformer 是一種基於自注意力機制的新型神經網絡架構”，神奇的機器翻譯使得多語種互譯成為可能。</p><p><br>近年來，得益於機器學習的快速發展，自然語言處理（NLP）技術不斷突破，在人機交互、在線翻譯工具等領域的應用層出不窮，不同語種的人與人、人與機器之間的無障礙自由交流得以實現。</p><p><br>當前的主流機器翻譯主要是基於神經網絡機器翻譯，這類方法是一個 “編碼器-解碼器”（encoder-decoder）架構的系統，編碼器對源語言序列進行編碼，並提取信息，然後通過解碼器把信息轉換為目標語言，完成語言翻譯過程。</p><p><br>自 2017 年問世以來，基於“編碼器-解碼器”架構設計的 Transformer 模型憑藉其優越的性能，已然成為機器翻譯領域的主流模型，在深度學習領域產生了巨大影響。</p><div class=pgc-img><img alt=谷歌Transformer再升級—新模型實現性能、速度雙提升 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/aa63807597454a2e875c855491eb7d82><p class=pgc-img-caption></p></div><p>然而，Transformer 模型並非完美，模型引入self-attention機制雖實現了快速並行的訓練，但在長序列文本的處理問題上，卻需要佔據大量計算資源，導致模型訓練成本提高。</p><p><br>近日，由 Google、劍橋大學、DeepMind 和<span style="color:#4d5156;--tt-darkmode-color: #8E939A">艾倫·圖靈研究院（Alan Turing Institute）</span>的研究人員組成的團隊基於正交隨機特徵的快速注意力（Fast Attention Via Positive Orthogonal Random Features，FAVOR+）機制，提出了一種新的 Transformer 模型——Performer。相比於 Transformer 模型，新模型無需做出過度調整就可以變得更加高效和節能。</p><p><br></p><h1 class=pgc-h-arrow-right>Performer 模型的技術突破<br></h1><p>2017 年，谷歌大腦（Google Brain）的 Ashish Vaswani 等人發表了一篇題為 “Attention Is All You Need” 的論文，<span style="color:#ab1942;--tt-darkmode-color: #DF285B">首次提出一種基於自注意力機制的 Transformer 模型</span>。</p><p><br></p><p><br>Transformer 模型顛覆了傳統神經網絡的架構，彌補了卷積神經網絡（CNN）和遞歸神經網絡（RNN）存在的不足，在語義特徵提取、長距離特徵捕獲、任務綜合特徵抽取等自然語言處理方面表現出了更優的性能，在自然語言處理、人機對話、圖像處理等許多領域都達到了當時最好的水平（<span style="color:#4c4c4c;--tt-darkmode-color: #9A9A9A">SOTA</span>）。</p><p><br>Transformer 架構的核心模塊是自注意力模塊，模型在處理每個單詞（輸入序列中的每個位置）時，自注意力模塊通過計算輸入序列中所有位置對的相似度分數，來尋找能夠幫助更好地編碼該單詞的線索。</p><p><br>然而，隨著輸入序列長度的增加，模型需要二次方的計算時間來產生所有相似度分數，所需計算內存也隨之增加，注意力機制面臨的效率問題也越來越突出。</p><p><br>針對那些需要長距離關注的應用，在 Transformer 基礎上已經有一些研究者提出了幾種快速的、空間利用率高的改進方法，但是大部分常見方法都依賴於稀疏注意力機制。</p><p><br>然而，稀疏注意力機制仍存在一定的侷限性。<br>（1）它們需要高效的稀疏矩陣乘法運算，而這些運算並不是在所有加速器上都能實現的；</p><p>（2）它們通常不能為其表示能力提供嚴格的理論保證；</p><p>（3）它們主要針對 Transformer 模型和生成式預訓練進行優化；</p><p>（4）它們通常會疊加更多的注意力層來補償稀疏表示，這使得它們很難與其他預訓練模型一起使用，因此需要重新訓練並消耗大量能量。</p><p><br>此外，稀疏注意機制通常仍然不足以解決常規注意方法應用的全部問題，如指針網絡。還有一些運算不能被稀疏化，如在工業級推薦系統中被大量應用的 softmax 運算。</p><p><br>Performer 使用了一個高效的（線性）廣義注意力框架，能夠對常規（softmax）全階注意力進行可證明的、準確的、實用的估計，不依賴於任何稀疏性或低階等先驗條件，從而實現更快的訓練速度，同時允許模型處理更長的序列，這一特性恰恰滿足了 ImageNet64 圖像數據集和PG-19文本數據集的要求。</p><p><br>Performer 模型通過正交隨機特徵（FAVOR+）算法實現快速注意力機制，並改用 Positive Orthogonal Random Features 估計 softmax 和高斯核函數，以實現在 FAVOR+ 機制中對常規 softmax 注意力進行魯棒且無偏的估計。</p><p><br>研究人員表示：<strong><span style="color:#ab1942;--tt-darkmode-color: #DF285B">“Performer 是第一個通過微調可以與常規 Transformers 進行完全兼容的線性架構”</span></strong>。</p><div class=pgc-img><img alt=谷歌Transformer再升級—新模型實現性能、速度雙提升 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/21726768b5c24ce99c02199b364b7589><p class=pgc-img-caption></p></div><blockquote class=pgc-blockquote-abstract><p><span style="color:#888;--tt-darkmode-color: #888888">圖 | 原點對稱的通用函數 r（定義為建立在：三角隨機特徵和正隨機特徵上的估計器的均方誤差（MSEs）的比值）是輸入特徵向量與其長度l之間的角度 φ（以弧度為單位）的函數, 函數的數值越大表示正隨機特徵性能越好的（φ，l）空間區域（左）；當 l 為定值 1 時，與變化的角度 φ 構成的函數 r 為正切函數，以及比較低 softmax 內核值區域中兩個估算器的 MSE（右）。</span></p><p><span style="color:#888;--tt-darkmode-color: #888888"><br></span></p></blockquote><p>作者通過比較發現，對於 φ 足夠大的臨界區域，該方法所使用的正交隨機特徵比任意的三角隨機特徵更精確。</p><p><br></p><div class=pgc-img><img alt=谷歌Transformer再升級—新模型實現性能、速度雙提升 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d584aba4a9764a9c95f02d5ad7d8be23><p class=pgc-img-caption></p></div><blockquote class=pgc-blockquote-abstract><p><span style="color:#888;--tt-darkmode-color: #888888">圖 | 將原始的經過預訓練的 Transformer 的權重轉移到 Performer 中，Performer 產的精度達到 0.07 （橙色虛線），但在原來的梯度步數的一小部分中，很快就恢復了精度。</span></p><p><span style="color:#888;--tt-darkmode-color: #888888">然而在 PG-19 上，三角法(TRIG) softmax 逼近變得非常不穩定，而正特徵(POS)(不重繪)和 Linformer (也是逼近 softmax)即使在重繪投影的情況下，也會在同樣的複雜度中趨於平穩。具有特徵重繪的正 softmax 是匹配 Transformer 的必要條件，SMREG 可實現更快的收斂。</span><br></p></blockquote><p>這篇論文利用詳細的數學定理，證明了與其單純依靠計算資源來提升性能，還不如開發出改進的、高效的 Transformer 架構，來顯著降低能耗。同時，由於 Performers 使用了與 Transformer 相同的訓練超參數，也可以有效訓練基於 softmax 的線性 Transformer。因此 FAVOR+ 機制可以作為一個簡單的插件，而無需進行過多的調整。</p><p><br></p><h1 class=pgc-h-arrow-right>Performer 模型應用前景廣泛<br></h1><p>研究人員表示，Performer 模型的提出，顯著降低了常規 Transformer 的空間和時間複雜度，並在 Transformer 的研究以及非稀疏注意機制的作用方面開闢了新的途徑。</p><p><br></p><p><br>該論文利用詳細的數學定理，證明了與其單純依靠計算資源來提升性能，還不如開發出改進的、高效的 Transformer 架構，來顯著降低能耗。同時，由於 Performers 使用了與 Transformer 相同的訓練超參數，因此 FAVOR+ 機制可以作為一個簡單的插件，而無需進行過多的調整。</p><p><br>該團隊在一系列豐富的場景下測試了 Performers 的性能，執行的任務包括像素預測、蛋白質序列建模。在實驗設置中，一個 Performer 只用 FAVOR+ 機制取代了常規 Transformer 的注意力組件。</p><p><br>在使用蛋白質序列訓練一個 36 層模型的挑戰性任務上，基於 Performer 的模型（Performer-RELU）的性能優於基線 Transformer 模型：Reformer 和 Linformer，後者的準確率顯著下降。</p><p><br>在標準的 ImageNet64 基準上，具有 6 層的 Performer 與具有 12 層的 Reformer 的準確性相當。優化後，Performer 的速度達到了 Reformer 的兩倍。</p><p><br>研究人員表示，由於基於 Performer 的可擴展 Transformer 架構可以處理更長的序列，而不受注意力機制結構的限制，同時保持準確和魯棒性，相信它們可以在生物信息學領域帶來新的突破，如蛋白質的語言建模等技術已經顯示出強大的潛力。</p><p><br></p><p><strong><span style="color:#888;--tt-darkmode-color: #888888">代碼地址：</span></strong><span style="color:#888;--tt-darkmode-color: #888888">https://github.com/google-research/google-research/tree/master/performer<br></span><strong><span style="color:#888;--tt-darkmode-color: #888888">論文地址：</span></strong><span style="color:#888;--tt-darkmode-color: #888888">https://arxiv.org/abs/2009.14794</span></p><p><br></p><p><strong><span style="color:#888;--tt-darkmode-color: #888888">參考資料：</span></strong><span style="color:#888;--tt-darkmode-color: #888888"><br>https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html<br>https://syncedreview.com/2020/10/02/google-cambridge-deepmind-alan-turing-institutes-performer-transformer-slashes-compute-costs/<br>https://www.youtube.com/watch?v=xJrKIPwVwGM</span></p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>Transformer</a></li><li><a>再升級</a></li><li><a>實現</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/f09ac34c.html alt=彩色電子書在廣州率先實現量產 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RkPMb9G6tipobr style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f09ac34c.html title=彩色電子書在廣州率先實現量產>彩色電子書在廣州率先實現量產</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2d12804e.html alt=[玩轉MySQL之九]MySQL實現ACID之原子性 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb044d821f74107a3fd9119fc34c642 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2d12804e.html title=[玩轉MySQL之九]MySQL實現ACID之原子性>[玩轉MySQL之九]MySQL實現ACID之原子性</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fb2bc471.html alt="「譯」 Spring 的分佈式事務實現—使用和不使用 XA—第二部分" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fb2bc471.html title="「譯」 Spring 的分佈式事務實現—使用和不使用 XA—第二部分">「譯」 Spring 的分佈式事務實現—使用和不使用 XA—第二部分</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e41fd8de.html alt="撫順各項防汛工作實現“六到位” 確保全市安全度汛" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e41fd8de.html title="撫順各項防汛工作實現“六到位” 確保全市安全度汛">撫順各項防汛工作實現“六到位” 確保全市安全度汛</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f22ee5ad.html alt="Redis 設計與實現 : Lua 腳本" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f22ee5ad.html title="Redis 設計與實現 : Lua 腳本">Redis 設計與實現 : Lua 腳本</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/56e2a065.html alt=這位大叔在隨機的彩票上實現了90%的中獎率 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/50ab0003166decded7e4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/56e2a065.html title=這位大叔在隨機的彩票上實現了90%的中獎率>這位大叔在隨機的彩票上實現了90%的中獎率</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cf633068.html alt="Java 多態的實現機制，看了都說好" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9d3b0e55813d46b4982ae7d9b81d1802 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cf633068.html title="Java 多態的實現機制，看了都說好">Java 多態的實現機制，看了都說好</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d0662f03.html alt="廣西鐵路出海大通道 全面實現電氣化運營" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d0662f03.html title="廣西鐵路出海大通道 全面實現電氣化運營">廣西鐵路出海大通道 全面實現電氣化運營</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c78a6fe2.html alt=HashMap的底層實現 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/0f9ffe95e10e4ff5804a8fd9cf591cfc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c78a6fe2.html title=HashMap的底層實現>HashMap的底層實現</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7c3d9f70.html alt=新穎的混合材料或有助於實現高效的下一代顯示器 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/47050004da4f36dcec1b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7c3d9f70.html title=新穎的混合材料或有助於實現高效的下一代顯示器>新穎的混合材料或有助於實現高效的下一代顯示器</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8d93e49d.html alt=教程：採用梯度下降算法實現線性迴歸！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1537162000876f4501fb1c4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8d93e49d.html title=教程：採用梯度下降算法實現線性迴歸！>教程：採用梯度下降算法實現線性迴歸！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e8e17d90.html alt=重要突破！中國科學家實現基底細胞癌4.1秒人工智能精準識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RtbWBYL9koJMmN style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e8e17d90.html title=重要突破！中國科學家實現基底細胞癌4.1秒人工智能精準識別>重要突破！中國科學家實現基底細胞癌4.1秒人工智能精準識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1b218e68.html alt=圖像拼接算法及實現（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1534489824878547eee8fc2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1b218e68.html title=圖像拼接算法及實現（一）>圖像拼接算法及實現（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9eb3d63.html alt=PHP實現各種經典算法詳解 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/15357595091626cf296dc3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9eb3d63.html title=PHP實現各種經典算法詳解>PHP實現各種經典算法詳解</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/740f5b7b.html alt=PHP怎麼實現分頁功能？（圖文+視頻教程） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/e81c52a014dd4c34adcbfb26482bd690 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/740f5b7b.html title=PHP怎麼實現分頁功能？（圖文+視頻教程）>PHP怎麼實現分頁功能？（圖文+視頻教程）</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>