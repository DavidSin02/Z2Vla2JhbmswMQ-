<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>深度強化學習-深度Q網絡（DQN）介紹 | 极客快訊</title><meta property="og:title" content="深度強化學習-深度Q網絡（DQN）介紹 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/RXfpIwbEJgChY1"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a4fc7b.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a4fc7b.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2a4fc7b.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a4fc7b.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a4fc7b.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2a4fc7b.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2a4fc7b.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2a4fc7b.html><meta property="article:published_time" content="2020-10-29T20:59:30+08:00"><meta property="article:modified_time" content="2020-10-29T20:59:30+08:00"><meta name=Keywords content><meta name=description content="深度強化學習-深度Q網絡（DQN）介紹"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/2a4fc7b.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>深度強化學習-深度Q網絡（DQN）介紹</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><img alt=深度強化學習-深度Q網絡（DQN）介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RXfpIwbEJgChY1><blockquote><p>原標題 | Deep Reinforcement Learning. Introduction. Deep Q Network (DQN) algorithm.</p><p>作者 | Markus Buchholz</p><p>譯者 | qianyuhappy、AI小山</p></blockquote><p></p><h2>1.引言</h2><img alt=深度強化學習-深度Q網絡（DQN）介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RXmawGE4FPnoWk><p>由DeepDream生成的圖像</p><p>原始的深度強化學習是純強化學習，其典型問題為馬爾科夫決策過程（MDP）。馬爾科夫決策過程包含一組狀態S和動作A。狀態的轉換是通過概率P，獎勵R和一個折衷參數gamma決定的。概率轉換P反映了轉換和狀態轉變的獎勵之間的關係，狀態和獎勵僅依賴上一時間步的狀態和動作。</p><p>強化學習為Agent定義了環境，來實現某些動作以最大化獎勵（這些動作根據policy採取）。對Agent的優化行為的基礎由Bellman方程定義，這是一種廣泛用於求解實際優化問題的方法。為了解決Bellman優化問題，我們使用了一種動態編程的方法。</p><p>當Agent存在於環境中並轉換到另一個狀態(位置)時，我們需要估計狀態V(s)(位置)-狀態值函數的值。一旦我們知道了每個狀態的值，我們就可以找出執行Q(S, A)-動作值函數的最佳方法(只需遵循值最高的狀態)。</p><p>這兩個映射或函數相關性很高，可以幫助我們找到問題的最佳策略。從狀態值函數我們可以看出遵循策略的Agent，處於的S狀態有多好。</p><p>符號解釋：</p><p>E[X] — 隨機變量X的期望</p><p>?— policy</p><p>Gt — t時刻的折現收益</p><p>γ — 折現率</p><p>但是，動作值函數q（s，a）是從狀態S開始，採取動作A，並遵循策略π的折現收益，並告訴我們從特定狀態採取特定動作的效果。</p><p>很明顯，狀態值函數和Q函數之間的區別在於值函數體現狀態的良好性，而Q函數體現狀態中的動作的良好性。</p><p>MDP由Bellman方程求解，Bellman方程是以美國數學家Richard Bellman的名字命名的。該方程有助於尋找最優的策略和價值函數。代理根據所施加的策略選擇操作(策略——正式地說，策略定義為每種可能狀態下操作的概率分佈)。代理可以遵循的不同策略意味著狀態的不同值函數。然而，如果目標是使收集到的獎勵最大化，我們必須找到最好的可能的政策，稱為最優政策。</p><p>另一方面，最佳狀態值函數的值，比所有其它值函數（最大返回值）都要大，因此，最佳值函數也以通過代入最大Q值來進行估算：</p><p>最後，值函數的貝爾曼等式（Bellman equation）可表示如下：</p><p>類似地，Q函數的貝爾曼等式可表示如下：</p><p>基於最佳狀態值函數以及上述的狀態值函數、動作值函數的等式，我們可以寫出最終的最佳值函數的等式，該等式稱作貝爾曼最佳等式：</p><p>通常，強化學習的問題通過Q學習算法來解決。這裡，如上所言，智能體與環境交互並接收獎勵。目標是用足最佳策略（選擇動作的方法），以取得最大獎勵。在學習過程中，智能體更新Q(S,A)表（當回合結束時，任務完成，目標達到）。</p><p>Q學習算法通過以下步驟實現：</p><p>1、用隨機數初始化Q(S,A)表。</p><p>2、用epsilon貪心策略選取一個行動，然後進入下一個狀態S’</p><p>3、根據更新等式來更新前一個狀態的Q值：</p><img alt=深度強化學習-深度Q網絡（DQN）介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RXmawkgDZveksS><p>最好是從解決來自OpenAI gym的 Frozen Lake 開始。</p><p>在凍湖環境裡（最好能熟悉OpenAI的描述），智能體可處理16種狀態，執行4個不同的動作（在一個狀態中）。在這種情況下，我們的A(S,A)表的大小是16x4。</p><p>Frozen Lake 代碼如下，你也可以點擊此處查看~</p><img alt=深度強化學習-深度Q網絡（DQN）介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RXmawl1CqPgPqy><p>https://gist.github.com/markusbuchholz/af4e5b5891de6d3cf5528f83b6198311#file-qlearning_algorithm-py</p><img alt=深度強化學習-深度Q網絡（DQN）介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RXmawlK4sSiugY><p>請注意上面給出的Q算法屬於時序差分學習算法（Temporal Difference Learning algorithms）（由Richard S. Sutton於1988年提出）。Q算法是一種線下策略（off-policy）算法（這種算法具有從舊的歷史數據學習的能力）。Q學習算法的擴展是SARSA（在線策略（on-policy)算法）。唯一區別在於Q(S,A)表的更新：</p><p></p><h2>2. 深度強化學習（深度Q網絡--DNQ）</h2><p>當所有可到達的狀態處於可控（能夠迭代）並且能存儲在計算機RAM中時，強化學習對於環境來說是足夠好用的。然而，當環境中的狀態數超過現代計算機容量時（Atari遊戲有12833600個狀態），標準的強化學習模式就不太有效了。而且，在真實環境中，智能體必須面對連續狀態（不離散），連續變量和連續控制（動作）的問題。</p><p>知道了智能體所處的環境的複雜性（狀態數量，連續控制），標準的、定義明確的強化學習Q表就得被深度神經網絡（Q網絡）取代了，後者可以把環境狀態映射為智能體動作（非線性逼近）。網絡架構，網絡超參數的選擇以及學習都在訓練階段（Q網絡權重的學習）中完成。DQN允許智能體探索非結構化的環境並獲取知識，經過時間積累，他們可以模仿人類的行為。</p><p></p><h2>3. 學習算法DQN</h2><p>下圖（在訓練過程中）描述了DQN的核心概念，圖中，Q網絡做非線性逼近，把狀態映射為動作值。</p><p>在訓練過程中，智能體與環境交互，並接收數據，這些數據在Q網絡的學習過程中會用到。智能體探索環境，建立一個轉換和動作輸出的全圖。開始時，隨機進行動作，隨著時間推移，這樣做越來越沒效果。在探索環境時，智能體儘量查詢Q網絡（逼近）以決定如何行動。我們把這種方式（綜合了隨機行為和Q網絡查詢）稱為epsilon貪心方法（epsilon貪心動作選擇塊），也就是說利用概率超參數epsilon在隨機和Q策略間進行選擇。</p><p>我們所講的Q學習算法的核心來自於監督學習。</p><p>如前所述，我們的目標是用深度神經逼近一個複雜的非線性函數Q(S,A)。</p><p>跟監督學習一樣，在DQN中，我們定義損失函數為目標和預測值之間的方差，我們也更新權重儘量減少損失（假定智能體從一個狀態轉換到另一個狀態，進行了某個動作a，獲取獎勵r）。</p><p>在學習過程中，我們使用兩個不相關的Q網絡（Q_network_local和Q_network_target）來計算預測值（權重θ）和目標值（權重θ’）。經過若干步驟後，目標網絡會被凍結，然後拷貝實際的Q網絡的權重到目標網絡權重。凍結目標Q網絡一段時間再用實際Q網絡的權重更新其權重，可以穩定訓練過程。</p><img alt=深度強化學習-深度Q網絡（DQN）介紹 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RXmax2lF6xVxbf><p>圖1. DQN算法概念</p><p>為使訓練過程更穩定（我們要避免用比較關聯的數據來訓練網絡，如果基於連續更新最後轉換來進行訓練的話， 這種情況就有可能發生），我們引入重播緩衝區，它能記住智能體所經歷的行為。然後，用重播緩衝區裡的隨機樣本來進行訓練（這可以減少智能體的經歷之間的關聯性，並有助於智能體從更廣泛的經歷中進行學習）。</p><p>DQN算法可描述如下：</p><p>1. 初始化重播緩衝區。</p><p>2. 預處理環境，並把狀態S輸入DQN， 後者會返回該狀態中所有可能的動作的Q值。</p><p>3. 用epsilon貪心策略選取一個動作：當有概率epsilon時，我們選擇隨機動作A，當有概率1-epsilon時，選取具有最高Q值的動作，例如A=argmax(Q(S, A, θ))。</p><p>4. 選擇了動作A後，智能體在狀態S中執行所選的動作，並進行到新狀態S ，接收獎勵R。</p><p>5. 把轉換存儲在重播緩衝中，記作<s>。 </s><s>。</s></p><p>6. 下一步，從重播緩衝區中抽取隨機批次的轉換，並用以下公式計算損失：</p><p>7. 針對實際網絡參數，執行梯度下降，以使損失最小化。</p><p>8. 每隔k步之後，拷貝實際網絡權重到目標網絡權重中。</p><p>9. 重複這些步驟M回合。</p><p></p><h2>4. 工程設置.結果.</h2><p>在這一段中，我展示Udacity（深度強化學習）的工程代碼的結果。</p><p>a. 工程的目標</p><p>本工程的目標是訓練智能體如何在方塊環境中通過移動來採集黃色香蕉。工程要求在100個連續回合中獲取+13的平均分。</p><p>b. 在導航工程中，使用下列參數設置神經網絡架構和超參數：</p><p>以下是每回合的獎勵圖，顯示出智能體在玩了2247回合後，能收到的平均獎勵（超過100回合）有至少+13。</p><p>Q網絡架構：</p><p>輸入層FC1：37節點輸入，64節點輸出</p><p>隱藏層FC2：64節點輸入，64節點輸出</p><p>隱藏層FC3：64節點輸入，64節點輸出</p><p>輸出層：64節點輸入，4節點輸出----動作的大小</p><p>使用的超參數：</p><p>BUFFER_SIZE = int(1e5) # 重播緩衝區大小</p><p>BATCH_SIZE = 64 # 最小批量大小</p><p>GAMMA = 0.99 # 折扣率</p><p>TAU = 1e-3 # 用於目標參數的軟更新</p><p>LR = 5e-4 # 學習率</p><p>UPDATE_EVERY = 4 # 更快網絡的快慢</p><p>Epsilon start = 1.0</p><p>Epsilon start = 0.01</p><p>Epsilon decay = 0.999</p><img alt=深度強化學習-深度Q網絡（DQN）介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RXmax3KDPwhgd5><img alt=深度強化學習-深度Q網絡（DQN）介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RXmax3kHKwKjmG><p>圖2. 智能體學習的平均得分</p><p></p><h2>5. 未來工作的想法</h2><p>如果有深度學習的相關經驗，那麼未來工作將主要集中於圖像處理方面（從像素中學習）。下圖展示了DQN的架構，圖中，我們輸入遊戲畫面，Q網絡逼近遊戲狀態中所有動作的Q值。動作由我們討論過的DQN算法進行選擇。</p><img alt=深度強化學習-深度Q網絡（DQN）介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RXmax467D8Kwzn><p>圖3. 從像素學習的概念</p><p>其次，未來的工作將集中在生成一個決鬥（Dueling）式DQN。在這個新的架構中，我們指定新的優勢函數，這個函數計算出智能體執行的一個動作，比其它動作好了多少（優勢可為正也可為負）。</p><p>Dueling DQN架構與上面講的DQN相同，只不過最後的全連接層分成兩股（見下圖所描述）。</p><p>若環境的一個狀態有確定數量的動作空間，絕大多數計算出來的動作對狀態沒有什麼影響。此外，有些動作有冗餘效應。在這種情況下，新的dueling DQN將會比DQN架構估算出來的Q值更精確。</p><p>其中一股計算值函數，另一股計算優勢函數（用於決定哪個動作更優）。</p><img alt=深度強化學習-深度Q網絡（DQN）介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RXmaxKoDldMBTY><p>圖4. Dueling DQN架構</p><p>最後，我們考慮一下從人類的偏好中進行學習 (OpenAI和Deep Mind) 。這個新概念的核心思想是從人類的反饋中學習。接收人類反饋的智能體，將盡力進行人類期望的動作，並相應地設置獎勵。人類與智能體的直接交互 ，會有助於降低設計獎勵函數和複雜的目標函數的難度。</p><img alt=深度強化學習-深度Q網絡（DQN）介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RXmaxLEHRHDREl><p>你可以通過我的Github找到本工程的全部源碼：</p><p>https://github.com/markusbuchholz/deep-reinforcement-learning/tree/master/p1_navigation</p><p>本文編輯：王立魚</p><p>英語原文：https://medium.com/@markus.x.buchholz/deep-reinforcement-learning-introduction-deep-q-network-dqn-algorithm-fb74bf4d68621</p><p>想要繼續查看該篇文章相關鏈接和參考文獻？</p><p>點擊【深度強化學習-深度Q網絡（DQN）介紹】即可訪問！</p><p>福利大放送——滿滿的乾貨課程免費送！</p><p>「好玩的Python：從數據挖掘到深度學習」該課程涵蓋了從Python入門到CV、NLP實踐等內容，是非常不錯的深度學習入門課程，共計9節32課時，總長度約為13個小時。。</p><p>課程頁面：https://ai.yanxishe.com/page/domesticCourse/37</p><p>「計算機視覺基礎入門課程」本課程主要介紹深度學習在計算機視覺方向的算法與應用，涵蓋了計算機視覺的歷史與整個課程規劃、CNN的模型原理與訓練技巧、計算機視覺的應用案例等，適合對計算機視覺感興趣的新人。</p><p>課程頁面：https://ai.yanxishe.com/page/domesticCourse/46</p><p>現AI研習社將兩門課程免費開放給社區認證用戶，只要您在認證時在備註框裡填寫「Python」，待認證通過後，即可獲得該課程全部解鎖權限。心動不如行動噢~</p><p>認證方式：https://ai.yanxishe.com/page/blogDetail/13999</p><p>雷鋒網雷鋒網雷鋒網</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>深度</a></li><li><a>強化</a></li><li><a>學習</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/5a7c0dad.html alt=深度學習中的線性代數 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/dfic-imagehandler/677561e3-e0ec-4693-bd88-0cd182b21a17 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5a7c0dad.html title=深度學習中的線性代數>深度學習中的線性代數</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/31ece80.html alt="深度 | 劉群：基於深度學習的自然語言處理，邊界在哪裡？" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/RaimUrx1Qpfnvn style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/31ece80.html title="深度 | 劉群：基於深度學習的自然語言處理，邊界在哪裡？">深度 | 劉群：基於深度學習的自然語言處理，邊界在哪裡？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/06166af.html alt=強化學習應用於組合優化問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d7490d50d1ae4da4a0c46ff151cb54d2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/06166af.html title=強化學習應用於組合優化問題>強化學習應用於組合優化問題</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e1c0c28.html alt=利用深度強化學習生成測試輸入 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ab2a7fe4123846c38fda0899ec3157c9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e1c0c28.html title=利用深度強化學習生成測試輸入>利用深度強化學習生成測試輸入</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c5e8471.html alt=深度學習與醫學圖像分析 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/2490e2dab8ed41e59acad9c5a23348aa style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c5e8471.html title=深度學習與醫學圖像分析>深度學習與醫學圖像分析</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cda97a.html alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5f757f260bf1489db295e89fa564231d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cda97a.html title=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展>上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0610903.html alt=深度學習中的優化器對比 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/e6c889ca1bde4454985ccf697dd54117 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0610903.html title=深度學習中的優化器對比>深度學習中的優化器對比</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/8efd8d2.html alt=強化學習基礎-對偶梯度上升 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RGPaBiGC9XjT8U style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/8efd8d2.html title=強化學習基礎-對偶梯度上升>強化學習基礎-對偶梯度上升</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html alt=直流鍋爐給水控制學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/eba10edcc8d14d9f8cde6fd5b212d90e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html title=直流鍋爐給水控制學習>直流鍋爐給水控制學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html alt=HTMLCSS學習筆記（六）——元素類型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb5988349894ce9bf568c6418f85b7d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html title=HTMLCSS學習筆記（六）——元素類型>HTMLCSS學習筆記（六）——元素類型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html alt="web前端（從零開始），每天更新學習筆記 HTML5元素分類" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/46d70004fcd55e1ddad3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html title="web前端（從零開始），每天更新學習筆記 HTML5元素分類">web前端（從零開始），每天更新學習筆記 HTML5元素分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html alt="MySQL 學習筆記" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html title="MySQL 學習筆記">MySQL 學習筆記</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>