<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 | 极客快訊</title><meta property="og:title" content="終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/3553afe761fd4eea86e31ba84eb1ffeb"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2ab3719e.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2ab3719e.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2ab3719e.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2ab3719e.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2ab3719e.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2ab3719e.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2ab3719e.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2ab3719e.html><meta property="article:published_time" content="2020-11-14T21:08:22+08:00"><meta property="article:modified_time" content="2020-11-14T21:08:22+08:00"><meta name=Keywords content><meta name=description content="終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/2ab3719e.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><div class=pgc-img><img alt=終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3553afe761fd4eea86e31ba84eb1ffeb><p class=pgc-img-caption></p></div><p>【新智元導讀】雖然在Coursera、MIT、UC伯克利上有很多機器學習的課程，包括吳恩達等專家課程已非常經典，但都是面向有一定理科背景的專業人士。本文試圖將機器學習這本深奧的課程，以更加淺顯易懂的方式講出來，讓沒有理科背景的讀者都能看懂。</p><p>把複雜的東西簡單化，讓非專業人士也能短時間內理解，並露出恍然大悟的表情，是一項非常厲害的技能。</p><p>舉個例子。你正在應聘機器學習工程師，面對的是文科出身的HR，如果能在最短時間內讓她瞭解你的專業能力，就能極大地提升面試成功率。</p><p>現在，機器學習這麼火，想入行的人越來越多，然而被搞糊塗的人也越來越多。因為大眾很難理解機器學習是<span>幹嗎的</span>？那些神祕拗口的概念，比如邏輯迴歸、梯度下降到底是什麼？j</p><p>一個23歲的藥物學專業的學生說，當他去參加機器學習培訓課程的時候，感覺自己就家裡那位不懂現代科技的奶奶。</p><p>於是一名叫Audrey Lorberfeld的畢業生，試圖將大眾與機器學習之間的鴻溝，親手填補上。於是有了這個系列文章。</p><p>本系列第一講：梯度下降、線性迴歸和邏輯迴歸。</p><h1>算法 vs 模型</h1><p>在理解開始瞭解機器學習之前，我們需要先搞懂兩個基礎概念：算法和模型。</p><p>我們可以把模型<span>看做</span>是一個自動售貨機，輸入（錢），輸出（可樂）。算法是用來訓練這個模型的，</p><p>模型根據給定的輸入，做出對應的決策獲得預期輸出。例如，一個算法根據投入的金額，可樂的單價，判斷錢夠不夠，如果多了該找多少錢。</p><p>總而言之，算法是模型背後的數學生命力。沒有模型，算法只是一個數學方程式。模型的不同，取決於用的算法的不同。</p><h1>梯度下降/最佳擬合線</h1><p>（雖然這個傳統上並不被認為是一種機器學習算法，但理解梯度對於瞭解有多少機器學習算法可用，及如何優化至關重要。）梯度下降幫助我們根據一些數據，獲得最準確的預測。</p><p>舉個例子。你有一個大的清單，列出每個你認識的人身高體重。然後做成下面這種分佈圖：</p><div class=pgc-img><img alt=終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0ae29d9625f84e5298382dbcee6fb397><p class=pgc-img-caption></p></div><p>圖上面的數字比較奇怪？不用在意這些細節。</p><p>現在，小區居委會要舉辦一個根據身高猜體重的比賽，贏的人發紅包。就用這張圖。你怎麼辦？</p><p>你可能會想在圖上畫一根線，這個線非常完美的給出了身高和體重的對應關係。</p><p>比如，根據這條完美線，身高1.5米的人體重基本在60斤左右。啊那麼，這根完美線是怎麼找出來呢？答：梯度下降。</p><div class=pgc-img><img alt=終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c1b130d1135b480791073b69dfd33750><p class=pgc-img-caption></p></div><p>我們先提一個概念叫RSS（the residual sum of squares）。RSS是點和線之間差異的平方和，這個值代表了點和線的距離有多遠。梯度下降就是找出RSS的最小值。</p><p>我們把每次為這根線找的不同參數進行可視化，就得到了一個叫做成本曲線的東西。這個曲線的地步，就是我們的RSS最小值。</p><div class=pgc-img><img alt=終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/ae04aeb105254fc0a388ab305b20cfe3><p class=pgc-img-caption></p></div><p>Gradient Descent可視化（使用MatplotLib）</p><p>來自不可思議的數據科學家Bhavesh Bhatt</p><p>梯度下降還有其他的一些細分領域，比如“步長”和“學習率”（即我們想要採取什麼方向到底部的底部）。</p><p>總之，：我們通過梯度下降找到數據點和最佳擬合線之間最小的空間；而最佳你和線是我們做預測的直接依據。</p><h1>線性迴歸</h1><p>線性迴歸是分析一個變量與另外一個或多個變量（自變量）之間，關係強度的方法。</p><p>線性迴歸的標誌，如名稱所暗示的那樣，即自變量與結果變量之間的關係是線性的，也就是說變量關係可以連城一條直線。</p><div class=pgc-img><img alt=終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/203dd9bafd1445a88aeb81c2a48e3107><p class=pgc-img-caption></p></div><p>這看起來像我們上面做的！這是因為線性迴歸中我們的“迴歸線”之前的最佳實踐線。最佳擬合線顯示了我們的點之間最佳的線性關係。反過來，這使我們能夠做出預測。</p><p>關於線性迴歸的另一個重點是，結果變量或“根據其他變量而變化的”變量（有點繞哈）總是連續的。但這意味著什麼？</p><p>假設我們想測量一下紐約州影響降雨的因素：結果變量就是降雨量，就是我們最關係的東西，而影響降水的自變量是海拔。</p><p>如果結果變量不是連續的，就可能出現在某個海拔，沒有結果變量，導致我們沒辦法做出預測。</p><p>反之，任意給定的海拔，我們都可以做出預測。這就是線性迴歸最酷的地方！</p><h1>嶺迴歸與LASSO迴歸</h1><p>現在我們知道什麼是線性迴歸，接下來還有更酷的，比如嶺迴歸。在開始理解嶺迴歸之前，我們先來了解正則化。</p><p>簡單地說，數據科學家使用正則化，確保模型只關注能夠對結果變量產生顯著影響的自變量。</p><p>但是那些對結果影響不顯著的自變量會被正則忽略嗎？當然不會！原因我們後面再展開細講。</p><p>原則上，我們創建這些模型，投喂數據，然後測試我們的模型是否足夠好。</p><p>如果不管自變量相關也好不相關都投喂進去，最後我們會發現模型在處理訓練數據的時候超棒；但是處理我們的測試數據就超爛。</p><p>這是因為我們的模型不夠靈活，面對新數據的時候就顯得有點不知所措了。這個時候我們稱之為“Overfit”過擬合。</p><p>接下來我們通過一個過長的例子，來體會一下過擬合。</p><blockquote><p>比方說，你是一個新媽媽，你的寶寶喜歡吃麵條。幾個月來，你養成了一個在廚房餵食並開窗的習慣，因為你喜歡新鮮空氣。</p><p>接著你的侄子給寶寶一個圍裙，這樣他吃東西就不會弄得滿身都是，然後你又養成了一個新的習慣：喂寶寶吃麵條的時候，必須穿上圍裙。</p><p>隨後你又收養了一隻流浪狗，每次寶寶吃飯的時候狗就蹲在嬰兒椅旁邊，等著吃寶寶掉下來的麵條。</p><p>作為一個新媽媽，你很自然的會認為，開著的窗戶+圍裙+嬰兒椅下面的狗，是讓你的寶寶能夠開心吃麵條的必備條件。</p><p>直到有一天你回孃家過週末。當你發現廚房裡沒有窗戶你有點慌；然後你突然想起來走的匆忙圍裙也沒帶；最要命的是狗也交給鄰居照看了，天哪！</p><p>你驚慌到手足無措以至於忘記給寶寶餵食，就直接把他放床上了。看，當你面對一個完全新的場景時你表現的很糟糕。而在家則完全是另外一種畫風了。</p><p>經過重新設計模型，過濾掉所有的噪音（不相關的數據）後你發現，其實寶寶僅僅是喜歡你親手做的麵條。</p><p>第二天，你就能坦然的在一個沒有窗戶的廚房裡，沒給寶寶穿圍裙，也沒有狗旁邊，開開心心的喂寶寶吃麵條了。</p></blockquote><p>這就是機器學習的正則化所幹的事情：讓你的模型只關注有用的數據，忽略干擾項。</p><div class=pgc-img><img alt=終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/06667eae2e124f86bea66f4751ca582a><p class=pgc-img-caption></p></div><p>在左邊：LASSO迴歸（你可以看到紅色梯級表示的係數在穿過y軸時可以等於零）</p><p>在右邊：嶺迴歸（你可以看到係數接近，但從不等於零，因為它們從不穿過y軸）</p><p>圖片來源：Prashant Gupta的“機器學習中的正規化”</p><p>在各種正規化的，有一些所謂的懲罰因子（希臘字母拉姆達：λ）。這個懲罰因子的作用是在數學計算中，縮小數據中的噪聲。</p><p>在嶺迴歸中，有時稱為“L2迴歸”，懲罰因子是變量係數的平方值之和。懲罰因子縮小了自變量的係數，但從來沒有完全消除它們。這意味著通過嶺迴歸，您的模型中的噪聲將始終被您的模型考慮在內。</p><p>另一種正則化是LASSO或“L1”正則化。在LASSO正則化中，只需懲罰高係數特徵，而不是懲罰數據中的每個特徵。</p><p>此外，LASSO能夠將係數一直縮小到零。這基本上會從數據集中刪除這些特徵，因為它們的“權重”現在為零（即它們實際上是乘以零）。</p><p>通過LASSO迴歸，模型有可能消除大部分噪聲在數據集中。這在某些情況下非常有用！</p><h1>邏輯迴歸</h1><p>現在我們知道，線性迴歸=某些變量對另一個變量的影響，並且有2個假設：結果變量是連續的；變量和結果變量之間的關係是線性的。</p><p>但如果結果變量不是連續的而是分類的呢？這個時候就用到邏輯迴歸了。</p><p>分類變量只是屬於單個類別的變量。比如每一週都是週一到週日7個日子，那麼這個時候你就不能按照天數去做預測了。</p><p>每週的第一天都是星期一，週一發生的事情，就是發生在週一。沒毛病。</p><p>邏輯迴歸模型只輸出數據點在一個或另一個類別中的概率，而不是常規數值。這也是邏輯迴歸模型主要用於分類的原因。</p><p>在邏輯迴歸的世界中，結果變量與自變量的對數概率（log-odds）具有線性關係。</p><p><strong>比率（odds）</strong></p><p>邏輯迴歸的核心就是odds。舉個例子：</p><p>一個班裡有19個學生，其中女生6個，男生13個。假設女性通過考試的機率是5：1，而男性通過考試的機率是3:10。這意味著，在6名女性中，有5名可能通過測試，而13名男性中有3名可能通過測試。</p><p>那麼，odds和概率（probability）不一樣嗎？並不。</p><p>概率測量的是事件發生的次數與所有事情發生的總次數的比率，例如，投擲40次投幣10次是正面的概率是25%；odds測量事件發生的次數與事件的次數的比率，例如拋擲30次有10次是正面，odds指的是10次正面:30次反面。</p><p>這意味著雖然概率總是被限制在0-1的範圍內，但是odds可以從0連續增長到正無窮大！</p><p>這給我們的邏輯迴歸模型帶來了問題，因為我們知道我們的預期輸出是概率（即0-1的數字）。</p><p>那麼，我們如何從odds到概率？</p><p>讓我們想一個分類問題，比如你最喜歡的足球隊和另一隻球隊比賽，贏了6場。你可能會說你的球隊失利的機率是1：6，或0.17。</p><p>而你的團隊獲勝的機率，因為他們是一支偉大的球隊，是6：1或6。如圖：</p><div class=pgc-img><img alt=終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e32b6c72a81045628026cd579bb076bb><p class=pgc-img-caption></p></div><p>圖片來源：</p><p>https://www.youtube.com/watch?v=ARfXDSkQf1Y</p><p>現在，你不希望你的模型預測你的球隊將在未來的比賽中取勝，只是因為他們過去獲勝的機率遠遠超過他們過去失敗的機率，對吧？</p><p>還有更多模型需要考慮的因素（可能是天氣，也許是首發球員等）！因此，為了使得odds的大小均勻分佈或對稱，我們計算出一些稱為對數比率（log-odds）的東西。</p><p><strong>log-odds</strong></p><div class=pgc-img><img alt=終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/678f99722f174673abd34144ea2f1a30><p class=pgc-img-caption></p></div><p>我們所謂的“正態分佈”：經典的鐘形曲線！</p><p>Log-odds是自然對數odds的簡寫方式。當你採用某種東西的自然對數時，你基本上可以使它更正常分佈。當我們製作更正常分佈的東西時，我們基本上把它放在一個非常容易使用的尺度上。</p><p>當我們採用log-odds時，我們將odds的範圍從0正無窮大轉換為負無窮正無窮大。可以在上面的鐘形曲線上看到這一點。</p><p>即使我們仍然需要輸出在0-1之間，我們通過獲取log-odds實現的對稱性使我們比以前更接近我們想要的輸出！</p><p><strong>Logit函數</strong></p><p>“logit函數”只是我們為了得到log-odds而做的數學運算！</p><div class=pgc-img><img alt=終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d63a8d52eecd4ecd9033926fe0c0ec91><p class=pgc-img-caption></p></div><p>恐怖的不可描述的數學。呃，我的意思是logit函數。</p><div class=pgc-img><img alt=終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/73c99e065ab446f38e5643085a8c13dd><p class=pgc-img-caption></p></div><p>logit函數，用圖表繪製</p><p>正如您在上面所看到的，logit函數通過取其自然對數將我們的odds設置為負無窮大到正無窮大。</p><p><strong>Sigmoid函數</strong></p><p>好的，但我們還沒有達到模型給我們概率的程度。現在，我們所有的數字都是負無窮大到正無窮大的數字。名叫：sigmoid函數。</p><p>sigmoid函數，以其繪製時呈現的s形狀命名，只是log-odds的倒數。通過得到log-odds的倒數，我們將我們的值從負無窮大正無窮大映射到0-1。反過來，讓我們得到概率，這正是我們想要的！</p><p>與logit函數的圖形相反，其中我們的y值範圍從負無窮大到正無窮大，我們的sigmoid函數的圖形具有0-1的y值。好極了！</p><div class=pgc-img><img alt=終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ee1c528fc43b4c8c9b1220d0616684f1><p class=pgc-img-caption></p></div><p>有了這個，我們現在可以插入任何x值並將其追溯到預測的y值。該y值將是該x值在一個類別或另一個類別中的概率。</p><p><strong>最大似然估計</strong></p><p>你還記得我們是如何通過最小化RSS（有時被稱為“普通最小二乘法”或OLS法）的方法在線性迴歸中找到最佳擬合線的嗎？</p><p>在這裡，我們使用稱為最大似然估計（MLE）的東西來獲得最準確的預測。</p><p>MLE通過確定最能描述我們數據的概率分佈參數，為我們提供最準確的預測。</p><p>我們為什麼要關心如何確定數據的分佈？因為它很酷！（並不是）</p><p>它只是使我們的數據更容易使用，並使我們的模型可以推廣到許多不同的數據。</p><div class=pgc-img><img alt=終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/04603ab28ea84e1f949725ac1cb80839><p class=pgc-img-caption></p></div><p>一般來說，為了獲得我們數據的MLE，我們將數據點放在s曲線上並加上它們的對數似然。</p><p>基本上，我們希望找到最大化數據對數似然性的s曲線。我們只是繼續計算每個log-odds行的對數似然（類似於我們對每個線性迴歸中最佳擬合線的RSS所做的那樣），直到我們得到最大數量。</p><p>好了，到此為止我們知道了什麼是梯度下降、線性迴歸和邏輯回顧，下一講，由Audrey妹子來講解決策樹、隨機森林和SVM。</p><p>參考鏈接：</p><p>https://towardsdatascience.com/machine-learning-algorithms-in-laymans-terms-part-1-d0368d769a7b</p><hr><p class=ql-align-center><strong>【新智元春季招聘開啟，一起弄潮AI之巔！】</strong></p><p>崗位詳情請戳：<a class=pgc-link data-content=mp href="https://www.toutiao.com/i6667310233787826702/?group_id=6667310233787826702" target=_blank>【春招英雄貼】新智元呼召智士主筆，2019勇闖AI之巔！</a></p><p><strong>【2019新智元 AI 技術峰會倒計時14天】</strong></p><p>​ 2019年的3月27日，新智元再匯AI之力，在北京泰富酒店舉辦AI開年盛典——2019新智元AI技術峰會。峰會以“<strong>智能雲•芯世界</strong>“為主題，聚焦智能雲和AI芯片的發展，重塑未來AI世界格局。</p><p>同時，新智元將在峰會現場權威發佈若干<strong>AI白皮書</strong>，聚焦產業鏈的創新活躍，評述華人AI學者的影響力，助力中國在世界級的AI競爭中實現超越。</p><div class=pgc-img><img alt=終於有人把梯度下降、線性迴歸、邏輯迴歸講明白了 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/5d66266694df45eeb05a20e32c8ff69b><p class=pgc-img-caption></p></div><p><strong>購票：</strong></p><p>活動行購票鏈接：<a class=pgc-link data-content=mp href=http://hdxu.cn/9Lb5U target=_blank>2019新智元AI技術峰會--智能雲•芯世界_精彩城市生活，盡在活動行！！</a></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>終於</a></li><li><a>線性</a></li><li><a>邏輯</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/4dc2e8d0.html alt=取消、返回、關閉的交互邏輯 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/15307556563726c719f07ce style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4dc2e8d0.html title=取消、返回、關閉的交互邏輯>取消、返回、關閉的交互邏輯</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c67451a1.html alt=看了這臺美國和麵機，終於知道為什麼外國人每天的早餐都那麼豐盛 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/39ba000437f591377966 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c67451a1.html title=看了這臺美國和麵機，終於知道為什麼外國人每天的早餐都那麼豐盛>看了這臺美國和麵機，終於知道為什麼外國人每天的早餐都那麼豐盛</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a0cd0159.html alt="「基底 / 線性組合 / 線性無關（相關）」-圖解線性代數 02" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/46f2000254161c118e69 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a0cd0159.html title="「基底 / 線性組合 / 線性無關（相關）」-圖解線性代數 02">「基底 / 線性組合 / 線性無關（相關）」-圖解線性代數 02</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8d93e49d.html alt=教程：採用梯度下降算法實現線性迴歸！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1537162000876f4501fb1c4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8d93e49d.html title=教程：採用梯度下降算法實現線性迴歸！>教程：採用梯度下降算法實現線性迴歸！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d555d04c.html alt=風華高科跌停！國產替代的邏輯還在嗎？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/f3a39a80-fb7f-46fc-8e42-9c9e71421db2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d555d04c.html title=風華高科跌停！國產替代的邏輯還在嗎？>風華高科跌停！國產替代的邏輯還在嗎？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/48c90bcc.html alt=熬了兩個通宵寫的！終於把多線程和多進程徹底講明白了！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8e306d12189741e4a3b8a74c64dec558 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/48c90bcc.html title=熬了兩個通宵寫的！終於把多線程和多進程徹底講明白了！>熬了兩個通宵寫的！終於把多線程和多進程徹底講明白了！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9ed0ee97.html alt=邏輯函數中IF函數判斷是與不是，NOT函數對參數值求反 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4757b6dd22c84a46b4bfaa7ee6165d20 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9ed0ee97.html title=邏輯函數中IF函數判斷是與不是，NOT函數對參數值求反>邏輯函數中IF函數判斷是與不是，NOT函數對參數值求反</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/28832a87.html alt=幾個底層邏輯，讓品牌年輕化 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/65157e16a2614181b3a9d4ae2c8c9737 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/28832a87.html title=幾個底層邏輯，讓品牌年輕化>幾個底層邏輯，讓品牌年輕化</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4a9bc155.html alt=終於！經過68個小時鏖戰！西昌經久鄉森林火災明火被撲滅 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RuyNTGl1MtABy4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4a9bc155.html title=終於！經過68個小時鏖戰！西昌經久鄉森林火災明火被撲滅>終於！經過68個小時鏖戰！西昌經久鄉森林火災明火被撲滅</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/68e43757.html alt=各種元器件主要物質的線性熱膨脹係數總彙 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/152974163166004c9f2ef5b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/68e43757.html title=各種元器件主要物質的線性熱膨脹係數總彙>各種元器件主要物質的線性熱膨脹係數總彙</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e3a19efd.html alt=線性膨脹係數在注塑產品中的應用（附常用塑料線型膨脹係數） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d26c39925f264665bd3f98663b4e253e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e3a19efd.html title=線性膨脹係數在注塑產品中的應用（附常用塑料線型膨脹係數）>線性膨脹係數在注塑產品中的應用（附常用塑料線型膨脹係數）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3f0b36db.html alt=線性代數精華——從正交向量到正交矩陣 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/3b3ba2948df841a4b0390e47943645f6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3f0b36db.html title=線性代數精華——從正交向量到正交矩陣>線性代數精華——從正交向量到正交矩陣</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/adeb1730.html alt=為什麼說形式邏輯讓人明辨是非，辯證邏輯讓人深刻睿智？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/dfic-imagehandler/10d027d9-a500-45e9-95e2-b677851d511b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/adeb1730.html title=為什麼說形式邏輯讓人明辨是非，辯證邏輯讓人深刻睿智？>為什麼說形式邏輯讓人明辨是非，辯證邏輯讓人深刻睿智？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e967f4e9.html alt=AI「王道」邏輯編程的復興？清華提出神經邏輯機，已入選ICLR class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d61270ccefc24f7b9482ec49590f94ba style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e967f4e9.html title=AI「王道」邏輯編程的復興？清華提出神經邏輯機，已入選ICLR>AI「王道」邏輯編程的復興？清華提出神經邏輯機，已入選ICLR</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2f276b81.html alt="USB 設備的邏輯組織結構" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7aeb24bbafcf4bcd9e8e58e04702f35c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2f276b81.html title="USB 設備的邏輯組織結構">USB 設備的邏輯組織結構</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>