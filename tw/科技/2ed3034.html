<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>ICLR2020推薦閱讀論文50篇 | 极客快訊</title><meta property="og:title" content="ICLR2020推薦閱讀論文50篇 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/87595ce587984dcc96abcfc72e6cb0a3"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2ed3034.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2ed3034.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2ed3034.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2ed3034.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2ed3034.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2ed3034.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/2ed3034.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/2ed3034.html><meta property="article:published_time" content="2020-10-29T21:08:29+08:00"><meta property="article:modified_time" content="2020-10-29T21:08:29+08:00"><meta name=Keywords content><meta name=description content="ICLR2020推薦閱讀論文50篇"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/2ed3034.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>ICLR2020推薦閱讀論文50篇</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/87595ce587984dcc96abcfc72e6cb0a3><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ba69c4810c764e0bba3b045edb3eb519><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/99015da0c9b14d64b1186ca61fccf640><p class=pgc-img-caption></p></div><p>文章發佈於公號【數智物語】 （ID：decision_engine），關注公號不錯過每一篇乾貨。</p><blockquote><p>來源 | 香儂科技</p><p>作者 | 香儂科技</p></blockquote><p>我們為大家整理了ICLR2020的相關論文，此次分享的是從Openreview中選取的部分論文，共50篇，其中大部分為NLP相關。文中涉及的相關論文推薦指數與推薦理由僅為小編個人觀點，利益無關。希望大家可以從中獲得啟發。</p><p>01</p><p><strong>推薦指數4.0論文</strong></p><p><strong>01</strong></p><p><strong>論文1</strong></p><p><strong>Reducing Transformer Depth on Demand with Structured Dropout</strong></p><p>https://openreview.net/pdf?id=SylO2yStDr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6fe71251e9ab48efa58b936588ed8e1b><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>還是那個問題，Transformer、BERT等模型太大，我們想要給模型瘦身而效果不減。</p><p>本文提出使用LayerDrop——隨機丟棄一些層，來給模型減肥。這種Drop方法既可以起到正則化的效果，更重要的是，它可以無痛剪枝，不需要再次微調，一勞永逸，簡直令人激動。</p><p><strong>推薦理由：</strong>方法非常優雅，在多個任務上的實驗也表明了這種方法的有效性，怎一個好字了得。</p><p><strong>推薦指數：</strong>4.0</p><p><strong>02</strong></p><p><strong>論文2</strong></p><p><strong>On Layer Normalization in the TransformerArchitecture</strong></p><p>https://openreview.net/pdf?id=B1x8anVFPr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8ecd0364079b4ab98dc7a1daf4bff917><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>我們知道，在原始的Transformer中，Layer Norm在跟在Residual之後的，我們把這個稱為Post-LN Transformer。</p><p>而且用Transformer調過參的同學也知道，Post-LN Transformer對參數非常敏感，需要很仔細地調參才能取得好的結果，比如必備的warm-up學習率策略，這會非常耗時間。</p><p>所以現在問題來了，為什麼warm-up是必須的？能不能把它去掉？</p><p>本文的出發點是：既然warm-up是訓練的初始階段使用的，那肯定是訓練的初始階段優化有問題，包括模型的初始化。</p><p>從而，作者發現，Post-LN Transformer在訓練的初始階段，輸出層附近的期望梯度非常大，所以，如果沒有warm-up，模型優化過程就會炸裂，非常不穩定。</p><p>既然如此，本文作者嘗試把LayerNorm換個位置，比如放在Residual的過程之中（稱為Pre-LN Transformer），再觀察訓練初始階段的梯度變化，發現比Post-LN Transformer不知道好到哪裡去了，甚至不需要warm-up，從而進一步減少訓練時間，這一結果的確令人震驚。</p><p><strong>推薦理由：</strong>本文別出心裁，用實驗和理論驗證了Pre-LN Transformer結構不需要使用warm-up的可能性，其根源是LN層的位置導致層次梯度範數的增長，進而導致了Post-LN Transformer訓練的不穩定性。</p><p>本文第一次將warm-up、LayerNorm、gradient和initialization聯繫起來，非常值得一讀！</p><p><strong>推薦指數：</strong>4.0</p><p>02</p><p><strong>推薦指數3.5論文</strong></p><p><strong>03</strong></p><p><strong>論文3</strong></p><p><strong>Copy That! Editing Sequences by CopyingSpans</strong></p><p>https://openreview.net/pdf?id=SklM1xStPB</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4b1a33ca4ed04ee7883833aae628507d><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>對於文本修改任務如糾錯，SEQ2SEQ方法其實並不好，這是因為大部分文本沒有錯，只需要單純copy即可。<br></p><p>本文基於大篇幅copy現象提出copy spans方法去動態地修改序列。方法非常簡單，但在代碼修復、語法糾錯任務上表現很好。</p><p><strong>推薦理由：</strong>方法簡潔有效，未來有很大的擴展空間。</p><p><strong>推薦指數：</strong>3.5</p><p><strong>04</strong></p><p><strong>論文4</strong></p><p><strong>SuperBloom: Bloom Filter MeetsTransformer</strong></p><p>https://openreview.net/pdf?id=SJxy5A4twS</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/60914ba8151142e891ba1225807e41d3><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>Bloom Filter是用來判定一個元素“是否一定不在集合”中的方法，也就是說，BF只有兩種結果：可能在與一定不在。BF使用的是哈希表方法。</p><p>另一方面，詞向量表一般非常大，這會降低訓練和推理的效率。本文在Transformer中使用BF算法，對詞向量矩陣大幅瘦身，並且在多個實驗上達到Recall的提升。</p><p><strong>推薦理由：</strong>方法非常有趣，效果也很明顯。</p><p><strong>推薦指數：</strong>3.5</p><p><strong>05</strong></p><p><strong>論文5</strong></p><p><strong>Calibration, Entropy Rates, And Memory inLanguage Models</strong></p><p>https://openreview.net/pdf?id=B1eQcCEtDB</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ba45951065d34bbfadc503630b83b332><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文集中回答一個問題：語言模型生成句子的長距離依賴和真實語言的長距離依賴之間應該如何比較。以往的度量是用熵率衡量，越小越好。</p><p>但是，GPT-2儘管實現了23.7的PPL，它的熵率卻達到了61.2，這說明這兩個指標並不是在說同一件事。本文提出一種基於校準的方法檢測和修正語言模型長距離依賴和真實分佈之間不一致問題。</p><p><strong>推薦理由：</strong>本文有助於學界重視LM中長距離依賴和future-aware的問題。</p><p>推薦指數：3.5</p><p><strong>06</strong></p><p><strong>論文6</strong></p><p><strong>DEFINE: Deep Factorized Input WordEmbeddings for Neural Sequence Modeling</strong></p><p>https://openreview.net/pdf?id=rJeXS04FPH</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/27d798c06aa04785be606a84b686b656><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>還是那個問題，詞典太大導致首尾兩個embedding參數量龐大（實際上也沒必要，因為很多詞出現頻率很低）。</p><p>本文提出使用“低維嵌入—>分層分組變換—>高維嵌入”的方法降低參數量，直接把Transformer-XL的參數化降一半。</p><p>在WikiText-103和PTB上的實驗表明，參數量大幅減少的同時效果卻不差；在WMT2014 En-De上的實驗表明，參數量的降低是顯著的。</p><p><strong>推薦理由：</strong>這種方法真的挺優雅的，感覺還可以改進，比如把FFN換成Attention和Conv啥的魔改一下。</p><p><strong>推薦指數：</strong>3.5</p><p><strong>07</strong></p><p><strong>論文7</strong></p><p><strong>Understanding Knowledge Distillation inNon-Autoregressive Machine Translation</strong></p><p>https://openreview.net/pdf?id=BygFVAEKDH</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/062cd605456a4209849ee227d5dc6706><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>NAT，其中基本都使用到了知識蒸餾（就是用teacher模型的輸出當作NAT模型的輸出目標），但是為什麼非要這麼做呢？難道用有噪音的輸出不會效果更差嗎？</p><p>本文使用大量實驗告訴我們：蒸餾降低了NAT模型的條件熵。基於此，本文還提出了幾種進一步提高NAT效果的方法，已經和AT效果差不多了。</p><p><strong>推薦理由：</strong>知識蒸餾恐怖如斯！</p><p><strong>推薦指數：</strong>3.5</p><p><strong>08</strong></p><p><strong>論文8</strong></p><p><strong>Encoder-Agnostic Adaptation forConditional Language Generation</strong></p><p>https://openreview.net/pdf?id=B1xq264YvH</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4e43100791774bafb0ddd7cd3f1ff517><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>眾所周知，對於非生成類任務，大型預訓練模型已經成為刷SOTA的標配，但是怎麼把預訓練用到語言生成上去呢？</p><p>本文提出了三種encoder-agnostic的方法，將任一encoder得到的特徵表示送入decoder並生成相應任務的句子。這裡的encoder是具體任務相關的、隨機初始化的。</p><p>而這三種方法的區別在於，decoder的不同部分有不同的初始化，但總的來說都分為使用預訓練模型初始化或隨機初始化。整個模型使用監督方法訓練。在多個生成任務上的實驗表明，這種方法的確可以顯著提高生成效果。</p><p><strong>推薦理由：</strong>方法簡單、有效、通用。</p><p><strong>推薦指數：</strong>3.5</p><p><strong>09</strong></p><p><strong>論文9</strong></p><p><strong>Combiner: Inductively Learning TreeStructured Attention in Transformers</strong></p><p>https://openreview.net/pdf?id=B1eySTVtvB</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7f6a82d3f3d2486aa1c4e13334a966c8><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文使用了Sparse Attention和Hierarchical Attention Block的方法去得到句子的語法樹，只需要對Transformer稍作修改，就可以取得相當好的結果。</p><p>和前一篇GrammarInduction文章不同，本文注重得到句子結構，也就是語法樹，而前者注重語法規則。</p><p><strong>推薦理由：</strong>效果是真的不錯，方法也很優雅，推薦閱讀。</p><p><strong>推薦指數：</strong>3.5</p><p><strong>10</strong></p><p><strong>論文10</strong></p><p><strong>Encoding Word Order in Complex Embeddings</strong></p><p>https://openreview.net/pdf?id=Hke-WTVtwr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1e77f7128a4e48dfa687914c43fbf550><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文提出一種通用的基於複數的方法去直接導出詞向量，從而綜合了詞向量和位置向量，還可以表示詞之間的相對距離。</p><p>而且Transformer中的positional embedding和word embedding結合方式可以看作是本文方法的一種特殊情況。在多個任務上的實驗表明了這種方法的有效性。</p><p><strong>推薦理由：</strong>複數空間非常有趣，很有發展潛力。</p><p><strong>推薦指數：</strong>3.5</p><p><strong>11</strong></p><p><strong>論文11</strong></p><p><strong>Large-scale Pretraining for NeuralMachine Translation with Tens of Billions of Sentence Pairs</strong></p><p>https://openreview.net/pdf?id=Bkl8YR4YDB</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/deee771a49db4e9998bd8904a4a67d1c><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文在40B的超大數據上驗證了NMT模型的效果，並且提出了一種平衡數據集利用和預訓練效率的方法。</p><p>實驗結果表明，大規模預訓練的確可以顯著提高NMT效果，而且也從經驗上驗證了在大語料下大模型的必要性。我們在WMT17 Ch-En上實現了32.3的BLEU值，達到新的SOTA。</p><p><strong>推薦理由：</strong>本文簡直是暴力出奇跡的典範，雖然暴力，但是也確實驗證了數據集和模型大小的有用性，還提出了大力下的訓練方法，為廣大“貧窮”的科研人員開闢了一條研究的路。</p><p><strong>推薦指數：</strong>3.5</p><p>03</p><p><strong>推薦指數3.0論文</strong></p><p><strong>12</strong></p><p><strong>論文12</strong></p><p><strong>An Exponential Learning Rate Schedule forBatch Normalized Networks</strong></p><p>https://openreview.net/pdf?id=rJg8TeSFDH</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6a0a152e5a1445b6bfc6c91d239d67c0><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>這是一篇無關NLP的論文。本文提出了一個每次乘以(1+a)的指數遞增學習率schedule方法，並在數學上證明它等價於BN + SGD + WD + Momentum + Standard Rate Tuning。</p><p>本文的出發點基於各種Normalization方法的scale-invariant性質：rescale模型的參數不會改變模型的預測結果。</p><p>因此，本文的方法可以總結為：（在使用如BN的Normalization方法的模型上）將初始學習率調為一個合適的值（如0.1），然後以（1+a）指數增長；當驗證集的loss不再降低之後，再以（1+b,b&lt;a）的指數增長學習率；重複這個過程直到收斂。在CNN和ResNets上的實驗表明了這種方法的真實性（有效性還需要更多實驗證明）。</p><p><strong>推薦理由：</strong>本文突破了以往的固有想法——學習率要不斷下降，從理論上驗證了在Normalization下學習率指數上升的等價性，具有一定新穎性。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>13</strong></p><p><strong>論文13</strong></p><p><strong>Improving the Gating Mechanism of Recurrent Neural Networks</strong></p><p>https://openreview.net/pdf?id=r1lnigSFDr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/86745bc8b2ba44f48ff1243e005f84fb><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>各種模型內的門控機制期望可以動態地實現信息的更迭，保留重要的信息而丟棄不重要的信息。</p><p>從BP的角度看，保留信息實際上是梯度的保留，也就是接近函數的飽和區域（梯度約為1，也即函數值接近0或1）。但問題是，一旦接近飽和區域，模型實際上很難建模長距離信息。</p><p>本文使用兩種方法緩解這些問題：（1）對遺忘門初始化為[0,1]上的均勻分佈；（2）提出一個refine gate。在多個任務的實驗驗證了這種方法的有效性。</p><p><strong>推薦理由：</strong>本文探究了一系列門控方法，並且提出了一種新的門控方法——UR-LSTM，可以更好地建模長距離依賴。另外，本文的實驗圖很好看。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>14</strong></p><p><strong>論文14</strong></p><p><strong>Masked Translation Model</strong></p><p>https://openreview.net/pdf?id=HygaSxHYvH</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ad713bf68a584c9a8641e1414e81b4a5><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>長期以來，機器翻譯都是基於seq2seq架構和AR生成。</p><p>本文提出的Masked Translation Model把編碼器和解碼器融合在一起，基於Mask方法，既可以AR生成，也可以Non-AR生成，還提供多種解碼策略，非常flexible，可以看作一種更通用的KERMIT模型。</p><p><strong>推薦理由：</strong>這種翻譯方法近來非常流行，本文繼續提高了這種方法的翻譯結果，不但可以做翻譯，甚至可以做LM、Text Editing等，很方便。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>15</strong></p><p><strong>論文15</strong></p><p><strong>Multichannel Generative Language Models</strong></p><p>https://openreview.net/pdf?id=r1xQNlBYPS</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a0c0f750b25e40e1ac175d7008ebd2b2><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文提出了一個改進的KERMIT模型，用於建模多語言（這裡稱為多channel）的聯合分佈，如此既可以進行條件生成（如MT），也可以進行無條件生成（直接生成句子），不但可以AR生成，也可以Non-AR生成，功能豐富。</p><p><strong>推薦理由：</strong>再次強調這種通用模型，是非常厲害的！</p><p><strong>推薦指數：</strong>3.0</p><p><strong>16</strong></p><p><strong>論文16</strong></p><p><strong>Semi-Supervised Named Entity Recognitionwith CRF-VAEs</strong></p><p>https://openreview.net/pdf?id=BkxnKkrtvS</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/89a05933f410404a9d2bc4296736fe11><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文獨具匠心地把CRF和VAE相融合去做半監督下的NER。</p><p>標準情況下的NER是編碼器->CRF->輸出標籤，而本文還加入了未標註文本，把CRF當成一種近似後驗加入優化ELBO，從而實現半監督下的模型學習。實驗證明了這種方法具有一定有效性。</p><p><strong>推薦理由：</strong>方法很有趣，但不太優雅。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>17</strong></p><p><strong>論文17</strong></p><p><strong>A Probabilistic Formulation ofUnsupervised Text Style Transfer</strong></p><p>https://openreview.net/pdf?id=HJlA0C4tPS</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f475fcfeb26e4c2e893a9e84e618773a><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文以一種概率生成模型的角度去分析無監督文本風格遷移，在沒有平行語料的情況下，訓練兩個領域的隱平行句子。</p><p>然後使用兩個LM先驗模型+兩個共享參數的領域轉換模型直接去建模整體聯合分佈，然後使用ELBO優化模型。在多個數據集上的實驗結果表明本文的方法可以提高Acc和BLEU值。</p><p><strong>推薦理由：</strong>以一種概率分佈的視角搭建起了無監督文本風格遷移各種方法的橋樑，可以讀一下。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>18</strong></p><p><strong>論文18</strong></p><p><strong>Word Embedding Re-Examined: Is theSymmetric Factorization Optimal?</strong></p><p>https://openreview.net/pdf?id=HklCk1BtwS</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c8af9f0da1fc46dfae7ad84989791fde><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文從理論上分析了word embedding具有一些良好性質的原因，本質上是共現矩陣到嵌入空間的低秩變換，這使得詞之間的相對距離得以保留，從而共現矩陣有的性質，word embedding也得以繼承。</p><p><strong>推薦理由：</strong>本文的證明過程具有一定的指導意義。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>19</strong></p><p><strong>論文19</strong></p><p><strong>Learning to Contextually AggregateMulti-Source Supervision for Sequence Labeling</strong></p><p>https://openreview.net/pdf?id=HJe9cR4KvB</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2a03788346d94ecb903343944b712a3e><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文學習了一個聚合同一任務不同數據源數據的序列標註模型，可以通過這種方法緩解數據內標註分佈偏移和噪聲的問題，也可以用於學習跨領域的數據。</p><p><strong>推薦理由：</strong>方法簡單，效果不錯，跨領域學習也有吸引力。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>20</strong></p><p><strong>論文20</strong></p><p><strong>BERTScore: Evaluating Text Generationwith BERT</strong></p><p>https://openreview.net/pdf?id=SkeHuCVFDr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/899b5fb63ed146e79705092e618ce497><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>BERT既然這麼牛逼，為什麼不能用它作為一種度量標準呢？直接把生成句子和參考句子懟進BERT，然後計算token之間的cos相似度，然後用一個max-pool，再玄學算一下，暴力有效，因吹斯聽！</p><p><strong>推薦理由：</strong>據說和人類評估更接近，而且也比較魯棒（這篇文章好像是某會被拒了，重投ICLR）。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>21</strong></p><p><strong>論文21</strong></p><p><strong>Parallel Neural Text-to-Speech</strong></p><p>https://openreview.net/pdf?id=BJeFQ0NtPS</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/faf8f88baf0d4b05bfa317739450fc2f><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>傳統的TTS一般是follow了一個多流程的pipeline，近年來基於深度學習的方法使用的是seq2seq，比如WaveNet、ClariNet等。</p><p>本文提出ParaNet，一種Non-AR的TTS方法，直接提取Mel頻譜，再使用Attention蒸餾，速度很快，效果不錯。</p><p><strong>推薦理由：</strong>文本轉語音的研究比較少，可以拿出來給大家復（預）習下。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>22</strong></p><p><strong>論文22</strong></p><p><strong>Contextual Text Style Transfer</strong></p><p>https://openreview.net/pdf?id=HkeJzANFwS</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/13fbebf6dca546eeaeb816d7b4e8a1a7><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文介紹了一種新的任務：上下文相關文本風格遷移。所謂上下文相關，就是在遷移的時候要保持上下文語義的一致性，這就在單純的文本風格遷移上多了一層要求。</p><p>本文提出的方法基於半監督，使用了多個損失，比較複雜，但是不難理解，效果卻也不錯。</p><p><strong>推薦理由：</strong>挖了個新坑，妙啊。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>23</strong></p><p><strong>論文23</strong></p><p><strong>Are Transformers Universal Approximationsof Sequence-to-Sequence Functions?</strong></p><p>https://openreview.net/pdf?id=ByxRM0Ntvr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4de25dff32b547e3a7fd51acf8af7a2d><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文從理論上證明了：Transformer是一個支持連續和置換不變性序列到序列函數的通用近似器，只要有足夠好的positional embedding。</p><p>此外，本文還發現把self-attention替換成其他的一些上下文映射函數可以取得更好的效果。</p><p><strong>推薦理由：</strong>滿頁的公式，還是你們來。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>24</strong></p><p><strong>論文24</strong></p><p><strong>TABNET: Attentive Interpretable TabularLearning</strong></p><p>https://openreview.net/pdf?id=BylRkAEKDH</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/03b29e99202b4ef8a57af7a6c214be8a><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文提出使用sequential attention去建模表格型數據，適用於各種分類和迴歸任務。</p><p><strong>推薦理由：</strong>Kaggle福利，現成模板，童叟無欺。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>25</strong></p><p><strong>論文25</strong></p><p><strong>Mixout: Effective Regularization toFinetune Large-Scale Pretrained Language Models</strong></p><p>https://openreview.net/pdf?id=HkgaETNtDB</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/342a39599df4433485a2230d68ef45ef><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文提出一種新的用於微調預訓練模型的L2正則化方法：mixout。</p><p>所謂mixout，就是在微調的整個過程中，動態地融合兩個階段模型的參數的方法，這樣可以使得微調漸次進行，不至於導致效果的崩塌。在GLUE幾個任務上的實驗驗證了該方法的有效性。</p><p><strong>推薦理由：</strong>方法挺有趣，可以進一步推廣到其他任務看看效果。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>26</strong></p><p><strong>論文26</strong></p><p><strong>Language GANs Falling Short</strong></p><p>https://openreview.net/pdf?id=BJgza6VtPB</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/324d2ecfb76d4cf491842e4f4c71f1db><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>評價一個模型生成句子的效果有兩方面的指標：質量和多樣性。只重視前者而忽略後者會導致exposure bias，尤其對於像GAN這種生成模型。</p><p>本文創造性地使用temperature作為一種評估模型quality-diversitytrade-off的指標。通過這種方法，作者發現基於MLE的生成模型在quality/diversity上都超過了基於GAN的方法。</p><p><strong>推薦理由：</strong>很有意思的文章，不過GAN真的是弟弟？</p><p><strong>推薦指數：</strong>3.0</p><p><strong>27</strong></p><p><strong>論文27</strong></p><p><strong>Unifying Question Answering, TextClassification, and Regression via Span Extraction</strong></p><p>https://openreview.net/pdf?id=HketzTNYwS</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/05e405c18d5a40f8a65e6e68d80ef43c><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>用BERT去做QA、文本分類和迴歸任務時，都是使用的最後一層單元的Span（或長或短），本文認為可以把這些任務綜合起來進行。</p><p>對QA，按照原BERT處理，直接抽取即可；對文本分類，在文本前加上“positive ornegative”，然後讓模型去判別“positive/negative”對應位置的span即可。</p><p>對句對分類，在文本前加上“entailment, contradictionor neutral”；對迴歸問題，直接對可能的值均勻分割成一些離散的值。</p><p>使用這種方法，我們有望把這些任務合在一種模型裡，減少了模型的設計量，並且如果做成多任務，還可以提高有限數據量下的效果。</p><p><strong>推薦理由：</strong>本文來自Salesforce論文小分隊，不用我多說了嗷。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>28</strong></p><p><strong>論文28</strong></p><p><strong>Are Pre-trained Language Models Aware ofPhrases? Simple but Strong Baselines for Grammar Induction</strong></p><p>https://openreview.net/pdf?id=H1xPR3NtPB</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/ec25f414be37418e921d2cf2ee4107c0><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文提出直接從預訓練的語言模型中提取語法結構成分，形成語法樹。</p><p>本文提出直接從Transformer-like的預訓練語言模型中，利用其多頭機制，直接提取各種短語結構，包括動詞短語、名詞短語、副詞短語等。</p><p>本文在英語語法推導任務上驗證了該方法的有效性，可以作為一個很強的baseline。</p><p><strong>推薦理由：</strong>不再訓練，不用規則！語法推導，點擊就送！</p><p><strong>推薦指數：</strong>3.0</p><p><strong>29</strong></p><p><strong>論文29</strong></p><p><strong>Dynamically Pruned Message PassingNetworks for Large-scale Knowledge Graph Reasoning</strong></p><p>https://openreview.net/pdf?id=rkeuAhVKvB</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4f3174b712724a79b8cb98d7639f559a><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>知識圖譜推理最近變得很火，這其實也是一個有巨大潛力的方向。以往的研究大多是給出一個推理路徑（path-based）。</p><p>本文提出一種更靈活、更具解釋力的方法，能夠在推理的同時進行網絡圖剪枝得到子圖（sub-graph），提高大規模知識圖譜下的推理效率。</p><p>本文采用了attention、consciousness prior等多種方法，十分novel。</p><p><strong>推薦理由：</strong>新的推理方法，有探索價值。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>30</strong></p><p><strong>論文30</strong></p><p><strong>An Empirical Study on Post-ProcessingMethods for Word Embeddings</strong></p><p>https://openreview.net/pdf?id=Byla224KPr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/506bd5e2130747d8832aff55031f934f><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文又是一篇偏理論的文章，研究了詞向量的後處理方法，把這種方法視為對Gram矩陣的縮放。</p><p>兩個詞向量集應該滿足各向同性縮放，從而中心核對齊（CKA）可以作為一種度量，因此我們目標就是去最大化這個相似性度量。</p><p>為此，本文提出在半黎曼流形（semi-Riemannian manifold）上優化，並且scale了其中的單位矩陣。實驗表明本文的方法具有一定效果。</p><p><strong>推薦理由：</strong>本文的詞向量後處理方法看似很複雜，但實際上作者給出了步驟，還是非常直觀的。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>31</strong></p><p><strong>論文31</strong></p><p><strong>Reinforcement Learning BasedGraph-to-Sequence Model for Natural Question Generation</strong></p><p>https://openreview.net/pdf?id=HygnDhEtvr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9df8b870a5db4e31822f647ba7e55432><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>作為一個NLP中比較小眾的領域，問題生成（QG）可以說是小眾中偏困難的了。它比QA難在，它是一個純粹的文本生成任務，而QA一般是作為分類任務。</p><p>本文首先提出一個對齊網絡進行詞層面和隱層層面的對齊；然後使用一個雙向的圖到序列生成器獲得進一步的特徵表示；最後使用一個RNN解碼器生成句子。</p><p>而且，本文還使用了CE損失和RL損失相結合的方法保證語法和語義的兼顧。</p><p><strong>推薦理由：</strong>本文提出了一個基於RL損失的Graph2Seq模型用於QG，實驗效果也是槓槓的。</p><p><strong>推薦指數：</strong>3.0</p><p><strong>32</strong></p><p><strong>論文32</strong></p><p><strong>Topology of Deep Neural Networks</strong></p><p>https://openreview.net/pdf?id=SkgBfaNKPr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/dd52af0223264eb4828b68fe6a5e0eee><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文認為，從拓撲學的角度看，深度模型實際上是把拓撲上非常複雜的數據轉化為簡單的拓撲結構，使得在輸出的時候線性可分，並且ReLU會使得這種簡化過程更快。</p><p><strong>推薦理由：</strong>把深度學習模型置於拓撲學的觀點下，給出瞭解釋深度學習模型的一個非常直觀的途徑。</p><p><strong>推薦指數：</strong>3.0</p><p>04</p><p><strong>推薦指數2.5論文</strong></p><p><strong>33</strong></p><p><strong>論文33</strong></p><p><strong>Toward Controllable Text ContentManipulation</strong></p><p>https://openreview.net/pdf?id=Skg7TerKPH</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/48c5ee69d9614169bd0168edfeea28a8><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>表格轉文本和受限文本生成是近期比較火熱的議題，把二者結合起來又如何呢。</p><p>本文提出受限表格轉文本生成方法，對給定表格和一個參考句，模型需要生成一段文本，文本的內容是描述表格，但風格卻是參考句。</p><p>在這種設置下，本文首先構建了兩個專門的數據集，然後使用seq2seq+attention+copy+reconstruction的方法訓練模型。</p><p><strong>推薦理由：</strong>本文的出發點比較有趣，提出了一個簡單實用的模型結構，並且結合了表格轉文本和文本風格遷移兩大主題，可以閱讀。</p><p><strong>推薦指數：</strong>2.5</p><p><strong>34</strong></p><p><strong>論文34</strong></p><p><strong>A Syntax-Aware Approach for UnsupervisedText Style Transfer</strong></p><p>https://openreview.net/pdf?id=Bkll_kHFPB</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/690e495b6dd94a19b61250b6f9100dc2><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文提出把預測語法標籤作為附加任務去做無監督的文本風格轉換；</p><p>損失還有三個：reconstruction, back-translation和discrimination。在幾個常用數據集的實驗上取得了一致的提高。</p><p><strong>推薦理由：</strong>這種語法多任務可以看作一種防止過擬合的手段，出乎我意料的是，效果竟然還不錯。</p><p><strong>推薦指數：</strong>2.5</p><p><strong>35</strong></p><p><strong>論文35</strong></p><p><strong>Compressive Transformers for Long-RangeSequence Modeling</strong></p><p>https://openreview.net/pdf?id=SylKikSYDH</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/2e3bc4af143d447b81be27aa5eee791b><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>LSTM不能建模長距離序列，Transformer存儲代價又太大，Transformer-XL會把很舊的單元丟掉，基於此，本文提出把Transformer-XL中要丟棄的部分壓縮，當作一種“壓縮記憶”。</p><p>怎麼壓縮呢？用花式CNN和Pooling……當然了，本文還提出一個新的數據集——PG-19，在這個數據集、WikiText-103和Enwiki8上達到SOTA。</p><p>通過各種實驗，本文證明了對超長文本建模，CompressedTransformer的確表現最好。</p><p><strong>推薦理由：</strong>方法簡單，但是有效呀！</p><p><strong>推薦指數：</strong>2.5</p><p><strong>36</strong></p><p><strong>論文36</strong></p><p><strong>Contextual Temperature for LanguageModeling</strong></p><p>https://openreview.net/pdf?id=H1x9004YPr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dc401009e1e04ec69757a16a25655768><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>在softmax中使用temperature已經是一個基本操作了，但是之前的溫度設置要麼全程固定，要麼隨訓練保持規律地變化。</p><p>本文認為，溫度大小應該和上下文有關，從而使用當前步的上文學習一個對當前步的溫度，在MoS上進行LM實驗，並且採用了多個正則項。大概有2個點的PPL提升。</p><p><strong>推薦理由：</strong>其實溫度和上下文相關還是很有道理的，但是本文的方法比較簡單，好在實驗還做得不錯。</p><p><strong>推薦指數：</strong>2.5</p><p><strong>37</strong></p><p><strong>論文37</strong></p><p><strong>On Variational Learning of ControllableRepresentations for Text without Supervision</strong></p><p>https://openreview.net/pdf?id=Hkex2a4FPr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4fe7ae5493594fa18dff2d7c7ff68e52><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文分析了使用VAE控制無監督文本生成的困難所在——後驗空間的潛在空缺（latent vacancy）問題。</p><p>由此，本文提出使用兩個損失項將後驗高斯均值映射到一個受限空間中，從而鼓勵該空間能夠被“填滿”，進行更好的優化。</p><p>這兩個損失，一是結構化重構損失——一種max margin方法把目標往兩個方向推，二是正則項——約束embeddingmatrix為正交。</p><p><strong>推薦理由：</strong>方法簡單，效果尚可。</p><p><strong>推薦指數：</strong>2.5</p><p><strong>38</strong></p><p><strong>論文38</strong></p><p><strong>Neural Markov Logic Networks</strong></p><p>https://openreview.net/pdf?id=SkeGvaEtPr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/121b658196c54e088fce15d8c03498f5><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>馬爾科夫邏輯網絡（MLN）是一種將一階邏輯和概率圖模型相結合的統計學習方法。</p><p>本文提出的神經馬爾科夫邏輯網絡（NMLN）不需要顯式地指定一階邏輯規則，而通過神經網絡隱式地學習。在知識庫填充和分子數據生成的實驗上取得顯著效果。</p><p><strong>推薦理由：</strong>看起來很厲害，有空學習下。</p><p><strong>推薦指數：</strong>2.5</p><p><strong>39</strong></p><p><strong>論文39</strong></p><p><strong>EINs：LongShort-Term Memory with Extrapolated Input Network Simplification</strong></p><p>https://openreview.net/pdf?id=B1l5m6VFwr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/04ae17541390486f9738d103e24cd9f6><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文通過改寫LSTM和GRU的方程，將它們作為一種動態Hopfield網絡，推出一種參數更少的RNN結構——EINS。</p><p><strong>推薦理由：</strong>雖然參數少了，但是更復雜了呢。</p><p><strong>推薦指數：</strong>2.5</p><p>05</p><p><strong>推薦指數2.0論文</strong></p><p><strong>40</strong></p><p><strong>論文40</strong></p><p><strong>Faster and Just as Accurate: A SimpleDecomposition for Transformer Models</strong></p><p>https://openreview.net/pdf?id=B1gKVeBtDH</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c5c8874ce12d415e8f237402284db964><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>儘管Transformer和BERT如此成功，但是訓練它們需要耗費巨量的時間。</p><p>本文認為，我們不需要在self-attention的時候使得每個token去關注所有的token，一部分就夠了。</p><p>由此，本文提出前k層分成兩部分，各自self-attention，剩下的層再合起來self-attention，並且使用了知識蒸餾和層級相似度減少信息損失。在QA和句子相似度分類任務上的實驗驗證了這種方法的高效率和低損耗。</p><p><strong>推薦理由：</strong>顯卡燃燒的味道不香嗎？</p><p><strong>推薦指數：</strong>2.0</p><p><strong>41</strong></p><p><strong>論文41</strong></p><p><strong>Incorporating BERT into Neural MachineTranslation</strong></p><p>https://openreview.net/pdf?id=Hyl7ygStwB</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/355f9f72427a4334a8e7613a2f7baf32><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>如何把預訓練模型，如BERT融入到MT中是自BERT誕生以來的問題。</p><p>不直接使用BERT初始化模型，本文是把BERT的輸出融入到SEQ2SEQ模型的每一層，用在每個self-attention之中。</p><p>此外，本文還提出drop-net——隨機丟棄BERT表示或模型本身的表示，充分利用兩個方面的信息。實驗結果還不錯。</p><p><strong>推薦理由：</strong>本文可以算是成功地把BERT加入到了MT中，方法比較奇特，但是還不夠優雅。</p><p><strong>推薦指數：</strong>2.0</p><p><strong>42</strong></p><p><strong>論文42</strong></p><p><strong>Robustness Verification for Transformers</strong></p><p>https://openreview.net/pdf?id=BJxwPJHFwS</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e96604644a5648c58e32056af6fe1aaf><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文給Transformer提出了首個魯棒性驗證方法，從理論和實驗兩個方面給出了界，並通過這種方法分析了情感分析中不同詞的重要性。</p><p><strong>推薦理由：</strong>來點偏理論的研究增增味。</p><p><strong>推薦指數：</strong>2.0</p><p><strong>43</strong></p><p><strong>論文43</strong></p><p><strong>On Predictive Information Sub-Optimalityof RNNs</strong></p><p>https://openreview.net/pdf?id=HklsHyBKDr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/072b605f2c254b2880bd97590c64a497><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>現實世界中，人之所以如此智能，就是因為人能夠記住很久之前的信息，從而能夠幫助預測未來，而各種RNN從形式上好像也是記住過去，預測未來。</p><p>然而本文通過兩個數據集上的實驗表明，RNN簡直弱爆了。</p><p><strong>推薦理由：</strong>本文其實不太偏CS，算是給RNN的缺點提供了另外一種視角。</p><p><strong>推薦指數：</strong>2.0</p><p><strong>44</strong></p><p><strong>論文44</strong></p><p><strong>MUSE: Multi-Scale Attention Model forSequence to Sequence Learning</strong></p><p>https://openreview.net/pdf?id=SJe-3REFwr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/453b5318bb3447e2a7f3f3fccbdaab9e><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文認為，像Transformer這樣的模型對於建模長距離語義依然存在困難，從而提出多種建模方式（Attention、Dynamic Conv、FFN）平行計算，然後再聚合起來的方法。實驗結果出乎意料地不錯。</p><p><strong>推薦理由：</strong>為什麼不在WMT14 En-De上做呢？</p><p><strong>推薦指數：</strong>3.0</p><p><strong>45</strong></p><p><strong>論文45</strong></p><p><strong>Dropout: Explicit Forms and Capacity Control</strong></p><p>https://openreview.net/pdf?id=Bylthp4Yvr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3bab1929265a452fba5533655960b96b><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文回答了一個問題：dropout是如何控制模型的容量的。本文通過一些數學推導給出了dropout下深度模型的泛化界，並在多種任務上進行了實驗。</p><p><strong>推薦理由：</strong>又是一篇數學學渣殺手，不過已經相對容易了。</p><p><strong>推薦指數：</strong>2.0</p><p><strong>46</strong></p><p><strong>論文46</strong></p><p><strong>Mitigating Posterior Collapse in StronglyConditioned VAE</strong></p><p>https://openreview.net/pdf?id=rJlHea4Kvr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f03779f1c7c04a6aa1149b2e2f6b5ab5><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>對於有很強條件的CVAE，decoder往往會忽略隱變量，使得模型退化為一個AE模型。</p><p>本文發現，這種情況發現的原因之一是條件變量的融入方式：直接concat。本文提出一種新的重參數方法使得條件變量和隱變量能夠更好地融合，緩解後驗坍塌問題。</p><p><strong>推薦理由：</strong>本文融合兩個變量的方法比較巧妙，值得學習。</p><p><strong>推薦指數：</strong>2.0</p><p><strong>47</strong></p><p><strong>論文47</strong></p><p><strong>SSE-PT: Sequential Recommendation viaPersonalized Transformer</strong></p><p>https://openreview.net/pdf?id=HkeuD34KPH</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/35fc145584374dc68c6c6fc742fc23cc><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>使用深度學習的方法構建推薦系統是一個正在探索的領域，現在流行的方法是用RNN、CNN加Attention或Transformer模型，比如SASRec模型。</p><p>但是這些模型大都不能有個性化的推薦，要麼就是效果不太好。本文把item embedding和user embedding結合起來，並使用SSE正則化方法減緩embedding的過擬合現象。</p><p><strong>推薦理由：</strong>推薦系統還是要介紹一下的。</p><p><strong>推薦指數：</strong>2.0</p><p>06</p><p><strong>推薦指數1.0論文</strong></p><p><strong>48</strong></p><p><strong>論文48</strong></p><p><strong>Neural Phrase-to-Phrase MachineTranslation</strong></p><p>https://openreview.net/pdf?id=S1gtclSFvr</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9e9641ce72754c908bde82c3f33eca9b><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>本文提出一種新的短語到短語的機器翻譯方法，使用短語層級的attention，然後用greedy或beam 的方法decode，再結合一個記憶力模塊，實現了與Transformer相較的結果。</p><p><strong>推薦理由：</strong>看個新鮮也是看。</p><p><strong>推薦指數：</strong>1.0</p><p><strong>49</strong></p><p><strong>論文49</strong></p><p><strong>Lossless Single Image Super Resolutionfrom Low-Quality JPG Images</strong></p><p>https://openreview.net/pdf?id=r1l0VCNKwB</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cc0f450949de4353a62523d6e9f6361d><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>各種不可描述的圖片往往會被各種壓縮，給廣大宅男同志帶來很多困擾，要是能得到（無碼[劃掉]）高清圖該多好啊！本文對JPG圖像進行分辨率修復，算法簡單，有沒有效不知道。</p><p><strong>推薦理由：</strong>我選擇無損圖像。</p><p><strong>推薦指數：</strong>1.0</p><p>07</p><p><strong>推薦指數0.5論文</strong></p><p><strong>50</strong></p><p><strong>論文50</strong></p><p><strong>Emergence of Functional and StructuralProperties of the Head Direction System by Optimization of RNN</strong></p><p>https://openreview.net/pdf?id=HklSeREtPB</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b6cf915f4cea49bd873d30db06820206><p class=pgc-img-caption></p></div><p><strong>推薦說明：</strong>這篇什麼意思，求懂的同學講解一下……</p><p><strong>推薦理由：</strong>？</p><p><strong>推薦指數：</strong>0.5</p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/664557f82edc40e28cdfb72cf5d66637><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5370b869335c4228b7384bfaab622de1><p class=pgc-img-caption></p></div><p><strong>星標我，每天多一點智慧</strong></p><div class=pgc-img><img alt=ICLR2020推薦閱讀論文50篇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/aaa2bd51c1b84c9aa46c72ee0bacd3c9><p class=pgc-img-caption></p></div></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>ICLR2020</a></li><li><a>推薦</a></li><li><a>閱讀</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/fdc48726.html alt=後端好書閱讀與推薦（續五） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/37425581639e49c7a4ebd28fc6c90a1f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fdc48726.html title=後端好書閱讀與推薦（續五）>後端好書閱讀與推薦（續五）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/aabe7612.html alt=「推薦閱讀」C語言中內存的管理與使用—堆與棧 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/0dd69fd4ecb0474e955dbf37110fd824 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/aabe7612.html title=「推薦閱讀」C語言中內存的管理與使用—堆與棧>「推薦閱讀」C語言中內存的管理與使用—堆與棧</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/bf669198.html alt="「燦媽閱讀推薦」DK博物大百科 超過6000張插圖 展示了奇妙自然界" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d8341f9defe44e6692f6c93b36dc7b45 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/bf669198.html title="「燦媽閱讀推薦」DK博物大百科 超過6000張插圖 展示了奇妙自然界">「燦媽閱讀推薦」DK博物大百科 超過6000張插圖 展示了奇妙自然界</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ecc8f13.html alt=焊接界深度閱讀推薦，讀懂國內外焊接技術的發展趨勢 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1532567873522ed742c3699 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ecc8f13.html title=焊接界深度閱讀推薦，讀懂國內外焊接技術的發展趨勢>焊接界深度閱讀推薦，讀懂國內外焊接技術的發展趨勢</a></li><hr><li><a href=../../tw/%E9%81%8A%E6%88%B2/714dc3d.html alt=「推薦閱讀」李白寫詞嗎可以稱為詞人嗎 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=http://p9.pstatp.com/large/pgc-image/caf87567cd6248c98417044d9c75870f style=border-radius:25px></a>
<a href=../../tw/%E9%81%8A%E6%88%B2/714dc3d.html title=「推薦閱讀」李白寫詞嗎可以稱為詞人嗎>「推薦閱讀」李白寫詞嗎可以稱為詞人嗎</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e55aaf9b.html alt="論文推薦 | 袁修孝：航攝影像密集匹配的研究進展與展望" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/R6Ieh75DBRtmcY style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e55aaf9b.html title="論文推薦 | 袁修孝：航攝影像密集匹配的研究進展與展望">論文推薦 | 袁修孝：航攝影像密集匹配的研究進展與展望</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4653cf8e.html alt=論文推薦｜王濤：國產機載大視場三線陣CCD相機GNSS偏心矢量和IMU視軸偏心角標定技術 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/R6Ieh75DBRtmcY style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4653cf8e.html title=論文推薦｜王濤：國產機載大視場三線陣CCD相機GNSS偏心矢量和IMU視軸偏心角標定技術>論文推薦｜王濤：國產機載大視場三線陣CCD相機GNSS偏心矢量和IMU視軸偏心角標定技術</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/43600b7e.html alt=推薦10個值得一去的國外神器網站！登錄無需任何手段，實用又有趣 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8716c57443b84bf39d12f7181c19e5d7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/43600b7e.html title=推薦10個值得一去的國外神器網站！登錄無需任何手段，實用又有趣>推薦10個值得一去的國外神器網站！登錄無需任何手段，實用又有趣</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c4bdc082.html alt=化學老師推薦：按元素歸類的初中化學方程式集合！考前複習好資料 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/e672e532fceb4bde92dc10ca986250ad style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c4bdc082.html title=化學老師推薦：按元素歸類的初中化學方程式集合！考前複習好資料>化學老師推薦：按元素歸類的初中化學方程式集合！考前複習好資料</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5718fd72.html alt="論文推薦 | 閆廣峰：L1範數探測粗差失效的觀測量識別方法" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/R6W0QpMHySg0Qb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5718fd72.html title="論文推薦 | 閆廣峰：L1範數探測粗差失效的觀測量識別方法">論文推薦 | 閆廣峰：L1範數探測粗差失效的觀測量識別方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1fb68281.html alt=新品推薦｜你要的三防漆新品來啦 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/0d18e71454334714a9ab8ac0e0e8d019 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1fb68281.html title=新品推薦｜你要的三防漆新品來啦>新品推薦｜你要的三防漆新品來啦</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d2c7df1e.html alt=「論文推薦」左建平教授談岩層移動研究進展及重點 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RmTmvrAHAwNOUb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d2c7df1e.html title=「論文推薦」左建平教授談岩層移動研究進展及重點>「論文推薦」左建平教授談岩層移動研究進展及重點</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/811e0e2a.html alt=中科地信推薦丨這些地質動圖，帶你看盡地球板塊變化理解地質構造 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/1622aa4e323b417592715cfb657cdede style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/811e0e2a.html title=中科地信推薦丨這些地質動圖，帶你看盡地球板塊變化理解地質構造>中科地信推薦丨這些地質動圖，帶你看盡地球板塊變化理解地質構造</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6d77a0e0.html alt=「論文推薦」郭廣禮等：無井式煤炭地下氣化岩層及地表移動與控制 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ReafrDd7EHXniF style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6d77a0e0.html title=「論文推薦」郭廣禮等：無井式煤炭地下氣化岩層及地表移動與控制>「論文推薦」郭廣禮等：無井式煤炭地下氣化岩層及地表移動與控制</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/52d575a2.html alt=如何在轉轉輕鬆賣出二手物品？吐血推薦這幾條實戰經驗 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/c54abd15210f441ebd596b09560bbeca style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/52d575a2.html title=如何在轉轉輕鬆賣出二手物品？吐血推薦這幾條實戰經驗>如何在轉轉輕鬆賣出二手物品？吐血推薦這幾條實戰經驗</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>