<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 | 极客快訊</title><meta property="og:title" content="自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/d7ede3dac2f24fdf812a4f5b18d4900a"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/316466b.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/316466b.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/316466b.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/316466b.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/316466b.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/316466b.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/316466b.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/316466b.html><meta property="article:published_time" content="2020-10-29T21:04:19+08:00"><meta property="article:modified_time" content="2020-10-29T21:04:19+08:00"><meta name=Keywords content><meta name=description content="自動駕駛感知-3D目標跟蹤及防碰撞系統搭建"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/316466b.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>自動駕駛感知-3D目標跟蹤及防碰撞系統搭建</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p><strong>來源：汽車電子與軟件</strong></p><p><br></p><h1 class=pgc-h-arrow-right><strong>Collision detection model</strong></h1><p>碰撞檢測是自動駕駛安全的基石，是最後一道保險槓，需要大量的工程實踐和測試才能確保碰撞檢測功能的絕對安全和可靠。因此碰撞檢測即使在整車廠，也是汽車工程師的一個重要的研究方向。在本文中，我會帶大家瞭解經典基於攝像頭的碰撞檢測原理，以及使用激光雷達為碰撞檢測功能提供冗餘。現在讓我們來了解下什麼是碰撞檢測。</p><p><strong>防碰撞系統(CAS)</strong> 是在行駛路徑上和物體即將發生碰撞時，警告司機甚至觸發緊急制動的主動安全系統。它允許汽車通過單一傳感器或多傳感器組合感知其他車輛，行人和障礙物。</p><p>防碰撞系統市場，按類型：</p><p>•自適應巡航控制（ACC）</p><p>•自主緊急制動（AEB）</p><p>•車道偏離警告系統（LDWS）</p><p>•停車輔助系統</p><p>•其他（盲點檢測和夜視）</p><p>而按照技術分類，分為激光雷達，毫米波雷達，超聲波雷達以及相機。</p><p>人類的反應時間大約是1 s，因此一個好的CAS應該至少在觸發緊急剎車前2s向我們發出警告。如果存在前車，則CAS會持續估算碰撞時間TTC　。當TTC降至較低閾值以下時，CAS可以決定警告駕駛員迫在眉睫的危險，或者根據系統情況自動使用車輛制動器。</p><p>讓我們看下下面的場景:</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d7ede3dac2f24fdf812a4f5b18d4900a><p class=pgc-img-caption></p></div><p>在這個場景中，綠色前車在t0時刻，相距d0的位置突然減速，經過Δt ，綠車位於與黃車僅僅相距d1的位置了，我們的目標是計算剩餘<strong>碰撞時間Time to collision(TTC)</strong>，以便系統可以警告黃色車輛的駕駛員，甚至自動觸發剎車。</p><p>在汽車領域凡是涉及到汽車隨時間的運動，就必須考慮車輛動態學中的相對運動數學模型。</p><p>恆定速度模型與恆定加速度模型</p><p>車輛動態學中的相對運動數學模型最常用的有兩種，恆定速度模型與恆定加速度模型。在本項目中，我會使用恆定速度模型分別計算相機和激光雷達的TTC。</p><p>要計算TTC，我們需要對自身車輛與先前車輛的物理行為進行假設。<strong>恆定速度模型(constant velocity model CVM)</strong>假設上圖中黃色和綠色車輛之間的相對速度是恆定的：</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d2e4dfc969c64ed4bb4e8496a05d767c><p class=pgc-img-caption></p></div><p>從上述公式看出，我們需要一種能夠在恆定時間dt內精確地測量到前車距離的傳感器。由於激光雷達的空間分辨率很高，通過激光雷達傳感器可以很好地實現測距。</p><p>但恆定速度模型在實際動態交通情況下很少用到，因為實際情況下車輛可能會急剎車，導致在不同時刻的測量下，兩車之間的相對速度呈現急速動態變化。</p><p>為了解決這一問題，我們可以採用恆定加速度模型(constant acceleration model CAM)，將速度看成時間的函數。大多數市售的<strong>碰撞檢測系統</strong>都是恆定加速度模型：</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/fff4e20895c0464db91e25ae83f7f05e><p class=pgc-img-caption></p></div><p>2004年Mobileye在“Forward Collision Warning with a Single Camera ”論文中基於CAM模型單獨使用相機計算TTC。下面我簡單介紹下Mobileye的基於CAM碰撞檢測推導。基於CAM碰撞檢測推導需要恆速模型的理論做支持。</p><p>對於恆速模型，此處Tm是碰撞時間。</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cc507024ae2741c5b0c245477eecd5ec><p class=pgc-img-caption></p></div><p>我們通過針孔投射模型可以推導:</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f14a82cbbb224f158f4a2815c14c1e93><p class=pgc-img-caption></p></div><p>S表示Δt連續兩幀時間間隔內同一物體連續兩張圖片的尺度比例：</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/72e7b07f6e8742c4987fd253a5fe19db><p class=pgc-img-caption></p></div><p>從Z0運動到Z1:</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0229614210e04bd9834723f9cea5d963><p class=pgc-img-caption></p></div><p>由此可以推導出:</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e3f7ad2e10944b5f96ed7fa35532a2e6><p class=pgc-img-caption></p></div><p>現在考慮恆加速度模型：</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15268054485f424fa26d00afaabe589f><p class=pgc-img-caption></p></div><p>另Z=0，求解一元二次方程:</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4ca36b38536a4a2aa43dd6283dd4fa8d><p class=pgc-img-caption></p></div><p>由於對於攝像頭來說，速度和加速度都不是可以直觀觀察到的。由於恆定速度模型下Tm是已知的。</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9d21c72ea91544b69f845f1ce4032c20><p class=pgc-img-caption></p></div><p>對上述等式兩邊求導，Tm‘可以理解為前一時刻的Tm值減掉後一時刻Tm的值，然後再除以時間Δt：</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1f3089daa103449a9659118661883425><p class=pgc-img-caption></p></div><p>由於Ż = V and V̇ = a，繼續簡化：</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c8a7230d5da2497baf1a8f45e1ef13da><p class=pgc-img-caption></p></div><p>我們定義輔助參數C：</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d69b3aaabcae4088af12f397930b9b34><p class=pgc-img-caption></p></div><p>可以求得α：</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f2791cd4b3e0494da7030633837c6beb><p class=pgc-img-caption></p></div><p>帶入一元二次方程式，得：</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/97c2dec8397d480bae69f0df8a59f3d1><p class=pgc-img-caption></p></div><p>最後將Tm計算公式帶入，得：</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1d9118fedd7548deb023822afa73eee2><p class=pgc-img-caption></p></div><p>在知道前後兩幀主目標的寬度(或高度)變化比例後，通過求得Tm和C，我們就能求得基於CAM模型的碰撞時間。我在我的代碼庫中實現了這一算法，但是由於恆定加速度模型需要恆速模型的理論支持，而恆速模型求出的攝像頭碰撞時間Tm數值可能會存在誤差，並且時間間隔Δt很短，導致C的數值存在很大波動。也就是說恆加速度模型對於前後兩幀S尺度比的計算精度要求很高，因此對於在本項目場景下，恆加速度模型並沒有很大的使用必要。</p><p>但是在本項目中，我們將使用CVM而不是CAM，是因為我們假定CVM模型足夠準確，能為我們提供TTC的合理的估算，而且CVM在編程上會很相對簡單很多。但是，請記住如果你要構建此類系統的商業版本，一定要使用恆加速度模型。接下來我們來討論基於恆速模型的攝像頭TTC計算。</p><p><br></p><h2 class=pgc-h-arrow-right>Estimating TTC with a Camera</h2><p>單眼相機無法測量公制距離。它們是無源傳感器，其依賴於環境光，該環境光將物體反射回相機鏡頭。因此，不可能像激光雷達技術那樣測量光的運行時間。</p><p>一般為了測量距離，將需要第二個攝像機。給定兩個經過仔細對準的相機同時拍攝的圖像（也稱為多目設置），在兩個圖像中定位共同的興趣點（例如，前車的尾燈），然後使用相機幾何學和透視投影對它們的距離進行三角測量 。目前汽車研究人員已經開發出了用於 ADAS 產品的立體相機，其中一些已經投入市場。尤其是梅賽德斯-奔馳是這項技術的先驅。然而，隨著更先進的 ADAS 技術和自動駕駛汽車的出現，由於雙目相機的包裝尺寸、高昂的價格和尋找相應特徵所需的高計算量，導致雙目相機已經逐漸從市場上消失。</p><h3 class=pgc-h-arrow-right>恆速模型TTC計算</h3><p>儘管單目相機無法測量距離，但是我們可以使用一種<strong>無需測量距離</strong>即可計算TTC的方法。如將公制距離d用在像面上的像素距離來代替的方法。在下圖中，您可以看到高度H可以使用透視投影將先前車輛的“位置”映射到圖像位置。我們可以看到相同的車輛高度H，取決於車輛距離 d_0 和 d_1， 映射到圖像平面上不同的高度 h_0 和 h_1 。顯然， h, H, d和 針孔相機焦距 f 之間存在幾何關係。我們看下圖：</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/196daa19e38d48eb8aec3a1eeca231dd><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dc420accb39f4bc7a89e41211a6f98bb><p class=pgc-img-caption></p></div><p>經過一系列公式推導，最終結果為：</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/eb3fa67a2a1f4a918a58dcb8bd6ce373><p class=pgc-img-caption></p></div><h3 class=pgc-h-arrow-right>邊界框檢測問題</h3><p>在本項目中，我們會使用YOLO V3在單目相機的連續圖像中定位車輛。對於每輛車，網絡都會返回一個邊界框，該邊界框的寬度或高度原則上可以用於計算我們在上一節中得出的TTC方程中的高度比。</p><p>但是，當仔細觀察時，可以看到邊界框並不總是反映真實的車輛尺寸，並且圖像之間的縱橫比有所不同。因此，使用邊界框高度或寬度進行TTC計算會導致明顯的估計誤差。</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0acfcca46eef4a0181bbfce0eabb33ba><p class=pgc-img-caption></p></div><p>在大多數工程任務中，僅依靠單個測量或屬性是不夠可靠的。對於與安全相關的產品尤其如此。因此，我們要考慮的是，在圖像中還能觀察到車輛和物體的其他屬性。</p><p>改用紋理關鍵點</p><p>現在，我們不再依賴整個車輛的檢測，而是希望小範圍分析其結構。如果有可能找到可以從一幀追蹤到下一幀的唯一可識別關鍵點，則我們可以使用車輛上所有關鍵點之間的相對距離來計算出TTC方程中高度比的可靠估計。下圖說明了該概念。</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/62dd7d67bf7d4b7ea55eda48381ece20><p class=pgc-img-caption></p></div><p>在（a）中，已檢測到一組關鍵點，並且已計算出關鍵點1-7之間的相對距離。在（b）中，已使用稱為關鍵點描述符在連續圖像之間匹配了4個關鍵點。彼此之間的所有相對距離之比可以用來代替高度比，從而計算出可靠的TTC估算值h1/h0。備註：對所有距離比率的求平均值或中位數 dk/dk′可以減少異常數值的干擾。</p><p>下圖為關鍵點相對距離的示意圖：</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/48b99817da3e4b2281302caad0086c21><p class=pgc-img-caption></p></div><p><br></p><h2 class=pgc-h-arrow-right>Estimating TTC with Lidar</h2><p>由於激光雷達能直接測量與前車的距離，因此我們不再需要獲得針孔成像的高寬比了。讓我們來看下如何使用激光傳感器測量碰撞時間。</p><p>假設配備CAS的車輛正在使用激光雷達傳感器對先前車輛進行距離測量。在這種情況下，傳感器將為我們提供到行駛路徑中最接近的3D點的距離。在下圖中，最接近的點由CAS車輛頂部的激光雷達傳感器發出的紅線表示。</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/118786c07ed84342a753c1f68bf657d3><p class=pgc-img-caption></p></div><p>在前文中，我們基於恆速模型討論了速度 v0可以通過兩次連續的激光雷達測量來計算，如下所示：</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3a2411ffd44c43948044e9ecff8e7eb8><p class=pgc-img-caption></p></div><p>一旦知道相對速度v0後，碰撞時間可以很容易地通過將兩輛車之間的剩餘距離除以 v0獲得。因此，只要給定一個能夠進行精確距離測量的激光雷達傳感器，就可以基於CVM和上述方程組開發用於TTC估算的系統。</p><p>下圖顯示了激光雷達點雲，它是在高速公路場景中拍攝的攝像機圖像的疊加，前車直接在行駛路徑中。到傳感器的距離用顏色編碼（綠色很遠，紅色很近）。在左側，還顯示了激光雷達點的鳥瞰圖。</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/238dba842a664f8eb6e72eaf95434063><p class=pgc-img-caption></p></div><p>由於測量精度與從物體反射的光量相關，因此有必要考慮除x，y和z座標之外每個激光雷達點的反射率r。下圖以綠色突出顯示高反射率，而具有低反射率的區域顯示為紅色。對點雲的相關反射率的分析表明，此類偏差通常發生在反射率降低的區域中。</p><p>為了從給定的點雲中獲得穩定的TTC測量，必須執行兩個主要步驟：</p><p>1、刪除路面的尺寸</p><p>2、去除反射率低的測量</p><p>我在本項目的代碼庫中，將Lidar點打包到一個名為LidarPoints的數據結構中。如下所示，該結構由公制座標中的點座標x（正向），y（左側）和z（向上）以及點反射率r（介於0和1之間（高反射率））組成。</p><pre><code>struct LidarPoint { // single lidar point in space    double x, y, z; // point position in m    double r; // point reflectivity in the range 0-1};</code></pre><p>為了計算TTC，我們需要找到到行駛路徑中最接近的激光雷達點的距離。後擋板一般為激光雷達反射最佳的位置。</p><p>這是由於車輛後擋板處反射面積大，較為平整，對於激光點雲的反射效果相對更佳。因此我們將點雲裁剪到車輛後擋板處，以便檢測後車到前車的最短距離。</p><p>在下圖中，我們可以獲得前車後擋板上的激光雷達測量值t_0 （綠色）和 t_1（紅色）。可以看出，在兩個時刻之間到車輛的距離略有減小。</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/318d52b62aa54a4db83b7d482b42cb92><p class=pgc-img-caption></p></div><p>以下代碼在與點關聯的點雲中搜索最接近的點 t0（lidarPointsPrev）以及與t_1（lidarPointsCurr）。在分別找到最接近點的距離後，根據我們在本節開始時得出的公式來計算TTC。</p><pre><code>void computeTTCLidar(std::vector&lt;LidarPoint&gt; &amp;lidarPointsPrev,                     std::vector&lt;LidarPoint&gt; &amp;lidarPointsCurr, double &amp;TTC){    // auxiliary variables    double dT = 0.1; // time between two measurements in seconds    // find closest distance to Lidar points    double minXPrev = 1e9, minXCurr = 1e9;    for(auto it=lidarPointsPrev.begin(); it!=lidarPointsPrev.end(); ++it) {        minXPrev = minXPrev&gt;it-&gt;x ? it-&gt;x : minXPrev;    }    for(auto it=lidarPointsCurr.begin(); it!=lidarPointsCurr.end(); ++it) {        minXCurr = minXCurr&gt;it-&gt;x ? it-&gt;x : minXCurr;    }    // compute TTC from both measurements    TTC = minXCurr * dT / (minXPrev-minXCurr);}</code></pre><p>注意即使激光雷達是可靠的傳感器，也可能會發生錯誤的測量。如上圖所示，少量點位於後擋板的後面，似乎沒有與車輛連接。當搜索最接近的點時，由於估計的距離太小，因此此類測量會帶來問題。有一些方法可以通過對點雲進行後處理來避免此類錯誤，但不能保證此類問題在實踐中永遠不會發生。因此，在本項目中為了對minXCurr和minXPrev進行更可靠的計算，我使用了所有距離的中值，來處理一定數量的離群值，其次通過融合另一個能夠計算TTC的傳感器 如相機，來過濾離群值。</p><p>儘管我們使用了相機及激光雷達計算TTC，但是實際上毫米波雷達傳感器仍然是TTC計算的最佳解決方案，因為它可以利用多普勒效應在返回的電磁波中利用頻移來對速度進行直接測量，而對於激光雷達傳感器，我們需要通過兩個距離之間的差值測量速度。對於實際解決方案來說，通過攝像頭或激光雷達測量TTC更多的是為了提供足夠的安全冗餘。</p><p><br></p><h2 class=pgc-h-arrow-right>Camera based 2D feature detection</h2><p>現在我們已經瞭解瞭如何使用恆速運動模型來設計碰撞檢測系統，該系統將計算碰撞檢測時間。我們在基於攝像頭的TTC計算章節提過，我們可以使用車輛上所有關鍵點之間的相對距離來計算出TTC方程中高度比的可靠估計。那麼問題來了，我們如何實現相鄰兩幀圖片間的關鍵點提取與匹配。在我之前的文章自動駕駛汽車視覺- 圖像特徵提取與匹配技術中，我詳細介紹了關鍵點檢測，描述以及匹配的背後原理，不同的組合方式之間的優劣，並提出幾組根據實踐結果得出的最佳組合。因此在本文中我只簡單講述特徵匹配的流程。目前存在通過多種檢測器，描述符以及匹配子組合來執行圖像關鍵點檢測，我所使用的是FAST+Brisik+MAT_BF，但是你們也可以在我的代碼庫中嘗試其它組合。</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ddfc79753552411ba56c8d8ede9b99b3><p class=pgc-img-caption></p></div><p>以下是關鍵點提取和匹配的順序：</p><p>1、從File中加載圖像。在實際項目中，您將使用相機並拍攝實時圖像，為了減少對於實時存儲的消耗，我們將從File中加載圖像並將其放入環形緩衝區。現在，此緩衝區為我們提供了每個時間點的圖像。</p><p>2、檢測圖像關鍵點。我們可以通過多種方法實現關鍵點檢測，所有方法返回的結果都是一個關鍵點集合。</p><p>3、提取關鍵點描述符。此模塊需要輸入關鍵點及原圖。通過在每個關鍵點位置周圍，計算出一個關鍵點描述符，以反映關鍵點附近的像素強度結構或其周圍的其他屬性，從而在圖像局部鄰域中唯一標識此關鍵點。</p><p>4、描述符匹配。匹配是基於關鍵點及關鍵點的描述符。由於是不同幀圖之間的關鍵點匹配，因此會輸入前後幀兩組關鍵點和描述符，基於這兩個輸入，輸出是兩個連續的圖像的一組關鍵點匹配。我們會在兩個圖像中找到多組匹配的關鍵點集合，用來計算碰撞時間。</p><p>現在，我們以及解決了關鍵點匹配的問題，但是同樣面臨一個嚴重的問題，大量的匹配關鍵點並非在目標車輛上。因此我們必須以某種方式隔離位於前車上的關鍵點，並排除路面上以及其他靜態對象上的所有其他關鍵點。對象檢測算法能夠輸出對象的邊界框，從而有效的幫我們隔離前車上的關鍵點，接下來我們來了解對象檢測算法。</p><p><br></p><h2 class=pgc-h-arrow-right>Object Detection with YOLO</h2><p>在本節中，我們將使用深度學習方法來檢測圖像中的車輛。我們已經在整個圖像上識別出了關鍵點。為了識別單個車輛並計算TTC ，我們需要將前後兩幀圖像上兩兩關鍵點歸類並與場景中的車輛關聯起來。</p><p>通過對象檢測算法，我們可以檢測圖像中的車輛，以此隔離匹配的關鍵點以及投影的激光雷達點，並將它們與特定對象相關聯。也就是說為了計算特定車輛的碰撞時間，我們需要隔離該車輛上的關鍵點，以使TTC不會因包含例如路面，靜止物體或其他車輛中的匹配項而失真。</p><p>對象檢測算法的輸出將是場景中所有對象周圍的一組2D邊界框。基於這些邊界框，我們可以輕鬆地將關鍵點匹配與對象相關聯，並獲得穩定的TTC估算值。</p><p>對於激光雷達測量，我們也可以用對相機圖像進行對象檢測生成的2D邊界框，將點雲聚類到單個對象中。在我之前的一篇文章自動駕駛激光雷達物體檢測技術中我提到過可以通過PCL點雲庫中的算法對點雲進行直接聚類，但是激光雷達對象檢測的能力是遠弱於攝像頭的，因此在本文中我們使用基於相機圖像的物體識別算法YOLOv3給定一組對象的邊界框，將激光雷達投影到圖像平面的點進行聚類並與特定對象相關聯。關於如何將激光雷達點投影到圖像平面，請參考我的另外一篇文章: 自動駕駛視覺融合-相機校準與激光點雲投影。</p><p>我在上一篇文章自動駕駛目標檢測- YOLO v3 深入解析中詳解介紹了YOLOv3的原理及實現，因此在本文中我只講述YOLOv3在C++環境下基於OPENCV快捷實現。</p><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/aba70988165943d08456c6a8676d2e42><p class=pgc-img-caption></p></div><p>YOLOv3工作流程</p><p>主要算法步驟：</p><p>首先，將圖像劃分為13x13的單元格網格。根據輸入圖像的大小，這些像元的大小（以像素為單位）會有所不同。在下面的代碼中，使用了416 x 416像素的大小，因此單元格大小為32 x 32像素。</p><p>如上圖所示，每個單元然後用於預測一組邊界框。對於每個邊界框，網絡還預測邊界框包圍特定對象的置信度以及該對象屬於特定類的概率（取自COCO數據集）。</p><p>最後，使用非最大抑制來消除具有低置信度的邊界框以及包圍同一對象的冗餘邊界框。</p><p>主要代碼步驟：</p><p><strong>初始化參數</strong></p><ul><li>“confThreshold”用於刪除分數值較低的所有邊界框</li><li>“ nmsThreshold”將非最大抑制NMS應用於剩餘的邊界框。</li><li>“ inpWidth”和“ inpHeight”，YOLO作者所建議，將其設置為416。其他值例如可以是320（更快）或608（更準確）。</li></ul><p><strong>準備模型</strong></p><ul><li>文件“ yolov3.weights”包含預先訓練的網絡的權重，並且已由YOLO的作者在此處提供。</li><li>包含網絡配置的文件“ yolov3.cfg”在此處可用。</li><li>而包含COCO數據集中使用的80個不同類名稱的coco.names文件可在此處下載。</li></ul><p>以下代碼顯示瞭如何加載模型權重以及相關的模型配置：</p><pre><code>// load image from filecv::Mat img = cv::imread("./images/img1.png");// load class names from filestring yoloBasePath = "./dat/yolo/";string yoloClassesFile = yoloBasePath + "coco.names";string yoloModelConfiguration = yoloBasePath + "yolov3.cfg";string yoloModelWeights = yoloBasePath + "yolov3.weights";vector&lt;string&gt; classes;ifstream ifs(yoloClassesFile.c_str());string line;while (getline(ifs, line)) classes.push_back(line);// load neural networkcv::dnn::Net net = cv::dnn::readNetFromDarknet(yoloModelConfiguration, yoloModelWeights);net.setPreferableBackend(cv::dnn::DNN_BACKEND_OPENCV);net.setPreferableTarget(cv::dnn::DNN_TARGET_CPU); </code></pre><p><strong>從輸入圖像生成4D Blob</strong></p><ul><li>從文件加載的圖像通過blobFromImage函數傳遞，然後轉換為神經網絡的輸入塊。像素值以1/255的縮放因子縮放到0到1的目標範圍。它還可以將圖像的大小調整為指定的大小（416、416、416），而不會進行裁切。</li></ul><pre><code> // generate 4D blob from input image    cv::Mat blob;    double scalefactor = 1/255.0;    cv::Size size = cv::Size(416, 416);    cv::Scalar mean = cv::Scalar(0,0,0);    bool swapRB = false;    bool crop = false;    cv::dnn::blobFromImage(img, blob, scalefactor, size, mean, swapRB, crop);　</code></pre><ul><li>輸出Blob將作為輸入傳遞到網絡。然後，將執行前向傳遞以獲得從網絡輸出的預測邊界框的列表。這些框經過一個後處理步驟，以過濾掉具有低置信度值的那些框.</li><li><strong>通過網絡運行正向傳遞</strong></li><li>我們必須將剛創建的Blob作為輸入傳遞給網絡。然後，我們運行OpenCV的轉發功能，以執行通過網絡的單個轉發。為此，我們需要確定網絡的最後一層，併為該功能提供相關的內部名稱。這可以通過使用OpenCV函數“ getUnconnectedOutLayers”來完成，該函數給出所有未連接輸出層的名稱，這些輸出層實際上是網絡的最後一層。</li></ul><pre><code> // Get names of output layers    vector&lt;cv::String&gt; names;    vector&lt;int&gt; outLayers = net.getUnconnectedOutLayers(); // get indices of output layers, i.e. layers with unconnected outputs    vector&lt;cv::String&gt; layersNames = net.getLayerNames(); // get names of all layers in the network    names.resize(outLayers.size());    for (size_t i = 0; i &lt; outLayers.size(); ++i) // Get the names of the output layers in names    {        names[i] = layersNames[outLayers[i] - 1];    }    // invoke forward propagation through network    vector&lt;cv::Mat&gt; netOutput;    net.setInput(blob);    net.forward(netOutput, names);</code></pre><ul><li>前向傳遞的結果以及網絡的輸出是大小為C的向量，每個類中的前四個元素表示中心x，y，相關邊界框的寬度和高度。第五個元素表示各個邊界框實際上包圍對象的信任度或置信度。矩陣的其餘元素是與coco.cfg文件中包含的每個類相關聯的置信度。</li></ul><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e7b3dc1243614acd8bc1dadaeafaea3d><p class=pgc-img-caption></p></div><p><br></p><p>以下代碼顯示瞭如何掃描網絡結果以及如何將具有足夠高置信度得分的邊界框組裝到向量中。該函數cv::minMaxLoc使用在整個數組中搜索的極值來找到最小和最大元素值及其位置。</p><pre><code>// Scan through all bounding boxes and keep only the ones with high confidence    float confThreshold = 0.20;    vector&lt;int&gt; classIds;    vector&lt;float&gt; confidences;    vector&lt;cv::Rect&gt; boxes;    for (size_t i = 0; i &lt; netOutput.size(); ++i)    {        float* data = (float*)netOutput[i].data;        for (int j = 0; j &lt; netOutput[i].rows; ++j, data += netOutput[i].cols)        {            cv::Mat scores = netOutput[i].row(j).colRange(5, netOutput[i].cols);            cv::Point classId;            double confidence;            // Get the value and location of the maximum score            cv::minMaxLoc(scores, 0, &amp;confidence, 0, &amp;classId);            if (confidence &gt; confThreshold)            {                cv::Rect box; int cx, cy;                cx = (int)(data[0] * img.cols);                cy = (int)(data[1] * img.rows);                box.width = (int)(data[2] * img.cols);                box.height = (int)(data[3] * img.rows);                box.x = cx - box.width/2; // left                box.y = cy - box.height/2; // top                boxes.push_back(box);                classIds.push_back(classId.x);                confidences.push_back((float)confidence);            }        }    } </code></pre><ul><li><strong>網絡輸出的後處理</strong></li><li>OpenCV庫提供了一種現成的函數，用於抑制重疊的邊界框。此功能稱為NMSBoxes，可以按以下簡短代碼示例所示使用它：</li></ul><pre><code>// perform non-maxima suppression    float nmsThreshold = 0.4;  // Non-maximum suppression threshold    vector&lt;int&gt; indices;    cv::dnn::NMSBoxes(boxes, confidences, confThreshold, nmsThreshold, indices);</code></pre><ul><li>應用非最大抑制後，多餘的邊界框將被成功刪除。下圖顯示了結果，其中綠色表示保留的邊界框，而紅色的邊界框在NMS期間已刪除。</li></ul><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e61989748c214bbcb8e5c1d37433f98c><p class=pgc-img-caption></p></div><p>至此我們已經使用對象檢測成功地自動識別了場景中的車輛並輸出場景中所有對象周圍的一組2D邊界框。</p><p>讓我來看看整體的代碼框架。</p><p><br></p><h2 class=pgc-h-arrow-right>Flow schematic</h2><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c5ac75149c444248a257b67ddd7e9066><p class=pgc-img-caption></p></div><p>本項目的目的是同時使用攝像頭和激光雷達測量TTC，在這過程中既有傳感器各自的處理部分，也有相互融合和補充的部分。</p><p>在上文中我們介紹了基於攝像機和激光雷達的TTC計算方法，關鍵點匹配，Yolov3對象識別，激光雷達3D點雲投影到2D圖像平面，以及如何將相機圖像中的區域與自由空間中的點相關聯。現在讓我們看一下程序原理圖，看看我們已經完成了什麼，以及仍然缺少什麼。</p><ul><li>在第一部分（黃色框）中，我們已經完成了編寫有關關鍵點檢測和關鍵點匹配的代碼。黃色矩形是其程序流程。首先我們加載圖像並使用環形緩衝區，然後檢測圖像關鍵點，提取關鍵點描述符，最後匹配相鄰兩幀圖像的關鍵點描述符。雖然關鍵點匹配存在於每個圖像中，但是它們沒有聚類並綁定到任何對象。</li><li>在第二部分中（綠色框），我們使用Yolov3完成了對對象的檢測和分類，這為我們提供了一組感興趣對象邊界框。對於相機圖像，我們可以將對象上的關鍵點關聯到對應的邊界框。而對於激光雷達，我們也可以將激光雷達3D點雲投影到2D圖像平面，使用對象檢測的邊界框對點雲進行聚類，到第四步時該邊界框變量既包含有關對象位置的信息，還包括車輛在圖像中的關鍵點信息，以及對應的3D激光雷達點雲。</li><li>接下來我們需要時刻跟蹤3D對象邊界框。因此，我們需要通過查看邊界框內的關鍵點對應關係來關聯相鄰幀之間的邊界框。</li><li>在完成第八步之後，我們可以計算相機及激光雷達傳感器正前方物體的TTC。值得注意的是，我們將只關注本車道線正前方最近的對象。</li></ul><h2 class=pgc-h-arrow-right>Code Implementation</h2><p>載入圖像Buffer，在每次循環中只需要前後兩個圖像數據</p><pre><code>if (dataBuffer.size() &gt; dataBufferSize) {	dataBuffer.erase(dataBuffer.begin());	}dataBuffer.push_back(frame);</code></pre><p>使用YOLOv3檢測和分類對象，獲得汽車對象的邊界框</p><pre><code>float confThreshold = 0.2;float nmsThreshold = 0.4;detectObjects((dataBuffer.end() - 1)-&gt;cameraImg, (dataBuffer.end() - 1)-&gt;boundingBoxes, confThreshold,nmsThreshold,yoloBasePath, yoloClassesFile, yoloModelConfiguration, yoloModelWeights, bVis);</code></pre><p>載入激光雷達點雲，根據距離屬性裁剪點雲</p><pre><code>/ remove Lidar points based on distance propertiesfloat minZ = -1.5, maxZ = -0.9, minX = 2.0, maxX = 20.0, maxY = 2.0, minR = 0.1; // focus on ego lanecropLidarPoints(lidarPoints, minX, maxX, maxY, minZ, maxZ, minR);(dataBuffer.end() - 1)-&gt;lidarPoints = lidarPoints;</code></pre><p>將3D點雲投射到2D圖像平面，並使用邊界框對點雲進行過濾和聚類</p><pre><code>// associate Lidar points with camera-based ROIfloat shrinkFactor = 0.20; // shrinks each bounding box by the given percentage to avoid 3D object merging at the edges of an ROIclusterLidarWithROI((dataBuffer.end()-1)-&gt;boundingBoxes, (dataBuffer.end() - 1)-&gt;lidarPoints, shrinkFactor, P_rect_00, R_rect_00, RT);</code></pre><p>檢測圖像關鍵點，選擇描述符，匹配相鄰兩幀圖像的關鍵點</p><pre><code>// choose keypoint detectordetKeypointsModern(keypoints, imgGray, detectorType, false);// choose keypoint descriptordescKeypoints((dataBuffer.end() - 1)-&gt;keypoints, (dataBuffer.end() - 1)-&gt;cameraImg, descriptors,descriptorType);// choose keypoint matcher and match corresponding kepointsmatchDescriptors((dataBuffer.end() - 2)-&gt;keypoints, (dataBuffer.end() - 1)-&gt;keypoints,(dataBuffer.end() - 2)-&gt;descriptors, (dataBuffer.end() - 1)-&gt;descriptors,matches, descriptorclass, matcherType, selectorType);// store matches in current data frame(dataBuffer.end() - 1)-&gt;kptMatches = matches;</code></pre><p>追蹤3D對象邊界框</p><pre><code>map&lt;int, int&gt; bbBestMatches;matchBoundingBoxes(matches, bbBestMatches, *(dataBuffer.end() - 2), *(dataBuffer.end() -1)); // associate bounding boxes between current and previous frame using keypoint matches// store matches in current data frame(dataBuffer.end() - 1)-&gt;bbMatches = bbBestMatches;</code></pre><p>分別計算基於恆速模型攝像頭以及激光雷達測量的碰撞時間</p><pre><code>for (auto it1 = (dataBuffer.end() - 1)-&gt;bbMatches.begin();it1 != (dataBuffer.end() - 1)-&gt;bbMatches.end(); ++it1) {    // find bounding boxes associates with current match    BoundingBox *prevBB, *currBB;    for (auto it2 = (dataBuffer.end() - 1)-&gt;boundingBoxes.begin();it2 != (dataBuffer.end() - 1)-&gt;boundingBoxes.end(); ++it2) {        if (it1-&gt;second == it2-&gt;boxID) // check wether current match partner corresponds to this BB            {            currBB = &amp;(*it2);            }        }    for (auto it2 = (dataBuffer.end() - 2)-&gt;boundingBoxes.begin();it2 != (dataBuffer.end() - 2)-&gt;boundingBoxes.end(); ++it2) {        if (it1-&gt;first == it2-&gt;boxID) // check wether current match partner corresponds to this BB            {            prevBB = &amp;(*it2);            }    	}    // compute TTC for current match    if (currBB-&gt;lidarPoints.size() &gt; 0 &amp;&amp;prevBB-&gt;lidarPoints.size() &gt; 0)    // only compute TTC if we have Lidar points        {        // compute time-to-collision based on Lidar data (implement -&gt; computeTTCLidar)        double ttcLidar;        computeTTCLidar(prevBB-&gt;lidarPoints, currBB-&gt;lidarPoints, sensorFrameRate, ttcLidar);        // assign enclosed keypoint matches to bounding box (implement -&gt; clusterKptMatchesWithROI)        // compute time-to-collision based on camera (implement -&gt; computeTTCCamera)        double ttcCamera;        clusterKptMatchesWithROI(*currBB, (dataBuffer.end() - 2)-&gt;keypoints,(dataBuffer.end() - 1)-&gt;keypoints, (dataBuffer.end() - 1)-&gt;kptMatches);        computeTTCCamera((dataBuffer.end() - 2)-&gt;keypoints, (dataBuffer.end() - 1)-&gt;keypoints,currBB-&gt;kptMatches, sensorFrameRate, ttcCamera);        currBB-&gt;kptMatches, sensorFrameRate, ttcCameraCAM,TmPrev);        }   　}</code></pre><p>至此我們完成了整個3D對象跟蹤及防碰撞系統搭建項目。</p><h2 class=pgc-h-arrow-right><strong>Conclusion</strong></h2><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/8313b062695247ab843e54f8cb7c594d><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=自動駕駛感知-3D目標跟蹤及防碰撞系統搭建 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3c5d8d6b6f8c4d409580036a52db113d><p class=pgc-img-caption></p></div><p>本文的目的是教會初學者如何利用關鍵點提取和匹配技術，對象檢測技術，3D點雲投影技術，激光雷達對象檢測技術來完成3D對象追蹤，並且計算與前車的碰撞時間TTC。</p><p>我們可以看到隨著時間變化，前後車的相對距離和速度改變，激光雷達和攝像機計算出的碰撞時間也隨之變化。</p><p>值得注意的是，在我們的使用場景中前後兩個時刻之間的Δt很短，恆速模型能夠滿足我們的要求。但在市售的防碰撞系統CAS中，幾乎都是使用恆定加速度模型，這一模型所需要的參數更多，對於相機前後兩幀圖像尺度比的精度要求很高。而恆速模型跳過了對於速度的測量，但是如果我們能獲得精確的速度，我們可以大大減少工作量，並且提高碰撞時間的計算精度。對於單目攝像頭來說，我們也以使用光流法，檢測圖像像素點的強度隨時間的變化進而推斷出物體移動速度及方向。</p><p>在實際汽車供應商的市售產品中，往往使用毫米波雷達搭建防碰撞系統，這是由於多普勒效應能直接獲得前車的精確速度。對於毫米波雷達的工作原理，大家可以參考我之前的文章: 自動駕駛毫米波雷達物體檢測技術-硬件, 自動駕駛毫米波雷達物體檢測技術-算法。其次對於防碰撞系統來說，系統的絕對安全可靠和及時是非常重要的，因此除了算法開發，大量研究人員的重心都放在測試與調優上了。如果你們有機會在你們後續的工作中接觸到CAS系統的設計和測試，很希望能聽到你們的技術分享。</p><p>最後在實際項目的後續處理中，我們還會用到卡爾曼濾波器，對來自不同傳感器的特徵級數據如速度，距離，位置等進行融合，從而獲得遠比單個傳感器更可靠的測量結果。在我之前的文章: 自動駕駛感知融合-無跡卡爾曼濾波(Lidar&Radar), 技術文章：自動駕駛感知融合-卡爾曼及擴展卡爾曼濾波(Lidar&Radar)中，我詳細講述了卡爾曼，擴展卡爾曼及無跡卡爾曼濾波器的原理以及實踐，可以供大家參考。通過多傳感器融合技術，感知的性能與穩定將會得到很大的提升。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>自動</a></li><li><a>駕駛</a></li><li><a>3D</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/b7f123aa.html alt="北京覺非科技：做自動駕駛最務實的領路人 | ChinaBang 創新企業" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/Rlt8aQM4wM8wAU style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b7f123aa.html title="北京覺非科技：做自動駕駛最務實的領路人 | ChinaBang 創新企業">北京覺非科技：做自動駕駛最務實的領路人 | ChinaBang 創新企業</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/f60f4d90.html alt=易成自動駕駛獲得高工智能汽車評選年度最具競爭力品牌獎！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/f60f4d90.html title=易成自動駕駛獲得高工智能汽車評選年度最具競爭力品牌獎！>易成自動駕駛獲得高工智能汽車評選年度最具競爭力品牌獎！</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/746c4e71.html alt="易成自動駕駛：聚焦人工智能 加快智能汽車產業化進程" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/78a90010850ff24a316c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/746c4e71.html title="易成自動駕駛：聚焦人工智能 加快智能汽車產業化進程">易成自動駕駛：聚焦人工智能 加快智能汽車產業化進程</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b0c9b37.html alt="自動駕駛公路技術規範將出臺 哪些領域將迎利好？" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1a562b35e499423c81f8452a7a99bccf style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b0c9b37.html title="自動駕駛公路技術規範將出臺 哪些領域將迎利好？">自動駕駛公路技術規範將出臺 哪些領域將迎利好？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/476a9dd.html alt=自動駕駛還遠嗎？核心傳感器都被安森美半導體佈局齊全了 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d2211d5127e347389a668165e732b5a9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/476a9dd.html title=自動駕駛還遠嗎？核心傳感器都被安森美半導體佈局齊全了>自動駕駛還遠嗎？核心傳感器都被安森美半導體佈局齊全了</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/90f2b6f.html alt=淺析自動駕駛的重要一環：感知系統發展現狀與方向 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/b6444ffa88e84982baccc7f2a22c5e29 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/90f2b6f.html title=淺析自動駕駛的重要一環：感知系統發展現狀與方向>淺析自動駕駛的重要一環：感知系統發展現狀與方向</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/97612fb.html alt=自動駕駛汽車難題——如何隱藏各種傳感器？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/74dfc2006f0a487fb06d18d0706c0c84 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/97612fb.html title=自動駕駛汽車難題——如何隱藏各種傳感器？>自動駕駛汽車難題——如何隱藏各種傳感器？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d15571b.html alt="演繹自動駕駛“黑科技” 上海車展歐拉R1女神版重磅來襲" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1903dac3aa5348b490825f847b9e672f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d15571b.html title="演繹自動駕駛“黑科技” 上海車展歐拉R1女神版重磅來襲">演繹自動駕駛“黑科技” 上海車展歐拉R1女神版重磅來襲</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d49e7c9.html alt="電機涉水 自動駕駛 中車電動演繹智能客車“黑科技”" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/RAzITpSBVRZDec style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d49e7c9.html title="電機涉水 自動駕駛 中車電動演繹智能客車“黑科技”">電機涉水 自動駕駛 中車電動演繹智能客車“黑科技”</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b998277.html alt=用於自動駕駛汽車安全性驗證的可重定位故障注入框架 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3a93acfd24e5411a8c579f891c5d4784 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b998277.html title=用於自動駕駛汽車安全性驗證的可重定位故障注入框架>用於自動駕駛汽車安全性驗證的可重定位故障注入框架</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/37b9dfe.html alt=自動駕駛研發中的軟件架構問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/fb53fc5aead048d7a2d3425aa66b729f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/37b9dfe.html title=自動駕駛研發中的軟件架構問題>自動駕駛研發中的軟件架構問題</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2afa690.html alt=自動駕駛“洩密”跨國纏鬥：特斯拉訴小鵬誰是贏家 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2afa690.html title=自動駕駛“洩密”跨國纏鬥：特斯拉訴小鵬誰是贏家>自動駕駛“洩密”跨國纏鬥：特斯拉訴小鵬誰是贏家</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b7dfc22c.html alt=3D打印和虛擬製造助福特大大降低事故率 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6737/2835227606 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b7dfc22c.html title=3D打印和虛擬製造助福特大大降低事故率>3D打印和虛擬製造助福特大大降低事故率</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/60f3eef5.html alt="全球首個3D AI主播火了 虛擬製作時代將至" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/68cc49aeb805412eb4a9d126b12284bd style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/60f3eef5.html title="全球首個3D AI主播火了 虛擬製作時代將至">全球首個3D AI主播火了 虛擬製作時代將至</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/93ce5334.html alt=3D打印3.07米高C919中央翼緣條 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RFk6uEkHthhMNl style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/93ce5334.html title=3D打印3.07米高C919中央翼緣條>3D打印3.07米高C919中央翼緣條</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>