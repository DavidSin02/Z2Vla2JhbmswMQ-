<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>深度研究自然梯度優化，從入門到放棄 | Deep Reading | 极客快訊</title><meta property="og:title" content="深度研究自然梯度優化，從入門到放棄 | Deep Reading - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/2cc1b8ef47a5458190c22d26d8bd164c"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3d54fe0b.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3d54fe0b.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/3d54fe0b.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3d54fe0b.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3d54fe0b.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/3d54fe0b.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/3d54fe0b.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/3d54fe0b.html><meta property="article:published_time" content="2020-11-14T21:08:22+08:00"><meta property="article:modified_time" content="2020-11-14T21:08:22+08:00"><meta name=Keywords content><meta name=description content="深度研究自然梯度優化，從入門到放棄 | Deep Reading"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/3d54fe0b.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>深度研究自然梯度優化，從入門到放棄 | Deep Reading</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><blockquote>作者 | Cold Marie Wild譯者 | 劉暢責編 | Jane出品 | AI科技大本營（公眾號id：rgznai100）</blockquote><p class=ql-align-justify>【導語】根據自然梯度的支持者提出一種建議：我們不應該根據參數空間中的距離來定義值域空間，而是應該根據分佈空間中的距離來定義它。這樣真的有效？關於自然梯度優化，今天這篇文章值得大家一讀！作者要以一個大家很少關注的角度講一個肯定都聽過的故事。</p><p class=ql-align-justify>現在的深度學習模型都使用梯度下降法來進行訓練。在梯度下降法的每個步驟中，參數值通常是從某個點開始，然後逐步將它們移動到模型最大損失減少的方向。我們通常可以通過從整個參數向量中計算損失的導數來實現這一點，也稱為雅可比行列式。然而，這只是損失函數的一階導數，它沒有任何關於曲率的信息，換言之就是一階導數改變的速度。由於該點可能位於一階導數的局部近似區域中，並且它可能離極值點並不遠（例如，在巨大的山峰之前的向下曲線），這時我們需要更加謹慎，並且不要以較大步長向下降低。因此，我們通常會採用下面等式中的步長 α 來控制我們的前饋速度。</p><div class=pgc-img><img alt="深度研究自然梯度優化，從入門到放棄 | Deep Reading" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2cc1b8ef47a5458190c22d26d8bd164c><p class=pgc-img-caption></p></div><p class=ql-align-center>這個步長正在做這樣一些事情：它以一個固定的值約束了你要在漸變方向上更新每個參數的距離。在這個算法的最簡單版本中，我們採用一個標量，alpha，假設其值為 0.1，並將其乘以對損失函數求的梯度。注意，我們的梯度實際上是一個向量，也就是相對於模型中每個參數的損失梯度。因此當我們將它乘以標量時，實際上我們將沿著每個參數軸按照相同的固定量，按比例更新一個歐幾里德參數距離。而且，在最基本的梯度下降版本中，我們在訓練過程中使用的是相同步長。</p><p class=ql-align-justify><strong>但是......這樣做真的有意義嗎？</strong>使用較小學習率的前提是我們知道單個梯度的局部估計值可能僅在該估計周圍的小局部區域中有效。但是，參數可以存在於不同的尺度上，並且可以對學習的條件分佈產生不同程度的影響。而且，這種程度的影響可能會在訓練過程中波動。從這個角度來看，在歐幾里德參數空間中去固定一個全局範圍看起來並不像是一件特別明智或有意義的事情</p><p class=ql-align-justify><strong>由自然梯度的支持者提出的另一種建議是，我們不應該根據參數空間中的距離來定義值域空間，而是應該根據分佈空間中的距離來定義它。</strong>因此，不應該是“在符合當前梯度變化時，將參數向量保持在當前向量的 epsilon 距離內”，而應該是“在符合當前梯度變化時，要保持模型的分佈是在之前預測分佈的 epsilon 距離內”。這裡的概念是兩個分佈之間的距離，而且對於任何縮放移位或一般的參數重置是具有不變性的。例如，可以使用方差參數或比例參數（1 /方差）來參數化相同的高斯分佈；如果你查看參數空間，根據它們是使用方差還是比例進行參數化，兩個分佈將是不同的距離。但是如果你在原始概率空間中定義了一個距離，它就會是一致的。</p><div class=pgc-img><img alt="深度研究自然梯度優化，從入門到放棄 | Deep Reading" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0b194be8f734407e9ba346f2570d1f05><p class=pgc-img-caption></p></div><p class=ql-align-center><strong>接下來，本文將嘗試建立一種更強大，更直觀的理解方法，稱為自然梯度學習</strong>，這是一種在概念上很優雅的想法，是為了糾正參數空間中尺度的任意性。我將深入探討它是如何工作的，如何在不同數學思想之間構建起橋樑，並在最後討論它是否以及在何處實際有用。</p><p class=ql-align-justify><strong>所以，第一個問題是：計算分佈之間的距離有什麼用？</strong></p><h1 class=ql-align-center><strong>KL散度賦能</strong></h1><p class=ql-align-justify>KL散度，更確切地說是 Kullback-Leibler 散度，在技術上並不是兩個分佈之間的距離度量（數學家對於所謂的度量或合適的距離是非常挑剔的），但這是一個非常近似的想法。</p><div class=pgc-img><img alt="深度研究自然梯度優化，從入門到放棄 | Deep Reading" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9bc5e636172742cc8db7542ed7c27f1e><p class=pgc-img-caption></p></div><p class=ql-align-center>在數學上，它是通過計算從一個分佈或另一個分佈採樣的 x 值取得的對數概率的比率的期望值（即，概率值的原始差異）來獲得的。於是，取自其中一個分佈或另一個分佈的期望使得它成為了一個非對稱度量，其中 KL（P || Q）！= KL（Q || P）。但是，在許多其他方面，KL 散度帶給我們關於概率距離是這樣的概念：它是直接根據概率密度函數的定義來衡量，也就是說，在定義的分佈上的一堆點的密度值的差異。這有一個非常實用的地方，對於“X的概率是多少”這樣的一個問題，當X沒限定範圍時，可以有各種不同的分佈。</p><p class=ql-align-justify>在自然梯度的背景下，KL散度是一種用來衡量我們模型預測的輸出分佈變化的方式。如果我們正在解決多分類問題，那麼我們模型的輸出將是可以看作多項分佈的 softmax，每個類都有不同的概率。當我們談論由當前參數值定義的條件概率函數時，也就是我們討論的概率分佈。如果我們採用 KL 散度作為縮放我們梯度步長的方式，這意味著對於給定的輸入特徵集，如果它們導致預測的類別分佈在 KL 散度方面非常不同，那麼我們在這個空間中看到的兩個參數配置也會大相徑庭。</p><h1 class=ql-align-center><strong>相關的費舍爾理論</strong></h1><p class=ql-align-justify>到目前為止，我們已經討論過為什麼在參數空間中縮放更新每一步的距離是令人不滿意的，並給出了一個不那麼隨意的替代方案：也就是我們模型預測的類分佈最多只能在一個 KL 散度範圍內，去更新我們的每一步距離。對我來說，理解自然梯度最困難的部分是另一部分：KL 散度和 Fisher 信息矩陣之間的聯繫。</p><p class=ql-align-justify>先講故事的結尾，自然梯度是像下面公式這樣實現的：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="深度研究自然梯度優化，從入門到放棄 | Deep Reading" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7aa76d4621fe48228502dc691ad530db><p class=pgc-img-caption></p></div><p class=ql-align-center>在等號上面的 def 意思是右邊的內容是左邊符號的定義。右邊是由兩部分組成，首先，是參數關於損失函數的梯度（這是與正常梯度下降步驟中使用的一樣的梯度）。“自然”位來自第二個部分：Z 的對數概率函數的平方梯度的期望值。我們將這整個部分，稱為 Fisher 信息矩陣，然後將損失梯度乘以其逆。</p><p class=ql-align-justify>p-theta（z）項是由我們的模型定義的條件概率分佈，也就是說：神經網絡末端的 softmax。 我們正在研究所有 p-theta 項的梯度，因為我們關心的是模型預測的類概率因參數變化而變化的量。預測概率的變化越大，我們參數更新前和更新後的預測分佈之間的 KL 差異越大。</p><p class=ql-align-justify>對自然梯度優化感到困擾的部分原因在於，當你正在閱讀或思考它時，你必須理解兩個不同的梯度對象，也就是兩個不同的事物。順便說一句，這對於想要深入瞭解它是不可避免的一步，特別是在討論概率時。我們沒有必要去抓住這樣的一個直覺; 如果您不喜歡瀏覽所有的細節，請隨時跳到下一部分</p><h1 class=ql-align-center><strong>關於損失函數的梯度</strong></h1><p class=ql-align-justify>通常，分類損失是一個交叉熵函數，但更廣泛地說，它是一些函數，它將模型的預測概率分佈和真實目標值作為輸入，並且當模型的預測分佈遠離目標時，損失值就更高。這個對象的梯度是梯度下降學習的核心; 它表示如果將每個參數移動一個單位，損失值就會發生變化。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="深度研究自然梯度優化，從入門到放棄 | Deep Reading" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/79da3f6c87e44974b0e96158ff7ea1a3><p class=pgc-img-caption></p></div><p class=ql-align-center></p><p class=ql-align-justify>對數似然的梯度</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="深度研究自然梯度優化，從入門到放棄 | Deep Reading" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7df02b51364448be808bc7be50000c08><p class=pgc-img-caption></p></div><p class=ql-align-center></p><p class=ql-align-justify><br></p><p class=ql-align-justify><strong>對於我來說，這是學習自然梯度最令人困惑的部分</strong>。因為，當您去閱讀有關 Fisher 信息矩陣的內容，將獲得許多說明它與模型的對數似然的梯度有關的內容。我之前對似然函數的理解是，它代表了你的模型在某些數據集上預測的正確可能性有多大，特別是，您需要目標值才能計算它，因為您的目標是計算模型預測出真實目標的概率。在討論可能性的大多數情況下，例如非常常見的最大似然技術，您會關心對數似然的梯度，因為似然越高，模型從真實分佈中採樣的值的概率越高，當然我們就很樂意看到這樣的結果。實際上，這看起來像計算 p（class | x）梯度的期望值，其中概率是在數據的實際類分佈中得出。</p><p class=ql-align-justify>但是，你也可以用另一種方式來估計似然值，而不是根據真實目標值計算您的似然（也就是採用非零梯度，因為它可能增加模型參數對目標預測準確的概率）。你可以使用從條件分佈本身中提取的概率來計算期望。也就是說，如果網絡是採用 softmax，而不是基於給定觀察數據中的真實類別，以 0/1 概率取得 logp（z）的期望，那麼使用該模型的估計概率作為其權重，將導致整體期望梯度值為 0，因為我們的模型是以當前的置信度作為基本事實，但我們仍然可以得到對梯度方差的估計（即梯度平方），也就是在 Fisher 矩陣中（隱含地）計算預測類空間中的 KL 散度時，需要用到的值。</p><p class=ql-align-justify><strong>所以......它有幫助嗎？</strong></p><p class=ql-align-justify>這篇文章花了很多時間來談論機制：究竟什麼叫做自然梯度估計，以及關於它如何以及為什麼能起作用的更好的一種直覺。但是，如果我不回答這個問題，我覺得自己會失職：這件事真的有價值嗎？</p><p class=ql-align-justify></p><p class=ql-align-justify>簡短的回答是：實際上，它並沒有為大多數深度學習應用程序提供足夠引人注目的價值。 有證據表明自然梯度僅需要很少的步驟就能讓模型收斂，但正如本文稍後將討論的那樣，這是一個複雜的比較過程。對於那些被參數空間中隨意更新步驟的方法困擾的人來說，自然梯度的想法是優雅並且令人滿意的。但是，除了優雅之外，我不清楚它是否提供了更多的價值。</p><p class=ql-align-justify></p><p class=ql-align-justify><strong>據我所知，自然梯度提供了兩個關鍵的價值來源：</strong></p><p class=ql-align-justify>1、它提供有關曲率的信息</p><p class=ql-align-justify>2、它提供了一種直接控制模型在預測分佈空間中移動的方法，而且與模型在損失空間中的移動是分開的</p><p class=ql-align-justify></p><p class=ql-align-center><br></p><div class=pgc-img><img alt="深度研究自然梯度優化，從入門到放棄 | Deep Reading" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9e6be3ee4387440abd91b0721a6cc052><p class=pgc-img-caption></p></div><p class=ql-align-center><strong>曲率</strong></p><p class=ql-align-justify>現有的梯度下降法的一大奇蹟是它通過一階求導完成。一階方法是僅計算與要更新的參數相關的導數。使用一階導數，您所知道的是曲線在特定點處的切線。您不知道切線的變化速度有多快。而二階導數，會更具有描述性，即函數在任何給定方向上的曲率水平。要知道曲率是一個非常有用的東西，因為在高曲率區域，梯度從一點到另一點的急劇變化，所以優化時需要謹慎的邁出一大步，以免被正在攀登的陡峭山峰的局部信息誤導，而跳下就在眼前的懸崖。我喜歡這樣思考的方法，如果你處於一個點到點的梯度變化很大的地區（也就是說：高方差），那麼你 minibatch估計的梯度在某種意義上就更加的不確定。相比之下，如果梯度在給定點幾乎沒有變化，那麼下一步就不需要謹慎了。二階導數信息非常有用，因為它可以根據曲率水平來縮放每一步的大小。</p><p class=ql-align-justify>實際上，自然梯度是將參數更新除以梯度的二階導數。梯度相對於給定參數方向變化越大，Fisher 信息矩陣中的值越高，那麼在該方向上更新的步幅大小越低。這裡討論的梯度是批次中各點的經驗似然的梯度。這與損失函數方面的梯度不同。但是，直觀地說，似然的巨大變化與損失函數的劇烈變化並不相符。因此，通過捕獲關於給定點處的對數似然導數空間的曲率的信息，自然梯度也給出了真實的損失空間中的曲率信息。有一個非常有力的論據，當自然梯度已被證明可以加速收斂（至少在所需梯度步幅的數量方面），這就是價值的來源。</p><p class=ql-align-justify>然而請注意，本文提到自然梯度可以在梯度步驟方面加速收斂。這種精確度來自於自然梯度的每個單獨步驟需要更長時間，因為它需要計算 Fisher 信息矩陣，記住，這是一個存在於 n_parameters² 空間中的數量。事實上，這種急劇放緩類似於通過計算真實損失函數的二階導數引起的減速。雖然是這種情況，但我還沒有看到計算自然梯度的 Fisher 矩陣能夠比計算相關損失函數的二階導數更快。以此為假設，與對損失函數本身進行直接二階優化的（也可能是同樣的代價高昂）方法相比，很難看出自然梯度提供的值域是多少。</p><p class=ql-align-justify>現代神經網絡能夠在理論預測只有一階導數失敗的情況下取得成功的原因有很多，深度學習從業者已經找到了一堆巧妙的技巧來憑經驗逼近二階導數矩陣中所包含的信息。</p><p class=ql-align-justify><strong>Momentum</strong> 作為一種優化策略，它是通過保持上一次梯度值的加權平均值並將任何給定的梯度更新偏向該值移動的方向來起作用。這有助於解決在梯度值變化很大時的一部分問題：如果你經常得到相互矛盾的梯度更新，他們通常會取一個平均值，類似於減慢你的速度學習率。而且，相比之下，如果你反覆得到相同方向的梯度估計值，那就表明這是一個低曲率區域，並會建議採用更大的步長，而 Momentum 正是遵循這一規律。</p><p class=ql-align-justify><strong>RMSProp</strong>，令人捧腹的是，它是由 Geoff Hinton 在課程中期發明的，它是對以前存在的稱為 Adagrad 的算法的修改。RMSProp 通過獲取過去平方梯度值的指數加權移動平均值，或者換句話說，過去的梯度方差，並將更新的步長除以該值。這可以大致被認為是梯度的二階導數的經驗估計。</p><p class=ql-align-justify><strong>Adam（自適應的矩估計方法）</strong>，它基本上是結合了上述的兩種方法，估計梯度的實際平均值和實際方差。它是當今最常見，也是最常用的優化策略之一，主要是因為它具有平滑掉嘈雜的一階梯度信號的效果。</p><p class=ql-align-justify>還值得一提的是，以上這些方法除了通常根據函數曲率縮放更新步長之外，它們還能根據這些特定方向上的曲率值不同地縮放不同的更新方向。這與我們之前討論的內容有點類似，也就是按相同的數量縮放所有參數可能不是一件明智的事情。您甚至可以根據距離來考慮這一點：如果某方向上的曲率很高，那麼在歐幾里德參數空間中相同數量的步長將使我們在梯度值的移動上比預期的更遠。</p><p class=ql-align-justify>因此，雖然在定義參數更新的連貫方向方面不具備自然梯度的優雅，但它確實能夠在曲率不同的地方，準時的調整方向和更新步長。</p><p class=ql-align-center><strong>根據分佈直接決定</strong></p><p class=ql-align-justify><strong>OK，最後一節論述的是：既然直接分析 N² 的計算似乎都是非常耗時的，如果我們的目標是使用對數似然的分析曲率估計去替代損失曲率估計，為什麼我們不直接採用後者，或接近後者的方式。但是，如果您處於這樣一種情況，即您實際上是在關注預測類別分佈的變化，而不僅僅是損失變化？ 這種情況甚至會是什麼樣的？</strong></p><p class=ql-align-justify>這種情況的一個例子，是當前自然梯度方法的主要領域之一：強化學習領域的信任區域策略優化（Trust Region Policy Optimization, TRPO）。在策略梯度的設置中，您在模型結束時預測的分佈是以某些輸入狀態為條件的動作分佈。而且，如果您正在從您模型當前預測的策略中收集下一輪訓練的數據訓練來學習 on-policy，則有可能將您的策略更新為一個死循環。這就是策略遭受災難性打擊的意義所在。為了避免這種情況，我們要謹慎行事，而不是做可以顯著改變我們策略的梯度更新。如果我們在預測概率發生變化的過程中保持謹慎，那就極少會出現死循環的情況。</p><p class=ql-align-justify>這就是自然梯度的一個案例：在這裡，我們關心的實際上是在新參數配置下不同行為的預測概率變化了多少。我們關心的是模型本身，而不僅僅是損失函數的變化。</p><p class=ql-align-justify><strong>最後，我想通過兩個我仍然存在的困惑來結束這篇文章</strong></p><p class=ql-align-justify>1、我不敢確定計算對數似然的 Fisher 矩陣是否比僅僅計算損失函數的 Hessian 更有效（如果是，那將是自然梯度獲得關於損失曲面曲率信息的更容易的方式）</p><p class=ql-align-justify>2、我比較不相信，當我們在 z 的對數概率上計算期望時，那個期望值能夠替代我們模型預測的概率（期望必須相對於某些概率集來定義）。</p><p class=ql-align-justify><br></p><p class=ql-align-justify>（本文由AI科技大本營編譯，轉載請聯繫微信1092722531）</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>優化</a></li><li><a>從入</a></li><li><a>門到</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/b477de87.html alt=史上最屌php從入門到精通學習路線 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b477de87.html title=史上最屌php從入門到精通學習路線>史上最屌php從入門到精通學習路線</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9ed469c8.html alt=從入門到精通，每天一個電氣知識：單項雙電容電動機接線 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/3c40786761ee451da3d62b6e3977d639 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9ed469c8.html title=從入門到精通，每天一個電氣知識：單項雙電容電動機接線>從入門到精通，每天一個電氣知識：單項雙電容電動機接線</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/aaecdea6.html alt=DIY從入門到放棄：兩顆CPU性能更強嗎？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9e5d81f95ef44faa4269be3a98c7ef7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/aaecdea6.html title=DIY從入門到放棄：兩顆CPU性能更強嗎？>DIY從入門到放棄：兩顆CPU性能更強嗎？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/de7620c1.html alt=穿越指南—科技樹重建從入門到精通5：風嘴與風箱的製作 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d750c367b1f047aca3fd763fd3b7c72b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/de7620c1.html title=穿越指南—科技樹重建從入門到精通5：風嘴與風箱的製作>穿越指南—科技樹重建從入門到精通5：風嘴與風箱的製作</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e4385529.html alt=從入門到精通！3分鐘帶你瞭解養老投資 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e4385529.html title=從入門到精通！3分鐘帶你瞭解養老投資>從入門到精通！3分鐘帶你瞭解養老投資</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f15436bf.html alt=從入門到專業，十款專業錄音麥克風推薦 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/b209080743654cbe8db0da2b1902a1e8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f15436bf.html title=從入門到專業，十款專業錄音麥克風推薦>從入門到專業，十款專業錄音麥克風推薦</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b00bb593.html alt=從入門到精通，每天一個電氣小知識：五孔插座接線圖 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3c1df553f6054444a38ee5ba272cbe7a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b00bb593.html title=從入門到精通，每天一個電氣小知識：五孔插座接線圖>從入門到精通，每天一個電氣小知識：五孔插座接線圖</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc8f31cd.html alt="Spring 從入門到入土——AOP 就這麼簡單！| 原力計劃" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/S2d9sYtGLaUEMv style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc8f31cd.html title="Spring 從入門到入土——AOP 就這麼簡單！| 原力計劃">Spring 從入門到入土——AOP 就這麼簡單！| 原力計劃</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b447430b.html alt=信息抽取算法從入門到精通，一文了解自然語言處理的前世今生 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/007b4884a10342e39988c1777d5965c0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b447430b.html title=信息抽取算法從入門到精通，一文了解自然語言處理的前世今生>信息抽取算法從入門到精通，一文了解自然語言處理的前世今生</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/54d17c3b.html alt=Java從入門到放棄（6）面向對象 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/54d17c3b.html title=Java從入門到放棄（6）面向對象>Java從入門到放棄（6）面向對象</a></li><hr><li><a href=../../tw/%E9%81%8A%E6%88%B2/b1218f82.html alt=魔獸世界：從入門到精通，冰DK大型PVE指南 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/b7bdca7b6088452fabc580cf41dfe08e style=border-radius:25px></a>
<a href=../../tw/%E9%81%8A%E6%88%B2/b1218f82.html title=魔獸世界：從入門到精通，冰DK大型PVE指南>魔獸世界：從入門到精通，冰DK大型PVE指南</a></li><hr><li><a href=../../tw/%E9%81%8A%E6%88%B2/a7c55f37.html alt=從入門到入土：劍聖萬字大型攻略（野區篇） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2f78229599fa425091245a7264352684 style=border-radius:25px></a>
<a href=../../tw/%E9%81%8A%E6%88%B2/a7c55f37.html title=從入門到入土：劍聖萬字大型攻略（野區篇）>從入門到入土：劍聖萬字大型攻略（野區篇）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3bfacf7c.html alt=從入門到專家，手把手教你混凝土攪拌站選型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7a9e801295fe4e8984d4655675e331e9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3bfacf7c.html title=從入門到專家，手把手教你混凝土攪拌站選型>從入門到專家，手把手教你混凝土攪拌站選型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fb492cbe.html alt=商業分析從入門到精通第7.2講：驗證需求 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/06ff6a74c5e14420ac5a2f92ff6fee8f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fb492cbe.html title=商業分析從入門到精通第7.2講：驗證需求>商業分析從入門到精通第7.2講：驗證需求</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/de04add.html alt=機械加工如何從入門到精通，麻花鑽的結構組成及作用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/4d179f9aac3c49f4bf56d30990bd95ee style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/de04add.html title=機械加工如何從入門到精通，麻花鑽的結構組成及作用>機械加工如何從入門到精通，麻花鑽的結構組成及作用</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>