<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書 | 极客快訊</title><meta property="og:title" content="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RqSgmP9HojtMNK"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/42f9ed8.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/42f9ed8.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/42f9ed8.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/42f9ed8.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/42f9ed8.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/42f9ed8.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/42f9ed8.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/42f9ed8.html><meta property="article:published_time" content="2020-10-29T21:05:34+08:00"><meta property="article:modified_time" content="2020-10-29T21:05:34+08:00"><meta name=Keywords content><meta name=description content="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/42f9ed8.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RqSgmP9HojtMNK><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5Tvks4FOi7jjF><blockquote class=pgc-blockquote-abstract toutiao-origin=blockquote><div><div><p><strong>本文內容節選自《集成學習：基礎與算法》一書。由南京大學人工智能學院院長周志華教授編著，中文版由其學生李楠博士翻譯。</strong></p><p>回顧機器學習最近30 年的發展歷程，各種學習方法推陳出新、不斷演進。但是，在此歷程中，通過構建並結合多個學習器來完成學習任務的<strong>集成學習方法</strong>，始終是提升學習效果的重要手段，成為機器學習領域的“常青樹”，受到學術界和產業界的廣泛關注。</p><p>集成學習技術已在人工智能實踐中被廣泛使用，例如，對搜索、推薦、廣告的核心任務——點擊率預估而言，GBDT （Gradient Boosting Decision Trees）因其穩定、優異的效果一直是事實上的工業標準；在語音識別領域，基於集成深度學習的聲學模型極大提升了識別效果；在異常檢測上，iForest 因其極高的檢測效率在實踐中備受關注。</p><p>本書化繁為簡，用通俗易懂的表述方式重點講解集成學習的主流代表性技術 Boosting ，並詳釋了重要算法的實現。集成學習方法在實踐中獲得了巨大成功，本書也向讀者闡述了集成學習在如計算機視覺、醫療、信息安全和數據挖掘競賽等領域中的應用實踐 。</p><p>想要了解關於集成學習的更多幹貨知識，<strong>關注AI科技大本營</strong><strong>並評論分享你對本文的學習心得或集成學習的見解，我們將從中選出10條優質評論，各送出《集成學習：基礎與算法》一本。活動截止時間為7月25日晚8點。</strong></p></div></div></blockquote><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqLxPCXik3SR><p><strong class=highlight-text toutiao-origin=span>基本概念</strong></p><p>機器學習、模式識別和數據挖掘的一項主要任務就是基於“數據集”(data set)構建“好”的“模型”(model)。</p><p>一個“數據集”通常由一組特徵向量構成，其中每個特徵向量使用一組“特徵”(feature)來描述一個對象。例如，在圖 1.1 中所示的 3-高斯人造數據集中， 每個對象就是一個被特徵 x 座標、y 座標和形狀所表述的數據點，相應的特徵 向量可寫成(.5, .8, cross)或(.5, .8, circle)。數據集中特徵的數量被稱為“維 度”(dimension);例如，上述數據集的維度為 3。通常，“特徵”也會被稱為</p><p>“屬性”，一個“特徵向量”也會被稱為一個“示例”(instance)，一個“數據集” 也會被稱為一個“樣本集”(sample)。</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5Tvksd4wfLWHg><p>一個“模型”通常指一個預測模型或者從數據集中構建的數據結構的模 型;例如，決策樹、神經網絡、支持向量機等。從數據中構建模型的過程稱為“學習”(learning)或“訓練”(training)，這一過程由“學習算法”(learning algorithms)來完成。學習獲得的模型被稱為“假設”(hypothesis)，在本書中 會被稱為“學習器”(learner)。</p><p>現實中有不同類型的學習問題，其中最常見的是“監督學習”(supervised learning)和“無監督學習”(unsupervised learning)。監督學習的目標是預測 未見樣本的目標特徵的值，此時學習獲得的模型被稱為“預測器”(predictor)。例如，在 3-高斯數據集上，如果要預測數據點的形狀，“cross”和“circle”被 稱為“標記”(labels)，預測器應該能夠預測未知標記樣本的標記，如預測數據點(.2, .3)的形狀。</p><p>如果標記是類別(categorical)變量，如這裡的“形 狀”，此學習任務被稱為“分類”(classification)，相應的學習器被稱為“分類 器”(classifier);如果標記是數值(numerical)變量，如這裡的“x 座標”，此學 習任務被稱為“迴歸”(regression)，相應的學習器被稱為“迴歸模型”(fitted regression model)。在兩種情況下，學習過程都是在具有標記信息的數據集上 完成的;此時，一個具有標記的示例稱為一個“樣本”(example)。在“二分 類”(binary classification)中，我們通常使用“正”和“負”表示兩個類別標記。</p><p>無監督學習不依賴於標記信息，它的目標是發現數據的一些內在分 布信息。一個典型的任務就是“聚類”(clustering)，即:發現數據點內在的 “簇”(cluster)結構。在本書中，我們主要關注監督學習，尤其是分類。在 1.2 節中，我們將簡要介紹一些常用的學習算法。</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S5Tvkt39EAqUPA><p>通常來講，一個模型“好”還是“不好”取決於它是否能滿足用戶的需求。由於不同的用戶對學習結果有不同的期望，在相應任務沒有完成前，很難知道 什麼是“正確的期望”。一個常用的策略就是評測並估計模型的效果，並讓用戶 決定一個模型是否可用，或者讓用戶從一組候選模型中選擇最好的模型。</p><p>由於學習的根本目標是“泛化”(generalization)，即:能夠把從訓練樣本 學習獲得的知識推廣到未見樣本，因此一個好的學習器應該具有好的泛化能力， 即:具有較小的“泛化誤差”(generalization error)。但是，直接估計泛化誤差 需要知道未見樣本的“真實”(ground-truth)標記，因此實際操作中是不可行 的。</p><p>此時，代表做法是讓學習器對已知真實標記信息的“測試數據”(test data) 進行預測，並將計算獲得的“測試誤差”(test error)作為泛化誤差的估計。將 學習獲得的模型應用到未見數據上的過程，稱為“測試”(testing)。在測試前， 通常需要對學習獲得的模型進行合適的配置，如:調整參數，此過程需要使用具有真實標記的數據來評估學習效果，被稱為“驗證”(validation)，使用的相應數據被稱為“驗證數據”(validation data)。一般來講，測試數據不能包含 訓練數據和驗證數據，否則估計的效果會過於樂觀。1.3 節將介紹效果評估。</p><p>學習過程可以形式化地描述為如下過程。假設 X 為樣本空間，D 為 X 上 的分佈，f 為潛在目標函數。給定訓練集 D = {(x1, y1), (x2, y2), . . . , (xm, ym)}， 其中 xi 是來自分佈 D 的獨立同分布取樣且 yi = f(xi)。以分類為例，學習的 目標是構建學習器 h 以最小化泛化誤差</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvlJgHbh00YQ><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqLy93x95UEx><p><strong class=highlight-text toutiao-origin=span>常用學習算法</strong></p><p><strong>1.2.1 線性判別分析</strong></p><p>線性分類器由權重向量 w 和偏差項 b 構成。給定樣例 x，其按如下規則預測獲得類別標記 y，即</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S5TvlJyAlrLJUF><p>分類過程分為如下兩步:首先，使用權重向量 w 將樣本空間投影到直線上;然 後，尋找直線上的一個點把正樣本和負樣本分開。</p><p>為了尋找最優的線性分類器(即:w 和 b)，一個經典的學習算法是線性判別分析(Fisher’s linear discriminant analysis，LDA)。簡要來講，LDA 的基本想法是使不同類的樣本儘量遠離，同時使同類樣本儘可能靠近。這一目標可通 過擴大不同類別樣本的類中心距離，同時縮小每個類的類內方差來實現。</p><p>在一個二類數據集上，分別記所有正樣本的均值和協方差矩陣為 μ+ 和Σ+，所有負樣本的均值和協方差矩陣為</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvlKI6v2gNkB><p>。投影后的類中心的距離為</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvlKVvLQwGO><p>同時，類內方差可寫為</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S5TvlgMC7PG8Pz><p>基於此，線性判別分析最大化以下目標以獲取最優的權重向量，即</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S5TvlgmI4PR7Wt><p>此問題的最優解可寫為</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5Tvlh2AguJrYf><p>獲取權重向量 w 後，不難獲得偏差項 b。例如，如果兩類樣本都來自具有相同 方差的正態分佈，偏差項 b 的最優解就是兩個類中心的均值，即</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvlhHZ0ugpg><p>圖 1.2 給出了線性判別分析分類器在 3-高斯數據集上的分類邊界。</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S5TvlhaJIVb8w8><p><strong>1.2.2 決策樹</strong></p><p>一棵決策樹(decision tree)由一系列按“分而治之”(divide-and-conquer) 方式組織的樹狀決策測試構成。每個非葉節點上有一個特徵測試(也稱“分 割”);基於特徵測試中不同的特徵取值，節點中的數據被分成不同的子集。每 個葉節點具有一個類別標記，每個落到此葉節點的示例會被設置為此類別標記。預測時，從根節點開始，樣本經過一系列特徵測試到達葉節點，進而獲得預測 結果。以圖 1.3 為例，分類過程以檢測 y 座標的值是否大於 0.73 開始;如果是， 此示例被分類為“cross”，否則檢測決策樹測試 x 座標是否大於 0.64;如果是， 示例將被分類為“cross”，否則為“circle”。</p><p>決策樹學習算法通常是遞歸過程。在每一步執行中，給定數據集並選定分 割，數據集被此分割分成幾個子集;每個子集作為下一步執行中的給定數據集。不難發現，決策樹學習算法的核心是如何選擇分割。</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvmAbILrVac><p>ID3 算法 [Quinlan，1986] 選擇信息增益(information gain)作為分割準則。給定訓練集 D，此數據集的熵(entropy)定義為</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S5TvmB13l0RTtv><p>如果數據集 D 被分成子集 D1,...,Dk，相應的熵會減少，所減少的熵被稱為“信息增益”，即</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S5TvmBD5MrlzsP><p>基於此，能夠產生最大信息增益的“特徵-值”組合會被選作分割。</p><p>信息增益準則的一個問題是它會偏愛那些具有很多可能取值的特徵，而忽 略其和分類的相關性。例如，考慮我們在處理一個二分類任務，且每個樣例有一 個唯一的“id”，此時若選擇“id”作為一個特徵並且將其作為分割的特徵，將 會獲得很大的信息增益，因為它能夠正確分類所有的訓練樣本;但是，這樣的 分類結果卻不能泛化，不能用於對未見樣例進行預測。</p><p>信息增益的這一不足在著名的 C4.5 算法 [Quinlan，1993] 中被解決。具體 而言，C4.5 算法使用“信息增益率”(gain ratio)作為選擇分割的標準，即</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvmBlBrZiEc7><p>不難發現，在信息增益的基礎上，信息增益率使用特徵的取值數做歸一化。這 樣，在所有具有較好信息增益的特徵中，具有最高信息增益率的特徵被選擇作 為分割。</p><p>CART 算法 [Breiman et al.，1984] 也是一種著名的決策樹算法，它使用“基尼係數”(Gini index)作為分割準則，即</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S5TvmD6BdANah5><p>對於決策樹來講，經常可以觀察到一種現象:相對於在訓練集上表現不那麼 好的決策樹，一棵在訓練集上表現完美的決策樹可能具有更差的泛化能力。這種 現象被稱為“過擬合”(overfitting)，這是由於學習器誤把訓練數據集上的一些 特質當成潛在的真實數據分佈造成的。例如，學習器可能擬合了訓練數據集上 的噪音。</p><p>為了降低過擬合的風險，常用策略是使用“剪枝”(pruning)去除由於 訓練數據集上的噪聲或特質而產生出來的分枝。通常，“預剪枝”(pre-pruning) 在樹生長時執行剪枝操作，而“後剪枝”(post-pruning)在樹生長完成後再檢 查並決定去除哪些分枝。如果有驗證數據集，可以根據驗證誤差執行剪枝，即: 對預剪枝，如果驗證誤差上升，分枝就不能生長;對後剪枝，如果去除一個分 枝會造成驗證誤差降低，則執行剪枝。</p><p>早期的決策樹算法，如 ID3，僅能處理離散特徵;後期的算法，如 C4.5 和CART，能夠處理數值特徵。處理數值特徵最簡單的方式就是，評估選擇數值特 徵的每個取值作為分割點，並據此將數據集分成兩個子集，其中一個包含比分 割點大的樣本，另一個包含其餘樣本。</p><p>當決策樹的高度被限制為 1 時，它預測時僅執行一個測試，這種決策樹被 稱為“決策樹樁”(decision stump)。通常來講，決策樹是非線性分類器，而決 策樹樁是線性分類器。</p><p>圖 1.4 給出了一個典型的決策樹在 3-高斯數據集上的分類邊界。</p><p><strong>1.2.3 神經網絡</strong></p><p>神經網絡(neural network)，也稱人工神經網絡，源自對生物神經網絡的 仿真和模擬。一個神經網絡的功能由神經元(neuron)模型、網絡結構和學習 算法共同決定。</p><p>神經元，也稱神經單元(unit)，是神經網絡中的基本計算組件。最常用的神經元模型是 McCulloch-Pitts 模型(簡稱 M-P 模型)。如圖 1.5(a) 所示，在M-P 模型中，輸入信號首先和連接權重相乘，然後相加累計後和一個稱為激發閾值的偏置項比較。若累計信號值大於激發閾值，將激發神經元並由激活函數(activation function)產生輸出信號。</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvmiwB2weU2B><p>神經元由帶權重的鏈接連接構成網絡，並能形成多種可能的網絡結構。其中最著名的就是如圖 1.5(b) 所示的多層前饋網絡(multi-layer feed-forwardnetwork)。在該網絡中，神經元按層連接，沒有層內和跨層連接。輸入層 (input layer)接收輸入特徵向量，每個神經元對應特徵向量的一維，且其激活 函數被設置為 f(x) = x。輸出層(output layer)輸出標記，其中每個神經元通 常對應一個可能的標記或標記向量的一維。輸入層和輸出層之間的層被稱為隱 層(hidden layer)。隱層神經元和輸出層神經元是功能性單元，常用的激活函數是 sigmoid 函數，即</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvmjPFZ5zFMS><p>雖然可以使用多個隱層，但是大家還是經常使用具有一個或兩個隱層的神 經網絡。這是由於:第一，理論上具有一個隱層的前饋網絡已經有能力逼近任 意連續函數;第二，訓練具有多個隱層的網絡需要複雜的學習算法防止其陷入 發散狀態(即:神經網絡難以收斂到穩定狀態)。</p><p>神經網絡訓練的目的是決定其連接權重和神經元的激發閾值，這些值決定 了神經網絡所代表的函數。不難發現，如果激活函數可微，整個多層前饋神經 網絡就是關於這些參數的可微函數，此時最為常用的訓練思路就是梯度下降(gradient descent)方法。</p><p>誤差逆傳播(BP)算法 [Werbos，1974;Rumelhart et al.，1986] 是最為成功的神經網絡訓練算法。在該算法中，輸入經過輸入層、隱層前饋計算到達輸 出層，在輸出層中對網絡輸出和樣本標記進行比較並計算誤差;然後，這些誤 差再經過隱層反向傳播回輸入層，在傳播的過程中算法會調整連接權重和激發 閾值以降低誤差。整個過程按梯度方向調節各個參數的值，並執行多輪，直到 訓練誤差減小到預定目標範圍。</p><p><strong>1.2.4 樸素貝葉斯</strong></p><p>為了對測試樣例 x 進行分類，學習算法可以構建概率模型來估計後驗概 率 P(y | x)，並選取具有最大後驗概率值的 y 作為輸出;這就是最大後驗(maximum a posteriori，MAP)準則。基於貝葉斯定理，</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvmjlAiz0QN4><p>在上式中，P(y) 可以通過計算訓練數據中每個類的比例獲得;由於是在同一個 x 上比較不同 y，P(x) 可被忽略。因此，僅需考慮 P(x | y)，如果能夠獲 得 P(x | y) 的精確估計，就能獲得理論上的最優分類器，即具有理論上最小 錯誤率——貝葉斯錯誤率(Bayes error rate)——的貝葉斯最優分類器(Bayes optimal classifier)。但是，由於 P (x | y) 是聯合概率分佈，需要估計指數多的特徵組合數量，直接估計將十分困難。為此，需要引入一些假設，以使估計變得可行。</p><p>樸素貝葉斯分類器(naïve Bayes classifier)假設給定類別標記，n 個特徵之間是獨立的。因此，這意味著，僅需在每個類內部計算每個特徵取值的比例，從而避免了對聯合概 率的估計。</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S5Tvmk99wseSAY><p>在訓練階段，樸素貝葉斯分類器對所有類別 y ∈ Y 估計 P (y)，並對所有特 徵 xi 估計 P (xi | y)。在測試階段，在所有類別標記中，樸素貝葉斯分類器通過最大化</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S5TvmkOBVPHX9K><p>選擇測試樣例 x 的類別標記。</p><p><strong>1.2.5 k-近鄰</strong></p><p>k-近鄰(k-NN)算法的基本假設是輸入空間中相似的樣本在輸出空間中也 應該相似。它沒有顯式的訓練過程，僅需存儲所有的訓練樣本，是一個懶惰學習 (lazy learning)方法。測試時，對測試樣例 x，k-近鄰算法在訓練樣本中尋找與 測試樣本最近的 k 個近鄰。對分類任務，測試樣例被分類為 k 個近鄰樣本中的投票最高的類別;對迴歸任務，測試樣例被預測為 k 個近鄰樣本的標記的均值。圖 1.6(a) 給出了一個 3-近鄰算法分類的示意圖。</p><p>圖 1.6(b) 給出了在 3-高斯數據集上 1-近鄰分類器——也稱最近鄰分類器——的分類邊界。</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvnFo7rBiHPk><p><strong>1.2.6 支持向量機和核方法</strong></p><p>支持向量機(SVMs)[Cristianini & Shawe-Taylor，2000] 是用來解決二分類問題的大間隔分類器(large margin classifier)，它嘗試尋找具有最大間隔(margin)的超平面來區分不同類別的樣本。其中，間隔定義為不同類別的樣本到分類超平面的距離。</p><p>考慮線性分類器 y = sign(w⊤x + b) (縮寫為 (w, b))，並使用 hinge 損 失來評估其對數據的擬合程度，即</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvnGJ9GqbxoD><p>同時，樣本 xi 距離超平面 w⊤x + b 的歐氏距離為</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvnGZ8pYddxu><p>此時，如果要求對所有樣本</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvnH6FqVwKOh><p>成立，距離超平面的最小距離為∥w∥−1，那麼支持向量機嘗試最大化 ∥w∥−1。</p><p>具體來講，支持向量機通過求解如下優化問題來獲得分類器，即</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvnHUBfelAMV><p>其中，C 是參數，ξi 是鬆弛變量，用來處理數據不完全可分的情況(如數據噪聲)。圖 1.7 給出了支持向量機的示意圖。</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S5Tvnmr1WbA19J><p>式 (1.19) 稱為優化問題的主形式(primal form)，其等價的對偶形式(dual form)為</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S5TvnnIC6M1VU4><p>線性分類的侷限在於當數據是非線性時，它們難以進行較好的分類。這種 情況下，一個常用方法是將數據點映射到一個高維空間，在這個高維空間中原 本線性不可分的樣本點變得線性可分。但是，由於在高維空間中計算內積比較困難，學習過程將變得很慢甚至不可行。</p><p>值得慶幸的是，核函數可以幫助解決這個問題。由核函數代表的特徵空間 稱為再生核希爾伯特空間(Reproducing Kernel Hilbert Space，RKHS)，而此 空間上的內積等於原空間上內積的核映射。換言之，對所有 xi，有</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5Tvnna2X6gliY><p>其中 φ 是從原空間到高維空間的映射，K 為核函數。基於此，可以將對偶問題(1.20) 中的內積替換為核函數。</p><p>根據 Mercer 定理 [Cristianini & Shawe-Taylor，2000]，每個半正定對稱函數都是核函數。常用的核函數包括線性核</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S5TvnoGCHhgsGt><p>其中 σ 為高斯核的寬度。</p><p>核技巧 (kernel trick)，即通過核函數將數據點映射到 RKHS 完成學習的方法，是一種常用的學習策略，可以和任何考慮輸入空間內積的學習方法配合 使用。當使用核技巧後，相應的學習方法稱為核方法。不難發現，支持向量機是 一類特殊的核方法，即使用核技巧的線性分類器。</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RqMqMYY7LcGio6><p><strong class=highlight-text toutiao-origin=span>評估和對比</strong></p><p>通常，我們需要從多個學習算法中挑選合適的學習算法，或者選擇合適的 參數。挑選最優算法和參數的任務稱為模型選擇，為此需要估計學習器的效果。基於實驗方法，這需要設計一系列的實驗並通過假設檢驗來比較不同的模型。</p><p>顯然，使用學習器在訓練集上的誤差，即訓練誤差，來評估學習器的泛化 性能是不合適的，這是因為訓練誤差會偏好複雜模型而非具有較好泛化能力的 模型。通常，一個高度複雜的學習器，如完全生長的決策樹，具有很低的訓練誤 差，但會有過擬合問題，從而在未見樣本上表現較差。此時，合理的學習性能評 估過程應該在驗證集上進行。需要注意的是，在訓練過程中，訓練集和驗證集 的類別標記都是已知的，當模型選定後，應該共同使用這些數據來獲得最終的 學習器。</p><p>事實上，我們常把一個數據集劃分成兩部分來獲得訓練集和驗證集。當進 行數據集劃分時，應該儘量保留原始數據的各種性質，否則驗證集很可能會產 生誤導性結果。舉一個極端的例子，如果訓練集僅包含正樣本，驗證集僅包含 負樣本，這顯然是不合適的。在分類問題中，如果要隨機劃分原始數據集，應該 保持訓練集和驗證集具有相似的類別比例;這種保留類別比例的採樣方式通常 稱為分層採樣。</p><p>如果沒有足夠的標記數據來獲得一個獨立的驗證集，一個常用的驗證方法是交叉驗證(cross-validation)。在 k 折交叉驗證中，原始數據集 D 首先被分層劃分成 k 個相同大小的互斥子集 D1,...,Dk，然後執行 k 次訓練-測試過程。在第 i 次執行中，使用 Di 作為驗證集，並使用其他所有子集(即</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S5TvoRBCV7U4yC><p>)作j=i為訓練集。最終返回 k 個測試結果的均值作為交叉驗證的結果。為了降低劃分 數據時隨機性帶來的影響，k 折交叉驗證通常需要使用不同的隨機劃分重複 t次，即 t 次 k 折交叉驗證。常用的實驗配置包括 10 次 10 折交叉驗證，以及Dietterich [1998] 建議使用的 5 次 2 折交叉驗證。當 k 等於原始數據集中的樣本 數量時，驗證集僅包含一個樣本，這種方法就是留一法(leave-one-out)驗證。</p><p>基於估計誤差可以比較不同的學習算法。由於數據劃分具有一定的隨機性， 簡單地比較平均誤差可能並不可靠。為此，通常使用假設檢驗。</p><p>若學習算法可高效執行 10 次，5 次 2 折交叉驗證成對 t 檢驗是一個好的 選擇 [Dietterich，1998]。該檢驗首先執行 5 次 2 折交叉驗證。在每次的 2 折交 叉驗證中，數據集 D 被隨機劃分為相同大小的 D1 和 D2 兩個子集，兩個算法a 和 b 分別在一個子集上訓練並在另一個子集上測試，從而獲得四個誤差估計:</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvoRTFwjHbS4><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S5TvoReHmseZX4><p>將服從自由度為 5 的 Student’s t 分佈。選定顯著程度α，如果t落在區間[−t5(α/2), t5(α/2)] 內，將接受零假設，即表示兩個算法沒有顯著差異。常用的 顯著程度 α 包括 0.05 和 0.1。</p><p>若算法僅能被執行一次，可以使用 McNemar 檢驗 [Dietterich，1998]。用err01 表示第一個算法分類錯誤但第二個算法分類正確的樣本數，err10 表示第一 個算法分類正確但第二個算法分類錯誤的樣本數。如果兩個算法具有相同的效 果，err01 應該和 err10 具有基本相同的值，因此，統計量服從χ2 -分佈。</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5TvoRsw1M7Rt><p>有時，需要在多個數據集上評估多個學習算法。在這種情況下，可以使用Friedman 檢驗 [Demšar，2006]。首先，基於平均誤差，在每個數據集上對每個 算法排序，具有最小誤差的最優算法排序值(rank)設為 1，其他算法依次獲得 較大的排序值。然後，可以獲得每個算法在所有數據集上的平均排序值，並用Nemenyi post-hoc 檢驗 [Demšar，2006] 來計算臨界差(critical difference)值</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S5TvoS7GCNA3R4><p>其中 k 是算法數量，N 是數據集數量，qα 是臨界值(critical value)[Demšar，2006]。如果兩個算法的平均排序值的差大於臨界差，那麼它們的效果具有顯著 差別。</p><p>如圖 1.8 所示，Friedman 檢驗的結果可以用臨界差圖來展現;其中，每個 算法對應一條線段，線段的中點是該算法的平均排序值，線的寬度是臨界差。在 圖 1.8 中，在該顯著程度下，算法 A 顯著優於其他算法，算法 D 顯著劣於其他 算法，算法 B 和 C 沒有顯著差別。</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5Tvp033AcaZSE><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqMZM85xvsbJ><p><strong class=highlight-text toutiao-origin=span>集成學習方法</strong></p><p>和傳統學習方法訓練一個學習器不同，集成學習方法訓練多個學習器 並結合它們來解決一個問題。通常，集成學習也被稱為基於委員會的學習(committee-based learning)或多分類器系統(multiple classifier system)。圖 1.9 給出了一個通用的集成學習框架。一個集成由多個基學習器(base learner)構成，而基學習器由基學習算法(base learning algorithm)在訓練數據上訓練獲得，它們可以是決策樹、神經網絡或其他學習算法。</p><p>大多數集成學習方法使用同一種基學習算法產生同質的基學習器，即相同種類的學習器，生成同質集成(homogeneous ensemble)；同時，也有一些方法使用多種學習算法 訓練不同種類的學習器，構建異質集成(heterogeneous ensemble)。在異質集 成中，由於沒有單一的基學習算法，相較於基學習器，人們更傾向於稱這些學 習器為個體學習器(individual learner)或組件學習器(component learner)。</p><p>通常，集成具有比基學習器更強的泛化能力。實際上，集成學習方法之所以 那麼受關注，很大程度上是因為它們能夠把比隨機猜測稍好的弱學習器(weak learner)變成可以精確預測的強學習器(strong learner)。因此，在集成學習 中基學習器也稱為弱學習器。</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S5Tvp1N8JuWnWL><p>由於使用多個模型解決問題的思想在人類社會中有悠久的歷史，我們難以 對集成學習方法的歷史進行溯源。例如，作為科學研究的基本假設，當簡單假 設和複雜假設都符合經驗觀測時，奧卡姆剃刀(Occam’s razor)準則偏好簡單 假設;但早在此之前，希臘哲學家伊壁鳩魯(Epicurus，公元前 341—270)提 出的多釋準則(principle of multiple explanations)[Asmis，1984] 主張應該保 留和經驗觀測符合的多個假設。</p><p>集成學習領域的發展得益於三個方面的早期研究，即:分類器結合、弱分類器集成和混合專家模型(mixture of experts)。分類器結合主要來自模式識別 領域。這方面的研究關注強分類器，試圖設計強大的結合規則來獲取更強的結 合分類器，在設計和使用不同的結合規則上積累深厚。弱分類器集成方面的研究主要集中在機器學習領域。這方面的研究關注弱分類器，試圖設計強大的算法提升弱分類器的效果，產生了包括AdaBoost 和 Bagging 等眾多著名的集成學習算法，並且在將弱學習器提升為強學習器方面有深入的理論理解。混合專家模型的研究主要集中在神經網絡領域。在此，人們通常考慮使用分而治之的策略來共同學習一組模型，並結合使用它們獲得一個總體解決方案。</p><p>20 世紀 90 年代以來，集成學習方法逐漸成為一個主要的學習範式，這主 要得益於兩項先驅性工作。其中，[Hansen & Salamon，1990] 是實驗方面的工 作, 如圖 1.10 所示，它指出一組分類器的集成經常會產出比其中最優個體分類 器更精準的預測;[Schapire，1990] 是理論方面的工作，它構造性地證明了弱學 習器可以被提升為強學習器。雖然我們所需的高精度學習器難以訓練，但弱學 習器在實踐中卻容易獲得，這個理論結果為使用集成學習方法獲得強學習器指明瞭方向。</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S5Tvp1dEucowDm><p>一般來講，構建集成有兩個步驟:首先產生基學習器，然後將它們結合起 來。為了獲得一個好的集成，通常認為每個基學習器應該儘可能準確，同時儘可能不同。</p><p>值得一提的是，構建一個集成的計算代價未必會顯著高於構建單一學習器。這是因為使用單一學習器時，模型選擇和調參經常會產生多個版本的模型，這 與在集成學習中構建多個基學習器的代價是相當的;同時，由於結合策略一般 比較簡單，結合多個基學習器通常只會花費很低的計算代價。</p><img alt="周志華教授力作，豆瓣10分好評，集成學習如何破解AI實踐難題 | 贈書" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqNMhMM9NBY><p><strong class=highlight-text toutiao-origin=span>集成學習方法的應用</strong></p><p>KDD Cup 作為最著名的數據挖掘競賽，自 1997 年以來每年舉辦，吸引了全球大量數據挖掘隊伍參加。競賽包含多種多樣的實際任務，如網絡入侵檢測(1999)、分子生物活性和蛋白質位點預測(2001)、肺栓塞檢測(2006)、客戶 關係管理(2009)、教育數據挖掘(2010)和音樂推薦(2011)等。在諸多機器 學習技術中，集成學習方法獲得了高度的關注和廣泛的使用。例如，在連續三 年的 KDD Cup 競賽中(2009—2011)，獲獎的冠軍和亞軍都使用了集成學習方法。</p><p>另一項著名的賽事 Netflix Prize由 Netflix 公司舉辦。競賽任務是基於用 戶的歷史偏好提升電影推薦的準確度，如果參賽隊伍能在 Netflix 公司自己的算 法基礎上提升 10% 的準確度，就能夠獲取百萬美元大獎。2009 年 9 月 21 日，Nexflix 公司宣佈，百萬美元大獎由 BellKor’s Pragmatic Chaos 隊獲得，他們的 方案結合了因子模型、迴歸模型、玻爾茲曼機、矩陣分解、k-近鄰等多種模型。另外還有一支隊伍取得了和獲獎隊伍相同的效果，但由於提交結果晚了 20 分鐘 無緣大獎，他們同樣使用了集成學習方法，甚至使用“The Ensemble”作為隊名。</p><p>除了在競賽上獲得顯赫戰績，集成學習方法還被成功應用到多種實際應用 中。實際上，在幾乎所有的機器學習應用場景中都能發現它的身影。例如，計算 機視覺的絕大部分分支，如目標檢測、識別、跟蹤，都從集成學習方法中受益。</p><p>基於 AdaBoost 和級聯結構，Viola & Jones [2001，2004] 提出了一套通用 的目標檢測框架。Viola & Jones [2004] 顯示在一臺 466MHz 計算機上，人臉檢 測器僅需 0.067 秒就可以處理 384×288 的圖像，這幾乎比當時最好的技術快 15倍，且具有基本相同的檢測精度。在隨後的十年間，這個框架被公認為計算機 視覺領域最重大的技術突破。</p><p>Huang et al. [2000] 設計了一套集成學習方法解決姿態無關的人臉識別問 題。它的基本思路是使用特殊設計的模型集成多個特定視角的神經網絡模型。和需要姿態信息作為輸入的傳統方法相比，這個方法不需要姿態信息，甚至能 在輸出識別結果的同時輸出姿態信息。Huang et al. [2000] 發現這個方法的效果 甚至優於以完美姿態信息作為輸入的傳統方法。類似的方法後來被用於解決多 視圖人臉檢測問題 [Li et al.，2001]。</p><p>目標跟蹤的目的是在視頻的連續幀中對目標對象進行連續標記。通過把目 標檢測看成二分類問題，並訓練一個在線集成來區分目標對象和背景，Avidan [2007] 提出了集成跟蹤(ensemble tracking)方法。該方法通過更新弱分類器來學習由於對象外觀和背景發生的變化。Avidan [2007] 發現這套方法能處理多種 具有不同大小目標的不同類別視頻，並且運行高效，能應用於在線任務。</p><p>在計算機系統中，用戶行為會有不同的抽象層級，相關信息也會來自多個 渠道，集成學習方法就非常適合於刻畫計算機安全問題 [Corona et al.，[2009]。Giacinto et al. [2003] 使用集成學習方法解決入侵檢測問題。考慮到有多種特徵 刻畫網絡連接，他們為每一種特徵構建了一個集成，並將這些集成的輸出結合 起來作為最終結果。Giacinto et al. [2003] 發現在檢測未知類型的攻擊時，集成 學習方法能夠獲得最優的性能。此後，Giacinto et al. [2008] 提出了一種集成方 法解決基於異常的入侵檢測問題，該方法能夠檢測出未知類型的入侵。</p><p>惡意代碼基本上可以分為三類:病毒、蠕蟲和木馬。通過給代碼一個合適 的表示，Schultz et al. [2001] 提出了一種集成學習方法用以自動檢測以往未見 的惡意代碼。基於對代碼的 n-gram 表示，Kolter & Maloof [2006] 發現增強決 策樹(boosted decision tree)能夠獲得最優的檢測效果，同時他們表示這種方法可以在操作系統中檢測未知類型的惡意代碼。</p><p>集成學習方法還被應用於解決計算機輔助醫療診斷中的多種任務，尤其用 於提升診斷的可靠性。周志華等人設計了一種雙層集成架構用於肺癌細胞檢測 任務 [Zhou et al.，2002a]，其中當且僅當第一層中的所有個體學習器都診斷為“良性”時才會預測為“良性”，否則第二層會在“良性”和各種不同的癌症類 型間進行預測。他們發現雙層集成方法能同時獲得高檢出率和低假陽性率。</p><p>對於老年痴呆症的早期診斷，以往的方法通常僅考慮來自腦電波的單信道數據。Polikar et al. [2008] 提出了一種集成學習方法來利用多信道數據;在此方法中，個體學習器基於來自不同電極、不同刺激和不同頻率的數據進行訓練，同時它 們的輸出被結合起來產生最終預測結果。</p><p>除了計算機視覺、安全和輔助診斷，集成學習方法還被應用到多個其他 領域和任務中。例如，信用卡欺詐檢測 [Chan et al.，1999;Panigrahi et al.，2009]，破產預測 [West et al.，2005]，蛋白質結構分類 [Tan et al.，2003;Shen & Chou，2006]，種群分佈預測 [Araújo & New，2007]，天氣預報 [Maqsood et al.，2004;Gneiting & Raftery，2005]，電力負載預測 [Taylor & Buizza，2002]， 航空發動機缺陷檢測 [Goebel et al.，2000;Yan & Xue，2008]，音樂風格和藝 術家識別 [Bergstra et al.，2006] 等。</p><p><strong>贈書活動</strong></p><p>想要了解關於集成學習的更多幹貨知識，<strong toutiao-origin=span>關注AI科技大本營並在評論區分享你對本文的學習心得或集成學習的見解</strong>，我們將從中選出10條優質評論，各送出《集成學習：基礎與算法》一本。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>周志華</a></li><li><a>10</a></li><li><a>分好評</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/602bcbcd.html alt="每天這樣陪玩 10 分鐘，勝過上萬元早教班" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RlIqLlfA7COpuB style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/602bcbcd.html title="每天這樣陪玩 10 分鐘，勝過上萬元早教班">每天這樣陪玩 10 分鐘，勝過上萬元早教班</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1017785d.html alt=手機收不到短信？這10種排查方法，看看你都會了沒？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/557c2fac09c941f69a7359acbaa810b4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1017785d.html title=手機收不到短信？這10種排查方法，看看你都會了沒？>手機收不到短信？這10種排查方法，看看你都會了沒？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/831354a0.html alt=一看就懂！10年老電工經驗總結：3個區分光纖是多模還是單模方法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/5bfee759920547be9fa569ef49842ad0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/831354a0.html title=一看就懂！10年老電工經驗總結：3個區分光纖是多模還是單模方法>一看就懂！10年老電工經驗總結：3個區分光纖是多模還是單模方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4a9d2887.html alt=10塊錢，可以撬動多大的槓桿？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8d6ace340ff04af9945e80694c8429cc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4a9d2887.html title=10塊錢，可以撬動多大的槓桿？>10塊錢，可以撬動多大的槓桿？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e342e11c.html alt="2019掌上生活10元風暴怎麼玩攻略 快速獲取小招喵方法" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/Rk8ZSn39iz0Be8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e342e11c.html title="2019掌上生活10元風暴怎麼玩攻略 快速獲取小招喵方法">2019掌上生活10元風暴怎麼玩攻略 快速獲取小招喵方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/75414528.html alt=招行掌上生活10元風暴續-10元購的膳魔師電飯煲到貨 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/c737032a00314e3091e2d81d467d777f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/75414528.html title=招行掌上生活10元風暴續-10元購的膳魔師電飯煲到貨>招行掌上生活10元風暴續-10元購的膳魔師電飯煲到貨</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/43600b7e.html alt=推薦10個值得一去的國外神器網站！登錄無需任何手段，實用又有趣 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8716c57443b84bf39d12f7181c19e5d7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/43600b7e.html title=推薦10個值得一去的國外神器網站！登錄無需任何手段，實用又有趣>推薦10個值得一去的國外神器網站！登錄無需任何手段，實用又有趣</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/65c5ac44.html alt=跟新型冠狀病毒有關的10個謠言，別被騙了 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/8d29a0c1-76c9-462f-93a3-12ac1fb91bb1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/65c5ac44.html title=跟新型冠狀病毒有關的10個謠言，別被騙了>跟新型冠狀病毒有關的10個謠言，別被騙了</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/256535bb.html alt="小米10又推'億級像素'，方向錯了為何還要越走越遠？" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/e25a6761fe874f28a49818dabfc7d87a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/256535bb.html title="小米10又推'億級像素'，方向錯了為何還要越走越遠？">小米10又推'億級像素'，方向錯了為何還要越走越遠？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4f8d6ade.html alt=耗時4年，奴役勞工10萬餘人，偽滿時期水豐、水滿水電站施工場景 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/152094458108473fb60ec01 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4f8d6ade.html title=耗時4年，奴役勞工10萬餘人，偽滿時期水豐、水滿水電站施工場景>耗時4年，奴役勞工10萬餘人，偽滿時期水豐、水滿水電站施工場景</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f448e7cc.html alt=10月市區日照時數僅27.3小時創十年新低，這真的很“貴陽” class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1541124052858d1740db0d4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f448e7cc.html title=10月市區日照時數僅27.3小時創十年新低，這真的很“貴陽”>10月市區日照時數僅27.3小時創十年新低，這真的很“貴陽”</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1bbb42bf.html alt=2020年10月7日，世界職業拳壇新鮮事兒 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/0939c8b4612246ab8bd720d63240562e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1bbb42bf.html title=2020年10月7日，世界職業拳壇新鮮事兒>2020年10月7日，世界職業拳壇新鮮事兒</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5cb94ee8.html alt=60年來新低！佛山10天日照時數僅0.8小時！去年洗的衣服，今年幹了嗎？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/R8aij9CAatiYf5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5cb94ee8.html title=60年來新低！佛山10天日照時數僅0.8小時！去年洗的衣服，今年幹了嗎？>60年來新低！佛山10天日照時數僅0.8小時！去年洗的衣服，今年幹了嗎？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d989bb55.html alt=創60年新低！佛山10天日照時數僅0.8小時！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/88a602fd3e244777a8e3504eba69edac style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d989bb55.html title=創60年新低！佛山10天日照時數僅0.8小時！>創60年新低！佛山10天日照時數僅0.8小時！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/73a10505.html alt="甘肅冬季降水量為近10年最多 日照時數為近50年最少" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RKI3zXA8zfQbRR style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/73a10505.html title="甘肅冬季降水量為近10年最多 日照時數為近50年最少">甘肅冬季降水量為近10年最多 日照時數為近50年最少</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>