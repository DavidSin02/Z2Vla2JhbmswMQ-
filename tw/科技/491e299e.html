<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>愛可可AI論文推介(10月9日) | 极客快訊</title><meta property="og:title" content="愛可可AI論文推介(10月9日) - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/6bf012fba2904c8cbafc6dad1079a51f"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/491e299e.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/491e299e.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/491e299e.html><meta property="article:published_time" content="2020-10-29T21:12:41+08:00"><meta property="article:modified_time" content="2020-10-29T21:12:41+08:00"><meta name=Keywords content><meta name=description content="愛可可AI論文推介(10月9日)"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/491e299e.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>愛可可AI論文推介(10月9日)</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>AI - 人工智能 LG - 機器學習 CV - 計算機視覺 CL - 計算與語言</p><p><br></p><p><strong>1、[CV]*Contrastive Learning of Medical Visual Representations from Paired Images and Text</strong></p><p>Y Zhang, H Jiang, Y Miura, C D. Manning, C P. Langlotz</p><p>[Stanford University]</p><p><strong>用無監督對比學習方法(ConVIRT)從圖像-文本對學習醫學視覺表示，用圖像表示與文本數據兩模態間的雙向對比目標，進行醫學圖像編碼器的預訓練，ConVIRT是領域不可知(domain-agnostic)的，無需額外的專家輸入。在4個醫學圖像分類任務和2個圖像檢索任務中，ConVIRT的表現優於其他同樣使用文本數據的強域內初始化方法，表示質量顯著提高。與ImageNet預訓練相比，ConVIRT能以更少的標記數據實現同水平的分類精度。</strong></p><blockquote><p>Learning visual representations of medical images is core to medical image understanding but its progress has been held back by the small size of hand-labeled datasets. Existing work commonly relies on transferring weights from ImageNet pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. We propose an alternative unsupervised strategy to learn medical visual representations directly from the naturally occurring pairing of images and textual data. Our method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test our method by transferring our pretrained weights to 4 medical image classification tasks and 2 zero-shot retrieval tasks, and show that our method leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classification tasks, our method requires only 10% as much labeled training data as an ImageNet initialized counterpart to achieve better or comparable performance, demonstrating superior data efficiency.</p></blockquote><p>https://weibo.com/1402400261/JokrT9o4s</p><div class=pgc-img><img alt=愛可可AI論文推介(10月9日) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6bf012fba2904c8cbafc6dad1079a51f><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=愛可可AI論文推介(10月9日) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/abba76e01694402ab0f7c62199bf09ff><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=愛可可AI論文推介(10月9日) onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/20c8135a263d459a82df8efc234d43d8><p class=pgc-img-caption></p></div><p><br></p><p><strong>2、[CL]Autoregressive Entity Retrieval</strong></p><p>N D Cao, G Izacard, S Riedel, F Petroni</p><p>[University of Amsterdam & Facebook AI Research]</p><p><strong>自迴歸實體檢索(GENRE)，用自迴歸方式生成實體名，以上下文為條件，通過從左到右逐詞條自迴歸方式生成實體唯一名稱來檢索實體。自迴歸允許直接捕捉上下文和實體名稱之間的關係，有效地對兩者進行交叉編碼；編解碼器架構的參數隨詞彙表大小而非實體數量而變化，大大減少了內存佔用；可有效計算精確的軟最大損失，而不需要對負數據進行子採樣。</strong></p><blockquote><p>Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity information such as descriptions. This approach leads to several shortcomings: i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; ii) a large memory footprint is needed to store dense representations when considering large entity sets; iii) an appropriately hard set of negative data has to be subsampled at training time. We propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion, and conditioned on the context. This enables to mitigate the aforementioned technical issues: i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new SOTA, or very competitive results while using a tiny fraction of the memory of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name.</p></blockquote><div class=pgc-img><img alt=愛可可AI論文推介(10月9日) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/70cc8bd78b324e17a7547137297644af><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=愛可可AI論文推介(10月9日) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c283709ae67e45d680ab6897b041d0fd><p class=pgc-img-caption></p></div><p><br></p><p><strong>3、[AI]Human-Level Performance in No-Press Diplomacy via Equilibrium Search</strong></p><p>J Gray, A Lerer, A Bakhtin, N Brown</p><p>[Facebook AI Research]</p><p><strong>用均衡搜索玩經典棋類桌遊《外交風雲》達到人類水平，《外交風雲》是個複雜遊戲，涉及合作與競爭，對AI技術提出了重大的理論和實踐挑戰。新的AI智能體通過對人類數據的監督學習並使用外部遺憾最小化的單步前向搜索，在該遊戲中實現了人類水平性能。用該智能體在流行的《外交風雲》網站上匿名玩遊戲，在1128名人類玩家中排名第23。</strong></p><blockquote><p>Prior AI breakthroughs in complex games have focused on either the purely adversarial or purely cooperative settings. In contrast, Diplomacy is a game of shifting alliances that involves both cooperation and competition. For this reason, Diplomacy has proven to be a formidable research challenge. In this paper we describe an agent for the no-press variant of Diplomacy that combines supervised learning on human data with one-step lookahead search via external regret minimization. External regret minimization techniques have been behind previous AI successes in adversarial games, most notably poker, but have not previously been shown to be successful in large-scale games involving cooperation. We show that our agent greatly exceeds the performance of past no-press Diplomacy bots, is unexploitable by expert humans, and achieves a rank of 23 out of 1,128 human players when playing anonymous games on a popular Diplomacy website.</p></blockquote><div class=pgc-img><img alt=愛可可AI論文推介(10月9日) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/270c5c909cdb4c44a934e9c0df8f0a78><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=愛可可AI論文推介(10月9日) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4f7fd456f3364850afbdc71a85c5aca6><p class=pgc-img-caption></p></div><p><br></p><p><strong>4、[CL]CATBERT: Context-Aware Tiny BERT for Detecting Social Engineering Emails</strong></p><p>Y Lee, J Saxe, R Harang</p><p>[Sophos AI]</p><p><strong>用上下文感知的超小BERT檢測釣魚郵件，為了識別那些不包含惡意代碼、不與已知攻擊共享單詞選擇的手工社會工程郵件，通過微調一個預訓練的、大規模修剪的BERT模型，加上來自郵件頭的附加上下文特徵，從中學習郵件內容和上下文特性間的上下文表示，即使存在故意拼寫錯誤和惡意逃避的狀況，也能有效檢測出有針對性的釣魚郵件。該方法優於用適配器和上下文層的強基線模型，釣魚郵件檢出率達到87%。</strong></p><blockquote><p>Targeted phishing emails are on the rise and facilitate the theft of billions of dollars from organizations a year. While malicious signals from attached files or malicious URLs in emails can be detected by conventional malware signatures or machine learning technologies, it is challenging to identify hand-crafted social engineering emails which don't contain any malicious code and don't share word choices with known attacks. To tackle this problem, we fine-tune a pre-trained BERT model by replacing the half of Transformer blocks with simple adapters to efficiently learn sophisticated representations of the syntax and semantics of the natural language. Our Context-Aware network also learns the context representations between email's content and context features from email headers. Our CatBERT(Context-Aware Tiny Bert) achieves a 87% detection rate as compared to DistilBERT, LSTM, and logistic regression baselines which achieve 83%, 79%, and 54% detection rates at false positive rates of 1%, respectively. Our model is also faster than competing transformer approaches and is resilient to adversarial attacks which deliberately replace keywords with typos or synonyms.</p></blockquote><div class=pgc-img><img alt=愛可可AI論文推介(10月9日) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b7caec640ef945f9a9811c7f39653da5><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=愛可可AI論文推介(10月9日) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/704ec79ab0bc4a3ab1d91318d20331a7><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=愛可可AI論文推介(10月9日) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6dd5d2d9bcea48f89cb21f7510af24f7><p class=pgc-img-caption></p></div><p><br></p><p><strong>5、[CL]WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization</strong></p><p>F Ladhak, E Durmus, C Cardie, K McKeown</p><p>[Columbia University & Cornell University]</p><p><strong>跨語種抽象摘要新基準WikiLingua，一個跨語言和多語言抽象摘要的基準數據集，從WikiHow中提取了18種語言的文章和摘要對，WikiHow是個高質量的協作資源，提供了人工撰寫的一系列不同主題的操作指南。通過對齊文章中用於描述每個how-to步驟的圖像，創建了跨語言的金標準文章-摘要對齊。</strong></p><blockquote><p>We introduce WikiLingua, a large-scale, multilingual dataset for the evaluation of crosslingual abstractive summarization systems. We extract article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. We create gold-standard article-summary alignments across languages by aligning the images that are used to describe each how-to step in an article. As a set of baselines for further studies, we evaluate the performance of existing cross-lingual abstractive summarization methods on our dataset. We further propose a method for direct crosslingual summarization (i.e., without requiring translation at inference time) by leveraging synthetic data and Neural Machine Translation as a pre-training step. Our method significantly outperforms the baseline approaches, while being more cost efficient during inference.</p></blockquote><p>https://weibo.com/1402400261/JokOvyCDJ</p><div class=pgc-img><img alt=愛可可AI論文推介(10月9日) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6a010acb47e847f3bd332d7818936a00><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=愛可可AI論文推介(10月9日) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d5daad8a4f514f5b9e01f10f02c33fdd><p class=pgc-img-caption></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>AI</a></li><li><a>論文</a></li><li><a>10</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E9%81%8A%E6%88%B2/b811f6c.html alt="論文提出AI Programmer：使用遺傳算法自動創造軟件程序" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=http://p9.pstatp.com/large/3b0e00016071a70e22b3 style=border-radius:25px></a>
<a href=../../tw/%E9%81%8A%E6%88%B2/b811f6c.html title="論文提出AI Programmer：使用遺傳算法自動創造軟件程序">論文提出AI Programmer：使用遺傳算法自動創造軟件程序</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/602bcbcd.html alt="每天這樣陪玩 10 分鐘，勝過上萬元早教班" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RlIqLlfA7COpuB style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/602bcbcd.html title="每天這樣陪玩 10 分鐘，勝過上萬元早教班">每天這樣陪玩 10 分鐘，勝過上萬元早教班</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e55aaf9b.html alt="論文推薦 | 袁修孝：航攝影像密集匹配的研究進展與展望" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/R6Ieh75DBRtmcY style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e55aaf9b.html title="論文推薦 | 袁修孝：航攝影像密集匹配的研究進展與展望">論文推薦 | 袁修孝：航攝影像密集匹配的研究進展與展望</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4653cf8e.html alt=論文推薦｜王濤：國產機載大視場三線陣CCD相機GNSS偏心矢量和IMU視軸偏心角標定技術 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/R6Ieh75DBRtmcY style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4653cf8e.html title=論文推薦｜王濤：國產機載大視場三線陣CCD相機GNSS偏心矢量和IMU視軸偏心角標定技術>論文推薦｜王濤：國產機載大視場三線陣CCD相機GNSS偏心矢量和IMU視軸偏心角標定技術</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1017785d.html alt=手機收不到短信？這10種排查方法，看看你都會了沒？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/557c2fac09c941f69a7359acbaa810b4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1017785d.html title=手機收不到短信？這10種排查方法，看看你都會了沒？>手機收不到短信？這10種排查方法，看看你都會了沒？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1801773a.html alt="AI芯天下丨虛擬工廠 ，2025智能製造新路徑" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/9fc63c421a37451d950eb1efcde49091 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1801773a.html title="AI芯天下丨虛擬工廠 ，2025智能製造新路徑">AI芯天下丨虛擬工廠 ，2025智能製造新路徑</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/831354a0.html alt=一看就懂！10年老電工經驗總結：3個區分光纖是多模還是單模方法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/5bfee759920547be9fa569ef49842ad0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/831354a0.html title=一看就懂！10年老電工經驗總結：3個區分光纖是多模還是單模方法>一看就懂！10年老電工經驗總結：3個區分光纖是多模還是單模方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/60f3eef5.html alt="全球首個3D AI主播火了 虛擬製作時代將至" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/68cc49aeb805412eb4a9d126b12284bd style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/60f3eef5.html title="全球首個3D AI主播火了 虛擬製作時代將至">全球首個3D AI主播火了 虛擬製作時代將至</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4a9d2887.html alt=10塊錢，可以撬動多大的槓桿？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8d6ace340ff04af9945e80694c8429cc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4a9d2887.html title=10塊錢，可以撬動多大的槓桿？>10塊錢，可以撬動多大的槓桿？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e342e11c.html alt="2019掌上生活10元風暴怎麼玩攻略 快速獲取小招喵方法" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/Rk8ZSn39iz0Be8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e342e11c.html title="2019掌上生活10元風暴怎麼玩攻略 快速獲取小招喵方法">2019掌上生活10元風暴怎麼玩攻略 快速獲取小招喵方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/75414528.html alt=招行掌上生活10元風暴續-10元購的膳魔師電飯煲到貨 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/c737032a00314e3091e2d81d467d777f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/75414528.html title=招行掌上生活10元風暴續-10元購的膳魔師電飯煲到貨>招行掌上生活10元風暴續-10元購的膳魔師電飯煲到貨</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/098d4a35.html alt=「火爐煉AI」機器學習048-Harris檢測圖像角點 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d756b20a1dbc4ab4b4f22d6b61be2043 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/098d4a35.html title=「火爐煉AI」機器學習048-Harris檢測圖像角點>「火爐煉AI」機器學習048-Harris檢測圖像角點</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/43600b7e.html alt=推薦10個值得一去的國外神器網站！登錄無需任何手段，實用又有趣 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8716c57443b84bf39d12f7181c19e5d7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/43600b7e.html title=推薦10個值得一去的國外神器網站！登錄無需任何手段，實用又有趣>推薦10個值得一去的國外神器網站！登錄無需任何手段，實用又有趣</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/51723e35.html alt=AI也有偏見：你在機器“眼裡”是好人還是壞蛋？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RfDgA10IBHJqK9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/51723e35.html title=AI也有偏見：你在機器“眼裡”是好人還是壞蛋？>AI也有偏見：你在機器“眼裡”是好人還是壞蛋？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fbd8ef39.html alt=破解AI大腦黑盒邁出新一步！谷歌現在更懂機器，還開源了研究工具 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/6c330004a1c9ab23e6a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fbd8ef39.html title=破解AI大腦黑盒邁出新一步！谷歌現在更懂機器，還開源了研究工具>破解AI大腦黑盒邁出新一步！谷歌現在更懂機器，還開源了研究工具</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>