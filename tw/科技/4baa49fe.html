<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>Rasa 入門教程 Core 系列（五） | 极客快訊</title><meta property="og:title" content="Rasa 入門教程 Core 系列（五） - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/4a7ff1c942e5461c96528277a1df87d8"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4baa49fe.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4baa49fe.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/4baa49fe.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4baa49fe.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4baa49fe.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/4baa49fe.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/4baa49fe.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4baa49fe.html><meta property="article:published_time" content="2020-10-29T21:12:41+08:00"><meta property="article:modified_time" content="2020-10-29T21:12:41+08:00"><meta name=Keywords content><meta name=description content="Rasa 入門教程 Core 系列（五）"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/4baa49fe.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>Rasa 入門教程 Core 系列（五）</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div class=pgc-img><img alt="Rasa 入門教程 Core 系列（五）" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4a7ff1c942e5461c96528277a1df87d8><p class=pgc-img-caption>rasa_tutorial_core_background.png</p></div><p>Rasa 入門教程 Core 系列包括十一個部分，前面介紹了 Rasa 框架 Core 系列的第四部分：動作。本文主要介紹 Rasa 框架 Core 系列的第五部分：<strong>策略</strong>。</p><p>本文的目錄結構：</p><ol start=1><li>策略配置</li><li>動作選擇</li><li>Keras 策略</li><li>Embedding 策略</li><li>Mapping 策略</li><li>Memoization 策略</li><li>Augmented Memoization 策略</li><li>Fallback 策略</li><li>Two-Stage Fallback 策略</li><li>Form 策略</li></ol><h1 class=pgc-h-arrow-right>1. 策略配置</h1><p>類 rasa.core.policies.Policy 決定在對話的每個步驟中應採取的什麼動作。有多種策略可供選擇，您可以在 rasa.core.agent.Agent 中包含多個策略。</p><p>項目中的 config.yml 文件有關鍵字 policies，你可以使用該關鍵字來自定義助手使用的策略。在下面的示例中，最後兩行顯示瞭如何使用自定義策略類並將參數傳遞給它。</p><pre><code>policies:  - name: "KerasPolicy"    featurizer:    - name: MaxHistoryTrackerFeaturizer      max_history: 5      state_featurizer:        - name: BinarySingleStateFeaturizer  - name: "MemoizationPolicy"    max_history: 5  - name: "FallbackPolicy"    nlu_threshold: 0.4    core_threshold: 0.3    fallback_action_name: "my_fallback_action"  - name: "path.to.your.policy.class"    arg1: "..."</code></pre><p><strong>1.1 最長曆史記錄</strong></p><p>Rasa 核心策略的一個重要超參數是 max_history。這可以控制模型查看多少對話歷史記錄，以決定下一步應採取什麼動作。</p><p>你可以在 yaml 文件中的配置策略裡通過設置 max_history 值將其傳遞到 Featurizer。</p><p>例如：假設你有一個 out_of_scope 表示超綱的意圖。如果你的機器人連續多次看到此意圖，則可能要告訴用戶你可以提供哪些幫助。因此你的 story 可能如下所示：</p><pre><code>* out_of_scope   - utter_default* out_of_scope   - utter_default* out_of_scope   - utter_help_message</code></pre><p>為了使 Rasa Core 學習這種模式，max_history 至少為 4。</p><p>如果增加 max_history，你的模型將變得更大，並且訓練將花費更長的時間。如果你有會影響將來對話的一些信息，則應將其作為槽位存儲，槽位信息可以用於每個特徵提取器。</p><p><strong>1.2 數據增強</strong></p><p>訓練模型時，默認情況下，Rasa Core 隨機將 stories 文件中的 stories 粘合在一起來創建更長的 stories。stories 文件如下所示：</p><pre><code># thanks* thankyou   - utter_youarewelcome# bye* goodbye   - utter_goodbye</code></pre><p>實際上你想教自己的策略在不相關時忽略對話歷史記錄，而無論以前發生了什麼，都只需以相同的動作進行響應即可。</p><p>你可以使用 --augmentation 標誌更改此行為，允許你設置 augmentation_factor。augmentation_factor 決定在訓練中有多少增強的 stories 是子採樣。增強後的 stories 在訓練之前會進行二次抽樣，因為它們的數量會很快變得非常大，我們希望對其進行限制。樣本 stories 的數量是augmentation_factorx10。默認情況下，增強設置為 20，最多可生成200個增強 stories。</p><p>--augmentation 0 表示禁用所有增強行為。基於 Memoization 策略不受增強影響（與 augmentation_factor 無關），並且會自動忽略所有增強的 stories。</p><h1 class=pgc-h-arrow-right>2. 動作選擇</h1><p>在任何時候，配置中定義的每個策略都會以一定的置信度預測下一個動作。有關每個策略如何做出決策的更多信息，請閱讀下面的策略說明。然後，該機器人的下一步動作將由具有最高置信度的預測策略決定。</p><p>在兩個相同的置信度策略進行預測的情況下（例如：Memoization 和 Mapping 策略，始終以 0 或 1 的置信度進行預測），則將考慮策略的優先級。Rasa 策略有默認優先級，這些默認優先級用於在平局時的預期結果。如下所示，數字越高優先級越高：</p><pre><code>5. FormPolicy4. FallbackPolicy and TwoStageFallbackPolicy3. MemoizationPolicy and AugmentedMemoizationPolicy2. MappingPolicy1. EmbeddingPolicy, KerasPolicy, and SklearnPolicy</code></pre><p>上述優先級層次結構可確保，在 NLU 的置信度值在 nlu_threshold 之下（例如：如果有一個 mapped 動作），機器人仍將回退。通常，不建議每個優先級使用一個以上的策略，並且某些優先級相同的策略（例如兩個 fallback 策略）嚴格不能一併使用。</p><p>如果創建自己的策略，請使用這些優先級作為確定策略優先級的指南。如果你的策略是機器學習策略，則它最有可能具有與 Rasa 機器學習策略相同的優先級 1。</p><h1 class=pgc-h-arrow-right>3. Keras 策略</h1><p>KerasPolicy 策略在 Keras 中使用神經網絡實現選擇下一個動作。默認體系結構是基於 LSTM，但是你可以重寫 KerasPolicy.model_architecture 方法以實現自己的體系結構。</p><pre><code>def model_architecture(    self, input_shape: Tuple[int, int], output_shape: Tuple[int, Optional[int]]) -&gt; tf.keras.models.Sequential:    """Build a keras model and return a compiled model."""    from tensorflow.keras.models import Sequential    from tensorflow.keras.layers import (        Masking,        LSTM,        Dense,        TimeDistributed,        Activation,    )    # Build Model    model = Sequential()    # the shape of the y vector of the labels,    # determines which output from rnn will be used    # to calculate the loss    if len(output_shape) == 1:        # y is (num examples, num features) so        # only the last output from the rnn is used to        # calculate the loss        model.add(Masking(mask_value=-1, input_shape=input_shape))        model.add(LSTM(self.rnn_size, dropout=0.2))        model.add(Dense(input_dim=self.rnn_size, units=output_shape[-1]))    elif len(output_shape) == 2:        # y is (num examples, max_dialogue_len, num features) so        # all the outputs from the rnn are used to        # calculate the loss, therefore a sequence is returned and        # time distributed layer is used        # the first value in input_shape is max dialogue_len,        # it is set to None, to allow dynamic_rnn creation        # during prediction        model.add(Masking(mask_value=-1, input_shape=(None, input_shape[1])))        model.add(LSTM(self.rnn_size, return_sequences=True, dropout=0.2))        model.add(TimeDistributed(Dense(units=output_shape[-1])))    else:        raise ValueError(            "Cannot construct the model because"            "length of output_shape = {} "            "should be 1 or 2."            "".format(len(output_shape))        )    model.add(Activation("softmax"))    model.compile(        loss="categorical_crossentropy", optimizer="rmsprop", metrics=["accuracy"]    )    if obtain_verbosity() &gt; 0:        model.summary()    return model</code></pre><p>下面是訓練部分：</p><pre><code>def train(    self,    training_trackers: List[DialogueStateTracker],    domain: Domain,    **kwargs: Any,) -&gt; None:    # set numpy random seed    np.random.seed(self.random_seed)    training_data = self.featurize_for_training(training_trackers, domain, **kwargs)    # noinspection PyPep8Naming    shuffled_X, shuffled_y = training_data.shuffled_X_y()    self.graph = tf.Graph()    with self.graph.as_default():        # set random seed in tf        tf.set_random_seed(self.random_seed)        self.session = tf.compat.v1.Session(config=self._tf_config)        with self.session.as_default():            if self.model is None:                self.model = self.model_architecture(                    shuffled_X.shape[1:], shuffled_y.shape[1:]                )            logger.info(                "Fitting model with {} total samples and a "                "validation split of {}"                "".format(training_data.num_examples(), self.validation_split)            )            # filter out kwargs that cannot be passed to fit            self._train_params = self._get_valid_params(                self.model.fit, **self._train_params            )            self.model.fit(                shuffled_X,                shuffled_y,                epochs=self.epochs,                batch_size=self.batch_size,                shuffle=False,                verbose=obtain_verbosity(),                **self._train_params,            )            # the default parameter for epochs in keras fit is 1            self.current_epoch = self.defaults.get("epochs", 1)            logger.info("Done fitting keras policy model")</code></pre><p>你可以重寫這些方法來實現你要的模型，或者使用預定義 keras model 初始化 KerasPolicy。</p><p>為了在相同輸入的情況下，獲得可重複訓練結果，你可以將 KerasPolicy 的屬性 random_seed 設置為任意整數值。</p><h1 class=pgc-h-arrow-right>4. Embedding 策略</h1><p>TEDP（Transformer Embedding Dialogue Policy），該策略的預定義體系結構主要包括以下步驟：</p><ul><li>將每個時間步長中的用戶輸入（用戶意圖和實體），先前的系統動作，槽位值和動作表單連接到輸入矢量中，然後傳遞給 pre-tranformer 的嵌入層；</li><li>餵給 Transformer；</li><li>在 Transformer 的輸出上增加一個 dense 層，以便在每個時間步長中嵌入對話；</li><li>在每個時間步長中使用 dense 層為系統動作創建嵌入；</li><li>計算對話嵌入和嵌入式系統動作之間的相似度。這一步是基於 StarSpace 想法。</li></ul><p>推薦使用 state_featurizer=LabelTokenizerSingleStateFeaturizer(...)（詳見 Featurization）。</p><p><strong>4.1 配置</strong></p><p>可以將配置參數作為參數傳遞到 EmbeddingPolicy 策略配置文件中。以下是可調的超參數：</p><ol start=1><li>神經網絡的體系架構：hidden_layers_sizes_b 在為系統動作嵌入層之前設置隱藏層大小的列表，隱藏層的數量等於列表的長度；transformer_size 設置 transformer 的單元數；num_transformer_layers 設置 transformer 層數；pos_encoding 設置 transformer 中位置編碼的類型，應為timing或emb；max_seq_length 如果使用嵌入位置編碼，則設置最大序列長度；num_heads 設置多頭注意力中的頭數；</li><li>訓練：batch_size 設置一個前向/後向傳播中的訓練示例的數量，批次大小越大，需要的存儲空間就越大；batch_strategy 設置批處理策略的類型，應為sequence或balanced；epochs設置算法訓練數據的次數，其中一個 epoch 表示所有訓練示例中一次前向和後向傳播；random_seed 如果設置為int，則對於相同的輸入將獲得可重複的訓練結果；</li><li>嵌入：embed_dim 設置嵌入空間的維度；num_neg 設置不正確的意圖標籤的數量，該算法將在訓練過程中將其與用戶輸入的相似性降至最低；similarity_type設置相似性的類型，它應該是auto，cosine或者inner，如果auto要根據loss_type， inner就是softmax，cosine 就是 margin；loss_type設置損失函數的類型，應為softmax或margin；mu_pos控制算法應如何嘗試為正確的意圖標籤生成嵌入向量，僅當loss_type設置為margin時使用；mu_neg控制最大負相似性為不正確的意圖，僅當 loss_type 設置為 margin 時使用;use_max_sim_neg如果設置為true，表示在不正確意圖標籤上使用最小化最大相似度，僅當 loss_type 被設置為margin 時使用;scale_loss如果設置為true，會降低損失的比例，例如在高可信度下預測正確標籤的示例，僅當loss_type設置為softmax時使用；</li><li>正則化：C2 設置L2正則化的規模；C_emb設置最小化不同意圖標籤的嵌入之間最大相似性的重要性的標度，僅當loss_type設置為margin時使用；droprate_a 在用戶輸入嵌入層之前，設置層之間的丟失率；droprate_b 在系統動作嵌入層之前，設置層之間的丟失率；</li><li>計算訓練準確率：evaluate_every_num_epochs 設置計算訓練準確率的頻率，較小的值可能會損害性能；evaluate_on_num_examples 要使用多少示例來支持驗證集以計算驗證準確率，較大的值可能會損害性能。</li></ol><p>可以在策略配置文件中指定這些參數。默認值定義在 EmbeddingPolicy.defaults 中：</p><pre><code>defaults = {    # nn architecture    # a list of hidden layers sizes before user embed layer    # number of hidden layers is equal to the length of this list    "hidden_layers_sizes_pre_dial": [],    # a list of hidden layers sizes before bot embed layer    # number of hidden layers is equal to the length of this list    "hidden_layers_sizes_bot": [],    # number of units in transformer    "transformer_size": 128,    # number of transformer layers    "num_transformer_layers": 1,    # type of positional encoding in transformer    "pos_encoding": "timing",  # string 'timing' or 'emb'    # max sequence length if pos_encoding='emb'    "max_seq_length": 256,    # number of attention heads in transformer    "num_heads": 4,    # training parameters    # initial and final batch sizes:    # batch size will be linearly increased for each epoch    "batch_size": [8, 32],    # how to create batches    "batch_strategy": "balanced",  # string 'sequence' or 'balanced'    # number of epochs    "epochs": 1,    # set random seed to any int to get reproducible results    "random_seed": None,    # embedding parameters    # dimension size of embedding vectors    "embed_dim": 20,    # the type of the similarity    "num_neg": 20,    # flag if minimize only maximum similarity over incorrect labels    "similarity_type": "auto",  # string 'auto' or 'cosine' or 'inner'    # the type of the loss function    "loss_type": "softmax",  # string 'softmax' or 'margin'    # how similar the algorithm should try    # to make embedding vectors for correct labels    "mu_pos": 0.8,  # should be 0.0 &lt; ... &lt; 1.0 for 'cosine'    # maximum negative similarity for incorrect labels    "mu_neg": -0.2,  # should be -1.0 &lt; ... &lt; 1.0 for 'cosine'    # the number of incorrect labels, the algorithm will minimize    # their similarity to the user input during training    "use_max_sim_neg": True,  # flag which loss function to use    # scale loss inverse proportionally to confidence of correct prediction    "scale_loss": True,    # regularization    # the scale of L2 regularization    "C2": 0.001,    # the scale of how important is to minimize the maximum similarity    # between embeddings of different labels    "C_emb": 0.8,    # dropout rate for dial nn    "droprate_a": 0.1,    # dropout rate for bot nn    "droprate_b": 0.0,    # visualization of accuracy    # how often calculate validation accuracy    "evaluate_every_num_epochs": 20,  # small values may hurt performance    # how many examples to use for hold out validation set    "evaluate_on_num_examples": 0,  # large values may hurt performance}</code></pre><h1 class=pgc-h-arrow-right>5. Mapping 策略</h1><p>MappingPolicy 策略可直接映射到意圖動作。通過給意圖屬性 triggers 來分配映射，例如：</p><pre><code>intents: - ask_is_bot:     triggers: action_is_bot</code></pre><p>一個意圖最多隻能映射到一個動作。機器人在收到觸發意圖消息後將運行映射的動作。之後，它將監聽下一條消息。隨著下一條用戶消息，將恢復正常預測。</p><p>如果你不希望意圖動作映射影響對話歷史記錄，則映射動作必須返回一個 UserUtteranceReverted() 事件。這將從對話歷史記錄中刪除用戶的最新消息及其後面發生的任何事件。這意味著你不應該將意圖-動作交互包括在 stories 中。</p><p>例如：如果用戶在對話流程中詢問“你是機器人嗎？”，你可能想回答用戶後，該交互不要影響到下一個動作預測。自定義動作可以做任何事，下面是一個簡單的示例，調度機器人的話語，然後恢復原交互：</p><pre><code>class ActionIsBot(Action):"""Revertible mapped action for utter_is_bot"""def name(self):    return "action_is_bot"def run(self, dispatcher, tracker, domain):    dispatcher.utter_template(template="utter_is_bot")    return [UserUtteranceReverted()]</code></pre><h1 class=pgc-h-arrow-right>6. Memoization 策略</h1><p>MemoizationPolicy 策略是記住訓練數據中的會話。如果訓練數據中存在正確的對話，它會將置信度設置為 1.0 並預測下一個動作，否則置信度為 0.0 預測為 None。</p><h1 class=pgc-h-arrow-right>7. Augmented Memoization 策略</h1><p>AugmentedMemoizationPolicy 策略記得從訓練的故事長達例子max_history轉彎，就像MemoizationPolicy。此外，它具有遺忘機制，可以遺忘對話歷史記錄中的某些步驟，並嘗試在歷史記錄短的情況下找到匹配項。它預測有信心下一個動作1.0 ，如果發現匹配，否則就預測None有信心0.0。</p><h1 class=pgc-h-arrow-right>8. Fallback 策略</h1><p>當出現以下情況就會觸發 FallbackPolicy 策略調用撤銷動作：</p><ol start=1><li>意圖識別的置信度值低於 nlu_threshold。</li><li>排名最高意圖的置信度與排名第二意圖的置信度值相差小於 ambiguity_threshold。</li><li>沒有一項對話政策預測高於置信度 core_threshold 的行動。</li></ol><p><strong>8.1 配置</strong></p><p>閾值和撤銷動作可以在策略配置文件中作為 FallbackPolicy 參數進行調整：</p><pre><code>policies:  - name: "FallbackPolicy"    nlu_threshold: 0.3    ambiguity_threshold: 0.1    core_threshold: 0.3    fallback_action_name: 'action_default_fallback'</code></pre><p><strong>參數說明</strong>nlu_threshold接受 NLU 預測所需的最低置信度ambiguity_threshold最高意圖的置信度必須超過第二高意圖的置信度的最小值core_threshold接受 Rasa Core 的動作預測所需的最低置信度fallback_action_name如果意圖或動作的置信度低於各自的閾值，則調用撤銷動作。</p><p>您也可以在 python 代碼中配置 FallbackPolicy：</p><pre><code>from rasa.core.policies.fallback import FallbackPolicyfrom rasa.core.policies.keras_policy import KerasPolicyfrom rasa.core.agent import Agentfallback = FallbackPolicy(fallback_action_name="action_default_fallback",                          core_threshold=0.3,                          nlu_threshold=0.3,                          ambiguity_threshold=0.1)agent = Agent("domain.yml", policies=[KerasPolicy(), fallback])</code></pre><h1 class=pgc-h-arrow-right>9. Two-Stage Fallback 策略</h1><p>TwoStageFallbackPolicy 策略用於處理 NLU 置信度值低的情況，試圖在多個階段對用戶輸入進行消歧。</p><ol start=1><li>如果 NLU 預測的置信度值低或不明顯高於排名第二預測的置信度值，則要求用戶確認意圖的分類。如果用戶確認，那麼 story 將繼續，就好像從一開始就對意圖進行了高置信度的分類。如果他們拒絕，則要求用戶重新表達他們的消息。</li><li>重新表達如果重新表達的意圖分類是確定的，則 story 將繼續，就好像用戶從一開始就有此意圖一樣。如果重新表達的意圖經過意圖分類的置信度值不高，則要求用戶確認意圖分類。</li><li>二次確認如果用戶確認了此意圖，則 story 將繼續，就好像用戶從一開始就有此意圖一樣。如果用戶拒絕，則將原始意圖分類為 deny_suggestion_intent_name 指定的意圖，並觸發最後的回退操作。</li></ol><p><strong>9.1 配置</strong></p><p>要使用TwoStageFallbackPolicy，請在策略配置中包括以下內容。</p><pre><code>policies:  - name: TwoStageFallbackPolicy nlu_threshold: 0.3 ambiguity_threshold: 0.1 core_threshold: 0.3 fallback_core_action_name: "action_default_fallback" fallback_nlu_action_name: "action_default_fallback" deny_suggestion_intent_name: "out_of_scope"</code></pre><p><strong>參數說明</strong>nlu_threshold接受 NLU 預測所需的最低置信度ambiguity_threshold最高意圖的置信度必須超過第二高意圖的置信度的最小值core_threshold接受 Rasa Core 的動作預測所需的最低置信度fallback_core_action_name如果 Rasa Core 動作預測的置信度值低於 core_threshold，則調用撤銷動作。該動作是為了提出建議的可識別意圖。fallback_nlu_action_name如果 Rasa NLU 意圖分類的置信度值低於 nlu_threshold，則調用的撤銷動作。當用戶第二次拒絕時調用此動作。deny_suggestion_intent_name用於用戶拒絕後建議的意圖分類名稱</p><h1 class=pgc-h-arrow-right>10. Form 策略</h1><p>FormPolicy 策略是 MemoizationPolicy 的擴展，用來處理表單填充。當 FormAction 被調用時，FormPolicy 策略將不斷預測 FormAction，直到表單中的所有必需的槽位值被填充。有關更多信息，請參見Forms。</p><hr><p><strong>備註：轉載請註明出處。</strong></p><p><strong>如發現錯誤，歡迎留言指正。</strong></p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>Rasa</a></li><li><a>入門</a></li><li><a>Core</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/6fe27eab.html alt="前端 | HTML入門基礎知識-網頁" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/e7a0b61194f445b8b9e5ae330961d2ea style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6fe27eab.html title="前端 | HTML入門基礎知識-網頁">前端 | HTML入門基礎知識-網頁</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/19664ba8.html alt=EXCEL入門基礎：對行和列選定數據求和 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1f03f82096d240dd92410709f3e19eb9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/19664ba8.html title=EXCEL入門基礎：對行和列選定數據求和>EXCEL入門基礎：對行和列選定數據求和</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5abb53a8.html alt=工程造價從入門到精通：造價員全能圖解+工程算量表，限時分享！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/49834ccb43ed42cb9a54c9827c3ab134 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5abb53a8.html title=工程造價從入門到精通：造價員全能圖解+工程算量表，限時分享！>工程造價從入門到精通：造價員全能圖解+工程算量表，限時分享！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1a5f8d5f.html alt=工程造價：入門知識全套講義，30章600頁，精通造價首選之作 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9e1b335c343a455f8777dd3144fc1c35 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1a5f8d5f.html title=工程造價：入門知識全套講義，30章600頁，精通造價首選之作>工程造價：入門知識全套講義，30章600頁，精通造價首選之作</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2d4007c7.html alt=“黑客”入門學習之“Windows組策略” class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ea21244d5f5c420ebef29650f3fafd1c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2d4007c7.html title=“黑客”入門學習之“Windows組策略”>“黑客”入門學習之“Windows組策略”</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9d08c7e6.html alt=PHP入門教程，5天86節課助力小白變大神！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/4366000004d4c98fd587 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9d08c7e6.html title=PHP入門教程，5天86節課助力小白變大神！>PHP入門教程，5天86節課助力小白變大神！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/923fd40e.html alt=Thinkphp6快速入門一 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/c4331ddc0ffb4c94a4aa80be95178354 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/923fd40e.html title=Thinkphp6快速入門一>Thinkphp6快速入門一</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3d2ce3d6.html alt="php新手入門教程， 最全最完整的教學視頻課程" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/666a38216ab04790a716bb1451c7fe44 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3d2ce3d6.html title="php新手入門教程， 最全最完整的教學視頻課程">php新手入門教程， 最全最完整的教學視頻課程</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/07454552.html alt=「素描入門」基礎不紮實，從排線練起 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/15252166717297d7af296ee style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/07454552.html title=「素描入門」基礎不紮實，從排線練起>「素描入門」基礎不紮實，從排線練起</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/07c706e2.html alt=「素描入門」素描排線的繪畫技法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1530098801296ab58189790 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/07c706e2.html title=「素描入門」素描排線的繪畫技法>「素描入門」素描排線的繪畫技法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a993b442.html alt=素描入門丨你說線條或者排線，容易嗎？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1536648317995f32cedaa40 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a993b442.html title=素描入門丨你說線條或者排線，容易嗎？>素描入門丨你說線條或者排線，容易嗎？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d0e2c5d2.html alt=零基礎入門要知道的素描知識總結 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/c2c16563-8821-4294-a8c6-9d76e62a2440 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d0e2c5d2.html title=零基礎入門要知道的素描知識總結>零基礎入門要知道的素描知識總結</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6f585f1e.html alt=JAVA入門到大神（玩轉正則表達式） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6f585f1e.html title=JAVA入門到大神（玩轉正則表達式）>JAVA入門到大神（玩轉正則表達式）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/806b0846.html alt=Java入門教程-日期和正則表達式入門 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/15352845316194ba375faa7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/806b0846.html title=Java入門教程-日期和正則表達式入門>Java入門教程-日期和正則表達式入門</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/52a05f19.html alt="Java入門 - 語言基礎 - 18.正則表達式" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ddf3412c71244bcaa3cb911e988fed3b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/52a05f19.html title="Java入門 - 語言基礎 - 18.正則表達式">Java入門 - 語言基礎 - 18.正則表達式</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>