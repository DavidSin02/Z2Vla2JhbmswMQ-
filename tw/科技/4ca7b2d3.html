<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習Bagging與隨機森林算法原理 | 极客快訊</title><meta property="og:title" content="機器學習Bagging與隨機森林算法原理 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/68b70854112d42a5a8a4cddbb5c871a1"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4ca7b2d3.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4ca7b2d3.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/4ca7b2d3.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4ca7b2d3.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4ca7b2d3.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/4ca7b2d3.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/4ca7b2d3.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4ca7b2d3.html><meta property="article:published_time" content="2020-10-29T21:12:41+08:00"><meta property="article:modified_time" content="2020-10-29T21:12:41+08:00"><meta name=Keywords content><meta name=description content="機器學習Bagging與隨機森林算法原理"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/4ca7b2d3.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習Bagging與隨機森林算法原理</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p>在集成學習原理小結中，我們講到了集成學習有兩個流派，一個是boosting派系，它的特點是各個弱學習器之間有依賴關係。另一種是bagging流派，它的特點是各個弱學習器之間沒有依賴關係，可以並行擬合。本文就對集成學習中Bagging與隨機森林算法做一個總結。</p><p>隨機森林是集成學習中可以和梯度提升樹GBDT分庭抗禮的算法，尤其是它可以很方便的並行訓練，在如今大數據大樣本的的時代很有誘惑力。</p><p>bagging的原理</p><p>在集成學習原理小結中，我們給Bagging畫了下面一張原理圖。</p><div class=pgc-img><img alt=機器學習Bagging與隨機森林算法原理 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/68b70854112d42a5a8a4cddbb5c871a1><p class=pgc-img-caption></p></div><p>從上圖可以看出，Bagging的弱學習器之間的確沒有boosting那樣的聯繫。它的特點在"隨機採樣"。那麼什麼是隨機採樣。</p><p>隨機採樣(bootsrap)就是從我們的訓練集裡面採集固定個數的樣本，但是每採集一個樣本後，都將樣本放回。也就是說，之前採集到的樣本在放回後有可能繼續被採集到。對於我們的Bagging算法，一般會隨機採集和訓練集樣本數m一樣個數的樣本。這樣得到的採樣集和訓練集樣本的個數相同，但是樣本內容不同。如果我們對有m個樣本訓練集做T次的隨機採樣，則由於隨機性，T個採樣集各不相同。</p><p>注意到這和GBDT的子採樣是不同的。GBDT的子採樣是無放回採樣，而Bagging的子採樣是放回採樣。</p><p>對於一個樣本，它在某一次含m個樣本的訓練集的隨機採樣中，每次被採集到的概率是1/m。不被採集到的概率為1-1/m。如果m次採樣都沒有被採集中的概率是(1-1/m)m，當m→∞時，(1-1/m)m→1/e≃0.368。也就是說，在bagging的每輪隨機採樣中，訓練集中大約有36.8%的數據沒有被採樣集採集中。</p><p>對於這部分大約36.8%的沒有被採樣到的數據，我們常常稱之為袋外數據(Out Of Bag, 簡稱OOB)。這些數據沒有參與訓練集模型的擬合，因此可以用來檢測模型的泛化能力。</p><p>bagging對於弱學習器沒有限制，這和Adaboost一樣。但是最常用的一般也是決策樹和神經網絡。</p><p>bagging的集合策略也比較簡單，對於分類問題，通常使用簡單投票法，得到最多票數的類別或者類別之一為最終的模型輸出。對於迴歸問題，通常使用簡單平均法，對T個弱學習器得到的迴歸結果進行算術平均得到最終的模型輸出。</p><p>由於Bagging算法每次都進行採樣來訓練模型，因此泛化能力很強，對於降低模型的方差很有作用。當然對於訓練集的擬合程度就會差一些，也就是模型的偏倚會大一些。</p><p><strong>bagging算法流程</strong></p><p>上一節我們對bagging算法的原理做了總結，這裡就對bagging算法的流程做一個總結。相對於Boosting系列的Adaboost和GBDT，bagging算法要簡單的多。</p><p>輸入為樣本集D={(x1,y1),(x2,y2),...(xm,ym)}，弱學習器算法, 弱分類器迭代次數T。</p><p>輸出為最終的強分類器f(x)</p><p>1.對於t=1,2...,T：</p><p>a)對訓練集進行第t次隨機採樣，共採集m次，得到包含m個樣本的採樣集Dt</p><p>b)用採樣集Dt訓練第t個弱學習器Gt(x)</p><p>2. 如果是分類算法預測，則T個弱學習器投出最多票數的類別或者類別之一為最終類別。如果是迴歸算法，T個弱學習器得到的迴歸結果進行算術平均得到的值為最終的模型輸出。</p><p><strong>隨機森林算法</strong></p><p>理解了bagging算法，隨機森林(Random Forest,以下簡稱RF)就好理解了。它是Bagging算法的進化版，也就是說，它的思想仍然是bagging,但是進行了獨有的改進。我們現在就來看看RF算法改進了什麼。</p><p>首先，RF使用了CART決策樹作為弱學習器，這讓我們想到了梯度提示樹GBDT。第二，在使用決策樹的基礎上，RF對決策樹的建立做了改進，對於普通的決策樹，我們會在節點上所有的n個樣本特徵中選擇一個最優的特徵來做決策樹的左右子樹劃分，但是RF通過隨機選擇節點上的一部分樣本特徵，這個數字小於n，假設為nsub，然後在這些隨機選擇的nsub個樣本特徵中，選擇一個最優的特徵來做決策樹的左右子樹劃分。這樣進一步增強了模型的泛化能力。</p><p>如果nsub=n，則此時RF的CART決策樹和普通的CART決策樹沒有區別。nsub越小，則模型約健壯，當然此時對於訓練集的擬合程度會變差。也就是說nsub越小，模型的方差會減小，但是偏倚會增大。在實際案例中，一般會通過交叉驗證調參獲取一個合適的nsub的值。</p><p>除了上面兩點，RF和普通的bagging算法沒有什麼不同，下面簡單總結下RF的算法。</p><p>輸入為樣本集D={(x1,y1),(x2,y2),...(xm,ym)}, 弱分類器迭代次數T。</p><p>輸出為最終的強分類器f(x)</p><p>1.對於t=1,2...,T：</p><p>a)對訓練集進行第t次隨機採樣，共採集m次，得到包含m個樣本的採樣集Dt</p><p>b)用採樣集Dt訓練第t個決策樹模型Gt(x)，在訓練決策樹模型的節點的時候， 在節點上所有的樣本特徵中選擇一部分樣本特徵， 在這些隨機選擇的部分樣本特徵中選擇一個最優的特徵來做決策樹的左右子樹劃分</p><p>2. 如果是分類算法預測，則T個弱學習器投出最多票數的類別或者類別之一為最終類別。如果是迴歸算法，T個弱學習器得到的迴歸結果進行算術平均得到的值為最終的模型輸出。</p><p>隨機森林的推廣</p><p>由於RF在實際應用中的良好特性，基於RF，有很多變種算法，應用也很廣泛，不光可以用於分類迴歸，還可以用於特徵轉換，異常點檢測等。下面對於這些RF家族的算法中有代表性的做一個總結。</p><p>Extra Trees</p><p>Extra trees是RF的一個變種, 原理幾乎和RF一模一樣，僅有區別有：</p><p>1. 對於每個決策樹的訓練集，RF採用的是隨機採樣bootstrap來選擇採樣集作為每個決策樹的訓練集，而extra trees一般不採用隨機採樣，即每個決策樹採用原始訓練集。</p><p>2.在選定了劃分特徵後，RF的決策樹會基於信息增益，基尼係數，均方差之類的原則，選擇一個最優的特徵值劃分點，這和傳統的決策樹相同。但是extra trees比較的激進，他會隨機的選擇一個特徵值來劃分決策樹。</p><p>從第二點可以看出，由於隨機選擇了特徵值的劃分點位，而不是最優點位，這樣會導致生成的決策樹的規模一般會大於RF所生成的決策樹。也就是說，模型的方差相對於RF進一步減少，但是偏倚相對於RF進一步增大。在某些時候，extra trees的泛化能力比RF更好。</p><p>Totally Random Trees Embedding</p><p>Totally Random Trees Embedding(以下簡稱 TRTE)是一種非監督學習的數據轉化方法。它將低維的數據集映射到高維，從而讓映射到高維的數據更好的運用於分類迴歸模型。我們知道，在支持向量機中運用了核方法來將低維的數據集映射到高維，此處TRTE提供了另外一種方法。</p><p>TRTE在數據轉化的過程也使用了類似於RF的方法，建立T個決策樹來擬合數據。當決策樹建立完畢以後，數據集裡的每個數據在T個決策樹中葉子節點的位置也定下來了。比如我們有3顆決策樹，每個決策樹有5個葉子節點，某個數據特徵x劃分到第一個決策樹的第2個葉子節點，第二個決策樹的第3個葉子節點，第三個決策樹的第5個葉子節點。則x映射後的特徵編碼為(0,1,0,0,0, 0,0,1,0,0, 0,0,0,0,1), 有15維的高維特徵。這裡特徵維度之間加上空格是為了強調三顆決策樹各自的子編碼。</p><p>映射到高維特徵後，可以繼續使用監督學習的各種分類迴歸算法了。</p><p>Isolation Forest</p><p>Isolation Forest（以下簡稱IForest）是一種異常點檢測的方法。它也使用了類似於RF的方法來檢測異常點。</p><p>對於在T個決策樹的樣本集，IForest也會對訓練集進行隨機採樣,但是採樣個數不需要和RF一樣，對於RF，需要採樣到採樣集樣本個數等於訓練集個數。但是IForest不需要採樣這麼多，一般來說，採樣個數要遠遠小於訓練集個數？為什麼呢？因為我們的目的是異常點檢測，只需要部分的樣本我們一般就可以將異常點區別出來了。</p><p>對於每一個決策樹的建立， IForest採用隨機選擇一個劃分特徵，對劃分特徵隨機選擇一個劃分閾值。這點也和RF不同。</p><p>另外，IForest一般會選擇一個比較小的最大決策樹深度max_depth,原因同樣本採集，用少量的異常點檢測一般不需要這麼大規模的決策樹。</p><p>對於異常點的判斷，則是將測試樣本點x擬合到T顆決策樹。計算在每顆決策樹上該樣本的葉子節點的深度ht(x)。從而可以計算出平均高度h(x)。此時我們用下面的公式計算樣本點x的異常概率:</p><div class=pgc-img><img alt=機器學習Bagging與隨機森林算法原理 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/661774381d41402492c9e81fa0b634ac><p class=pgc-img-caption></p></div><p>其中，m為樣本個數。c(m)的表達式為：</p><div class=pgc-img><img alt=機器學習Bagging與隨機森林算法原理 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/216bf3eaf3e84b7dbb78a5fe25cc2330><p class=pgc-img-caption></p></div><p>ξ為歐拉常數，s(x,m)的取值範圍是[0,1]，取值越接近於1，則是異常點的概率也越大。</p><p>隨機森林小結</p><p>RF的算法原理也終於講完了，作為一個可以高度並行化的算法，RF在大數據時候大有可為。 這裡也對常規的隨機森林算法的優缺點做一個總結。</p><p>RF的主要優點有：</p><p>1. 訓練可以高度並行化，對於大數據時代的大樣本訓練速度有優勢。個人覺得這是的最主要的優點。</p><p>2. 由於可以隨機選擇決策樹節點劃分特徵，這樣在樣本特徵維度很高的時候，仍然能高效的訓練模型。</p><p>3. 在訓練後，可以給出各個特徵對於輸出的重要性。</p><p>4. 由於採用了隨機採樣，訓練出的模型的方差小，泛化能力強。</p><p>5. 相對於Boosting系列的Adaboost和GBDT， RF實現比較簡單。</p><p>6. 對部分特徵缺失不敏感。</p><p>RF的主要缺點有：</p><p>1.在某些噪音比較大的樣本集上，RF模型容易陷入過擬合。</p><p>2. 取值劃分比較多的特徵容易對RF的決策產生更大的影響，從而影響擬合的模型的效果。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>Bagging</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/113c5015.html alt=機器學習：聊聊機器學習模型集成學習算法——Bagging class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/40160001d1fba60611f8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/113c5015.html title=機器學習：聊聊機器學習模型集成學習算法——Bagging>機器學習：聊聊機器學習模型集成學習算法——Bagging</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>