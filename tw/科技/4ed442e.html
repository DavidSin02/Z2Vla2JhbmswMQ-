<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>萬字長文帶你入門 GCN | 极客快訊</title><meta property="og:title" content="萬字長文帶你入門 GCN - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RqSgmP9HojtMNK"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4ed442e.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4ed442e.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/4ed442e.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4ed442e.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4ed442e.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/4ed442e.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/4ed442e.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/4ed442e.html><meta property="article:published_time" content="2020-10-29T21:01:04+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:04+08:00"><meta name=Keywords content><meta name=description content="萬字長文帶你入門 GCN"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/4ed442e.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>萬字長文帶你入門 GCN</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><h1 toutiao-origin=h1><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RqSgmP9HojtMNK></h1><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Rbc1Htm93uvAQn><p class=pgc-img-caption>來源 | 阿澤的學習筆記（ID: aze_learning）</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RqMqLxPCXik3SR><p></p><h1 toutiao-origin=h1>Convolutional Neural Network</h1><p>CNN 在圖像識別等任務中具有重要作用，主要是因為 CNN 利用了圖片在其域中的平移不變性。由於圖結構不存在平移不變性，所以 CNN 無法直接在圖上進行卷積。</p><p></p><h2 toutiao-origin=h2><strong toutiao-origin=span>1.1 Translational Invariance</strong></h2><p>剛剛提到 CNN 之所以可以應用到圖像而無法應用到圖網絡中主要是因為圖像具有<strong>「平移不變形（translational invariance）」</strong>，而圖網絡不具備這一屬性。那麼問題來了，什麼是平移不變形呢？</p><p>我們知道對於 CNN 來說，其核心在於使用了基於卷積核的卷積操作來提取圖像的特徵，這裡的卷積操作類似於對<strong>「計算區域內的中心節點和相鄰節點進行加權求和」</strong>：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGEucCTA7G2R><p class=pgc-img-caption>CNN 之所以能成為圖像領域的明珠卻很少應用於其他領域原因是：<strong>「圖片是一個規整的二維矩陣」</strong>，無論卷積核平移到圖片中的哪個位置都可以保證其運算結果的一致性，這就是我們所說的<strong>「平移不變性」</strong>。CNN 的卷積本質就是利用這種平移不變性來對掃描的區域進行卷積操作，從而實現了圖像特徵的提取。</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGEvK6eprEpv><p>而網絡是不規整的關係型數據，所以其不存在平移不變形（每個節點的周圍鄰居數不固定），這就使得傳統的 CNN 方法無法直接應用於網絡中。</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGFJYCZECJLJ><p></p><h2 toutiao-origin=h2><strong toutiao-origin=span>1.2 Convolution Kernels</strong></h2><p>既然是因為卷積核的原因，那麼可不可以不使用卷積核？</p><p>答案肯定是不可以，因為卷積神經網絡的一大核心就是利用卷積核實現<strong>「參數共享（Parameter Sharing）」</strong>。下圖為有卷積核的卷積操作：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGFKIAtFFdd2><p>此時的參數大小隻與卷積核大小有關，而如果不進行參數共享的話，參數的大小則與圖像的像素矩陣保持一致：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGFKk2g8QIDI><p>除此之外，卷積神經網絡還有另一大核心：<strong>「局部連接性（Locally Connection）」</strong>。局部連接是指卷積計算每次只在與卷積核大小對應的區域進行，也就是說輸入和輸出是局部連接的。如果不進行局部連接的話，相當於將圖片的矩陣展開成向量進行輸入，類似於全連接神經網絡，此時的參數量會變得非常巨大：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGFL14atVRwp><p>也就是說，通過參數共享和局部連接性我們可以將參數從 降低到 。其中，W H 和 K 分別為圖像的寬、高和通道，N 為隱藏層節點個數，m 為卷積核寬，k 為卷積核個數。</p><p>PS：CNN 有三大特點，除了上面說的局部連接和參數共享之外，還有<strong>「層次化表達（Hierarchical Expression）」</strong>。CNN 的層次化表達可以通過卷積層疊加得到，每一個卷積層都是在前一層的基礎上進行的，這樣的意義在於，網絡越往後，其提取到的特徵越高級。比如說：第一層可能是一些線條，第二層可能會是一些紋理，第三層可能是一些抽象圖案：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGFLG6CcL9aO><p>可能會有同學問：那我們還有其他辦法在圖上進行卷積嗎？答案是一定有的 = =。</p><p>目前的一大思路就是藉助譜圖理論（Spectral Graph Theory）來實現在拓撲圖上的卷積操作，大致步驟為將空域中的拓撲圖結構通過傅立葉變換映射到頻域中並進行卷積，然後利用逆變換返回空域，從而完成了圖卷積操作。</p><p>看到這裡，估計大家會有一堆疑問，包括：什麼是譜圖理論？什麼是傅立葉變換？什麼是頻域空域？逆變換是什麼？</p><p>想要清楚的回答這個問題，要從圖信號處理說起。</p><p></p><h1 toutiao-origin=h1>Graph Signal Processing</h1><p>圖信號處理（Graph Signal Processing，以下簡稱 GSP）用來處理那些定義在圖上的非規則域的信號，這句話有點拗口，拆開說就是處理圖上定義的信號，但信號所在域是規則的。</p><p></p><h2 toutiao-origin=h2><strong toutiao-origin=span>2.1 Simple Example</strong></h2><p>這裡我們舉一個圖信號處理的簡單例子：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGGOY2dEKymj><p>假設我們在一個地方測量溫度，並根據人口密度安排了溫度感應點（如上圖 a 所示），地區 n 的測量溫度可以表示為 （如上圖 b 所示），並且 ， 為真實溫度， 為隨機噪聲帶來的誤差。</p><p>現在我們想通過對測量地及周圍的溫度求平均來減少這些隨機噪聲，當然為了防止失去局部溫度（這個也叫 Over Smooth），我們會對每個點取其周圍區域進行平均：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGGZEGSQyKMo><p>上圖 c 展示了 y(1) 的計算方式。我們也可以用矩陣來表示：</p><p>其中，矩陣 A 為鄰接矩陣（測量點的連接情況如上圖 d 所示），測量位置及每個位置的測量溫度如上圖 e 所示。</p><p>我們還可以對其進行優化，根據距離來為不同測量點添加不同的權重：</p><p>當然，我們也需要對權重進行歸一化，以便產生無偏估計：</p><p>其中，對角矩陣 D 用於歸一化，其值為 ，這個矩陣還有另外一個名字，叫<strong>「度矩陣（Degree Matrix）」</strong>。</p><p>以上便是一個簡單的是圖信號處理過程，其框架大致為：</p><ol><li><p>測量點構成節點（圖 a），節點間的連通性和相關性構成邊；</p></li><li><p>節點和邊構成圖（圖 b），該圖是信號域，表示測量信號的點以及它們之間的關係，並使用該圖進行分析和處理；</p></li><li><p>測量溫度是圖的信號（圖 e），這裡的信號由真實溫度和測量噪聲所組成；</p></li><li><p>考慮測量位置，我們提出了局部平均和加權平均，這是最簡單的圖信號處理方式（Linear fist-order）。</p></li></ol><p>同樣的，我們也可以將其應用在多個領域，如民意調查、政治分析等。</p><p></p><h2 toutiao-origin=h2><strong toutiao-origin=span>2.2 Fourier Transformer</strong></h2><p>我相信如果我一上來就扔出傅立葉變換，很多人都會崩潰不想看，不信我們試試：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGGZaxM82mL><p>如果沒有崩潰的話就直接看下一節吧；如果崩潰了就接著看，但是筆掉了千萬別撿，否則可能就看不懂了。</p><p><strong toutiao-origin=span>2.2.1 Transformer</strong></p><p>為了讓大家無痛入門，我們先從最簡單變換的說起。</p><p>我們知道笛卡爾座標系中，每個點都會有一個座標，如下圖所示 A(-3,1) B(2,3)：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGGZt8DP25VR><p>那麼為什麼可以這麼表示呢？為什麼 A 的座標為 (-3,1) 而 B 的座標為 (2,3) ？</p><p>這是因為在笛卡爾座標系中，我們定義了一組標準正交基 ，基是向量有方向有大小。（正交是指不同基之間的內積為 0，即兩個基線性無關，而標準基是指基的模為 1）</p><p>A 的座標其實就表示在 x 軸座標上有 3 個 的長度且方向與 相反，在 y 軸座標上有 1 個 的長度，且方向相同。</p><p>這樣做的好處是什麼呢？主要是為了方便計算和表示，試想下，如果只給你一點點而沒有座標系，你怎麼表示兩個點之間的距離呢？而放在座標系中，這些問題就迎刃而解。</p><p>有同學可能疑問，不是說變換嗎？怎麼扯到笛卡爾座標系了？其實我們剛剛說的就是一種變換：<strong>「將圖上的節點變換到座標系中」</strong>。</p><p></p><h2 toutiao-origin=h3><strong toutiao-origin=span>2.2.2 Fourier Series</strong></h2><p>傅立葉變換分為傅立葉級數和連續傅立葉變換，我們先說傅立葉級數。</p><p>傅立葉級數適用於週期性函數，它能夠將任何週期性函數分解成簡單震盪函數的集合（正弦函數和餘弦函數），舉個例子，比如說下圖：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S2YGHGW1DOkMkz><p>左邊是一個周期函數，右邊是將周期函數分解成多個簡單震盪函數，所以這個周期函數用數學公式可以表達為：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGHRmAm00qrF><p>我們看到上圖中的信號是隨著時間變換的，所以我稱之為<strong>「時域（Time domain）」</strong>。</p><p>我們觀察到，不同的振盪函數具有不同的振幅和頻率，以上圖為例 的振幅為 1/3 而頻率為 ，考慮以頻率為橫座標，振幅為縱座標，我們有：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGHSWEy0Cfv4><p>這個就是我們所說的頻域（Frequency Domain），其和時域是等價的，不過是從另外一個角度去描述信號。我們把它放在一起看一下：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S2YGHT8FSiK1pw><p>我們可以放一張動圖感受一下：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGHccAPhYErT><p>給出傅立葉級數的公式：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGIHQJCmJtji><p>還可以將其稍作變換：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S2YGIHpIEa9bZn><p>這樣我們便能看出來，此時的標準正交基為</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGIIL6GfdhkW><p>，而對應的係數 其實就是傅立葉級數在這組標準正交基下的向量。這便是傅立葉變換，將信號從時域變換到頻域中。</p><p>這裡介紹下傅立葉變換後的基為正交基，因為有個知識點後面還會用到。</p><p>我們知道判斷兩個向量是否正交可以用向量點乘求和等於 0 來判斷，這種方法我們稱為點積（內積）：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGIJCC1wDuNP><p>與向量點積不同的是，函數是連續的，假設現在有兩個函數 f 和 g，f 的週期為 2n，我們也想用上述連續累加的方式來使得函數內積和向量內積的概念一致，而積分正是函數累加的概念，所以我們有：</p><p>對於上面我們說的傅立葉變換後的正交基，我們容易得到：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGIJS7EdJHPn><p>容易證明上述標準基為正交基。</p><p>在數學裡，希爾伯特空間（Hilbert Space）是有限維歐幾里得空間的一個推廣，是一個完備的內積空間，其定義了一個帶有內積的完備向量空間。在希爾伯空間中，一個抽象元素通常被稱為向量，它可以是一個複數或者函數。傅立葉分析的一個重要目的是將一個給定的函數表示成一族給定的基底函數的和，而希爾伯特空間為傅立葉分析提供了一種有效的表述方式。</p><p>可能大家看到這裡要爆炸了，不過不用擔心，我們只需要記住上面<strong>「兩個函數的內積形式」</strong>即可。</p><p></p><h2 toutiao-origin=h3><strong toutiao-origin=span>2.2.3 Fourier Transformer</strong></h2><p>我們剛剛說的都是週期性函數，但現實中大部分函數都是非週期的，那如果涉及到非週期性函數該怎麼辦呢？</p><p>在介紹非週期性函數之前，我們先簡單介紹下歐拉公式。</p><p>考慮橫軸為 1，縱軸為虛單位 i 的座標系，圖上任意一點都可以表示為 。</p><p>根據歐拉公式，我們可以寫成：</p><p>其中，e 為自然對數的底數。</p><p>所以座標軸上的點現在有了兩種表示方式：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S2YGIsj1JRyt38><p>考慮 ， 會隨著 t 的增大而逆時針旋轉。所以 可以表示為座標點 A 隨著時間 t 逆時針旋轉。我們以時間 t 為橫座標，則可以記錄到座標點 A 映射在虛軸的運動軌跡：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGItK1Jl5DUI><p>左邊圖是我們看到的旋轉頻率，稱為頻域，而右邊圖看到是時間流逝，稱為時域，是不是和我們剛剛介紹的（從時域變換到頻域）正好相反？也就是說，時域和頻域其實是可以相互轉換的。</p><p>回到正題，考慮非周期函數的傅立葉變換。</p><p>事實上，我們可以將非周期函數考慮為週期無窮大的函數，考慮頻域中的橫座標：，當週期 T 無窮大大時，頻域圖就從離散點變為連續的曲線，如下圖：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGItcAsWpdwd><p>那麼，我們該如何從這個非周期函數中分解出各種信號呢？答案就是利用正交！比如說，假設這函數中有一個 的信號，那麼我們用 就可以把它乘出來，而其他分量如</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGIu5I25sXW9><p>都會因為正交而消失。所以我們需要對函數做一個內積：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGIuWDIBqS35><p>其中， 剛剛介紹過，就是一組正交基的組合。我們用正交基去與函數求內積，如果原函數中包含頻率為 的三角函數，則 便為 0，反之為 0，這樣自然分離能分離出相應的信號，其圖示如上圖 c 中右部分所示。</p><p>細心的同學可能還會注意到上式的計算的結果中還有複數 i。其實是樣子的：<strong>「實數部分表示振幅」</strong>，<strong>「虛數部分表示相位」</strong>。相關資料同學們可以自己查閱，不再進行過多介紹。</p><p>以上就是我們所說的傅立葉變換（Fourier Transform，FT）。同樣的我們也存在逆變換：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGK9jCLQ8g7z><p>於是，我們便實現了將信號拆成多個正弦信號，再把正弦信號逆變換為原來信號的過程。</p><p>簡單介紹下傅立葉變換的應用吧， 省得看了那麼多不知道他能幹什麼。</p><p>一個很經典的例子就是：分離、降噪。如果男生和女生一起說話，該如何分離出兩者的聲音呢？答案就是對這一段聲音（時域）做傅立葉變換轉換到頻率，而男女生的聲音頻率不同，在頻域中，低頻為男生，中頻為女生，高頻可能為噪音，我們可以根據需要去除中頻和高頻的信號，並將其進行逆變換，這樣便分離出了男生的聲音。</p><p>PS：這裡再說一個好玩的，頻域中是不是不存在時間的概念？不存在時間卻可以表示時間，這有沒有一點像我們的人生，看似無規律，但是從上帝視角來看，一切皆命中註定。</p><p></p><h2 toutiao-origin=h2><strong toutiao-origin=span>2.3 Graph Laplacian</strong></h2><p>圖拉普拉斯矩陣可以定義為：</p><p>其中，D 為度矩陣，W 為考慮權值的鄰接矩陣。</p><p>考慮歸一化後的拉普拉斯矩陣：</p><p>以上為常規操作，不過介紹到這裡不知道大家會不會有一點疑問。</p><p>至少我是有疑問的：圖拉普拉斯矩陣為什麼要這樣定義的？</p><p>要想回答這個問題，首先我們得了解什麼是拉普拉斯算子。</p><p><strong toutiao-origin=span>2.3.1 Laplacian</strong></p><p>在數學中，拉普拉斯算子（Laplacian）是由歐幾里得空間中的一個函數的梯度的散度給出的微分算子，通常有以下幾種寫法：。所以對於任意函數 來說，其拉普拉斯算子的定義為：</p><p>這裡引入了一個新的概念——散度，這裡簡單介紹下：</p><p>散度（Divergence）是向量分析的一個向量算子，將向量空間上的向量場（矢量場）對應到一個標量場。散度描述的是向量場裡一個點是匯聚點還是發源點。值為正時表示該點為發源點，值為負時表示該點為匯聚點，值為零時表示該點無源。散度在物理上的含義可以理解為磁場、熱源等。</p><p>回到正文，我們看下拉普拉斯算子在 n 維空間中的笛卡爾座標系的數學定義：</p><p>數學表示為各個維度的二階偏導數之和。</p><p>以一維空間為例：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGKAAAgzNiYe><p>也就是說二階導數近似於其二階差分，可以理解為當前點對其在所有自由度上微擾之後獲得的增益。這裡自由度為 2，分別是 +1 和 -1 方向。</p><p>再以二維空間為例子：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGKB3Gr1d9pQ><p>看到上面可能大家會很可能很陌生，但是這個就是圖像中的拉普拉斯卷積核：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGKBn8Y0S5yX><p>此時共有 4 個自由度 (1,0),(-1,0),(0,1),(0,-1)，當然如果對角線後其自由度可以為 8。</p><p>對此我們可以進行歸納：<strong>「拉普拉斯算子是所有自由度上進行微小變化後所獲得的增益」</strong>。</p><p>我們將其推廣到網絡圖中，考慮有 N 個節點的網絡圖，其自由度最大為 N，那麼函數 可以是 N 維的向量，即：</p><p>其中， 表示函數 在網絡圖中節點 i 處的函數值，類比 為函數 在 (x,y) 的函數值。</p><p>在網絡圖中，兩個節點的之間的增益為 ，考慮加權圖則有 ，那麼對於節點 i 來說，總增益即為拉普拉斯算子在節點 i 的值：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S2YGKC45YcQ18K><p>其中， 為節點 i 的度；上式第二行去掉了 是因為 可以控制節點 i 的鄰接矩陣。</p><p>對於任意 都成立，所以我們有：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGKpoFVxGiNe><p>自此，我們便給出了圖拉普拉斯矩陣的推導過程，這個公式的全稱為：圖拉普拉斯算子作用在由圖節點信息構成的向量 上得到的結果等於圖拉普拉斯矩陣和向量 的點積。拉普拉斯矩陣反映了當前節點對周圍節點產生擾動時所產生的累積增益，直觀上也可以理解為某一節點的權值變為其相鄰節點權值的期望影響，形象一點就是拉普拉斯矩陣可以刻畫局部的平滑度。</p><p><strong toutiao-origin=span>2.3.2 Laplace Spectral decomposition</strong></p><p>拉普拉斯矩陣的譜分解就是矩陣的特徵分解：</p><p>對於無向圖來說，拉普拉斯矩陣是實對稱矩陣，而實對稱矩陣一定可以用正交矩陣進行正交相似對角化：</p><p>其中， 為特徵值構成<strong>「對角矩陣」</strong>， 為特徵向量構成的<strong>「正交矩陣」</strong>。</p><p>又因為正交矩陣的逆等於正交矩陣的轉置： ，所以我們有：</p><p>因為 L 是半正定矩陣，我們還可以有：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S2YGKqUCccKy8e><p>其中， 為節點 i 的信號。我們稱 為圖信號的總變差（Total Variation），可以刻畫圖信號整體的平滑度。</p><p>拉普拉斯的譜分解具有以下幾點性質：</p><ul><li><p>由於拉普拉斯矩陣以每行（列）元素之和為零，因此拉普拉斯矩陣的至少有一個特徵值為 0，對應的特徵向量</p></li><li><p>拉普拉斯矩陣的特徵值都大於零，歸一化的拉普拉斯矩陣的特徵值區間為 [0, 2]；</p></li><li><p>如果有 n 個特徵值為 0，則表示圖有 n 個子圖相互無連接；</p></li><li><p>特徵值的總和為矩陣的跡，對於歸一化的拉普拉斯矩陣，如果沒有孤立節點或子圖，其特徵值為 N。</p></li></ul><p></p><h2 toutiao-origin=h2><strong toutiao-origin=span>2.4 Graph Fourier Transformer</strong></h2><p>有同學看到這可能會感到疑問了：<strong>「我們剛介紹傅立葉變換，現在又介紹拉普拉斯譜分解的，到底想幹嘛」</strong>。</p><p>這是因為：<strong>「傅立葉分析是拉普拉斯譜分析的一個特例」</strong>！想不到吧，是不是很震驚？</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGKqt8bmXK6z><p>我們來證明下，首先考慮亥姆霍茲方程（Helmholtz Equation）：</p><p>其中， 為拉普拉斯算子， 為特徵函數， 為特徵值。</p><p>看不懂不要緊，把它當成廣義特徵方程就行：，狹隘的特徵方程只能用於處理向量和矩陣，而這個可以用於處理函數，最經典的應用是處理波動方程和擴散方程，所以我們可以用它處理信號。</p><p>回顧一下傅立葉變換：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGKrfGHT9Q8j><p>其實就是信號函數 與基函數 的內積（剛剛介紹過函數內積）。</p><p>對於基函數 ，我們讓其與拉普拉斯算子求內積：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGKsm8Db3Vsc><p>以上便證明 是<strong>「拉普拉斯算子的特徵函數」</strong>，同時也證明了<strong>「離散傅立葉變換是拉普拉斯譜分析的一個特例」</strong>。</p><p>寫到這我們有以下線索：首先拉普拉斯矩陣（離散拉普拉斯算子）可以應用在圖像上，理論上也可以應用到網絡上，而傅立葉變換是拉普拉斯的一個小弟，所以小弟也可以應用到圖上。</p><p>回顧下拉普拉斯譜分析：</p><p>我們類比一下：</p><table><thead><tr><td>信號中的傅立葉變換</td><td>網絡圖中的傅立葉變換</td></tr></thead><tbody><tr><td><div><p>頻率</p></div></td><td><div><p>特徵值</p></div></td></tr><tr><td><div><p>正交基中某個向量</p></div></td><td><div><p>正交矩陣中的某個向量</p></div></td></tr></tbody></table><p>是不是長得非常像，所以我們也有了網絡圖上的傅立葉變換：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S2YGLyTDYx1H8y><p>其中， 為網絡圖上的 n 維向量， 表示網絡中的節點 i 的第 k 個分量， 表示特徵向量 k 的第 i 個分量。做個類比解釋：特徵值（頻率） 下， 的圖傅立葉變換（振幅）等於 與 對應的特徵向量 的內積。</p><p>考慮矩陣乘法：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGLyt1nbowho><p>所以我們得到了<strong>「圖傅立葉變換的矩陣形式」</strong>，這裡的 為拉普拉斯譜分解的正交矩陣。</p><p>我們也可以得到傅立葉逆變換：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RqMqMYY7LcGio6><p></p><h1 toutiao-origin=h1>Graph Convolutional Network</h1><p>前面的鋪墊很多，終於要迎來 GCN 了。</p><p></p><h2 toutiao-origin=h2><strong toutiao-origin=span>3.1 Convolution</strong></h2><p>我們先來看一下卷積的定義，卷積是指通過兩個函數 和 生成第三個函數的一種數學算子，表徵函數 與經過翻轉和平移的 的乘積函數所圍成的曲邊梯形的面積：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGLzK3prFK0Q><p>對於離散卷積來說，我們可以定義為：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGLzx7ftgzai><p>計算卷積有很多種方法，除了直接計算外，我們還可以考慮<strong>「卷積定理」</strong>：在適當條件下，兩個信號的卷積的傅立葉變換是他們的傅立葉變換的點積。換句話說，一個域（如時域）的卷積等於另一個域（如頻域）的點乘：</p><p>其中 表示 的傅立葉變換。</p><p>藉助傅立葉逆變換 可以寫成：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGMUM3DbY7ZV><p>這樣做有什麼好處呢？或者說，我們為什麼要變換一個域後再去做卷積操作？</p><p>因為利用卷積定理可以簡化卷積的運算量。對於一個長度為 n 的序列，按照卷積的定義來計算則需要做 2n-1 組對位乘法，即時間複雜度為 ；而利用傅立葉變換後，只需要計算一組對位乘法，而且離散傅立葉變換有快速的算法（快速傅立葉變換），所以總的計算複雜度為 。</p><p></p><h2 toutiao-origin=h2><strong toutiao-origin=span>3.2 Graph Convolution</strong></h2><p>現在有了圖傅立葉變換，又有了離散卷積操作，那麼我們想：既然無法直接在空域進行卷積，可否將圖信號映射到頻域後再做卷積操作。</p><p>所以我們有：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGMUjBeQDoEg><p>其中，向量 與向量 的元素點積，等價於將 組織成對角矩陣的形式進行矩陣乘法，所以我們有：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGMVb2YICbu9><p>最後我們再左乘 進行逆變換：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGMWb7B5xZ85><p>這裡，我們不寫成 的主要原因在於，我們可以將其與深度學習相結合，在 GCN 中我們的卷積核是可訓練並且參數共享的，所以在此我們可以直接將 寫成 ，這個便是深度學習中的可學習參數。</p><p></p><h2 toutiao-origin=h2><strong toutiao-origin=span>3.3 GCN-1</strong></h2><p>第一代的卷積神經網絡也就是剛剛我們給出的公式：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGMWxH8AbPwC><p>這和論文中給出的公式是一樣的：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGNedIEklyy3><p>這邊補充一點，在這篇論文中，作者還給出了一個基於空域的<strong>「深度局部連接網絡」</strong>（Deep Locally Connected Networks），我們可以簡單看一下：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGNex144PK4E><p>每一層變換定義為：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S2YGNfNa7FMp><p>其中， 表示第 k 第 i 個節點； 表示第 k 層節點 i 和節點 j 的權值，考慮局部鄰居； 表示卷積運算； 表示第 k 層的池化操作。也就是說每個節點都是由其鄰居和自身卷積池化得到。</p><p>雖然看起來很簡單，但是優點在於它不需要很強的前提假設，其只需要網絡具有局部鄰域結構，甚至不需要很好的 Embedding 向量。</p><p>但這種結構下有一個很大的缺點：<strong>「沒有辦法實現共享參數」</strong>。</p><p>作者針對這種問題提出了我們所看到第一代圖卷積神經網絡。</p><p></p><h2 toutiao-origin=h2><strong toutiao-origin=span>3.4 GCN-2</strong></h2><p>第一代的圖卷積神經網絡很巧妙的利用圖譜理論來實現拓撲圖的卷積操作，但其有很多缺點，比如說：計算複雜度太高，我們需要對拉普拉斯矩陣進行譜分解得到特徵向量矩陣 ，時間複雜度為 ；</p><p>針對這個問題，學者提出了第二代 GCN。</p><p>首先我們回顧下圖傅立葉變換：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGNff5l2T8ql><p>可以看到這是一個和特徵值密切相關的函數，我們不妨將 寫成拉普拉斯矩陣 L 的特徵值函數 ：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGNgiF1KBH7N><p>然後這個卷積核有兩個侷限性：</p><ol><li><p>不具備局部連接性；</p></li><li><p>時間複雜度為 。</p></li></ol><p>為了克服這個缺點，我們引入 K 階多項式：</p><p>其中，參數 是多項式係數，這樣濾波器就具有了 K 階局部性了，複雜度也降低到 。</p><p>我們將這個公式帶入卷積運算中：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGOGOHX8ErO2><p>此時，我們計算圖卷積運算就不需要再乘上特徵向量矩陣 ，而是直接使用拉普拉斯矩陣 L 的 k 次方，這樣就避免了進行特徵分解。而我們可以事先計算好 ，這樣就只需要計算矩陣相乘。同時由於 L 為稀疏矩陣，所以時間複雜度為 ， 為節點邊數。</p><p>此外，作者還引入了切比雪夫展開式來近似 。</p><p>設 為切比雪夫多項式的第 k 階式子，切比雪夫多項式的遞歸式為：。所以我們有：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGOGnBbHZqgz><p>其中， ； 是指拉普拉斯矩陣 L 的最大值。</p><p>這是因為切比雪夫多項式的輸入要在 之間，由於拉普拉斯矩陣的半正定性，所以所有的特徵值都是大於等於 0 的，將其除以最大特徵值可以將特徵壓縮到 區間內，現在需要將其壓縮到 ，所以我們有：</p><p>我們將切比雪夫多項式引入到我們的卷積變換中：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGOHJ97E5kof><p>其中， 。這個表達式為拉普拉斯多項式中的一個 k 階近似函數，依賴於節點的 <strong>「k 階鄰域」</strong>（走 k 步能到的鄰居），時間複雜度與邊呈線形相關。</p><p></p><h2 toutiao-origin=h2><strong toutiao-origin=span>3.5 GCN-3</strong></h2><p>第二代 GCN 解決了圖卷機要求特徵分解的問題，但是在計算圖卷積操作時，依然每次都要進行矩陣乘法，時間複雜度為 ，於是學者繼續優化。</p><p>我們把上式拿下來：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGOHZ470idBz><p>GCN 通過上式的多層卷積層進行疊加，而每層都會逐點進行非線性疊加，考慮到時間複雜度問題，學者直接取 K=2，也就是說得到了一個拉普拉斯算子的二階近似函數。這樣我們既可以對網絡進行卷積操作，又不會引入太多的切比雪夫係數。而且這樣的線形運算允許我們構建更深的網路，提高模型的建模能力。</p><p>我們知道歸一化的拉普拉斯矩陣的特徵值區間為 [0, 2]，進一步近似 ，所以我們有新的表達式：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGOHw8BjwO1S><p>其中，</p><p>在實際訓練過程中，我們需要規範化參數來避免過擬合，所以我們令 ，從而有：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S2YGOsAg7OLnf><p>需要注意的是， 的特徵值範圍在 [0, 2] 之間，所以如果在很深的網絡中會引起梯度爆炸的問題，所以我們需要再次對他進行一次歸一化（原文也稱 <strong>「renormalization trick」</strong>）：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGOsf1ByEqg3><p>我們把這個公式從標量推廣到矩陣，對於輸入節點的向量 ，其中 N 為節點數，C 為節點的特徵向量維度，我們有：</p><p>其中， 是濾波器的參數矩陣， 是卷積後的信號矩陣，時間複雜度為 。節點的向量可以通過卷積操作從 C 維度 轉變為 F 維度。</p><p>依據上面的單層運算，我們給出了多層圖卷積網絡的傳播規則：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGOt4DyShw0j><p>其中， ，A 為鄰接矩陣， 為單位矩陣，所以 為添加自連接的鄰接矩陣； ， 可以理解為對角線為節點 i 的度數矩陣； 為神經網絡第 層的權重矩陣； 是激活函數； 是第 層的激活矩陣，並且 ， 是由節點 的特徵向量組成矩陣。</p><p>到此，便完成了 GCN 卷積操作的公式推導。</p><p></p><h2 toutiao-origin=h2><strong toutiao-origin=span>3.6 Model</strong></h2><p>再來關注一下模型。</p><p>圖卷積神經網絡是指在圖結構中做卷積操作的神經網絡，所以其輸入輸出的都是圖結構，區別於傳統的神經網絡結構，其隱藏層是直接在圖結構中進行激活：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S2YGOtU8q4Lx6w><p>為了方便理解，我們舉個分類任務例子，以包含一個隱藏層的 GCN 為例：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S2YGOtv6Oz11W6><p>由於知道了 GCN 的傳播規則，所以我們有最終的結果：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGPRWG8gV5U6><p>其中， 是輸入層到隱藏層的權重， 是隱藏層到輸出層的權重；用 Softmax 是因為這是一個節點分類任務，需要預測標籤。</p><p>然後，我們用交叉熵作為代價函數：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGPRxDYn07Oz><p>其中， 為有標籤的節點集合。</p><p>有了代價函數後，我們可以通過梯度下降來更新網絡的參數。</p><p></p><h2 toutiao-origin=h2><strong toutiao-origin=span>3.7 Experiment</strong></h2><p>簡單看下第三代 GCN 的試驗。</p><p>由於 GCN 比較複雜，所以這裡我將給出兩種實驗，一種是 GCN 的效果實驗，另一種是模擬 GCN 運行的實驗。</p><p></p><h2 toutiao-origin=h3><strong toutiao-origin=span>3.7.1 Effect</strong></h2><p>我們來看一下實驗部分，GCN 與其他模型的對比：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGPSUatrBxk><p>可以看到 GCN 的結果在不同數據集上都取得了非常好的效果，遠超其他模型。</p><p>我們再看一下，對於 GCN 而言不同程度的近似會有什麼樣的效果：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGPTB4WaIYGA><p>可以看到並不是模型越複雜效果越好。</p><p>GCN 還有除了訓練後模型精度高外，還有兩個非常硬核的地方，即使不訓練，直接隨機參數也可以獲得不錯的效果，下圖展示了在某一數據集下隨機賦權值的結果：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGPTz9iUCqHJ><p>另外，作為半監督學習，GCN 可以在只標註少量樣本的情況下學得出色的效果，下圖為每個類別只標註一個樣本的分類結果：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S2YGQ8eHGgTl5N><p></p><h2 toutiao-origin=h3><strong toutiao-origin=span>3.7.2 Simulation</strong></h2><p>為了更加形象的理解 GCN，我們來對 GCN 進行模擬。</p><p>首先，以一個簡單有向圖模型為例：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGQ9P6fFGWny><p>鄰接矩陣 A 和 節點的特徵向量 X 為：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGQ9h7MXfSSE><p>我們有一個簡單的傳播規則（不考慮參數矩陣和激活函數）：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGQA4A5Von5K><p><strong>「可以看到節點的特徵變成了其鄰居的特徵之和！」</strong></p><p>但這邊也就一些小問題：</p><ol><li><p>這種傳播過程沒有考慮節點自身的特徵；</p></li><li><p>度的大節點特徵值會越來越大，度小的節點特徵值會越來越小，傳播過程對特徵的尺度敏感。</p></li></ol><p>為了解決這個問題，我們需要：</p><ol><li><p>加一個單位矩陣，考慮自環路；</p></li><li><p>將鄰接矩陣 A 與度矩陣 D 的逆相乘對特徵進行歸一化。</p></li></ol><p>我們先看下加上單位矩陣的效果：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGQALFOHqzaA><p>可以看到，加上單位矩陣的計算考慮了節點的特徵。</p><p>再看下鄰接矩陣歸一化的效果：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGQvB6BSBqoL><p>鄰接矩陣被歸一化到 0 到 1 之間。</p><p>我們將兩個放在一起，並考慮參數矩陣 W：</p><p>所以我們有：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/S2YGQvg9Ea6YC7><p>以上便完成了 GCN 的簡單仿真。</p><p>我們回過頭來再來看一下網絡的傳播規則：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGQy8F25aF04><p>現在是不是更能明白為什麼這麼傳播了？</p><p>這裡解釋一下歸一化為什麼是兩邊乘上矩陣的 -1/2 次方。</p><p>這是因為對稱歸一化的拉普拉斯矩陣其元素定義為：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/S2YGQyR87KtjGB><p>我們仿真模擬的是用加權求和取平均的方式來聚合，而作者採用的是拉普拉斯變換。我這邊做一個化簡大家可能個就會明白了：</p><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S2YGQyjDUW2T5G><p>區別於加權求和取平均的方式，拉普拉斯變換不但考慮當前節點的 i 的度，還考慮其他節點 j 的度。</p><p></p><h1 toutiao-origin=h1>Conclusion</h1><p>GCN 的入門文章就介紹完了，大致思路為：CNN 中的卷積無法直接應用於網絡圖中，所以引出了圖信號處理（Graph Signal Processing）中的 Graph Fourier Transformation，進而定義 Graph Convolution，最後結合深度學習發展出來 GCN。</p><p></p><h1 toutiao-origin=h1>Reference</h1><ol><li><p>《Graph Convolutional Networks in 3 Minutes》</p></li><li><p>《如何理解卷積神經網絡中的權值共享？》</p></li><li><p>《HIERARCHICAL DEEP LEARNING ARCHITECTURE FOR 10K OBJECTS CLASSIFICATION》</p></li><li><p>《Introduction to Graph Signal Processing》</p></li><li><p>《Fourier series》</p></li><li><p>《Fourier Series Graph Interactive》</p></li><li><p>《Hilbert space》</p></li><li><p>《Laplace operator》</p></li><li><p>《如何理解 GCN？- Johnny Richards的回答》</p></li><li><p>《圖拉普拉斯算子為何定義為D-W》</p></li><li><p>《圖卷積神經網絡理論基礎》</p></li><li><p>《如何理解 GCN？- superbrother的回答》</p></li><li><p>《Fourier transform》</p></li><li><p>《Convolution》</p></li><li><p>《Convolution theorem》</p></li><li><p>《Spectral Networks and Deep Locally Connected Networks on Graphs》</p></li><li><p>《Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering》</p></li><li><p>《Semi-supervised Classification with Graph Convolutional Networks》</p></li><li><p>《How to do Deep Learning on Graphs with Graph Convolutional Networks》</p><br></li></ol><img alt="萬字長文帶你入門 GCN" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/R69FpRH4d90a7d></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>萬字長</a></li><li><a>文帶</a></li><li><a>入門</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1fb61f.html alt=萬字長文帶你瞭解變分自編碼器VAEs class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/0f0d438dcd3f4085902cc8c9214d8655 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1fb61f.html title=萬字長文帶你瞭解變分自編碼器VAEs>萬字長文帶你瞭解變分自編碼器VAEs</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6fe27eab.html alt="前端 | HTML入門基礎知識-網頁" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/e7a0b61194f445b8b9e5ae330961d2ea style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6fe27eab.html title="前端 | HTML入門基礎知識-網頁">前端 | HTML入門基礎知識-網頁</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/17eee2b1.html alt="小蜜團隊萬字長文 | 講透對話管理模型最新研究進展" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/249d781057744bdcb94e007250ac41bc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/17eee2b1.html title="小蜜團隊萬字長文 | 講透對話管理模型最新研究進展">小蜜團隊萬字長文 | 講透對話管理模型最新研究進展</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/19664ba8.html alt=EXCEL入門基礎：對行和列選定數據求和 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1f03f82096d240dd92410709f3e19eb9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/19664ba8.html title=EXCEL入門基礎：對行和列選定數據求和>EXCEL入門基礎：對行和列選定數據求和</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5abb53a8.html alt=工程造價從入門到精通：造價員全能圖解+工程算量表，限時分享！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/49834ccb43ed42cb9a54c9827c3ab134 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5abb53a8.html title=工程造價從入門到精通：造價員全能圖解+工程算量表，限時分享！>工程造價從入門到精通：造價員全能圖解+工程算量表，限時分享！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1a5f8d5f.html alt=工程造價：入門知識全套講義，30章600頁，精通造價首選之作 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9e1b335c343a455f8777dd3144fc1c35 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1a5f8d5f.html title=工程造價：入門知識全套講義，30章600頁，精通造價首選之作>工程造價：入門知識全套講義，30章600頁，精通造價首選之作</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2d4007c7.html alt=“黑客”入門學習之“Windows組策略” class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ea21244d5f5c420ebef29650f3fafd1c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2d4007c7.html title=“黑客”入門學習之“Windows組策略”>“黑客”入門學習之“Windows組策略”</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9d08c7e6.html alt=PHP入門教程，5天86節課助力小白變大神！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/4366000004d4c98fd587 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9d08c7e6.html title=PHP入門教程，5天86節課助力小白變大神！>PHP入門教程，5天86節課助力小白變大神！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/923fd40e.html alt=Thinkphp6快速入門一 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/c4331ddc0ffb4c94a4aa80be95178354 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/923fd40e.html title=Thinkphp6快速入門一>Thinkphp6快速入門一</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3d2ce3d6.html alt="php新手入門教程， 最全最完整的教學視頻課程" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/666a38216ab04790a716bb1451c7fe44 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3d2ce3d6.html title="php新手入門教程， 最全最完整的教學視頻課程">php新手入門教程， 最全最完整的教學視頻課程</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/07454552.html alt=「素描入門」基礎不紮實，從排線練起 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/15252166717297d7af296ee style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/07454552.html title=「素描入門」基礎不紮實，從排線練起>「素描入門」基礎不紮實，從排線練起</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/07c706e2.html alt=「素描入門」素描排線的繪畫技法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1530098801296ab58189790 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/07c706e2.html title=「素描入門」素描排線的繪畫技法>「素描入門」素描排線的繪畫技法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a993b442.html alt=素描入門丨你說線條或者排線，容易嗎？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1536648317995f32cedaa40 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a993b442.html title=素描入門丨你說線條或者排線，容易嗎？>素描入門丨你說線條或者排線，容易嗎？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d0e2c5d2.html alt=零基礎入門要知道的素描知識總結 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/c2c16563-8821-4294-a8c6-9d76e62a2440 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d0e2c5d2.html title=零基礎入門要知道的素描知識總結>零基礎入門要知道的素描知識總結</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6f585f1e.html alt=JAVA入門到大神（玩轉正則表達式） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6f585f1e.html title=JAVA入門到大神（玩轉正則表達式）>JAVA入門到大神（玩轉正則表達式）</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>