<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等 | 极客快訊</title><meta property="og:title" content="今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RsRP7Hp6AI3kGt"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/54538ef2.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/54538ef2.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/54538ef2.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/54538ef2.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/54538ef2.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/54538ef2.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/54538ef2.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/54538ef2.html><meta property="article:published_time" content="2020-11-14T21:06:58+08:00"><meta property="article:modified_time" content="2020-11-14T21:06:58+08:00"><meta name=Keywords content><meta name=description content="今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/54538ef2.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><h2><img alt="今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RsRP7Hp6AI3kGt><br></h2><p></p><h2><strong>目錄</strong></h2><p>UnrealText：從虛擬世界合成真實場景文本圖像</p><p>ScrabbleGAN：半監督變長手寫文本生成</p><p>ROAM：遞歸優化跟蹤模型</p><p>G2L-Net：用於實時6D姿態估計的嵌入矢量特徵的全局到局部網絡</p><p>用於人體姿勢估計的多視角圖像的可穿戴IMU融合：一種幾何方法</p><p></p><h2><strong>UnrealText：從虛擬世界合成真實場景文本圖像</strong></h2><p>論文名稱：UnrealText: Synthesizing Realistic Scene Text Images from the Unreal World</p><p>作者：Long Shangbang /Yao Cong</p><p>發表時間：2020/3/24</p><p>論文鏈接：https://paper.yanxishe.com/review/15414?from=leiphonecolumn_paperreview0408</p><p>推薦原因</p><p>這篇論文被CVPR 2020接收，提出了一種名為UnrealText的圖像合成方法，可以通過3D圖形引擎渲染逼真的圖像。3D合成引擎通過整體渲染場景和文本來提供逼真外觀，並允許訪問精確的場景信息。這篇論文通過大量實驗驗證了所提方法在場景文本檢測和識別方面的有效性。這篇論文還會生成多語言版本，以供將來對多語言場景文本檢測和識別進行研究。</p><img alt="今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RvX6vAJEsOday8><img alt="今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RvX6vAoA1xzmqv><p></p><h2><strong>ScrabbleGAN：半監督變長手寫文本生成</strong></h2><p>論文名稱：ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation</p><p>作者：Fogel Sharon /Averbuch-Elor Hadar /Cohen Sarel /Mazor Shai /Litman Roee</p><p>發表時間：2020/3/23</p><p>論文鏈接：https://paper.yanxishe.com/review/15413?from=leiphonecolumn_paperreview0408</p><p>推薦原因</p><p>這篇論文被CVPR 2020接收，考慮的是手寫文本生成的問題。</p><p>深度學習方法在手寫文本識別問題上取得了大幅的性能提高，然而由於手寫體的每個人都有獨特風格，基於深度學習的訓練樣本會受到數量的限制。收集數據是一項具有挑戰性且代價高昂的任務，而隨後的標註任務也非常困難。這篇論文使用半監督方法來減輕數據標註的負擔。與完全監督的方法相比，半監督方法除了使用標記數據之外，還使用一些未標記的樣本來提高性能，從而能更好地適應測試集中新出現的圖像。</p><img alt="今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RvX6vB56kgu1a7><p></p><h2><strong>ROAM：遞歸優化跟蹤模型</strong></h2><p>論文名稱：ROAM: Recurrently Optimizing Tracking Model</p><p>作者：Yang Tianyu /Xu Pengfei /Hu Runbo /Chai Hua /Chan Antoni B.</p><p>發表時間：2019/7/28</p><p>論文鏈接：https://paper.yanxishe.com/review/15412?from=leiphonecolumn_paperreview0408</p><p>推薦原因</p><p>這篇論文被CVPR 2020接收，提出了一個由反應生成和邊界框迴歸組成的追蹤模型，其中反應生成部分通過生成一個熱圖來顯示對象出現在不同的位置，邊界框迴歸部分通過迴歸相對的邊界框來定位滑動窗口的位置。為了有效地使模型適應外觀變化，這篇論文提出通過離線訓練一個遞歸神經優化器來更新追蹤模型，使模型在幾個梯度步驟內收斂，提高了更新跟蹤模型的收斂速度，同時獲得了更好的性能。在OTB, VOT, LaSOT, GOT-10K和TrackingNet基準數據集上評估了新提出的模型、ROAM和ROAM++這兩個模型，實驗結果表明新提出的方法明顯優於最先進的方法。</p><img alt="今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RvX6vBJCbt7x4J><img alt="今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RvX6vBa1TEShOS><p></p><h2><strong>G2L-Net：用於實時6D姿態估計的嵌入矢量特徵的全局到局部網絡</strong></h2><p>論文名稱：G2L-Net: Global to Local Network for Real-time 6D Pose Estimation with Embedding Vector Features</p><p>作者：Chen Wei /Jia Xi /Chang Hyung Jin /Duan Jinming /Leonardis Ales</p><p>發表時間：2020/3/24</p><p>論文鏈接：https://paper.yanxishe.com/review/15408?from=leiphonecolumn_paperreview0408</p><p>推薦原因</p><p>這篇論文被CVPR 2020接收，要處理的是姿態估計的問題。</p><p>這篇論文提出了一個名為G2L-Net的實時6D目標姿態估計框架，包含三個部分：首先通過二維檢測從RGB-D圖像中提取粗粒度目標點雲；然後將粗粒度目標點雲加入到遷移定位網絡中進行三維分割和目標遷移預測；最後通過預測得到的分割和平移信息，將細粒度目標點雲轉化為局部正則座標，用於訓練旋轉定位網絡來估計初始目標旋轉。在第三步中，G2L-Net通過定義逐點嵌入向量特徵來捕獲視圖感知的信息。為了計算出更精確的旋轉，G2L-Net還採用旋轉殘差估計器來估計初始旋轉與真實標籤之間的殘差，從而提高初始姿態估計的性能。在兩個基準數據集上的大量實驗表明，G2L-Net在精度和速度方面都達到了最新的水平。</p><img alt="今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RvX6vcd49pvaI8><img alt="今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RvX6vdX312WPce><p></p><h2><strong>用於人體姿勢估計的多視角圖像的可穿戴IMU融合：一種幾何方法</strong></h2><p>論文名稱：Fusing Wearable IMUs with Multi-View Images for Human Pose Estimation: A Geometric Approach</p><p>作者：Zhang Zhe /Wang Chunyu /Qin Wenhu /Zeng Wenjun</p><p>發表時間：2020/3/25</p><p>論文鏈接：https://paper.yanxishe.com/review/15407?from=leiphonecolumn_paperreview0408</p><p>推薦原因</p><p>這篇論文被CVPR 2020接收，要解決的是3D人體姿勢估計的問題。</p><p>利用可穿戴的慣性測量單元（Inertial measurement unit，IMU），這篇論文提出一種名為定向正則化網絡（Orientation Regularized Network，ORN）的幾何方法，來增強每對關節的視覺特徵。當一個關節被遮擋時，新方法可以顯著提高2D姿態估計的準確性。然後，這篇論文通過定向規則化圖形結構模型（Orientation Regularized Pictorial Structure Model，ORPSM）將多視圖2D姿勢提升到3D空間，來最小化3D和2D姿勢之間的投影誤差，以及3D姿勢和IMU方向之間的差異。這種兩步的方法明顯減少了公開數據集上的誤差。</p><img alt="今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RvX6vg982bQziQ><img alt="今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RvX6vgPFwPZZmL><img alt="今日 Paper | ScrabbleGAN；UnrealText；跟蹤模型；G2L-Net等" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RvXHBy6HrdQjRr><p>雷鋒網雷鋒網雷鋒網</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>Paper</a></li><li><a>ScrabbleGAN</a></li><li><a>UnrealText</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/dab0f30f.html alt="今日 Paper | 從純圖像重建世界；層次遞歸網絡序列；注意力神經網絡；命名實體識別等" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RqRS7P03AH4R3v style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dab0f30f.html title="今日 Paper | 從純圖像重建世界；層次遞歸網絡序列；注意力神經網絡；命名實體識別等">今日 Paper | 從純圖像重建世界；層次遞歸網絡序列；注意力神經網絡；命名實體識別等</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>