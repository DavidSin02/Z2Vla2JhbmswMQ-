<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 | 极客快訊</title><meta property="og:title" content="機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/84c47890a2c44654997e63bd5cdf0c72"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/59b3843e.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/59b3843e.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/59b3843e.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/59b3843e.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/59b3843e.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/59b3843e.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/59b3843e.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/59b3843e.html><meta property="article:published_time" content="2020-10-29T21:11:12+08:00"><meta property="article:modified_time" content="2020-10-29T21:11:12+08:00"><meta name=Keywords content><meta name=description content="機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/59b3843e.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/84c47890a2c44654997e63bd5cdf0c72><p class=pgc-img-caption></p></div><h1>指數分佈</h1><p>高斯分佈、二項分佈、多項分佈、泊松分佈、伽瑪分佈和貝塔分佈都屬於指數分佈。它的一般形式是</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2dab3b676e1449cba4a56e4995152744><p class=pgc-img-caption></p></div><p><em>A</em>（<em>η</em>）是累積量函數。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/c86e504f468d424fba2c24478113ec81><p class=pgc-img-caption></p></div><p>其指數eᴬ是歸一化因子，A（η）也稱為對數配分函數。η是自然參數。T（x）被稱為充分統計量。在許多特定的分佈中，如伯努利分佈，它等於x。</p><p>考慮以下伯努利分佈，其取值為1的概率為α，值為0的概率為1- α。我們可以用指數形式重寫伯努利分佈。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a24a467ac5364b64b3809318e3f0b2e7><p class=pgc-img-caption></p></div><p>然後</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cd78256ed665485a84e10e21832afd62><p class=pgc-img-caption></p></div><p>h，T和A的選固定擇將定義一個特定的指數分佈，如伯努利分佈。如果我們轉換η，它將成為恢復伯努利分佈的模型參數α的邏輯函數。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3bc8d6aa66f44605970a468f106a51ac><p class=pgc-img-caption></p></div><p>因此，它可以用自然參數η表示為指數，而不是用參數α來建模伯努利分佈。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a3e3791a62b742018380844576809c8b><p class=pgc-img-caption></p></div><p>對於二項式和泊松分佈</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9771756f45ea485da9c37714f33b5680><p class=pgc-img-caption></p></div><p>到目前為止，我們的分佈只需要一個參數來建模。對於由多個參數建模的分佈，<em>η</em>將包含值向量。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0be2fc9407ca4485872d867ec0854abd><p class=pgc-img-caption></p></div><p>許多概率模型中的概率密度，如在圖模型中由馬爾可夫隨機場MRF建模的概率密度，可以表示為指數。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/24f3cfa694f5415bb3e5ef4b080a7f3d><p class=pgc-img-caption></p></div><p>因此，指數族分佈成為建模概率模型的自然選擇。</p><p>讓我們來看看<em>A</em>（<em>η</em>）的導數</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/f6d85ecc969f41b9aa162ff5c4e6ce3a><p class=pgc-img-caption></p></div><p>它的一階導數是充分統計量T(x)的期望。對於T(x)=x，這個導數等於分佈的均值。</p><p>在泊松分佈中，用傳統的積分定義計算E[x](均值)並不容易。將T（x）定義為泊松分佈中的x，A '（η）等於E [ x ]。一般來說，微分比積分簡單，我們利用它來解期望。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2fb631f3975146d8a7ff114804118886><p class=pgc-img-caption></p></div><p>二階導數<em>A</em> '（<em>η</em>）等於方差。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/5d21567c01fc4cda8ee0b499e62d050b><p class=pgc-img-caption></p></div><p>A的導數實際上幫助我們定義了分佈。</p><h1>矩匹配</h1><p>矩定量地描述了函數的形狀。定義為</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ae971f8aee0346de834e58853fbcdd67><p class=pgc-img-caption></p></div><p>這一矩被稱為關於零的矩。但是如果我們先用平均值減去x，它將被稱為中心矩。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8783aca5718c42fd9ac28127bef06044><p class=pgc-img-caption></p></div><p>k階矩等於a（η）的k階導數。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/879d7a867d9345c5957c7bc10cf0faac><p class=pgc-img-caption></p></div><p>A（η）是凸函數（其二階導數大於0）。由於A'（η）= μ，η具有與μ（力矩參數）的一對一映射。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b0746a42c2f34158a4f9f7cf7c372cb9><p class=pgc-img-caption></p></div><p>根據充分統計量t（x）的定義，導數<em>A'</em>（<em>η</em>），<em>A''</em>（<em>η</em>），...... <em>Aᵏ</em>（<em>η</em>）具有特殊的意義，可以通過採樣數據進行估計。因此，我們在樣本數據、分佈矩和分佈參數之間創建一個鏈接。在機器學習中，我們要用q*來模擬種群密度p。在矩匹配中，我們從樣本數據中計算矩，以使它們的充分統計量的期望值相匹配。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/339b0bc70b2d4729a3e54424385b7b43><p class=pgc-img-caption></p></div><p>假設繪製的所有數據都是iid，最大似然估計將是：</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/41b6fdb422574d8189f339a34ff49861><p class=pgc-img-caption></p></div><p>可以通過從樣本數據中找出充分統計量的平均值來計算μ。這稱為矩匹配。估計後，我們可以找到分佈的參數。</p><p>考慮一個簡單的zero-centered分佈f</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3cbfb50bd7454f2691dae8de1e27a6fb><p class=pgc-img-caption></p></div><p>讓我們看看如何通過採樣計算分佈參數σ。矩計算如下：</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2589583d76f84b368fbaea99c51688d6><p class=pgc-img-caption></p></div><p>這些矩是鐘形分佈的均值和方差。我們可以通過採樣來估計二階矩。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/5bfec0107c184e0bae7bb9c0de7f7fd2><p class=pgc-img-caption></p></div><p>通過將理論矩和樣本矩聯繫起來，得到了對σ（sampled σ）的估計。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d7a983fc48284c01ad225c37b792b5af><p class=pgc-img-caption></p></div><p>在上面的例子中，通過積分求E (x)和E (x²)很容易。一般來說。對於許多其他指數分佈來說，這並不容易，比如gamma分佈。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cb25bdaa6f8c4cb880ca50e299e2eacc><p class=pgc-img-caption></p></div><p>自然參數及其逆定義為：</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0aa00228f0254e61b223cbf9f7ae065f><p class=pgc-img-caption></p></div><p>充分統計為（log x，x），a（η）為</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/af60aa1881da4a59976c2b40b2df2db3><p class=pgc-img-caption></p></div><p>使用<em>A</em>（<em>η</em>）的導數，我們找到了充分統計的期望</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5b6e400d43f6472895abe4c5f7e72e8f><p class=pgc-img-caption></p></div><p>然後利用樣本數據計算充分統計量的平均值，對上述參數α和β進行反求。</p><h1>貝葉斯推斷</h1><p>頻率推斷從事件的頻率得出結論。如果我們兩次擲硬幣兩次正面（head），p（head）等於100％嗎？然而，由於樣本量太小，頻率推斷不太可能發佈這樣的結果。</p><p>貝葉斯推斷利用貝葉斯定理從似然和先驗信念中導出後驗分佈。當有新的觀測結果時，我們將後驗轉換為先驗，並根據新的證據計算新的後驗。由於後驗是一個確定性分佈而不是一個點估計，我們可以繼續將其與新的證據相結合，形成一個新的belief。簡言之，我們從某個p（h）開始，並在新的證據下繼續更新後驗。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1dbc93582bd4436a8b081abda908ef29><p class=pgc-img-caption></p></div><p>例如，可以通過結合汽車如何移動的動態模型和GPS之前的測量數據來開始對汽車位置的預先判斷。或者我們甚至可以完全從直覺或經驗開始一個先驗。給定當前傳感器讀數，我們形成了給定不同位置假設的當前傳感器讀數的可能性。利用貝葉斯推理，我們可以得到給定傳感器讀數的當前汽車位置的概率分佈P(H|E)。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/146202a959104455ba37cc3dc0ba9b26><p class=pgc-img-caption></p></div><p>我們將後驗轉換為前驗，以便下一次迭代時進行新的觀察。樣本量越小，似然曲線越寬，峰值越低。我們還沒有畫出足夠的數據來排除許多可能性。因此，如果後驗是強的(窄的和尖的)，後驗將與前驗相似。當收集到的數據越多，似然值越尖，後驗分佈越接近似然曲線。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fe82cfdd7c0148438822f299f773221b><p class=pgc-img-caption></p></div><p><strong>Frequentist vs Bayesian</strong></p><p>Frequentist應用最大似然估計來找到解釋觀察結果的最佳模型參數。貝葉斯聚焦在模型參數θ上，並使用貝葉斯定理計算模型參數的後驗。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/01d78c49fb41478aae0d6814146e308b><p class=pgc-img-caption></p></div><p>貝葉斯推斷在給定觀察的情況下計算不同模型的概率。當然，對於高維或大的連續空間，這可能非常複雜。進一步簡化似然模型和先驗模型是可行的。或者我們可以通過採樣或近似來解決這個問題。</p><p>根據樣本收集的方式，回答P(x|y)可能比回答P(y|x)更容易。有時，概率很容易在相反的方向上建模。例如，P（y | x， θ）和P（θ）通常用高斯分佈或β分佈建模。下面是貝葉斯線性迴歸的一個例子。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/379d000acee74cb39f308b39a9e9bf0d><p class=pgc-img-caption></p></div><p>我們忽略貝葉斯定理中的分母P（y | X），因為它不是θ的函數。對於P（y | x， θ）和P（θ），我們在貝葉斯線性迴歸中用單獨的高斯模型對它們進行建模。實際上，P(y |X)或P(X)通常很難計算，所以這是優化後驗的一個很好的簡化。</p><p>在貝葉斯定理,我們有相對較大的自由選擇模型P(θ)。但並不是每個選擇都是相等的，這個選擇影響後驗分析計算的難易程度。如果相應的後驗函數屬於前驗函數的同一類分佈，則前驗函數是共軛前驗函數。由於後驗在下一次迭代中經常被用作先驗，我們可以簡單地重複同樣的數學計算後驗。例如，如果似然和先驗都可以用高斯函數建模，那麼後驗函數也是高斯函數，易於計算。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/05ef33ecf06e449db566a2cbde163fab><p class=pgc-img-caption></p></div><p>如果模型θ可以使用共軛先驗對應於特定似然分佈來建模，我們通常可以容易地和分析地解決後驗。</p><p><strong>Beta分佈的貝葉斯推斷</strong></p><p>對於二項分佈，我們可以使用beta分佈對其進行建模。如果可能性是二項式或伯努利，我們將在beta分佈之前選擇我們的共軛。這個選擇使得我們可以將後驗分佈為β分佈，並且可以容易地分析計算計算。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/5d593f82bcae429b9c2bb5caf58a9538><p class=pgc-img-caption></p></div><p>這是關於使用β分佈來尋找後驗的框架，其中我們對p（data|θ）和p（θ）都使用β分佈。後驗p（θ|data）將是β分佈，所涉及的數學只是一些補充。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/457431f176b84c77b686bfdad7d7460f><p class=pgc-img-caption></p></div><p>讓我們考慮一個人接觸病毒的感染率。如果我們沒有先驗知識，我們可以從均勻分佈開始先驗（如下）。貝葉斯推理中的後驗與頻率論的結果相似，因為我們的belief較弱。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/59179fd2b17b480b8318eda11fc409ee><p class=pgc-img-caption></p></div><p>否則，我們可以從一些基於過去經驗、知識甚至直覺的先驗知識開始。然而，如果我們的belief是錯的，我們需要收集更多的數據來逐漸重塑後驗曲線。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1ed53bd329894a848da39ccbcc593c8a><p class=pgc-img-caption></p></div><p>讓我們看看貝葉斯推理與頻率推斷的不同之處。在貝葉斯中，我們首先認為流感感染率可以建模為B（2,6）。這將是我們下面的第一張圖。假設我們只有一個實驗室結果，並測試呈陽性。一個普通的頻率推斷者會說根據樣本感染率是100％。但我們知道這在科學上是不合理的。但是對於貝葉斯來說，隨著結果的逐漸出現，我們仍然可以利用貝葉斯推理得出某種結論。從某種角度來看，如果我們先驗是合理的，貝葉斯推理給我們一個合理的圖像。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5ca1cc01c1674e41b5c8c0b5371f5f18><p class=pgc-img-caption></p></div><p><strong>Gamma分佈作為共軛先驗</strong></p><p>如果似然可以用高斯分佈來建模，我們可以用伽馬分佈作為共軛先驗。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/524494bfac234085916ec6a97674d7d6><p class=pgc-img-caption></p></div><p>似然<em>p</em>（<em>x |θ</em>）的高斯分佈可以用以下形式表示</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8d5a6456e145467882b8dcdc8e6a029b><p class=pgc-img-caption></p></div><p>應用貝葉斯定理，我們也可以以Gamma分佈的形式推導出後驗。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/48cc9cdb38ad409eb14481b57c86bf2c><p class=pgc-img-caption></p></div><p><strong>Dirichlet - 多項式的共軛先驗</strong></p><p>Dirichlet分佈是多項式的共軛先驗。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/90942599aa6a440ea822472700715d77><p class=pgc-img-caption></p></div><p>後驗是：</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e26f0f2c14df4fa795c3208afa8ea330><p class=pgc-img-caption></p></div><p>Dirichlet分佈也是分類分佈之前的共軛：</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8dc0280ee42145ae8fa800aeaad262b3><p class=pgc-img-caption></p></div><p><strong>共軛先驗概述</strong></p><p>以下是對應於特定似然分佈的一些其他共軛先驗。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1b12b65e0f22458e8b00e9cd4caa164f><p class=pgc-img-caption></p></div><p><strong>預測與正則化</strong></p><p>利用bayes定理，在給定觀測值的情況下，計算了θ模型的後驗概率。假設模型參數θ為zero-centered高斯分佈，則先驗p（θ）在目標函數中轉化為l2正則項。從概念上講，p（θ）可以看作是一個正則化因子。它可以懲罰成本函數。如下圖所示，如果我們事先知道θ是什麼樣子的，我們可以對p（θ）應用一個相當複雜的模型。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7e851ac97d2e4a609315361072aaebd4><p class=pgc-img-caption></p></div><p>為了進行新的預測，我們在訓練中使用後驗p（θ| X，y）作為p（θ）。然後我們通過積分θ得到邊際概率p（y 0 | x 0）。這是邊際推斷。我們通過將其他所有內容相加來計算變量的概率。</p><h1>導數</h1><p><strong>雅可比矩陣和Hessian矩陣</strong></p><p>這些矩陣分別是f的一階和二階導數。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/422078a1422747418764f292daffec89><p class=pgc-img-caption></p></div><p>這種表示法稱為分子佈局。hessian矩陣是對稱的。具有hessian矩陣和向量v的二次方程的上界是</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a1c801f4fe83445faf238da23f31357a><p class=pgc-img-caption></p></div><p>下面，我們使用分母佈局。它是分子佈局的轉置。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d70e7a2422a94fe68338c530c70a6c90><p class=pgc-img-caption></p></div><p>這是微分一個向量和一個矩陣的結果</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6a4a8d56832c489f91253c7989946005><p class=pgc-img-caption></p></div><h1>矩陣分解</h1><p><strong>圖形解釋</strong></p><p>我們可以通過將x投影到x軸和y軸來表示二維向量x。因此數據點可以表示為（xᵢ，yᵢ）。我們可以選擇單位向量q並計算x對q的投影。投影向量為qqᵀx，其大小等於qᵀx。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f2c1e2df0dd245cc9655a6a788616a7c><p class=pgc-img-caption></p></div><p>在機器學習（ML）中，我們將特徵從高維空間提取到低維潛在空間（比如k維）。概念上，我們把x投射到k個不同的向量q ⱼ上。選擇qⱼ是很重要的。如果做得正確，我們可以使用更少的成分來表示信息。例如，如果我們選擇下面的q 1和q 2，我們可以忽略q 2（藍點）。它們可能太小，我們可以忽略它們。但是，如果我們選擇x軸和y軸，則情況並非如此。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/59e012dc13564f8d881618c8b0fd9cb0><p class=pgc-img-caption></p></div><p>SVD將矩陣分解為獨立的成分。SVD中選取的所有q相互獨立(正交)，即提取的特徵不相關。從概念上講，SVD選擇第一個q，當其餘成分被刪除時，則最小化下面的最小平方誤差</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2dea1e12a3014f85a57fe9cf42e77df8><p class=pgc-img-caption></p></div><p><em>XXᵀ</em>是對稱的。<em> </em>最優<em>q</em>（命名為<em>q 1</em>）將是<em>XXᵀ</em>的特徵向量，具有最大特徵值<em>λ或</em>最大奇異值<em>σ</em>（<em>λ=σ²</em>）</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6b8b45d0c44641eb96395016d7bbf21b><p class=pgc-img-caption></p></div><p>然後我們基於相同的原理選擇下一個組件，條件是q彼此正交。因此，所選擇的q 2將具有第二大的特徵值。我們可以繼續這個過程，直到我們用完特徵向量。</p><p><strong>奇異值分解（SVD）</strong></p><p>SVD在線性代數中的表現方式不同。任何矩陣A都可以分解為</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6490b581a124486795a23c7c2de3d8ea><p class=pgc-img-caption></p></div><p>其中U由u構成- AAᵀ和uᵢ的本徵向量彼此正交。類似地，v由AᵀA的特徵向量vᵢ組成，該特徵向量也彼此正交。</p><p>從上面的等式，A也可以寫成</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b50fd0d37f46406594348fa31be8560c><p class=pgc-img-caption></p></div><p>其中uᵢ和vᵢ是單位向量。因此，當我們評估分解成分的重要性時，我們可以忽略那些具有非常小的σᵢ的項。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e226638d9fd0412f9ecca84f1233ab82><p class=pgc-img-caption></p></div><p>如果我們僅保留具有最大σᵢ的最頂部k項，我們有效地將A的維度減小為k，即，提取的特徵僅在k維度上。考慮到每個主成分的重要性，我們有效地減少了輸入的維度。這就是PCA所做的。</p><p><strong>主成分分析PCA</strong></p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/63ec90c2085a419e8dd415d287dad506><p class=pgc-img-caption></p></div><p>直觀地說，兩個輸入特徵可能相互關聯，因此您可以創建一個新特徵來表示這兩個特徵。對於主成分分析，我們希望找到k個獨立的特徵來表示我們的數據。</p><p><strong>PCA示例</strong></p><p>在機器學習（ML）中，SVD將包含訓練數據的矩陣分解為獨立的特徵。例如，矩陣的行包含來自用戶的電影評級。列包含電影的用戶評分。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7d5ef47bf81446918b0106024b90fb6d><p class=pgc-img-caption></p></div><p>如果我們選擇AAᵀ的前K個特徵值，其相應的特徵向量等效於下面的前K個優化q k向量：</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/f8384c10c0e64179b3fa8d442c3b139b><p class=pgc-img-caption></p></div><p>回想一下，我們將x投影到這些主成分qk中。求出最上面K個優化的qk，將x的維數降為K，就可以得到投影向量是x的第K個潛在因子。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/889ac86a13d94035a4161606fc6500a7><p class=pgc-img-caption></p></div><p>我們可以連接qᵢ形成矩陣Q。我們可以通過將Qᵀ與用戶的電影分級相乘得出userᵢ 的潛在特徵。（qᵢ是M ×1，其中M是電影的數量，Q是M × K）</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2bb17d17faa9492c8cee9c43a2889d2b><p class=pgc-img-caption></p></div><p>SVD發現用戶評級的模式（主成分）。我們可以想象一些主成分可能代表電影的類型或發行的年代。例如，zᵢ中的第一個成分可以指示用戶是否喜歡喜劇。</p><p><strong>概率PCA</strong></p><p>在svd中，我們將x分解為USVᵀ。而概率pca模型X≈WZ。我們將使用em算法來學習W和Z，其中Z可以作為X的潛在特徵。與svd不同，W不需要是正交的。列不需要是單位長度或彼此垂直。</p><p>首先，我們假設潛變量zᵢ是zero-centered高斯分佈。利用W，我們可以通過WZ重建原始數據X，其中x也由高斯建模。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e5f91daaf4ef481191b633f01c0f24d4><p class=pgc-img-caption></p></div><p>Z是EM算法中的潛在變量θ2，W是θ1。我們的目標是</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/91e1a64910b14b07b36228cef3721ded><p class=pgc-img-caption></p></div><p>在E步驟中，我們計算<em>q</em>（<em>zᵢ</em>）的高斯分佈</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2a534cf2a100415f8ca2f9b75db4e4de><p class=pgc-img-caption></p></div><p>在M步驟中，我們進行優化</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/56a1d503cbbf4247bd4b7b1342c59fa2><p class=pgc-img-caption></p></div><p>算法是：</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/adb0f03c212444e8a09a3c2de76e7e38><p class=pgc-img-caption></p></div><p><strong>Kernel PCA</strong></p><p>從一個角度來看，PCA找到一組最大化qᵀXXᵀq的向量q 。由於XXᵀ是對稱的，因此q將是具有最大特徵值的XXᵀ的特徵向量。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/41c2895573664bf19d0c2a0b706f9c4f><p class=pgc-img-caption></p></div><p>因此，問題變為找到具有最大特徵值的特徵向量。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c98a690b5a69436b9ef2872fbb724167><p class=pgc-img-caption></p></div><p>我們用核（Kernel）替換XXᵀ以將輸入映射到更高維度。這允許我們創建線性邊界來對在低維空間中不可線性分離的數據進行分類。相反，PCA通常被認為是降維技術。所以這兩種技術似乎都朝著相反的方向發展。然而，有時候，我們需要在變小之前變大。進入高維空間使我們能夠以更簡單明確的邊界對信息進行聚類。一旦信息清晰地聚類，將更容易將其映射到較低維度的空間。這是PCA kernel背後的動機。讓我們從以下等式開始</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2a2d2315cb374c068a05ecbda6d3b724><p class=pgc-img-caption></p></div><p>經過一些操作，我們得到</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/21e77b0305c3467b9f4e1a2cdfda96cb><p class=pgc-img-caption></p></div><p>因此，假設矩陣K保持核結果，我們可以通過找到K的特徵向量找到aᵢ。讓我們用高斯函數定義核函數。x的相應潛在因子可以計算為：</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/434c6e5b86284d01826cabb634108a42><p class=pgc-img-caption></p></div><p>下面是我們如何使用Kernel PCA 預測新輸入<em>x</em> 0</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/02737320e9304c258d80c360a7a77be7><p class=pgc-img-caption></p></div><p><strong>Cholesky分解</strong></p><p>Hermitian正定矩陣A的Cholesky分解是</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f6d0307272594e8fa5240634d77bfe7d><p class=pgc-img-caption></p></div><p>Hermitian矩陣是一個等於其轉置共軛的方陣。轉置共軛物取每個元素的複共軛，然後轉置矩陣。</p><p>協方差矩陣是對稱的（如果值都是real，則是Hermitian的特殊情況）和半正定。因此，Cholesky分解通常用於機器學習（ML)，以便更容易和更穩定地操作。</p><p><strong>Moore-Penrose Pseudoinverse</strong></p><p>對於線性方程組，我們可以計算方陣<em>A</em>的倒數來求解<em>x</em>。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1d348dfa93944f528a3d5b575c4a885b><p class=pgc-img-caption></p></div><p>但並非所有矩陣都是可逆的。在機器學習（ML）中，由於數據中存在噪聲，因此不太可能找到精確解。但x的解可以估算為</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bcaabfc4be6e48d1877c8fe8c4681a4e><p class=pgc-img-caption></p></div><p>其中</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e92e854844ef41af9bd92d929205b793><p class=pgc-img-caption></p></div><h1>統計顯著性</h1><p>空假設H 0表示兩個測量現象之間沒有關係，例如，財富和幸福之間沒有相關性。如果觀察到的數據具有統計顯著性，則拒絕零假設。例如，如果我們在100次拋硬幣中看到100個正面，我們可以“否定”硬幣是公平的假設。因此，備擇假設 H 1（一種與H 0相矛盾的假設）可能是真的（硬幣不均勻）。實際上，要量化兩個變量之間的關係比計算收集到的數據只是偶然發生的概率要難得多。因此，零假設是對兩種現象得出結論的較好方法。</p><p>p值(概率值)是零假設為真時觀測樣本的概率。一個小的p值(通常≤0.05或≤0.01)顯示出與原假設相反的有力證據，即偶然發生的情況很少見。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/efd0b06930e6477988b7f65afe539ad0><p class=pgc-img-caption></p></div><p>例如，在收集100個數據點之後，我們可以基於數據計算相關係數。如上所示，如果我們收集的100個數據點的相關性為-0.25，則其對應的PDF約為0.012。只有2.5％的群體可能具有小於-0.2的相關性。因此，零假設可能是錯誤的。</p><p><strong>置信區間</strong></p><p>在進行實驗收集樣本後。我們可以使用樣本數據點來估計一個像平均值這樣的總體參數(稱為estimator)。置信區間可以計算為這個樣本均值周圍的範圍。95%置信水平意味著在95%的實驗中，其置信區間包含總體的真實均值。換句話說，一個實驗的置信區間不包含真實均值的概率是1 / 20。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3908f06aeedb4524965312d01517233b><p class=pgc-img-caption></p></div><p>這是計算樣本均值的置信區間的骨架</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c30fd8cad097402dbbdfbb4ea176813d><p class=pgc-img-caption></p></div><p>樣本方差：</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2bb7b9e905524c66bf5d25fc20f50bd5><p class=pgc-img-caption></p></div><p><strong>卡方檢驗</strong></p><p>卡方檢驗(Chi-square test)是一種常用的檢驗方法，用於測量觀察到的數據之間的相關性只是偶然的可能性，而不是兩個變量之間的某種相關性。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/70f0b234780543a59bf197cc524c0b8b><p class=pgc-img-caption></p></div><p>利用上述公式計算卡方統計量。我們比較樣本的實際計數和假設不存在相關性的期望計數。下面是一個決定性別是否影響寵物選擇的例子。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/de3bb9a96be54a4dad4803209fee14dc><p class=pgc-img-caption></p></div><p>在這個例子中，如果性別不是一個因素，我們計算了擁有汽車的男性的實際數量減去預期數量之間的差額。我們平方它，除以期望的計數然後計算相應的卡方值。在我們的表格中，我們有四種可能的組合(雄貓、雄狗、雌貓、雌狗)。因此，我們有四個自由度，我們需要把所有四個值加起來來計算卡方統計量。</p><p>對於雙邊檢驗，我們將給定的顯著性水平α除以2。例如，對於α=0.05，如果卡方統計量只有0.05/2=0.025的概率是偶然的，我們可以接受相關。由於卡方分佈是不對稱的，我們通常會查表，看看對應的特定概率值的卡方統計量是多少。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7147c8255f09477da756b08265d37756><p class=pgc-img-caption></p></div><p>例如，當自由度為4時，如果upper-tail表卡方統計量大於11.1，我們將接受相關性。當然，我們也需要參考bottom-tail表來檢查卡方值是否太小。</p><h1>探索性數據分析</h1><p>為了探索數據，我們可以計算兩個變量之間的協方差，或執行如下所示的散點圖來發現趨勢。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/27ebd258722b427081c93bcdd0adf14d><p class=pgc-img-caption></p></div><p>例如，下面的綠點和藍點分別是SF和NY的房子。對於海拔高度>73英尺，我們有一個決策樹樁，滿足這個條件的很可能是SF。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8305ffefa69f402db5a4d48d658ab782><p class=pgc-img-caption></p></div><h1>範數</h1><p><strong>L1, L2-norm</strong></p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/4bdee4819cb140559e9398a01db05163><p class=pgc-img-caption></p></div><p><strong>Lp-norm, L∞-norm (max norm) & Frobenius norm</strong></p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/09975601a17c4c8699515c9bbc9e26b0><p class=pgc-img-caption></p></div><h1>相似度</h1><p><strong>Jaccard相似度</strong></p><p>Jaccard相似度測量交集大小與並集大小之間的比率。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/836d79022150426cb1a926bc082cd8a5><p class=pgc-img-caption></p></div><p><strong>餘弦相似度</strong></p><p>餘弦相似度測量兩個矢量之間的角度。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/07aa468aa90f490291cab5820db18575><p class=pgc-img-caption></p></div><p><strong>皮爾遜相似度</strong></p><p>Pearson相關係數ρ測量兩個變量之間的相關性。</p><div class=pgc-img><img alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9cbbe91d818a4bf6818125ee1776cc19><p class=pgc-img-caption></p></div></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>總結</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/5199ece.html alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/76d31ab63a7249a5abaeec98d8891354 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/5199ece.html title=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等>機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>