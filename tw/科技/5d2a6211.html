<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>神經網絡與圖靈機的複雜度博弈 | 极客快訊</title><meta property="og:title" content="神經網絡與圖靈機的複雜度博弈 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/4af200040ff1f5233c1c"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/5d2a6211.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/5d2a6211.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/5d2a6211.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/5d2a6211.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/5d2a6211.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/5d2a6211.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/5d2a6211.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/5d2a6211.html><meta property="article:published_time" content="2020-11-14T21:02:50+08:00"><meta property="article:modified_time" content="2020-11-14T21:02:50+08:00"><meta name=Keywords content><meta name=description content="神經網絡與圖靈機的複雜度博弈"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/5d2a6211.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>神經網絡與圖靈機的複雜度博弈</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4af200040ff1f5233c1c></p><p>1931年，天才數學家圖靈提出了著名的圖靈機模型，它奠定了人工智能的數學基礎。1943年，麥克洛克 & 皮茨（McCulloch & Pitts）兩人提出了著名的人工神經元模型，該模型一直沿用至今，它奠定了所有深度學習模型的基礎。那麼，這兩個開山之作究竟是怎樣一種相愛相殺的關係呢？天才數學家馮諾依曼指出，<strong>圖靈機和神經元本質上雖然彼此等價，我們可以用圖靈機模擬神經元，也可以用神經元模擬圖靈機，但是它們卻位於複雜度王國中的不同領地。</strong>神經網絡代表了一大類擅長並行計算的複雜系統，它們自身的結構就構成了對自己最簡潔的編碼；而圖靈機則代表了另一類以穿行方式計算的複雜系統，這些系統以通用圖靈機作為複雜度的分水嶺：一邊，系統的行為可以被更短的代碼描述；另一邊，我們卻無法繞過複雜度的溝壑。<strong>而先進的深度學習研究正在試圖將這兩類系統合成為一個：神經圖靈機。</strong></p><h1><strong>全書綱要：</strong></h1><p><strong>馮·諾依曼的遺產：尋找人工生命的理論根源</strong></p><p><strong>探尋計算的“原力”</strong></p><p>第二堂課：控制與信息理論</p><p>第三堂課：信息的統計理論</p><p>第四堂課：大數之道</p><p>第五堂課：複雜自動機的一些考量——關於層次與進化的問題</p><p>在翻譯過程中，做了以下的添加和修改：</p><p>1、為了方便閱讀，為原文進行了分段，並加上了段標題；</p><p>2、為了讓讀者感覺更親切，加上了若干副插圖。</p><p>3、為原文添加了大量的評論，東方和尚的評論和張江老師的評論都會標註出來，另外，因為這本書是馮·諾依曼的助手 Arthur W. Burks(遺傳算法之父 John Holland 的博士生導師)，所以在框中的文字是編者加的註解。大家要注意分辨。</p><h1><strong>概述</strong></h1><h1><strong>編者Arthur W. Burks注：</strong></h1><p>馮·諾依曼說信息理論包括兩大塊：嚴格的信息論和概率的信息論。以概率統計為基礎的信息理論大概對於現代計算機設計更加重要。但是在此之前，我們必須先弄清楚嚴格的信息論那部分，它其實就是形式邏輯的另一種處理方式。他接下來解釋了一些形式邏輯的基本概念，簡要地說明了真值函數的連接詞，比如“與”“非”“如果…那麼”“與非”以及它們之間的相互定義性。他還解釋了變量和量詞，包括“全稱”量詞和“存在”量詞。<strong>他的結論是“如果你有這樣的一臺計算機，就可以表達一切數學計算，或者能夠純粹地用數學計算表達出的任何主題。”</strong></p><p>對於計算機，我不打算深談。因為存在另外一種不一樣的機器，同信息理論也很有關係，就是 McCulloch & Pitts 提出的神經網絡理論，你可以說圖靈機和 McCulloch & Pitts 的神經網絡分別處於信息理論的兩個極端。這兩套理論都力圖建立一個公理化的體系，用某種假設的理想機器來建立形式邏輯系統，但並不實際去製作這樣的機器。他們都成功地說明了形式邏輯同他們設想的機器是完美兼容的，<strong>也就是說，機器能做到的一切工作，都能夠被形式邏輯所刻畫；反過來，任何能夠用形式邏輯描述的事物，也都能夠用這類機器來運行。</strong></p><p><strong>編者Arthur W. Burks注：</strong></p><p>馮·諾依曼這裡假設了 McCulloch & Pitts 的神經網絡有著一條無限長的紙帶，結果表明了它同圖靈機的等價性。這個結果也就是圖靈可計算性、函數的 λ 可定義性、以及一般遞歸的概念。請參見圖靈的論文“可計算性和 λ 可定義性（Computability and λ-Definability）”</p><p>我會簡單地介紹 McCulloch & Pitts 的神經網絡以及圖靈的工作。因為它們各自代表了一種重要的研究方式：組合方法和整體方法。McCulloch & Pitts 描述了一套方法，由非常簡單的零件組成複雜結構。因此只需要對底層的零件作公理化定義就可以得到非常複雜的組合；圖靈則是對於整個自動機進行了公理化的定義，他僅僅定義了自動機的功能，並沒有涉及到具體的零件。</p><p><strong>McCulloch & Pitts 的神經網絡</strong></p><p>McCulloch & Pitts 的工作具有很明確的目的，就是為了研究人類的神經系統而提出一套簡單的數學邏輯模型。結果驚喜地發現這個模型跟形式邏輯等價，這部分實現了 McCulloch &Pitts 的研究目標，但僅僅是一部分。他們的模型研究的是神經本身，我之前也對這個問題很感興趣。為了能夠切入本質，而不被神經系統的生理和化學的具體細節所幹擾，他們使用了一套公理化方法，用幾個簡單的假設來描述神經的行為。在此，我們暫時先不管這些假設具體是怎樣實現的，同實際又是否一致。</p><p>他們接下來又對神經元作了進一步的理想處理。這一步引來了許多指責，但在我看來卻是完全合理的。McCulloch & Pitts 並沒有按照實際的情形去公理化神經元，而是設計了一種完全理想化的模型，它要比實際的簡單許多。他們相信，這一極度簡化的模型包括了神經元最核心的那些特性；而所有的其它性質只是無關緊要的細節，在初步研究中應當忽略。我認為科學界還需要很長的時間，才能完全認同他們的觀點。我們並不知道在簡化模型中被忽略的那些細節是否真的無關緊要，但至少，在理想化處理以後，神經網絡變得相當容易理解了。</p><p><strong>神經元的定義如下：我們也許應該把它叫做形式神經元，因為它並不是真正的神經細胞，而是僅僅具有神經細胞的一些關鍵性質。我們用一個小圓圈代表一個神經元，從圓圈延伸出的直線則代表神經突觸。箭頭表示某神經元的突觸作用於另一個神經元之上，也就是信號的傳送方向。神經元有兩個狀態：激發和非激發。</strong>我們無需關心激發具體是什麼意思，因為激發的主要性質通過實際運行來體現。這兒有一點點循環定義：所謂激發態，就是能夠激發別的神經元的狀態。大致在神經網絡的末端，處於激發態的神經元會激發某些非神經元的東西來實現其目的。比如，肌肉會受刺激，從而產生身體的運動；或者刺激某種腺體產生化學變化，分泌荷爾蒙之類。所以，神經激發最終產生的結果是我們研究範圍以外的事情，我們不考慮這類現象。</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4af4000233bfccb27c59></p><p class=pgc-img-caption><strong>McCulloch-Pitts 的神經元模型（譯者加）</strong></p><p><strong>編者Arthur W. Burks注：</strong></p><p>馮·諾依曼說明了控制神經相互作用的原理：根據 McCulloch & Pitts 模型，他也假設神經激發之間有一段均勻的延遲，雖然他們暫且不考慮“神經疲勞這一重要現象，也就是說，神經被激活之後，有一段時間因疲勞而對刺激失去反應”，這種神經疲勞對於生命功能有著重要的作用。但是，就算存在疲勞因素，我們還是可以用串聯神經元的辦法來實現連續的運動。馮·諾依曼還定義了神經閾值，提出了抑制性突觸的概念，用小圈（而不是箭頭）來表示[48]接下來，馮·諾依曼講解了“McCulloch & Pitts 的重要結論”，想象有一個具有若干輸入和唯一輸出的黑箱。再選擇兩個時間點：t1 和 t2，並確定從 t1 到 t2 何種輸入組合會導致輸出，何種輸入得不到輸出。</p><p>不論你如何選擇這些條件，總可以在黑箱裡設計一套神經網絡來實現這些條件。這意味著神經網絡在一般意義上是同符號邏輯等價的。這是一個事實，即任何被設計出來的系統都可以用有限長度的描述來嚴格表示，因此他們是等價的。這裡我不打算證明它，因為任何形式系統的證明都是冗長且困難的。</p><p><strong>編者Arthur W. Burks注：</strong></p><p>這裡簡單說一下證明的方法，按照馮·諾依曼所說，所有的開關函數（真值函數、布爾函數）都可以用確定延遲的神經網絡來實現。利用循環的方法，我們可以用神經元來實現任意容量的內存，並附加到以上的開關函數上。這就相當於給一套符號邏輯加上了一根無限長的紙帶，也就是實現了圖靈機。因此，對於每一臺圖靈機 M，總可以製造一套等價的神經網絡，計算得到同樣的數字。</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4af0000470b1a0a80f52></p><p class=pgc-img-caption><strong>抑制不能傳遞的神經網絡</strong></p><p><strong>編者Arthur W. Burks注：</strong></p><p>馮·諾依曼展示瞭如何建立一些樣例網絡。如上圖，a 可以抑制 b，b 可以抑制 c，c 又可以抑制 a。如果輸入端被激發（箭頭處輸入），且抑制端（箭頭處輸入）沒有激發，則神經元進入激活狀態。根據此規則，如果 a 和 b 都被激發，那麼 α 會激發，而 β 不激發；如果 b和 c 都被激發，那麼 β 會激發，而 γ 不激發；並且，如果 a 和 c 都被激發，那麼 γ 會激發，而 α 不激發。馮·諾依曼利用這個例子來說明人類行為的不確定特性，比如 a 比 b 強，b 比 c 強，照理說應該 a 比 c 強。但是在上面的神經網絡中間，雖然 a 比 b 強，b 比 c 強，c 卻又比 a強。</p><p>下面，馮·諾依曼組合基本神經元得到了幾個其他的神經網絡：簡單的存儲器，計數器，基本的學習電路。也就是《馮·諾依曼著作選》5.342-345 中的電路。學習電路有兩個輸入端，a和 b。如果 b 接著 a 之後受到一次刺激，則該電路會記錄一次，當這個數字達到 256 以後，無論 a 是否受到刺激，一旦 b 受到刺激，電路都會發出一次脈衝。</p><p>請大家注意，這些電路雖然看上去很複雜，但是從它們的組合方式去觀察卻很簡單。而且它們的複雜度與語法的複雜度相當。如果你能用一句話來描述它們的功能，那麼你就能毫不費力地把電路圖畫出來。其實，McCulloch & Pitts 得到的結論也就是圖形描述和語義描述沒有太大的區別。中繼元件的組合描述是和嚴格的語義描述完全兼容的。</p><p>我想闡述一下這個觀點的哲學引申。<strong>根據上述觀點，</strong><strong>可以推論：只要能夠用人類語言描述的東西，都能夠用神經元方法表示出來。同時，神經元並不需要有超常的聰明程度及複雜程度。</strong>事實上，這種理想的模型甚至不必有真實的神經元那麼聰明或複雜，因為簡化後的神經元，雖然以死板的、缺乏變化的方式運行，卻已經能夠做到一切你想得到的事情了。另一方面，McCulloch & Pitts 沒有說的事情當然也同樣重要。首先，不是說我們這樣設計出來的電路一定存在於自然界中。其次，不是說在這個簡化處理中，神經元被忽略的那些功能是無關緊要的。最後，雖然理論上我們可以用神經元電路描述任何事情，但這並不是說實際上得到這樣的描述就會很簡單。換個角度講，就像人類神經活動一樣，也許你能夠理解所有單個神經元的運作原理，但是你仍然解釋不了這些神經元的整體湧現結果。</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4af10004752302395a3d></p><p class=pgc-img-caption><strong>沒有輪廓的三角形，但是你的眼睛卻可以幫你勾勒出它的輪廓（譯者加）</strong></p><p>當你看到一個三角形的時候，你認識到“這是一個三角形”，知道它的大小。怎麼解釋這個事實呢？三角形的幾何定義是很簡單的，就是三根直線的組合。但是實際上，三角形的辨認是非常任意的。比如三條邊是彎曲的、只能看到三角形的頂點（插入一張三角形的視覺補償圖片）、三角形用陰影表達等等，在這些情況下你都可以認出其中的三角形來。但是，如果你要把所有的這些情況都放到三角形的程序描述中，那麼這個描述就會變得無限的冗長。</p><p>不僅如此，辨認三角形的能力僅僅是你辨認幾何圖形的能力中間的極小一部分，幾何圖形的辨認能力又只不過是視覺認知能力的極小一部分。理論上，你都可以給這些能力寫出具體的描述出來。但是這並不是解讀一張圖片真正的視覺機制，因為要“進入”一張圖片之後，你才能把意義賦予它。這是超越圖形範圍以外的事情，因此不能夠用以上的幾何圖形方式來描述。比如給你看一張墨水染成的抽象圖案，每個人都會把自己的理解放進圖片中，但是這些放進去的東西取決於這個人過去的經歷和他的人格，而不能在圖片本身中找到。所以心理醫生可以利用此類方法來對人進行心理測試。</p><p>這些看上去好像是一些零碎的事情，但基本的事實是：人的大腦超乎想象的複雜，其中有五分之一都是用於視覺處理的。所以，按照我們之前的估算，大約有 20 億個神經元是專門用於建立視覺圖形的。除了直接描述視覺大腦以外，現在我們還沒有別的更簡單的辦法去描述什麼成分構成了視覺類比（visual analogy）。</p><p><strong>不同層次的複雜性</strong></p><p>一般地說，自動機的文字描述應該比給出自動機的全部設計圖紙要簡單。但是這個結論也不是在所有情況下都成立。<strong>在形式邏輯中，只要自動機的結構不是非常複雜，那麼它的文字描述的確要比自動機本身簡單。但在複雜度很高的情形下，實際的自動機結構反而要比它的描述更簡單一些。</strong></p><p>我實際上正在用自己的方式解釋一條非常漂亮的邏輯定理，這是由哥德爾曾經提出的一條定理：<strong>從邏輯上說，對於一個對象的描述要比這個對象本身高一個級別。因此，漸進地說，前者總是比後者要長。</strong>我覺得為了理解這種性質，必須運用上述定理，因為在你去深入研究這件事情的時候，你已經觸及到複雜度這個概念的本質。有理由懷疑，在一切模糊的、難以定義的事物背後，都存在著複雜度漸近的性質（比如“看上去相似是什麼意思？”），給人的感覺是，不管你用多少文字，都沒有辦法把問題徹底描述清楚。這類事物就屬於高複雜度的情況，也就是實際去做，讓機器實際去運行，反而比直接描述它要更快。因此，在這種情況下電路本身的執行要比描述出電路所有的功能和可能的條件更快。形式神經網絡能夠做任何你能用文字描述出來的事情，這個理念非常重要，在複雜度不太高的時候，這種處理方法能夠非常大程度地簡化問題。但是在複雜度很高的情形下，定理所描述的情況則完全有可能相反，也就是邏輯電路反而比文字形容更加簡單。</p><p><strong>編者Arthur W. Burks注：</strong></p><p>馮紐曼在討論完圖靈機之後再次提到了這個問題。馮紐曼接著舉了兩個例子，說明理想化的神經元並不能解釋實際的神經細胞是如何實現某一個具體功能的。在第一個例子中，一束神經元被用於傳送某一個連續的數字，用來代表血壓之類的身體指標。神經元並不是用二進制的方法來表示這個數字，而是以發射的頻率來代表血壓的高低，血壓越高則發射越頻繁。可以用神經疲勞來解釋這個現象，一旦神經發射之後，會有一段時間的疲勞期，在其中，神經細胞不應激，而之後的刺激越強，則神經細胞離開疲勞期應激的速度就越快。接下來的問題是“為什麼自然界從未使用二進制，而是使用脈衝頻率的方法來表示大小呢？”，馮紐曼說他對這類問題很感興趣。他提出了一個猜測：頻率調製的表示方法比二進制要更加可靠，請參見下面的1.1.2.3節，以及《計算機與人腦》的77-79頁，還有馮紐曼的著作選集5.306-308以及5.375-376。<strong>有關記憶</strong></p><p><strong>編者Arthur W. Burks注：</strong></p><p>第二個理想神經元同實際不符的例子是關於記憶的實現。馮紐曼之前用理想神經元建立過存儲器電路。他說這樣的內存電路可以做到任意大。但他覺得，實際上自然並不是用這樣的機制來實現記憶的。</p><p>這不是自然界實現記憶的機制，原因很簡單，像神經元這樣的開關元件很容易疲勞，因此要實現1bit的信息儲存，一個神經元是遠遠不夠的，可能要十幾個以上，這是一種嚴重的浪費，因為開關元件並不適合用作儲存信息。對於人工製造的計算機器，如世界第一臺計算機ENIAC中，內存不足一直是一個制約效率的嚴重問題。雖然ENIAC由龐大的2萬個電子真空管組成，它的內存容量卻只有可憐的20位數字。這個尷尬局面就是因為把真空管，或者說開關元件用來幹它不適合乾的工作所導致的。只要把內存實現的機制加以修改，計算機器的效率就可以大大提升[49]。</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4aef00047e2b49692bb4></p><p class=pgc-img-caption><strong>神經元結構</strong></p><p>人類神經系統的內存需求可能是非常巨大的，估計其數量級為10的16次方。關於這個估計本身是否合理，我不想多討論，因為說不清楚。但是我相信，因為人腦神經元的數量僅為 10 的 10 次方，遠不足以儲存這樣巨大的記憶容量，我們最好老老實實地承認，對於記憶究竟是什麼，我們還一無所知。當然我們可以做各種各樣的猜測，比如認為記憶是神經突觸的某種細微變化，因此不受上述數量級的限制等等。我不知道有沒有什麼證據證明這種猜測，但是我想應該沒有。你可能懷疑神經細胞遠不僅僅是開關電路那麼簡單，多餘的功能就包括記憶的實現，但是事實上我們什麼也不知道。很可能記憶的實現是同神經元是完全不同性質的機制[50]。</p><p><strong>尋找記憶元件的主要困難，是因為記憶似乎無處不在。</strong>很難在大腦中間簡單地定位任何東西，因為大腦具有強大的自組織能力。即便是你找到某一個特定部位所對應的功能，在手術切除大腦的相關部位之後，很可能大腦會進行重新組織，重新分配各部分的職責，從而使得這種功能因此又得到了恢復。大腦具有相當強大的靈活適應能力，因此難以給不同部位確定具體的功能，我懷疑在所有的大腦功能中間，記憶是分佈得最廣泛的。[參見：《計算機與人腦》63-68 頁[51]</p><p><strong>五、圖靈機</strong></p><p>我這裡想要提兩件事<strong>[疲勞和內存]</strong>，在 McCulloch & Pitts 對神經系統的分析中間，明顯沒有提到它們，之後再說一說圖靈機。McCulloch & Pitts 最後得出結論，認為公理化的神經元體系同形式邏輯是等價的。圖靈則得到了反向的結果，圖靈感興趣的是形式邏輯而不是自動機。他想要證明形式邏輯中的一個重要猜想，也就是所謂的可計算判定問題。<strong>這個問題是問，對於一類邏輯表達，是否存在著一套固定方法來判定這個表達式的真假。圖靈之所以討論自動機，其實是為了給他的可計算性證明提供一個鋪墊，讓這個形式系統中的理論顯得更加清楚和前後一致。</strong></p><p><strong>編者Arthur W. Burks注：</strong></p><p>接下來，馮·諾依曼簡述了圖靈自動機的定義。McCulloch & Pitts 自動機的基本結構是神經元，圖靈機則是由不同狀態組成的，任何時候，圖靈機都處在有限的狀態組合中的某一狀態中；“外部世界”則是由紙帶所代表。每個運行週期中，自動機讀入一格紙帶，可能修改這一格的內容，然後把紙帶向左或者向右移動一格。圖靈機按照預定的規則字典運行，其中規定了對於不同的狀態，讀入不同的符號時候下一個狀態為何，並且在這一格寫入何種符號。紙帶有特殊的一格，表明了開始位置，有限長度的程序可以預先寫在紙帶上，計算所得的二進制結果則寫在開始位置之後。</p><p>接著馮·諾依曼說到了圖靈關於有限自動機的結論：<strong>存在通用圖靈機AU，具有以下性質：對於任何圖靈機 A 和程序 I，存在模擬程序 IA，只要提供 IA 和 I 給AU，AU 就可以模擬 A 的運算，輸出和A 執行 I 得到的相同結果。</strong></p><p>AU 具有模擬任何圖靈機去執行任何指令的能力，哪怕這個圖靈機比 AU 本身要複雜得多也沒關係。因為通用圖靈機本身的複雜性不足，可以由模擬器程序包含的內容來填補。<strong>這項研究的深刻之處在於，只要把通用圖靈機恰當地製作出來，它就能夠完成任意複雜的工作，因為其他的各種需求可以通過編寫程序來滿足。</strong>但是，只有當任意的圖靈機 A 具有最起碼的複雜度，從而能夠實現通用圖靈機的時候，它才能有這樣的能力。換句話說，沒有達到通用圖靈機複雜度的系統，不管你給它寫什麼程序，都是不可能完成某些工作的。但是一旦複雜性超越了某一個確定的閾值，只要給予適當的程序，所有的自動機都能夠相互模擬了[52]。</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4af200040ff010499439></p><p class=pgc-img-caption><strong>圖靈機模型（譯者加）</strong></p><p><strong>編者Arthur W. Burks注：</strong></p><p>馮·諾依曼跟著解釋，通用圖靈機 AU 是怎麼模擬任意一個圖靈機 A 的。模擬程序 IA 就像一本對照詞典一樣，包括了所有 A 的狀態和每一個符號的組合，說明在這個情形下，A 的下 一個狀態是什麼，在紙帶上應該寫入什麼符號。AU 按照這本詞典運行，就可以得到跟 A 一 模一樣的輸出了。</p><p>到此，我想指出，人類第一次具有了某種通用的工具，理論上說，只要任何人能夠做到的事，這種工具也能復現此過程。這裡並沒有無中生有，憑空製造複雜度， 因為程序還是得具體的寫出來。而且，實現通用計算的這一過程也是基於嚴格的計算理論的， 亦即關於怎樣描述對象以及如何在字典中間查閱條目並執行的過程。</p><p><strong>不僅如此，圖靈還發現了自動機的極限。也就是說，無法制造這樣一種自動機，這臺自動機能夠預測其他自動機在有限時間內是否能解決某個問題，完成停機。</strong>換句話說，雖然你可以製造能夠模擬任何圖靈機的通用圖靈機，你卻無法制造可以預測任何圖靈機運行結果的“預測機”。做過的事情可以重複，但是沒有做過的事情，卻沒有辦法預測[53]。</p><p>以上結論同形式邏輯的結構本身具有深刻的聯繫。雖然在這堂課上不能深談，我想說一下大概的意思，以便熟悉形式邏輯的人去研究。這一點跟類型論（theory of types）和哥德爾定理有關，你可以在一個邏輯類型中間做任何可行的事情。但是一件事情是否可行這個問題本身，卻屬於高一個層次的邏輯類型。我之前提過，低複雜度的東西，很容易預先判斷它的行為。但是對於高複雜度的形式邏輯對象，卻很難提前預測它的行為，最好的辦法就是把它實際製造出來運行。在此情形下，一個問題的有效範圍要比問題本身處在高一個層次的類型。</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4af0000470b453c8fd06></p><p class=pgc-img-caption><strong>通用圖靈機工作原理示意圖（譯者加）</strong></p><p><strong>通用圖靈機可以被看作是一個可以變身為其它任意一臺圖靈機（甚至包括通用圖靈機它自己）的機器。只要給它輸入適當的指令，它就可以原封不動地模擬其它圖靈機（M,M’,M’’）的工作。</strong>我們通常用的通用可編程計算機實際上就是通用圖靈機，程序員給機器輸入的程序就相當於是任意一臺特定的圖靈機（例如求解 1+1 的值得圖靈機）的描述。</p><p><strong>補充材料：Arthur Burks 與 Kurt Godel 的通信</strong></p><blockquote><p><strong>補充材料內容均來自編者編者Arthur W. Burks注。</strong></p></blockquote><p>馮·諾依曼的第二堂課到此結束，關於最後兩段，編者想做一點補充[54]。</p><p>在課程最後一段中，馮·諾依曼提到了哥德爾的一條定理“你可以在一個邏輯類型中間做任何可行的事情。但是一件事情是否可行這個問題本身，卻屬於高一個層次的類型。”因為編者不知道哥德爾曾得到過這樣的定理，這段話顯得十分費解。另外他在之前引用的有關哥德爾的討論（47 頁），還包括馮·諾依曼的希克森研討會上的文章，“自動機的一般和邏輯的理論”（參見馮·諾依曼文集的5.310-311）。所以我給哥德爾教授寫信詢問此事，不過我認為他給出的答案是關於參考文獻最模糊的解釋。因此，編者把與哥德爾的通信稍作編輯，附在下面：</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4af100047526e64442b0></p><p><strong>我的信件如下：</strong></p><blockquote><p>鄙人正在編注已故馮·諾依曼教授關於自動機原理的未完成手稿，在 1949年伊利諾伊大學的以及此前的希克森會議上，他都引用了您的著作，但我卻查不到出處，因為有可能馮·諾依曼生前曾同您談及過相關問題，我冒昧給您致函，請求指點。</p><p>手稿的背景如下：馮·諾依曼當時在討論視覺識別，他說可以把人的眼睛和神經系統看成一部確定的有限自動機。他提出如下的可能，即描述這類自動機的行為的最簡方式，是把自動機的結構依樣畫葫蘆地描述出來。這是令人信服的。但是我不能明白他接下來說的話：“在這個範圍內，有可能實際對象本身就是它自己的最簡單描述。這是說，任何用文字或者形式邏輯來描述一個對象的嘗試，都可能導致比對象本身更加複雜，牽涉到更多關係的結果，也就是越說越複雜。現代邏輯研究中的某些結論暗示了，在遇到高度複雜的對象的時候，這樣的奇特性質反而是正常的。”這裡似乎提到了您的工作，鄙人隨信附上全文，敬請參閱。</p><p>在伊利諾伊大學的課上，馮·諾依曼也提出了類似的觀點：視覺產生過程最簡單的精確描述方法，就是把大腦的視覺部分神經元連接羅列出來。他接下來說，對於形式邏輯的研究表明，當自動機不太複雜的時候，對於自動機功能的描述要比自動機本身簡單。但對於複雜自動機，情況卻相反。他這裡指名道姓提到了您的定理‚我實際上正在曲解一條邏輯定理，但是這個定理本身是非常漂亮的。這是由哥德爾曾經提出的一條定理：從邏輯上說，對於一個對象的描述要比這個對象本身高一個級別。因此，漸進地說，前者總是比後者要長。我覺得為了理解這種性質，必須運用上述定理，因為在你去深入研究這件事情的時候，你已經觸及到複雜度這個概念的本質。</p><p>此後，在敘述了圖靈機以及圖靈對於停機問題不可判定性的論述之後，他再次回到這個問題上面。他說這些問題跟類型論和您的研究結果有著深刻聯繫。此處的錄音記錄有些亂，我儘量保留了原文：這一點跟類型論和哥德爾定理有關，你可以在一個邏輯類型中間做任何可行的事情。但是一件事情是否可行這個問題本身，卻屬於高一個層次的類型。我之前提過，低複雜度的東西，很容易預先判斷它的行為。但是高複雜度的形式邏輯對象，卻很難提前預測它的行為，最好的辦法就是把它實際製造出來運行。在此情形下，一個問題的有效範圍要比問題本身處在高一個層次的類型。</p><p>鄙人能夠理解一個對象的描述要比對象本身高一個類型層次。但是，除此以外，我不明白馮·諾依曼的意圖何在。我有兩個想法，但是似乎都不能夠自圓其說。一是把哥德爾配數當作一個公式的描述來對待。但是，至少在某些情況下，哥德爾配數的長度是可以小於公式的長度的，否則自指的不可判定的公式就是不可能的了。另外一種可能的理解是同您 1936年的論文《關於證明的長度》中間提出的證明長度定理有關。有兩個系統：S和較大的系統 S1，您的定理說：對於每一個遞歸函數 F，在兩個系統下都存在一個可證明的陳述使得在這兩個系統下的最短的證明滿足一個不等式，也就是在小系統 S下的證明的哥德爾配數要比 F作用在大系統 S1 中間證明的哥德爾配數大。這符合馮·諾依曼說的結論，問題是結論正好相反：類型越高，證明越短而不是越長。</p><p>馮·諾依曼教授的這段敘述令我十分迷惑不解，如您能不吝賜教，鄙人不勝感激。</p></blockquote><p><strong>哥德爾的覆信如下：</strong></p><blockquote><p>我對於馮·諾依曼當時的意圖，大致有了一些概念，但是因為我從未和他談及這些問題，這僅僅是我個人的猜測。</p><p>我認為馮·諾依曼提到在下的定理的時候，並非是指存在不可判定的斷言或者證明長度的問題。而是一個認識論上的事實，即語言 A的完全描述是無法用語言 A本身來給出的，這是因為在 A裡面，沒有辦法定義句子的真假。這條定理才是包含算術的形式系統不可判定性的真正原因。但在我 1931年的論文55中間沒有提出這條定理，直到 1934年我在普林斯頓的講課上才正式提出。同樣性質的定理也由塔斯基於 1933年提出[56]</p><p>上述定理清楚地表明，在某些情況下，相比對於機制本身，對機制會做什麼描述要牽涉更多的東西[57]。在這個意義上，它需要用更加抽象的術語來表達，也就是更高的層次。</p><p>但這僅僅是抽象的程度，同需要用到的符號數量沒有關係。正如您所說的，符號數量和層次高低可能是成反比的。但是，如果從通用圖靈機角度來看，馮·諾依曼的想法會變得更加清楚。對於圖靈機，我們可以說其行為的完全描述是無窮大的。因為既然不存在通用的停機判定方法，那麼要描述一臺具體圖靈機的行為，只有把其所有的狀態枚舉出來才行。當然，為了不陷入無限大帶來的困難，我們這裡做了一個假定，即只有可判定的描述才是完全的描述。通用圖靈機的兩種複雜度之比為無窮大，同時它又可以被當作其他各種有限機制的一種極限，這立即就導出了馮·諾依曼的結論[58]。”</p></blockquote><p><strong>Jake 點評</strong></p><p>馮·諾依曼在這一章花了大量的篇幅介紹了 McCulloch & Pitts 的人工神經元模型以及圖靈的圖靈機模型。在今天看來，這兩套模型都已經成為了教科書中的經典，甚至已經略顯過時。</p><p>但是，jake 認為，這一章最精彩的部分並不在這裡，而在於馮·諾依曼提出來的描述層次，以及不同層次的描述複雜性的問題。包括編者 Arthur Burks 就此問題請教著名的數理邏輯學家庫特.哥德爾，和哥德爾給出的回信與評論。</p><p>實際上，僅僅從字面意思上，馮·諾依曼在一堂課上同時介紹了 McCulloch & Pitts 的人工神經元模型以及圖靈機模型就已經為此問題埋下了伏筆。我們不妨思考這樣一個問題：<strong>對於一個複雜的人工神經網絡，我們如何造出一臺圖靈機來精確地模擬這個神經網絡的行為？</strong>首先，根據正文的敘述，McCulloch & Pitts 已經證明了人工神經網絡等價於通用計算機，所以必然可以用圖靈機來模擬<strong>。但是，這裡的關鍵問題是：如果我們造出了這臺模擬神經網絡的圖靈機，它的複雜度（簡單地可以理解為描述機器行為的代碼長度）與原神經網絡相比誰大誰小呢？</strong></p><p>我們知道圖靈機的運作是一種串行的方式，每一個時刻都由那個笨重的機器頭——這個中心集權者進行精細的控制操作；而人工神經網絡則是一種並行運作的機器，每一個時刻，決策的權力都下放到整個系統中所有的神經元，網絡整體所完成的運算是所有神經元的集體行為決定的。因此，如果要把這種複雜的集體行為“壓縮到”一臺串行運作的圖靈機的行為上面，勢必會大大增加圖靈機運算的複雜度。因而，馮·諾依曼傾向於認為，<strong>在這種情況下，對人工神經網絡行為最好的模擬無非是運行這個網絡本身。換句話說，神經網絡行為的最短描述長度就是它自身的代碼長度。</strong></p><p>這樣，我們實際上可以畫出一張有關自動機複雜度的示意圖：</p><p>行為複雜度</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/4af10004752a0943dc49></p><p>如圖所示，我們可以用一臺自動機的結構複雜度（用多少代碼來實現這個自動機）和它運行之後所表現出的行為複雜度兩個數軸來表示自動機。對於一般的自動機來說，它的行為複雜度會隨著它的結構複雜度增加而增加（不過這個結論似乎存在著反例，例如一維的元胞自動機，結構複雜度很小，但是其行為複雜度很大）。但是這種變化曲線並不是簡單線性的，而是存在著某個閾值點（C 點），這就是馮·諾依曼在書中反覆提到的複雜度閾值。當自動機的結構複雜度超過一定的閾值之後，自動機的行為會變得異常複雜而難以預測。</p><p>下面的條狀區域就是對所有自動機根據複雜度而完成的分類。在複雜度閾值 C 左側的自動機為普通的簡單圖靈機（例如計算 2+100 的圖靈機）；C 右側的自動機為諸如 McCulloch &Pitts 神經網絡的複雜自動機。而這個閾值 C 對應什麼樣的自動機呢？在本章中，馮·諾依曼懷疑通用圖靈機有可能位於這個 C 附近，因為當一個自動機的結構複雜度超過了實現通用圖靈機的複雜度的時候，它就可以通過吸收不同的輸入代碼而“變身”成任意特定的圖靈機。而在後續的章節中，馮·諾依曼懷疑這個 C 對應的恰恰是自複製自動機。</p><p>在編者與哥德爾的通信中，以及關於馮·諾依曼所說的不同層次的複雜度的討論中，我們可以明顯看出，馮·諾依曼暗含瞭如下的一種猜想：<strong>儘管我們還不知道這個複雜度閾值 C 是什麼，但是似乎我們可以比較有把握地斷定，這個 C 值點恰恰與自指——這個構成了圖靈停機問題、哥德爾定理以及程序自複製問題的核心概念有關。</strong></p><p>總之，我認為這個不同層次的複雜性的問題是馮·諾依曼在本章給我們留下的一個非常寶貴的遺產，也是理解複雜性問題的核心的地帶。但是由於我能力有限，還不足以回答馮·諾依曼這裡的問題，不過我這幾年來一直圍繞著這個問題展開探索，在此，我願意把瞭解到的相關的概念（更重要的是參考文獻）列舉出來，希望能夠起到拋磚引玉的作用。</p><p>1、Kolmogorov 複雜性</p><p>馮·諾依曼的這本書的出版是 1966 年，就在幾乎同一年，Kolmogorov 在 1965 年和 Chaitin在 1966 年提出了“Kolmogorov Complexity”或者也叫“算法複雜性（Algorithmic Complexity）”的概念。我不知道 Kolmogorov 等人是不是瞭解馮·諾依曼關於自動機複雜度，特別是複雜度閾值的討論，但是，它們要解決的問題似乎是同樣的。</p><p>Kolmogorov 複雜度定義在所有的 01 序列上。一個序列的複雜度定義為能夠生成這個序列的所有圖靈機（遞歸函數）程序中最短的圖靈機的代碼長度。這樣，序列的複雜度就對應為它的可壓縮性。對於一個隨機序列來說，我們找不到一段程序來生成它，於是我們只能用最笨的方法：把這個序列逐字的打印出來。因此，它的Kolmogorov 複雜性就是序列自己的長度。而對於一個有規則的序列，例如010101„„重複200 遍，我們可以寫出一個程序為：“Print(‘01’ 200 times)”，這個程序的代碼長度是 21，於是該序列的 Kolmogorov 複雜度就是在 log(200)數量級上。</p><p>關於 Kolmogorov 複雜性，我推薦這本書：Li, Ming and Vitányi, Paul, An Introduction toKolmogorov Complexity and Its Applications, Springer, 1997。</p><p>我們看到，Kolmogorov 複雜性已經非常接近馮·諾依曼要探索的自動機的複雜度的概念了。</p><p>2、自指與跨層次</p><p>在正文中，馮·諾依曼以及哥德爾曾多次提到了有關“邏輯類型”、“命題的真偽在形式系統內部無法判定”等問題。這其實都已經牽扯到了自指以及跨層次的纏結等問題（參見侯世達：《哥德爾、埃舍爾、巴赫——一集異璧之大成》，1994 商務印書館）。而對這些問題，jake已經做過一定的探索，並總結在了《系統中的觀察者 7：自指——連接圖形和襯底的金帶》這篇文章中，在此不再贅述<strong>（點擊閱讀原文查看）</strong>。</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4af4000233c4c90df2f3></p><p class=pgc-img-caption><strong>自噬的蛇</strong></p><p>在這裡，我想進一步指出的是，傳統的有關自指與跨層次纏結的問題多針對的是邏輯結論，例如一個命題是否可以被證明，一個圖靈機是否能停機，等等，這些問題都僅僅有“是或否”兩種答案。就我所知，我們還沒有看到有關這種自指與纏結層次的定量研究，例如產生悖論的概率、停機的概率有多大？也沒有將複雜度與自指聯繫起來的研究：例如如果要真正實現一個自指句子（或者自指的圖靈機）它在不同層次的複雜度（程序長度）上有何關係？我想，馮·諾依曼在此呼喚的有關自動機的新理論恰恰就是這類自指或者跨層次纏結的定量理論。例如馮·諾依曼懷疑複雜度閾值恰恰就在自指的自動機中產生，而對於這類自動機來說，從高層次來描述它的複雜性（預測它的行為）比從低層次來描述它的複雜性（對自動機直接描述）更大。</p><p>就我所知，數學家、計算機科學家 Chaitin 的確考慮過類似的問題（參見：Chaitin: Information, Randomness & Incompleteness, World Scientific 1987; Chaitin: Information-Theoretic Incompleteness, World Scientific 1992）。讀者不妨參考。</p><p>3、不確定性原理</p><p>此處純粹是我對馮·諾依曼問題的一種猜測，沒有太多的根據，請讀者斟酌其中的結論。我們都知道，在量子力學中有一個著名的“不確定性原理”（Uncertainty Principle），它指出動量的波動與位置的波動滿足不等式：</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4af300040045ce07f16b></p><p>雖然這個不確定性原理是量子物理學家海森堡提出來的，但是它實際上深深植根於數學之中。從一般意義上來說，任何一個時間域上的函數 f(t)和它的傅立葉變換即頻率域上的函數 g(w)都存在著不確定性關係，而位置和動量上的概率波恰恰構成了傅立葉變換對，所以它們才存在著不確定性原理（請讀者參考 R. N. Bracewell 殷勤業(譯)：《傅立葉變換及其應用》，西安交通大學出版社）。</p><p>實際上，從時間域和頻率域上對同一個信號的描述無非是從兩種不同的視角來對信號進行描述，而不確定性原理則告訴我們，我們從原則上不可能對未知的信號同時在時間域和頻率域上都給出精確的測量。</p><p>在這裡，如果我們描述的對象是一個未知的序列，我們可以從序列展開本身的數值來描述，也可以從能夠生成該序列的圖靈機程序這個角度來描述它。</p><p>我們猜想，在這種對未知序列的不同角度的描述中也存在著不確定性原理，也就是我們不能同時提高對序列的直接描述和程序生成描述的精度。聯繫到描述的長度，我們知道，對序列的描述精度一般與程序的長度正相關，程序越長說明描述的越詳細，精度越高。這樣，我們也許可以得到類似不確定性原理的不等式：</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4af30004004798c3aaf1></p><p>即近似描述未知序列的圖靈機程序的代碼長度 Lp 加上從序列本身的角度進行描述的代碼長度 Lt 的和有一個下限值，我們不能同時提高這兩種代碼長度值。（之所以中間變成了加 號而不是乘號是因為熵與對應概率之間的對數關係）。也許，這種不確定性原理就是馮·諾依曼 在這裡討論的不同層次複雜度之間的關係的定量刻畫。也是自指性的一種定量的反映？</p><p>4、神經圖靈機</p><p>近些年來，基於人工神經網絡的深度學習有了突飛猛進的發展，人們逐漸發現，在McCulloch-Pitts模型基礎上引入更復雜的結構會大大提升人工神經網絡解決問題的能力。<strong>例如，2014年，Google的科學家們就提出了神經圖靈機模型，這種模型巧妙地將神經網絡和圖靈機結合到了一起。</strong></p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4af0000470b6a6d16280></p><p class=pgc-img-caption><strong>神經圖靈機示意圖</strong></p><p>融合方法的關鍵是利用了某種類似於“注意力”的機制，它能夠利用一種連續化的操作來對外部存儲單元進行“軟化”的讀或寫。注意力機制就是指一種動態地將權重分配在外部存儲單元上的方法。當某一項的權重為1，其它退化成0的時候，這就回歸到了經典的圖靈機模型。實驗表明，這種裝置在問題求解、問答、閱讀理解等任務上的能力表現要遠超其它的神經網絡模型。</p><p><strong>參考文獻</strong></p><blockquote><p><strong>以下譯者按均指東方和尚的註解</strong></p></blockquote><p>[48] 譯者注：神經在激發後會疲勞，在一段時間中間不能再次被激發，這一實際生理規律在理想模型中也是應該被考慮加入的。這樣可以解決神經網絡因為正反饋，而陷入簡單同步的死循環等問題。在 Hebb 定律提出之後不久，Milner 就提出了可變閾值、抑制和疲勞的概念，補充了神經網絡的特性。只有考慮了這些因素之後，神經網絡才能產生比較高級的湧現現象，如同步、預測、和層級等等，才能較好地對於外界進 行識別。可參見霍蘭著《湧現》的相關章節。</p><p>[49]譯者按：現代計算機內存有兩種，高速內存還是由集成電路中的微型晶體管實現，稱為SRAM；低速內存則由電容等機制實現，稱為DRAM。</p><p>[50]譯者按：現代神經科學研究證明記憶並不侷限於大腦某一部位，大腦的微觀工作機制，即細胞突觸形成、神經環路的建立，本質上就是一種學習和記憶過程；此外，由於一個神經元可以具有成千上萬個突觸，而記憶則包含於大量神經元組合之中，所以可以輕易突破正文中提到的 10 的 16 次方的內存容量限制。當然， 要把某一記憶和具體神經元建立確切聯繫，尚有大量工作要做。</p><p>[51]譯者按：現代神經醫學和診斷科學的發展，已經可以把大腦的單個神經元活動和認知過程進行實時的比對，證明大腦的不同部位的確有特定的功能，這種功能常常可以定位到具體的神經元上。同時，大腦恢復能力並沒有馮·諾依曼所說的那麼強大，功能喪失常常是永久性的。另一方面，記憶很可能的確是一種分佈式 計算。</p><p>[52]譯者按：以上通用圖靈機的概念非常深刻，是現代計算機的理論依據，因為只有明白了這點，才能製造可編程的計算機，而不是僅僅具有特定功能的計算器，從計算器到計算機，是對自然認識的一次飛躍。</p><p>[53] 譯者注：這裡提到的“預測機”無法實現的問題實際上牽扯到了可計算理論的不可判定問題類，對圖靈機行為的預測就是一種不可判定問題。關於更多的不可判定問題的討論，請參考這裡。</p><p>[54]譯者按：當時計算機還是“尖端科技”，其原理還顯得十分神祕，故須作此介紹。而今天圖靈機早己家喻戶曉，我們集智也進行過多次科普（請讀者參見：《圖靈機遇計算理論》以及《系統中的觀察者 7：自指——連接圖形和襯底的金帶》），圖靈機介紹部分茲省略不譯。下面有一段編者 Burks 教授同哥德爾本人的通訊來往，很有史料及科學價值，全文照譯，請讀者留意。</p><p>[55]譯者按：即哥德爾最著名的論文：《〈數學原理〉及有關係統中的形式不可判定命題》，其中提出了哥德爾定理。</p><p>[56]譯者按：即塔斯基定理，說在任何形式系統中間，真值的概念不能夠在這個系統內部定義。關於這段討論，讀者也可參看《系統中的觀察者 7：自指——連接圖形和襯底的金帶》一文。</p><p>[57]譯者按：比如可以十分簡潔地寫出某個算術表達式，但是這個斷言是否為真，可能會牽涉到比這個算術表達本身多得多的內容，例如哥德巴赫猜想就遠不僅僅是兩個數加起來那麼簡單）</p><p>[58] 譯者按：通用圖靈機可以模擬一切圖靈機，它的行為描述是無限複雜的，但是它本身的描述卻是具體的，是有限長度的，故對於圖靈機，行為複雜度/機制複雜度=∞，所有具體產生複雜度的機制，隨著其複雜度 的提高，最後都無限接近於圖靈機。這個問題非常值得深思！</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4af10004752b4a865958></p><p class=pgc-img-caption>作者：John von Neumann 20世紀最重要的數學家之一，在現代計算機、博弈論、核武器和生</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/4af200040ff47dad80fc></p><p class=pgc-img-caption>編者：Arthur W. Burks 馮·諾依曼的助手，遺傳算法之父 John Holland</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4af10004752c39abe789></p><p class=pgc-img-caption><strong>譯者：東方和尚 </strong>集智俱樂部神祕粉絲 曾與張江在集智俱樂部的網站上互相過（hu）招（dui）</p><p><img alt=神經網絡與圖靈機的複雜度博弈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4aef00047e2ee7ce3166></p><p class=pgc-img-caption><strong>注者：Jake張江 </strong>集智俱樂部創始人，集智AI學園創始人，北師大教授</p><h1><strong>推薦閱讀</strong></h1><p><strong><a href="http://m.toutiao.com/i6489702066292785678/?group_id=6489702066292785678&group_flags=0">馮·諾依曼：探尋計算的“原力”</a></strong></p><p><strong><a href="http://m.toutiao.com/i6487519558885179917/?group_id=6487894928364929294&group_flags=0">馮·諾依曼的遺產：尋找人工生命的理論根源</a></strong></p><p><strong><a href="http://m.toutiao.com/i6479300655797240333/?group_id=6479300655797240333&group_flags=0">Google AutoML項目：自創生理論的實踐</a></strong></p><p><strong><a href="http://m.toutiao.com/i6461164541827875342/?group_id=6461545548389286157&group_flags=0">為人類編程</a></strong></p><p><strong><a href="http://m.toutiao.com/i6464330791194198542/?group_id=6464330791194198542&group_flags=0">引力、量子與人工智能的深度對話｜尤亦莊</a></strong></p><p><strong><a href="http://m.toutiao.com/i6437649076480639490/?group_id=6438025533618913538&group_flags=0">人工智能社會學—未來的新興學科？——AI視野（四）｜張江</a></strong></p><p><strong>集智QQ群｜292641157</strong></p><p><strong>商務合作｜zhangqian@swarma.org</strong></p><p><strong>投稿轉載｜wangting@swarma.org</strong></p><h1><strong>◆ ◆ ◆</strong></h1><p><strong>搜索公眾號：集智俱樂部</strong></p><p><strong>加入“沒有圍牆的研究所”</strong></p><p>讓蘋果砸得更猛烈些吧！</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>神經</a></li><li><a>網絡</a></li><li><a>圖靈機</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html alt=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/65c4000bda98898dcdbb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html title=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼>谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2bc1496a.html alt=為了更好的深度神經網絡視覺，只需添加反饋（循環） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/17fccfd7096d44eeb3921bbd0dc29a13 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2bc1496a.html title=為了更好的深度神經網絡視覺，只需添加反饋（循環）>為了更好的深度神經網絡視覺，只需添加反饋（循環）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fd4c22a3.html alt=你還不知道神經網絡是啥？十分鐘教你跟上人工智能熱潮 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/470f0001d893b2ad09e2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fd4c22a3.html title=你還不知道神經網絡是啥？十分鐘教你跟上人工智能熱潮>你還不知道神經網絡是啥？十分鐘教你跟上人工智能熱潮</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cc9d1be9.html alt=基於二維材料、用於人工神經網絡的高密度憶阻陣列的晶圓級集成 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/3c0b503678da4b15be05f6f56c0d213f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cc9d1be9.html title=基於二維材料、用於人工神經網絡的高密度憶阻陣列的晶圓級集成>基於二維材料、用於人工神經網絡的高密度憶阻陣列的晶圓級集成</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/6062a4c0.html alt=BP神經網絡的線性本質的理解和剖析-卷積小白的隨機世界 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/6d474536ff3d4b1fba0cbfc85968ff6f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/6062a4c0.html title=BP神經網絡的線性本質的理解和剖析-卷積小白的隨機世界>BP神經網絡的線性本質的理解和剖析-卷積小白的隨機世界</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/f149efd9.html alt=用於調整深度神經網絡的簡單參考指南 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/15379529924702cde52ac04 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/f149efd9.html title=用於調整深度神經網絡的簡單參考指南>用於調整深度神經網絡的簡單參考指南</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0396dba3.html alt=貝葉斯神經網絡(系列)：第二篇 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RKYlnth9DPo8ac style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0396dba3.html title=貝葉斯神經網絡(系列)：第二篇>貝葉斯神經網絡(系列)：第二篇</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a4bbdd29.html alt=針對深度神經網絡的簡單黑盒對抗攻擊 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/b9ec712cd33442338496141ebfcecb45 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a4bbdd29.html title=針對深度神經網絡的簡單黑盒對抗攻擊>針對深度神經網絡的簡單黑盒對抗攻擊</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cafcc06.html alt=模式識別與神經網絡的發展 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1523254283784d3d276a90f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cafcc06.html title=模式識別與神經網絡的發展>模式識別與神經網絡的發展</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fcf9e89.html alt=BP神經網絡學習筆記 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/fc5cec456c184c48b1ee22a233b9ee0b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fcf9e89.html title=BP神經網絡學習筆記>BP神經網絡學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d7196c1.html alt=手工打造神經網絡：透視分析 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6ee200033390f3f6b2ca style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d7196c1.html title=手工打造神經網絡：透視分析>手工打造神經網絡：透視分析</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9f3924a.html alt=機器學習：神經網絡學習之多層前饋神經網絡（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a0a4cd0f7d9244a6a12da3c0af6893a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9f3924a.html title=機器學習：神經網絡學習之多層前饋神經網絡（一）>機器學習：神經網絡學習之多層前饋神經網絡（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/556321d.html alt=機器學習：神經網絡學習之多層前饋神經網絡（二） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/2d53a815-ab09-4da3-94a2-5b6843366e3a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/556321d.html title=機器學習：神經網絡學習之多層前饋神經網絡（二）>機器學習：神經網絡學習之多層前饋神經網絡（二）</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/f3732f4.html alt=一文幫你梳理清楚深度神經網絡的基礎知識！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2f16bcb220e14085a04994454ea4998a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/f3732f4.html title=一文幫你梳理清楚深度神經網絡的基礎知識！>一文幫你梳理清楚深度神經網絡的基礎知識！</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/2d00c65.html alt=理解神經網絡 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/15409758920775d7570f483 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/2d00c65.html title=理解神經網絡>理解神經網絡</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>