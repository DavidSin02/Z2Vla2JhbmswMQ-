<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>基於Transformer模型的中文文本自動校對研究 | 极客快訊</title><meta property="og:title" content="基於Transformer模型的中文文本自動校對研究 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/RnA5iHG5yimfrr"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/5e37e483.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/5e37e483.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/5e37e483.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/5e37e483.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/5e37e483.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/5e37e483.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/5e37e483.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/5e37e483.html><meta property="article:published_time" content="2020-11-14T21:05:26+08:00"><meta property="article:modified_time" content="2020-11-14T21:05:26+08:00"><meta name=Keywords content><meta name=description content="基於Transformer模型的中文文本自動校對研究"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/5e37e483.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>基於Transformer模型的中文文本自動校對研究</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>在如今信息量與日俱增的大數據時代，每天都會產生大量的文本信息，隨之而來的文本校對任務也越來越繁重，傳統的人工校對不僅校對週期長、勞動強度大，而且效率低，研究快速高效的自動校對方法是十分必要的。</p><p>基於概率統計的方法通過構建混淆集，分析目標詞語與其上下文詞語的同現特徵及搭配特徵，判斷哪些錯誤需要校對，並在混淆集中選取概率值最高的詞語替換目標詞語，從而進行文本校對<sup>[1]</sup>。然而，對於長距離的文本錯誤及語義錯誤，此類方法往往效果不佳。近年來，隨著技術的快速發展，將深度學習技術應用於中文文本自動校對任務成為了一種新的發展方向。</p><p>深度學習技術中的Transformer模型在Google於2017年6月發佈在arXiv上的一篇文章《Attention is all you need》中提出<sup>[2]</sup>，使用完全基於Self-Attention即“自注意力機制”的網絡結構，拋棄了傳統Encoder-Decoder即“編碼器-解碼器”模型必須使用RNN或CNN的固有模式<sup>[3]</sup>。本文首次將其應用於中文文本自動校對任務，實驗結果表明該模型可以較好地解決中文文本自動校對中的遠距離信息丟失問題，進一步提升校對性能。</p><p><strong>1 背景</strong></p><p><strong>1.1 中文文本自動校對的國內外研究現狀</strong></p><p>國外最早於20世紀60年代開始展開英文文本自動校對研究，發展至今，已取得突破性進展，擁有高準確率和高召回率的校對方法也已經應用到實際產品當中<sup>[4]</sup>。而國內關於中文文本自動校對的研究起步較晚，於20世紀90年代開始在國外對英文文本自動校對研究的基礎上，進行中文文本自動校對技術研究<sup>[5]</sup>。</p><p>目前，國內對於中文文本自動校對技術的研究方法主要有以下3種：(1)基於規則或語言學知識的方法<sup>[6]</sup>；(2)基於統計、概率的方法，如N-gram模型<sup>[7]</sup>；(3)基於特徵和Winnow學習模型，通過構造混淆集進行校對<sup>[8]</sup>。以上3種方法均為傳統的中文文本校對方法<sup>[9]</sup>，本文則是通過深度學習技術進行中文文本自動校對，而傳統的深度學習技術通常採用RNN或CNN進行中文文本自動校對<sup>[10]</sup>。</p><p><strong>1.2 RNN與CNN在中文文本自動校對的侷限性</strong></p><p>使用RNN進行中文文本自動校對任務時，通常採用雙向RNN結構<sup>[11]</sup>。即通過一個RNN進行從左往右的壓縮表示，另一個RNN進行從右往左的壓縮表示，將兩種壓縮表示結合起來，作為最終序列的分佈式表示。因為對序列中的元素按順序處理，所以兩個詞之間的交互距離即相對距離。因此，RNN在長距離序列中存在信息丟失問題，即使是加入了門控機制的LSTM<sup>[12]</sup>和GRU，依然無法解決該問題。</p><p>使用CNN進行中文文本自動校對任務時，一般使用多層結構來實現序列從局部表示到全局表示<sup>[13]</sup>。一個詞首先會在底層CNN單元上與其距離較近的詞產生交互，然後在稍高層次的CNN單元上與更遠一些的詞產生交互。詞之間的交互距離，與它們的相對距離成正比，因此使用CNN進行中文文本自動校對任務同樣會導致遠距離信息丟失問題。</p><p><strong>2 Transformer模型</strong></p><p>Transformer模型是一種對序列信息建模的新方法，該模型依然沿用了經典的Encoder-Decoder結構，不同的是Transformer模型不再使用RNN或CNN，而是使用Self-Attention機制。該機制的優勢就是可以獲取文本語義信息而不受詞語之間的相對距離影響，從而解決遠距離信息丟失問題，提高校對性能。</p><p><strong>2.1 Self-Attention</strong></p><p>Self-Attention機制作為Transformer模型最核心的特點，其可以建立當前位置詞語與其上下文相關詞語之間的聯繫。實現方式為：首先，將輸入向量分別與3個隨機初始化的權值矩陣W<sub>Q</sub>、W<sub>K</sub>和W<sub>V</sub>相乘，計算得出3個新的向量，分別為查詢向量q、鍵向量k和值向量v。其中q向量表示為了編碼當前詞語，需要去注意文本中的其他詞；k向量是當前詞的可以被用於檢索的信息；v向量則表示當前詞語的實際內容。接下來計算Self-Attention的分數值，其決定了在對輸入句子中的某個詞語進行編碼時，對句子中其他部分的關注程度。</p><p>如圖1所示，針對“生度”這個詞，通過查詢向量與鍵向量點乘，計算出其他詞對於該詞的一個分數值。首先針對於本身即q<sub>1</sub>×k<sub>1</sub>，然後針對於第二個詞即q<sub>1</sub>×k<sub>2</sub>；為了減少乘積值過大對之後歸一化的影響，將點乘得到的結果分別除以向量維度的開方；之後再做歸一化處理，得到的結果即每個詞與當前位置詞語的相關性大小；最後將值向量v與歸一化後的結果相乘再相加，得到的結果即Self-Attetion在當前節點的值。計算方式如圖1所示。</p><img alt=基於Transformer模型的中文文本自動校對研究 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RnA5iHG5yimfrr><p>為了提高計算速度，本文采用矩陣的方式進行計算，計算方法如式(1)所示。</p><p>其中，Q、K、V分別表示對應的3個向量矩陣，即查詢矩陣、鍵矩陣和值矩陣；d<sub>key</sub>表示向量維度，本文取64維。</p><p><strong>2.2 Multi-Headed Attention</strong></p><p>為了提升模型性能，加入了 Multi-Headed Attention機制<sup>[14]</sup>，即“多頭注意力”機制，通過初始化多組權值矩陣進行計算，本文選取8組。將計算得到8個矩陣拼接為一個矩陣，再將一個隨機初始化的矩陣與拼接好的矩陣相乘，得到最終矩陣。該機制的結構如圖2所示。</p><img alt=基於Transformer模型的中文文本自動校對研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RnA5iI71XC11ZC><p>“多頭注意力”機制有多個“注意力頭”，因此可以並行計算，提高模型訓練速度。並且每個“注意力頭”都有對應的權值矩陣，實現不同的線性變換，因此擴展了模型關注序列不同位置的信息的能力，可以對文本的語義信息有更完整的理解，從而進一步提高校對性能。</p><p><strong>3 Transformer模型的實現</strong></p><p><strong>3.1 數據預處理</strong></p><p>在Transformer模型構造之前，需要對輸入的數據進行預處理<sup>[15]</sup>，包括：數據加載、數據清洗、分詞、語料轉換、分析統計。其中，數據加載是將數據集中的數據導入模型；數據清洗是去除語料中的特殊字符；分詞使用中文分詞工具“jieba分詞”，對語料進行分詞<sup>[16]</sup>；語料轉換是將中文詞語轉換成對應的數字編碼，以及對應的數字編碼轉換為中文詞語；分析統計是對語料進行分析，瞭解語料特徵。</p><p><strong>3.2 Transformer模型結構</strong></p><p>本文中的Transformer模型由6個編碼器層和6個解碼器層堆疊組成。待校對文本在編碼器端輸入，正確文本在解碼器端輸入，通過監督學習來訓練模型。即訓練階段將訓練集中的輸入語料作為編碼器輸入，將對應的正確語料作為解碼器的輸入；在測試階段將測試集的待校對語料作為編碼器的輸入，解碼器端無輸入，僅依賴前一時刻的解碼器輸出信息進行校對。其整體結構如圖3所示。</p><img alt=基於Transformer模型的中文文本自動校對研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RnA5iKq6ZZdFvi><p>編碼器層內部包含兩個子層，即一個Self-Attention層和一個全連接的前饋神經網絡層。Self-Attention層中的查詢向量、鍵向量和值向量均來自於前一個編碼器層，因此編碼器層的每個位置都能去關注前一層輸出的所有信息，使得當前節點不僅關注當前信息，還能獲得上下文的語義信息。前饋神經網絡層應用於Self-Attention層的輸出，由兩個線性變換和一個ReLU激活函數組成。計算方法如式(2)所示。</p><p>其中，W<sub>1</sub>和W<sub>2</sub>為模型中神經元的權值，b<sub>1</sub>和b<sub>2</sub>為偏置值，x為輸入向量。</p><p>編碼器層與解碼器層的結構如圖4所示。</p><img alt=基於Transformer模型的中文文本自動校對研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RnA5lEm17yCzEz><p>解碼器層不僅包含編碼器層中的兩個子層，還添加了一個注意力子層對編碼器的輸出執行多頭注意，其查詢向量來自於前一個解碼器層，鍵向量和值向量來自於編碼器的輸出，因此解碼器的每個位置都可以關注到編碼器輸入序列的全部信息，幫助當前解碼器節點獲取到需要關注的重點內容。此外，解碼器的Self-Attention子層加入了masked部分，其可以對某些值進行掩蓋，從而防止模型注意到後續位置信息。這種屏蔽確保了當前的預測只能依賴於之前的已知輸出。</p><p><strong>3.3 Transformer模型實現過程</strong></p><p>Transformer模型的內部結構如圖5所示。</p><img alt=基於Transformer模型的中文文本自動校對研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RnA5lFbJJaI2M><p><strong>3.3.1 Embedding</strong></p><p>首先，模型對輸入數據進行Embedding，即詞嵌入，將輸入詞語轉變成向量<sup>[17]</sup>。將向量輸入到編碼器和解碼器的第一層，經過多頭注意力機制處理後傳入前饋神經網絡，得到的輸出信息作為下一層編碼器和解碼器的輸入<sup>[18]</sup>。</p><p><strong>3.3.2 Positional Encoding</strong></p><p>因為Transformer模型缺少對輸入序列中詞語順序的表示，所以需要在編碼器層和解碼器層的輸入添加一個Positional Encoding向量，即位置編碼向量，維度與輸入向量的維度相同，本文取512維。該向量決定當前詞在序列中的位置，計算方法如式(3)、式(4)所示：</p><p>其中，pos是指當前詞語在句子中的位置；i表示pos對應的向量值，取值範圍為0～255；d<sub>model</sub>表示向量維度。在偶數位置，使用正弦編碼；在奇數位置，使用餘弦編碼。最後將位置編碼向量與輸入向量相加，作為輸入傳入模型。</p><p><strong>3.3.3 殘差連接和歸一化</strong></p><p>編碼器層和解碼器層中每個子層都加入了殘差連接和歸一化<sup>[19]</sup>。子層先進行殘差連接，避免誤差反向傳播時的梯度消失，接著對輸出進行歸一化，避免梯度消失或梯度爆炸。剩餘連接和歸一化後的輸出表示如式(5)所示：</p><p>其中，x為前一層前饋神經網絡或多頭注意力層的輸出向量，SubLayer為注意力機制函數，LayerNorm為歸一化函數。</p><p><strong>3.3.4 輸出層</strong></p><p>當解碼器層全部執行完畢後，為了將得到的向量映射為本文需要的詞語，需要在最後一個解碼器層後添加一個全連接層和Softmax層。全連接層輸出logits向量，作為Softmax層的輸入。假設詞典包括n個詞語，那最終Softmax層會輸出n個詞語分別對應的概率值，概率值最大的對應詞語就是最終的輸出結果。</p><p><strong>4 結果與分析</strong></p><p>針對上述模型，本文設計了以下的實驗。本實驗運行環境操作系統為Windows 10，CPU為Intel Core<sup>TM</sup>i5-8265，GPU為GTX 1070Ti，運行內存8 GB。一共進行4組實驗，分別為傳統的Seq2Seq、加入BiLSTM的Seq2Seq、基於注意力機制的BiLSTM Seq2Seq與Transformer共4種模型。實驗使用的數據集來自於2018 NLPCC共享的訓練數據集Task 2，其中提供了717 206條中文文本語句對，將其中的700 000條作訓練集，17 206條作測試集，劃分過程隨機。Src代表待校對語料，Trg表示原句所對應的正確語料。數據集的統計數據如表1所示，分為訓練集和測試集，統計了正確語料和錯誤語料，以及分詞後的詞語總數。</p><img alt=基於Transformer模型的中文文本自動校對研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RnA5lA33e36UbV><p>為了對不同模型的校對效果進行評價，本次實驗採用準確率、召回率和F1值作為指標，對實驗結果進行評估，實驗結果如表2所示。</p><img alt=基於Transformer模型的中文文本自動校對研究 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RnA5lVr5fTZTbg><p>實驗結果表明，BiLSTM和注意力機制均可以在傳統Seq2Seq模型的基礎上提升中文文本校對效果，然而，Transformer模型的表現更好。並且，深度學習模型的校對效果受到訓練次數和訓練數據集大小的影響，如果增加Transformer模型的訓練次數和增大訓練數據集，模型將擁有更好的校對效果。</p><p><strong>5 結論</strong></p><p>本文將Transformer模型應用於中文文本自動校對領域，通過公開數據集對該模型的校對性能進行驗證，得出該模型的校對效果相比較於其他的模型在準確率和召回率方面均有很大提升。</p><p>本次實驗由於數據集的侷限性<sup>[20]</sup>，導致並未展現出Transformer模型的全部能力，但是該模型在中文文本自動校對中的表現依然優異，也為Transformer模型應用於其他自然語言處理任務提供了可能性。</p><p><strong>參考文獻</strong></p><p>[1] 張仰森，俞士汶.文本自動校對技術研究綜述[J].計算機應用研究，2006(6)：8-12.</p><p>[2] VASWANI A，SHAZEER N，PARMAR N，et al.Attention is all you need[J].Comouting Research Respository，2017，arXiv：1706：87-90.</p><p>[3] ZHANG C，WANG X，YU S，et al.Research on keyword extraction of Word2vec model in Chinese corpus[C].IEEE/ACIS 17th International Conference on Computer and Information Science(ICIS).IEEE，2018：339-343.</p><p>[4] 張濤.中文文本自動校對系統設計與實現[D].成都：西南交通大學，2017.</p><p>[5] 劉亮亮，王石，王東昇，等.領域問答系統中的文本錯誤自動發現方法[J].中文信息學報，2013，27(3)：77-83.</p><p>[6] 劉亮亮，曹存根.中文“非多字詞錯誤”自動校對方法研究[J].計算機科學，2016，43(10)：200-205.</p><p>[7] 張仰森，曹大元，俞士汶.基於規則與統計相結合的中文文本自動查錯模型與算法[J].中文信息學報，2005，20(4)：1-8.</p><p>[8] 張仰森，唐安傑，張澤偉.面向政治新聞領域的中文文本校對方法研究[J].中文信息學報，2014，28(6)：79-84.</p><p>[9] 張仰森，鄭佳.中文文本語義錯誤偵測方法研究[J].計算機學報，2017，40(4)：911-924.</p><p>[10] Wu Yonghui，SCHUSTER M，Chen Zhifeng，et al.Google′s neural machine translation system bridging the gap between human and machine translation[J].Computing Research Repository，2016，arXiv：1609：56-59.</p><p>[11] Chung Junyoung，GULCEHRE C，CHO K，et al.Empirical evaluation of gated recurrent neural networks on sequence modeling[J].CoRR，2014，abs/1412：78-81.</p><p>[12] Cheng Jianpeng，LAPATA M.Long short-term memory-networks for machine reading[J].arXiv Preprint 2016，arXiv：1601：127-130.</p><p>[13] GEHRING J，AULI M，GRANGIER D，et al.Convolutional sequence to sequence learning[J].arXiv Preprint 2017，arXiv：1705：68-71.</p><p>[14] LUONG M T，LE Q V，SUTSKEVER I，et al.Multi-task sequence to sequence learning[J].arXiv Preprint 2015，arXiv：1511：114-117.</p><p>[15] 王潔，喬藝璇，彭巖，等.基於深度學習的美國媒體“一帶一路”輿情的情感分析[J].電子技術應用，2018，44(11)：102-106.</p><p>[16] 孫鐵利，劉延吉.中文分詞技術的研究現狀與困難[J].2012(1)：187-192.</p><p>[17] Lin Zhouhan，Feng Minwei，NOGUEIRA C，et al. A struc-tured self-attentive sentence embedding[J].arXiv Preprint 2017，arXiv：1703：116-119.</p><p>[18] PRESS O，WOLF L.Using the output embedding to improve language models[J].arXiv Preprint 2016，arXiv：1608：67-70.</p><p>[19] BA J L，KIROS J R，HINTON G E.Layer normalization[J].arXiv Preprint 2016，arXiv：1607：45-49.</p><p>[20] 字雲飛，李業麗，孫華豔.基於深度神經網絡的個性化推薦系統研究[J].電子技術應用，2019，45(1)：14-18.</p><p><strong>作者信息:</strong></p><p>龔永罡，裴晨晨，廉小親，王嘉欣</p><p>(北京工商大學 計算機與信息工程學院食品安全大數據技術北京市重點實驗室，北京100048)</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>Transformer</a></li><li><a>自動校</a></li><li><a>文本</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/e911ae58.html alt=要實現數據自動校對，這幾個Excel小技巧不可錯過。 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/f1380ab2c662411395ce974b73d251a6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e911ae58.html title=要實現數據自動校對，這幾個Excel小技巧不可錯過。>要實現數據自動校對，這幾個Excel小技巧不可錯過。</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2a1f2cc9.html alt=谷歌Transformer再升級—新模型實現性能、速度雙提升 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/aa63807597454a2e875c855491eb7d82 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2a1f2cc9.html title=谷歌Transformer再升級—新模型實現性能、速度雙提升>谷歌Transformer再升級—新模型實現性能、速度雙提升</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/84f64628.html alt=給PDF批量添加文本鏈接 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/b2563acb0efb4e7394cabb8fb4a7461b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/84f64628.html title=給PDF批量添加文本鏈接>給PDF批量添加文本鏈接</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f0ec806f.html alt="用 Vision Transformer 進行圖像分類" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/9529b2f9517b48fdbc59f29c866a50c1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f0ec806f.html title="用 Vision Transformer 進行圖像分類">用 Vision Transformer 進行圖像分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/308c027f.html alt=Inventor教程之工程圖文本標註 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/05b2d1214af84a4db766220598e8e2b9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/308c027f.html title=Inventor教程之工程圖文本標註>Inventor教程之工程圖文本標註</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/34678f5f.html alt=excel小技巧，限制文本長度 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1536886321532590f2ae066 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/34678f5f.html title=excel小技巧，限制文本長度>excel小技巧，限制文本長度</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4371907f.html alt=Transformer自動糾語法、改論文,我們試了這個免費英文寫作新神器 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/0da2d0c3568a4a07931ecc0b6f9d9d3b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4371907f.html title=Transformer自動糾語法、改論文,我們試了這個免費英文寫作新神器>Transformer自動糾語法、改論文,我們試了這個免費英文寫作新神器</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/75cee8d2.html alt=如何統一文本的格式——辦公必備 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/568800039e02905458a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/75cee8d2.html title=如何統一文本的格式——辦公必備>如何統一文本的格式——辦公必備</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/86fe6bc5.html alt=文本內容的處理技巧 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/c97c39f4269d4106a459879c91118d8e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/86fe6bc5.html title=文本內容的處理技巧>文本內容的處理技巧</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/815a0e38.html alt=Excel中如何將文本轉換方向 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/6539/4800626177 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/815a0e38.html title=Excel中如何將文本轉換方向>Excel中如何將文本轉換方向</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/003a6d76.html alt=網頁文本不讓複製粘貼？教你4個破解技巧，全網任意複製 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1810307723974cfc8ebd44d83bac1472 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/003a6d76.html title=網頁文本不讓複製粘貼？教你4個破解技巧，全網任意複製>網頁文本不讓複製粘貼？教你4個破解技巧，全網任意複製</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/54b66f71.html alt=為什麼說Transformer就是圖神經網絡？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RqSgmP9HojtMNK style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/54b66f71.html title=為什麼說Transformer就是圖神經網絡？>為什麼說Transformer就是圖神經網絡？</a></li><hr><li><a href=../../tw/%E9%81%8A%E6%88%B2/bf8ddee8.html alt=《飄》的文本細讀：一部宏大精緻的種族主義南方穢史 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/S5palgR7fORlvW style=border-radius:25px></a>
<a href=../../tw/%E9%81%8A%E6%88%B2/bf8ddee8.html title=《飄》的文本細讀：一部宏大精緻的種族主義南方穢史>《飄》的文本細讀：一部宏大精緻的種族主義南方穢史</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cecfe4b1.html alt=文本關鍵詞提取算法-TextRank class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/4a4eb849-d382-479d-b560-411b6afd4f24 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cecfe4b1.html title=文本關鍵詞提取算法-TextRank>文本關鍵詞提取算法-TextRank</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ddf7529.html alt="如何在 Mac 上輕鬆製作好看的文本長圖？" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1b6f37238d68407aa5ee026856cbbf83 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ddf7529.html title="如何在 Mac 上輕鬆製作好看的文本長圖？">如何在 Mac 上輕鬆製作好看的文本長圖？</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>