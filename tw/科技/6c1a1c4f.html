<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>所有學機器學習的人必須要懂的5個迴歸損失函數 | 极客快訊</title><meta property="og:title" content="所有學機器學習的人必須要懂的5個迴歸損失函數 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/e3705450196b412ba0d239f329ce5ed7"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c1a1c4f.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c1a1c4f.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6c1a1c4f.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c1a1c4f.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c1a1c4f.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6c1a1c4f.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6c1a1c4f.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c1a1c4f.html><meta property="article:published_time" content="2020-11-14T21:05:42+08:00"><meta property="article:modified_time" content="2020-11-14T21:05:42+08:00"><meta name=Keywords content><meta name=description content="所有學機器學習的人必須要懂的5個迴歸損失函數"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/6c1a1c4f.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>所有學機器學習的人必須要懂的5個迴歸損失函數</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><blockquote>作者：Prince Grover編譯：ronghuaiyang</blockquote><h1><strong>導讀</strong></h1><blockquote><p>為模型選擇合適的損失函數，讓模型具有最好的效果。</p></blockquote><p>機器學習中的所有算法都依賴於函數的最小化或最大化，我們稱之為“目標函數”。一組最小化的函數稱為“損失函數”。損失函數是衡量預測模型在預測預期結果方面做得有多好。求函數最小值的一種常用方法是“梯度下降法”。把損失函數想象成起伏的山，而梯度下降就像從山上滑下來到達最低點。</p><p>沒有一個單一的損失函數適用於所有類型的數據。它取決於許多因素，包括異常值的存在、機器學習算法的選擇、梯度下降的時間效率、求導數的易用性和預測的置信度。本系列博客的目的是瞭解不同的損失以及每種損失如何幫助數據科學家。</p><p>損失函數可以大致分為兩類：<strong>分類和迴歸損失</strong>。在這篇文章中，我主要關注迴歸損失。在以後的文章中，我將討論其他類別的損失。如果我忘記了什麼，請在評論中告訴我。此外，本博客所展示的所有代碼和圖表均可在 這裡中找到。</p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e3705450196b412ba0d239f329ce5ed7><p class=pgc-img-caption>迴歸函數預測的數值，分類函數預測的是標籤</p></div><h1><strong>迴歸損失</strong></h1><p><strong>1. 均方誤差，二次型損失，L2損失</strong></p><p>均方誤差(Mean Square Error, MSE)是最常用的迴歸損失函數。MSE是目標變量與預測值之間距離的平方和。</p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3e34e17d58004fe68bbcb921aba8e1de><p class=pgc-img-caption></p></div><p>下面是一個MSE函數的圖，其中真實目標值為100，預測值在-10,000到10,000之間。MSE損失(y軸)在預測(x軸)= 100時達到最小值。範圍是0到∞。</p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/28960ff2fa444a52945f93c7c9f32287><p class=pgc-img-caption>Plot of MSE Loss (Y-axis) vs. Predictions (X-axis)</p></div><p><strong>2. 平均絕對誤差，L1損失</strong></p><p>平均絕對誤差(MAE)是迴歸模型中使用的另一個損失函數。MAE是目標變量和預測變量之間的絕對差值之和。所以它測量的是一組預測的平均誤差大小，而不考慮它們的方向。(如果我們也考慮方向，那就叫做平均偏差誤差(Mean Bias Error, MBE)，它是殘差/誤差的和)。範圍也是0到∞。</p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b17d7f4ed3d045dfb38cde8ac7f04bc5><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/07b4c8c63bc64d55ab7683a5c39f90ba><p class=pgc-img-caption>Plot of MAE Loss (Y-axis) vs. Predictions (X-axis)</p></div><p><strong>MSE vs. MAE (L2 loss vs L1 loss)</strong></p><p><strong>簡而言之，使用平方誤差更容易，但是使用絕對誤差對異常值更有魯棒性。我們來理解一下為什麼！</strong></p><p>當我們訓練機器學習模型時，我們的目標是找到使損失函數最小化的點。當然，當預測值恰好等於真實值時，兩個函數都達到最小值。</p><p>下面快速回顧一下這兩種方法的python代碼。我們可以編寫自己的函數，也可以使用sklearn的內置度量函數：</p><pre> # true: Array of true target variable # pred: Array of predictions  def mse(true, pred):  return np.sum((true - pred)**2)  def mae(true, pred): return np.sum(np.abs(true - pred))  # also available in sklearn  from sklearn.metrics import mean_squared_error from sklearn.metrics import mean_absolute_error</pre><p>讓我們看看2種情況下MAE和均方根誤差的值(RMSE，也就是MSE的平方根，使其與MAE處於相同的規模)。在第一種情況下，預測值接近真實值，誤差在觀測值之間的方差很小。在第二種情況下，有一個異常值，誤差很大。</p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/276701f571ca4e38b3923af7adf6a8e0><p class=pgc-img-caption>左: 誤差很接近 右: 有一個誤差和其他差別很大</p></div><p><strong>我們從中觀察到什麼，它如何幫助我們選擇使用哪種損失函數？</strong></p><p>由於MSE平方誤差(y - y_prediction = e)，所以如果e > 1，誤差(e)的值會增加很多。如果我們有一個異常數據，e的值會變得很大並且e²>>e。這將使MSE損失模型比MAE損失模型給予離群值更多的權重。在上面的第二種情況中，以RMSE為損失的模型將被調整，以犧牲其他常見的例子來最小化單個的離群值情況，這將降低它的總體能力。</p><p>如果訓練數據被異常值破壞(例如，我們在訓練環境中錯誤地接收到巨大的負/正值，但在測試環境中卻沒有)，那麼MAE損失是有用的。</p><p>直覺上，我們可以這樣想：如果我們只需要對所有試圖最小化MSE的觀測結果給出一個預測，那麼這個預測應該是所有目標值的平均值。但是如果我們試圖最小化MAE，這個預測將是所有觀測值的中值。我們知道中值比平均值更對異常值更健壯，這使得MAE比MSE對異常值更健壯。</p><p>使用MAE loss的一個大問題是(尤其是對於神經網絡)，它的梯度始終是相同的，這意味著即使損失值很小，梯度也會很大。這不利於學習。為了解決這個問題，我們可以使用動態學習速率，當我們接近最小值時，動態學習速率會降低。MSE在這種情況下表現良好，即使有固定的學習速率也會收斂。MSE損失的梯度在損失值較大時較高，在損失接近0時減小，使得訓練結束時更加精確(見下圖)。</p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/863750a4510d49c5a2a8278495b1bae0><p class=pgc-img-caption></p></div><p><strong>決定使用哪個損失函數</strong> 如果異常值代表對業務重要而且是應該檢測的異常，那麼我們應該使用MSE。另一方面，如果我們認為離群值只是代表損壞的數據，那麼我們應該選擇MAE作為損失。</p><p>我建議閱讀這篇文章，並結合一項不錯的研究比較使用L1 loss和L2 loss的迴歸模型的性能，研究是否存在異常值。記住，L1和L2損失只是MAE和MSE的另一個名稱。</p><blockquote><p><em>L1損失對異常值的魯棒性更強，但其導數不是連續的，使得求解效率低下。L2損耗對異常值很敏感，但給出了一個更穩定、更封閉的解(通過將其導數設置為0.)</em></p></blockquote><p><strong>兩者都有問題</strong>：在某些情況下，損失函數都不能給出理想的預測。例如，如果我們的數據中90%的觀測值的真實目標值為150，其餘10%的目標值在0-30之間。然後，一個以MAE為損失的模型可能預測所有觀測值為150，忽略10%的異常情況，因為它將試圖接近中值。在同樣的情況下，使用MSE的模型會給出0到30範圍內的許多預測，因為它會偏向於離群值。這兩種結果在許多業務案例中都是不可取的。</p><p><strong>在這種情況下該怎麼辦</strong>？一個簡單的解決方法是轉換目標變量。另一種方法是嘗試不同的損失函數。這就是我們的第三個損失函數背後的動機，Huber損失。</p><p><strong>3. Huber Loss, 平滑的平均絕對誤差</strong></p><p>Huber loss對數據中異常值的敏感性小於平方誤差損失。它在0處也是可微的。它基本上是絕對誤差，當誤差很小的時候，它變成了二次函數。多小的時候變成二次誤差取決於超參數，，這是可調整的。Huber損失方法<strong>當~ 0時為MAE，當~∞時為MSE</strong></p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4abd8b5e7bc8418d85bac2ae67e7aa5f><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ba9edf182b1e4cecbcb7a561a5fcbaa9><p class=pgc-img-caption>Plot of Hoss Loss (Y-axis) vs. Predictions (X-axis</p></div><p>delta的選擇非常關鍵，因為它決定了你願意將什麼視為異常值。大於delta的殘差用L1最小化(L1對大的異常值不太敏感)，而小於delta的殘差用L2“適當地”最小化。</p><p><strong>為什麼使用Huber損失？</strong>使用MAE訓練神經網絡的一個大問題是它的梯度經常很大，這會導致在使用梯度下降法訓練結束時會錯過最小值。對於MSE，梯度隨著損失接近最小值而減小，使其更加精確。</p><p>Huber損失在這種情況下非常有用，因為它會圍繞著減小梯度的最小值曲線。它比MSE對異常值更有效。因此，它結合了MSE和MAE的優良性能。然而，Huber損失的問題是我們可能需要訓練超參數delta，這是一個迭代過程。</p><p><strong>4. Log-Cosh Loss</strong></p><p>Log-cosh是迴歸任務中使用的另一個比L2更平滑的函數。Log-cosh是預測誤差的雙曲餘弦的對數。</p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/24bf960567384b4c97e32ec24e8301af><p class=pgc-img-caption>Plot of Log-cosh Loss (Y-axis) vs. Predictions (X-</p></div><p><strong>優點:</strong> log(cosh(x))對於小的x，大約等於(x ** 2) / 2對，對於大的x，大約等於abs(x) - log(2)。這意味著logcosh的工作原理與均方誤差類似，但不會受到偶爾出現的嚴重錯誤預測的太大影響。它具有Huber損失的所有優點，不像Huber損失，它在任何地方都是二階可微的。</p><p><strong>為什麼我們需要二階導數？</strong>許多ML模型的實現，如XGBoost，都使用牛頓法來尋找最優解，這就是為什麼需要二階導數(Hessian)。對於像XGBoost這樣的ML框架，二階可微的函數更為有利。</p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1f611bf77f5a4805958eefaec3a6a1d7><p class=pgc-img-caption>XgBoost中用的目標函數，注意，依賴一階和二階導數</p></div><p>但是log-cosh損失並不是完美的。它仍然存在梯度和hessian問題，因為對於偏離目標非常大的的預測是恆定的，因此導致XGBoost不進行分割。</p><p>Huber和Log-cosh損失函數的Python代碼：</p><pre> # huber loss def huber(true, pred, delta): loss = np.where(np.abs(true-pred) &lt; delta , 0.5*((true-pred)**2), delta*np.abs(true - pred)- 0.5*(delta**2)) return np.sum(loss)  # log cosh loss def logcosh(true, pred): loss = np.log(np.cosh(pred - true)) return np.sum(loss)</pre><p><strong>5. 理解分位數損失</strong></p><p>在現實世界的大多數預測問題中，我們往往對預測中的不確定性感興趣。我們需要了解預測的範圍，而不是僅僅進行每個點的估算，這樣可以顯著改進許多業務問題的決策過程。</p><p>分位數損失函數在我們有興趣預測一個區間而不是僅僅預測點時非常有用。最小二乘迴歸的預測區間是基於殘差(y - y_hat)在自變量值之間具有恆定方差的假設。我們不能相信違反這一假設的線性迴歸模型。我們也不能拋棄將線性迴歸模型擬合為基線的想法，認為使用非線性函數或基於樹的模型對這種情況進行建模總是更好的。這就是分位數損失和分位數迴歸可以彌補的地方，因為基於分位數損失的迴歸甚至為非恆定方差或非正態分佈的殘差提供了合理的預測區間。</p><p>讓我們看一個例子，以更好地理解為什麼基於分位數損失的迴歸在異方差數據中表現良好。</p><p><strong>分位數迴歸vs.普通最小二乘迴歸</strong></p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6e0008d4fb6b4962adb611d3f57df06d><p class=pgc-img-caption>左: X1和Y是線性關係，具有固定的方差。右：X2和Y是線性關係，但是Y的方差隨著X2增加（異方差性</p></div><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b675b3318cd74c8c8d817a9c21143655><p class=pgc-img-caption>橘色線表示了兩種情況的OLS估計</p></div><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/075c01af44ec4a7ab07efa5d0b40f624><p class=pgc-img-caption>分位數迴歸。虛線表示基於0.05和0.09分位數損失的迴歸結果</p></div><p>上面的圖的分位數迴歸的代碼在這裡。</p><p><strong>理解分位數損失</strong></p><p>基於分位數的迴歸旨在估計給定一定預測變量值的響應變量的條件“分位數”。分位數損失實際上只是MAE的延伸(當分位數是50%時，它就是MAE)。</p><p>我們的想法是，根據我們想給正誤差或負誤差更多的值來選擇分位數值。損失函數試圖對於過高的估計和過低的估計不同的懲罰，在此基礎上選擇分位數(γ)。例如，γ= 0.25分位數損失函數對於過高的預測給出了更多的懲罰，試圖保持預測值略低於中位數。</p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d79c393489654a1abaf62b508e837327><p class=pgc-img-caption></p></div><p>γ是所需的分位數和值在0和1之間。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1e4a9984f6614cdb8becb8627c9abc67><p class=pgc-img-caption></p></div><p>Plot of Quantile Loss (Y-axis) vs. Predictions (X-axis). True value of Y = 0</p><p>我們也可以使用這個損失函數來計算神經網絡或基於樹的模型中的預測區間。下面是梯度增強樹迴歸的Sklearn實現示例。</p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d6d30f2101dc4b41ae5107d551040bcc><p class=pgc-img-caption>使用分位數損失預測間隔（梯度提升迴歸器）</p></div><p>上圖顯示了使用sklearn庫的GradientBoostingRegression中可用的分位數損失函數計算出的90%的預測區間。上限是γ= 0.95，下限γ= 0.05。</p><h1>對比學習：</h1><p>“Gradient boosting machines, a tutorial”中提供了一個不錯的模擬比較。證明上述損失函數的性質，他們模擬的使用sinc(x)對數據集採樣，人工模擬噪聲的函數有兩個來源：高斯噪聲ε~N (0,σ2)和脈衝噪聲組件ξ~Bern(p)。加入脈衝噪聲項來說明魯棒性效果。下面是使用不同損失函數擬合GBM迴歸函數的結果。</p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/04efbfd77b27499092cbbe366521d7b7><p class=pgc-img-caption></p></div><p><strong>連續損失函數：(A) MSE損失函數；(B) MAE損失函數；(C)Huber損失函數；(D)分位數損失函數</strong>。將平滑的GBM擬合到有噪聲的噪聲sinc(<em>x</em>)數據的演示：<strong>(E)</strong>原始sinc(<em>x</em>)函數；<strong>(F)</strong>平滑的GBM，使用MSE和MAE損失擬合；<strong>(G)</strong> 平滑的GBM，使用Huber損失擬合δ= {4，2，1}；<strong>(H)</strong>平滑的GBM使用分位數的損失α= {0.5，0.1，0.9}。</p><p><strong>模擬的一些觀察結果：</strong></p><ul><li class=ql-align-justify>具有MAE損失模型的預測受脈衝噪聲的影響較小，而具有MSE損失函數的預測由於噪聲所引起的差異而略有偏差。</li><li class=ql-align-justify>當模型存在huber損失時，預測對選取的超參數值不敏感。</li><li class=ql-align-justify>分位數損失很好地估計了相應的置信水平。</li></ul><p><strong>所有的損失函數畫在一個圖裡</strong></p><div class=pgc-img><img alt=所有學機器學習的人必須要懂的5個迴歸損失函數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a44a6edc8eae406ba88818ad9f3e3bac><p class=pgc-img-caption></p></div><p>英文原文：https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0</p><p><strong>更多文章，請關注微信公眾號：AI公園</strong></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>學機器</a></li><li><a>學習</a></li><li><a>必須</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/3813ce99.html alt=成為“黑客”前，必須學習的“操作系統架構體系” class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/f84e83ca27a749d7ad9f79857565dc4f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3813ce99.html title=成為“黑客”前，必須學習的“操作系統架構體系”>成為“黑客”前，必須學習的“操作系統架構體系”</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c6280d4c.html alt=學習形意拳必須要明白的，三體式的技法與效用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9c5840c178e645679455b9d9d80c97cd style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c6280d4c.html title=學習形意拳必須要明白的，三體式的技法與效用>學習形意拳必須要明白的，三體式的技法與效用</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/25df97f.html alt=高中生物學習，考生必須理清的知識點和難點 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/4b020002f016407c6ace style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/25df97f.html title=高中生物學習，考生必須理清的知識點和難點>高中生物學習，考生必須理清的知識點和難點</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/dafc1e3.html alt=絕對值學習必須掌握的知識點和題型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/7ce764ea-4d6a-47e9-875f-678af4b3435c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/dafc1e3.html title=絕對值學習必須掌握的知識點和題型>絕對值學習必須掌握的知識點和題型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9a72c81.html alt=學習公文必須熟悉掌握公文常用詞彙 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/4701000389f02f05fb3f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9a72c81.html title=學習公文必須熟悉掌握公文常用詞彙>學習公文必須熟悉掌握公文常用詞彙</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d0ae3ea.html alt=學習太極拳必須遵循一定的程序循序漸進，怎麼練我們來看看 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d0ae3ea.html title=學習太極拳必須遵循一定的程序循序漸進，怎麼練我們來看看>學習太極拳必須遵循一定的程序循序漸進，怎麼練我們來看看</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8f735cd3.html alt=網站建設中有哪些你必須要注意的？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8f735cd3.html title=網站建設中有哪些你必須要注意的？>網站建設中有哪些你必須要注意的？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html alt=直流鍋爐給水控制學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/eba10edcc8d14d9f8cde6fd5b212d90e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html title=直流鍋爐給水控制學習>直流鍋爐給水控制學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html alt=HTMLCSS學習筆記（六）——元素類型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb5988349894ce9bf568c6418f85b7d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html title=HTMLCSS學習筆記（六）——元素類型>HTMLCSS學習筆記（六）——元素類型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html alt="web前端（從零開始），每天更新學習筆記 HTML5元素分類" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/46d70004fcd55e1ddad3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html title="web前端（從零開始），每天更新學習筆記 HTML5元素分類">web前端（從零開始），每天更新學習筆記 HTML5元素分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html alt="MySQL 學習筆記" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html title="MySQL 學習筆記">MySQL 學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5b46ca88.html alt=做跨境電商站外推廣，你必須瞭解的Facebook像素 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/2df772dcb2d747f8b55ff9e4610cbcad style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5b46ca88.html title=做跨境電商站外推廣，你必須瞭解的Facebook像素>做跨境電商站外推廣，你必須瞭解的Facebook像素</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>