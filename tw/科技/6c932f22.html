<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>策略梯度的簡明介紹 | 极客快訊</title><meta property="og:title" content="策略梯度的簡明介紹 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/1527998215130569eb83799"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c932f22.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c932f22.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6c932f22.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c932f22.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c932f22.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6c932f22.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6c932f22.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6c932f22.html><meta property="article:published_time" content="2020-11-14T21:08:22+08:00"><meta property="article:modified_time" content="2020-11-14T21:08:22+08:00"><meta name=Keywords content><meta name=description content="策略梯度的簡明介紹"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/6c932f22.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>策略梯度的簡明介紹</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p>本文旨在為強化學習 - 策略梯度中最重要的一類控制算法提供簡明而全面的介紹。我將在進展中討論這些算法，從頭到尾得出眾所周知的結果。它針對的是具有合理背景的讀者，以及機器學習中的其他任何主題。</p><h1>介紹</h1><p>強化學習（RL）指的是學習問題和機器學習的子領域，近來出於很多原因而出現在新聞中。基於RL的系統現在已經打敗Go的世界冠軍，幫助更好地操作數據中心並掌握各種各樣的Atari遊戲。研究界看到許多更有前途的結果。有足夠的動力，讓我們現在看看強化學習問題。</p><p>強化學習是對學習問題的最一般的描述，其目的是最大化長期目標。系統描述包括一個代理，該代理通過其在不連續時間步驟的行為與環境交互並獲得獎勵。這將代理轉換為新狀態。典型代理 - 環境反饋迴路如下圖所示。</p><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1527998215130569eb83799><p class=pgc-img-caption></p></div><p>學習問題的強化學習風格與人類如何有效表現行為驚人地相似 - 體驗世界，積累知識並利用學習來處理新情況。</p><h1>背景和定義</h1><p>RL背後的大量理論都是建立在獎勵假設的基礎上的，這個假設概括地說，一個代理的所有目標和目的都可以用一個叫做獎勵的標量來解釋。更正式地說，獎勵假設如下</p><blockquote><p>獎勵假設：所有我們所說的目標和目標都可以被認為是所接收標量信號（稱為獎勵）的累積和的期望值的最大化。</p></blockquote><p>作為一個RL的實踐者和研究者，一個人的工作是為一個給定的問題找到一套合適的獎勵，這個問題被稱為獎勵塑造。</p><p>代理必須通過一個稱為馬爾可夫決策過程的理論框架進行正式工作，該決策過程由在每個狀態下要做出的決策(要採取什麼行動?)組成。這就產生了一系列的狀態，行為和獎勵被稱為軌跡，</p><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1527999114420e602ffd860><p class=pgc-img-caption></p></div><p>目標是最大化這套獎勵。更正式的，我們看看馬爾科夫決策過程框架。</p><blockquote><p>馬爾可夫決策過程：（Discounted）馬爾可夫決策過程（MDP）是一個元組（S，A，R， p， γ），例如</p></blockquote><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1527999212336cc2565aa5e><p class=pgc-img-caption></p></div><blockquote><p>其中 S_t， S_（t +1）∈S（狀態空間）， A_（t +1）∈A（動作空間）， R_（t +1）， R_t∈R（獎勵空間）， p定義過程的動態， G_t是折扣回報</p></blockquote><p>簡單地說，MDP定義了過渡到一個新狀態的概率，得到了給定當前狀態和執行操作的一些獎勵。這個框架在數學上是令人滿意的，因為它是一階馬爾可夫。這只是一種奇特的說法，說明接下來發生的一切都只取決於現在，而不是過去。一個人如何到達當前狀態並不重要，重要的是他如何到達當前狀態。這個框架的另一個重要部分是貼現因子γ。將這些回報與未來回報的不同程度的重要性相加，就會得出折現回報的概念。正如你所預料的那樣,γ導致更高的靈敏度更高回報的未來。然而,γ= 0的極端情況下不考慮來自未來的回報。</p><p>環境p的動態不在代理的控制之下。可以想象站在一個有風的環境中的場地，並在每秒四個方向中的一個方向邁出一步。風很強烈，你很難沿著與北，東，西，南完全一致的方向移動。在下一秒落在一個新的狀態的這個概率是由有風場的動力學p給出的。這當然不在你的（代理）控制之下。</p><p>但是，如果您以某種方式瞭解環境的動態，並朝著除北，東，西或南以外的方向移動，該怎麼辦。該策略是代理控制的。當代理遵循策略π時，它會產生稱為軌跡的狀態，行為和獎勵順序。</p><blockquote><p>策略：策略被定義為給定狀態的行為的概率分佈</p></blockquote><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1527999567326018ae717ca><p class=pgc-img-caption></p></div><h1>策略梯度</h1><p>強化學習代理的目標是在遵循策略π時最大化“預期”獎勵。像任何機器學習設置一樣，我們定義一組參數θ（例如複雜多項式的係數或神經網絡中單位的權重和偏差）來參數化這個策略 - π_θ（為簡潔起見也寫成π）。如果我們將給定軌跡τ的總回報表示為r（τ），我們得出以下定義。</p><blockquote><p>強化學習目標：根據參數化策略最大化“預期”獎勵</p></blockquote><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1527999686366f1917db8ad><p class=pgc-img-caption></p></div><p>所有有限的MDPs至少有一個最優策略(可以給出最大的獎勵)，在所有最優策略中，至少有一個是平穩和確定性的。</p><p>像任何其他機器學習的問題,如果我們能找到參數θ⋆最大化J,我們會解決這個任務。解決機器學習文獻中這種最大化問題的標準方法是使用梯度上升(或下降)。在梯度上升中，我們使用以下更新規則不斷地遍歷參數</p><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1527999796987405811a10d><p class=pgc-img-caption></p></div><p>挑戰來了，我們如何找到包含期望的目標的梯度。積分在計算環境中總是不好的。我們需要找到繞過他們的辦法。第一步是從期望的擴展開始重新構造梯度</p><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1527999872005a304fcc5cb><p class=pgc-img-caption></p></div><blockquote><p>策略梯度定理：期望回報的導數是策略π_θ對數的回報和梯度乘積的 期望值</p></blockquote><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15279999278267323554711><p class=pgc-img-caption></p></div><p>現在，讓我們擴展π_θ（τ）的定義</p><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1528000033417f453d3cf63><p class=pgc-img-caption></p></div><p>為了理解這個計算，讓我們把它分解下來 - P代表在某些狀態s_ 0 開始的遍歷分佈。從那時起，我們應用概率乘積法則，因為每個新的行動概率都與前一個概率無關（還記得馬爾可夫嗎?）。在每一步中，我們採取用策略有所行動π_θ和環境動態p決定哪些新的狀態過渡到。這些乘以T時間步長表示軌跡的長度。等同地，取得日誌，我們有</p><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1528000106906526d4971fd><p class=pgc-img-caption></p></div><p>這個結果本身很美，因為這告訴我們，我們並不需要知道狀態P的遍歷分佈和環境動力學p。這是至關重要的，因為對於大多數實際目的來說，很難對這兩個變量進行建模 擺脫他們，當然是很好的進展。因此，所有使用此結果的算法都被稱為“無模型算法 ”，因為我們不會“模擬”環境。</p><p>“期望”（或者相當於一個積分項）仍然存在。一個簡單而有效的方法是對大量軌跡進行採樣並將其平均。這是一個近似值，但是是一個沒有偏差的值，類似於近似在連續空間上的一個積分，其中有一組離散的點。這種技術在形式上被稱為馬爾可夫鏈蒙特卡羅（MCMC），廣泛用於概率圖形模型和貝葉斯網絡來近似參數概率分佈。</p><p>我們以上處理中未被觸及的一個術語是對軌跡r（τ）的獎勵。即使參數化策略的梯度不依賴於獎勵，該術語也會在MCMC採樣中增加很多方差。實際上，每個R_t都有T個方差來源。然而，我們可以使用回報G_t，因為從優化RL目標的角度來看，過去的回報沒有任何貢獻。因此，如果用折現回報G_t代替r（τ），我們得到經典算法Policy Gradient算法，稱為REINFORCE。隨著我們進一步討論，這並不能完全緩解這個問題。</p><h1>REINFORCE (and Baseline)</h1><p>重申一下，REINFORCE算法計算策略梯度為</p><p><strong>REINFORCE Gradient</strong></p><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1528000451904ee628075a8><p class=pgc-img-caption></p></div><p>我們仍然沒有解決採樣軌跡中的方差問題。解決此問題的一種方法是重新設計上面定義的RL目標，即極大似然估計（Maximum Likelihood Estimate）。在MLE設置中，眾所周知，數據壓倒了以前 - 簡單地說，無論初始估計多麼糟糕，在數據極限內，模型都會收斂到真實參數。然而，在數據樣本具有高方差的情況下，穩定模型參數可能是非常困難的。在我們的情況下，任何不穩定的軌跡都可能導致策略分配出現次優的轉變。獎勵的規模加劇了這個問題。</p><p>因此，我們試著通過引入另一個稱為基線b的變量來優化獎勵的差異。為了保持梯度估計無偏差，基線獨立於策略參數。</p><p><strong>基線增強（</strong>REINFORCE with Baseline<strong>）</strong></p><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15280005274152d65f75fd3><p class=pgc-img-caption></p></div><p>要知道為什麼，我們必須證明梯度與附加項保持不變</p><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1528000589847e645010013><p class=pgc-img-caption></p></div><p>在理論和實踐中使用基線可以減少方差，同時保持梯度仍然無偏。一個好的基準是使用狀態值當前狀態。</p><blockquote><p>狀態值：狀態值被定義為在策略π_θ之後給定狀態的預期回報 。</p></blockquote><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1528000637797bee93c57ff><p class=pgc-img-caption></p></div><h1>Actor-Critic方法</h1><p>找到一個好的基線本身就是另一個挑戰，並計算它的另一個挑戰。相反，讓我們使用參數ω來近似以得到V ^ω_（s）。所有使用可學習的V ^ω_（s）來引導梯度的算法被稱為Actor-Critic算法，因為這個值函數估計像“actor”（代理策略）對“ critic”（good v/s bad values） 。然而這一次，我們必須計算actor 和critic的梯度。</p><blockquote><p>One-Step Bootstrapped Return：單步引導回報獲取即時回報，並通過使用軌跡中下一狀態的引導值估計來估計回報。</p></blockquote><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15280011266342e78673c39><p class=pgc-img-caption></p></div><p><strong>Actor-Critic策略梯度</strong></p><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1528001174049828506be8c><p class=pgc-img-caption></p></div><p>不言而喻，我們還需要更新critic的參數ω。這裡的目標通常被認為是均方損失（或較不嚴重的Huber損失）並且使用隨機梯度下降來更新參數。</p><p><strong>Critic’的目標</strong></p><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1528001256935b3e28f3b3e><p class=pgc-img-caption></p></div><h1>確定性策略梯度</h1><p>很多時候，在機器人技術中，可用的可控制策略是可用的，但這些行為不是隨機的。在這樣的環境下，很難像以前看到的那樣建立隨機策略。一種方法是將噪聲注入控制器。更重要的是，隨著控制器越來越多的維度，先前看到的算法開始變差。由於這樣的場景，我們不是直接學習大量的概率分佈，而是直接瞭解給定狀態的確定性行為。因此，以最簡單的形式，貪婪的最大化目標就是我們所需要的</p><p><strong>確定性行為</strong></p><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1528002447582f43cb56440><p class=pgc-img-caption></p></div><p>然而，對於大多數實際目的而言，這種最大化操作在計算上是不可行的（因為沒有其他方式比在整個空間中搜索給定的動作值函數）。相反，我們可以期望做的是構建一個函數逼近器來逼近這個argmax，因此稱為確定性策略梯度（DPG）。</p><p>我們用下面的等式來總結這一點。</p><p>DPG目標</p><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/152800251423856c71a325b><p class=pgc-img-caption></p></div><p>確定性策略梯度</p><div class=pgc-img><img alt=策略梯度的簡明介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1528002540785e93fb4daa4><p class=pgc-img-caption></p></div><p>這個數值變成了我們可以用MCMC抽樣再次估計的另一個期望值。</p><h1>通用強化學習框架</h1><p>現在我們可以得到一個通用的算法，看看我們所學到的所有部分是如何結合在一起的。所有新算法通常都是下面給出的算法的變體</p><blockquote><p>Loop:</p><p>Collect trajectories (transitions - (state, action, reward, next state, terminated flag))</p><p>(Optionally) store trajectories in a replay buffer for sampling</p><p>Loop:</p><p>Sample a mini batch of transitions</p><p>Compute Policy Gradient</p><p>(Optionally) Compute Critic Gradient</p><p>Update parameters</p></blockquote><h1>Code</h1><p>對於熟悉Python的讀者來說，這些代碼片段旨在成為上述理論思想的更具體的表示。這些已經被從實際代碼的學習循環中取出。</p><p><strong>策略梯度(同步 Actor-Critic)</strong></p><blockquote><p># Compute Values and Probability Distribution</p><p>values, prob = self.ac_net(obs_tensor)</p><p># Compute Policy Gradient (Log probability x Action value)</p><p>advantages = return_tensor - values</p><p>action_log_probs = prob.log().gather(1, action_tensor)</p><p>actor_loss = -(advantages.detach() * action_log_probs).mean()</p><p># Compute L2 loss for values</p><p>critic_loss = advantages.pow(2).mean()</p><p># Backward Pass</p><p>loss = actor_loss + critic_loss</p><p>loss.backward()</p></blockquote><p>深層確定性策略梯度</p><blockquote><p># 從軌跡獲取動作的Q值</p><p>current_q = self.critic(obs_tensor, action_tensor)</p><p># 獲取目標Q值</p><p>target_q = reward_tensor + self.gamma * self.target_critic(next_obs_tensor, self.target_actor(next_obs_tensor))</p><p># L2 loss for the difference</p><p>critic_loss = F.mse_loss(current_q, target_q)</p><p>critic_loss.backward()</p><p># Actor loss based on the deterministic action policy</p><p>actor_loss = - self.critic(obs_tensor, self.actor(obs_tensor)).mean()</p><p>actor_loss.backward()</p></blockquote></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>簡明</a></li><li><a>介紹</a></li><li><a>梯度</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/4adca96d.html alt=不鏽鋼旋轉樓梯—不鏽鋼旋轉樓梯的原理和尺寸介紹 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1532917894689e20edadba9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4adca96d.html title=不鏽鋼旋轉樓梯—不鏽鋼旋轉樓梯的原理和尺寸介紹>不鏽鋼旋轉樓梯—不鏽鋼旋轉樓梯的原理和尺寸介紹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b11a0727.html alt=今天介紹的電纜型號是：RVVP class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/db5ce8c1-b1ad-424b-a931-d371d5aad449 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b11a0727.html title=今天介紹的電纜型號是：RVVP>今天介紹的電纜型號是：RVVP</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e9cbcc2d.html alt=什麼是“志願梯度”？為什麼填報志願時要注意“志願梯度”？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1524050959957fa1dbd0e0d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e9cbcc2d.html title=什麼是“志願梯度”？為什麼填報志願時要注意“志願梯度”？>什麼是“志願梯度”？為什麼填報志願時要注意“志願梯度”？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/087a8932.html alt=如何真正理解梯度的含義 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d4719d4d74574ed1a033aefbd26cee65 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/087a8932.html title=如何真正理解梯度的含義>如何真正理解梯度的含義</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/65d2605f.html alt=什麼是梯度：用形象的語言解讀梯度的本質原理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/557c7b377c5449bb885df35b2d354e03 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/65d2605f.html title=什麼是梯度：用形象的語言解讀梯度的本質原理>什麼是梯度：用形象的語言解讀梯度的本質原理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e087ca41.html alt=偏導數和函數的梯度 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/9d20a1e4cbff42a094d57df057fe9597 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e087ca41.html title=偏導數和函數的梯度>偏導數和函數的梯度</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/79e15fbc.html alt=如何使用pytorch自動求梯度 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/81815a9554cb45cba6a49188c07e579c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/79e15fbc.html title=如何使用pytorch自動求梯度>如何使用pytorch自動求梯度</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5fc113b1.html alt=梯度原理：梯度在每一點上都指向函數增長最快的方向 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/cdb8db41d5024f38a2e490e66baebdb4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5fc113b1.html title=梯度原理：梯度在每一點上都指向函數增長最快的方向>梯度原理：梯度在每一點上都指向函數增長最快的方向</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9451cf29.html alt=梯度組織工程用生物材料的製備 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/11e3fbe804ad4ef09d8f41415c48a879 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9451cf29.html title=梯度組織工程用生物材料的製備>梯度組織工程用生物材料的製備</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/931a18f4.html alt=高考志願填報院校梯度、專業梯度、批次梯度如何設置？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/931a18f4.html title=高考志願填報院校梯度、專業梯度、批次梯度如何設置？>高考志願填報院校梯度、專業梯度、批次梯度如何設置？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c99d724e.html alt=給大家介紹幾種常見的齒輪，學機械的收藏了 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/dfic-imagehandler/b864f3ca-e8bf-47f4-a940-027e7a96e4a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c99d724e.html title=給大家介紹幾種常見的齒輪，學機械的收藏了>給大家介紹幾種常見的齒輪，學機械的收藏了</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/37860fee.html alt=「小恩學堂」壁掛爐核心部件介紹第4期——循環水泵 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/5e7a000263651c843347 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/37860fee.html title=「小恩學堂」壁掛爐核心部件介紹第4期——循環水泵>「小恩學堂」壁掛爐核心部件介紹第4期——循環水泵</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a14a021d.html alt=火力發電廠主要設備及其作用介紹 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a14a021d.html title=火力發電廠主要設備及其作用介紹>火力發電廠主要設備及其作用介紹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/82ec221c.html alt="向三歲孩子介紹地球系列之一 - 天空，大地，水元素分類遊戲" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5e731406b21a45718433a22396b5282b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/82ec221c.html title="向三歲孩子介紹地球系列之一 - 天空，大地，水元素分類遊戲">向三歲孩子介紹地球系列之一 - 天空，大地，水元素分類遊戲</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4dce8a5a.html alt=庫爾勒景區介紹 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/70872229-1b85-4bda-b208-afc6881ea16b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4dce8a5a.html title=庫爾勒景區介紹>庫爾勒景區介紹</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>