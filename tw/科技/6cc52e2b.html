<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>使用機器學習的手寫數字識別 | 极客快訊</title><meta property="og:title" content="使用機器學習的手寫數字識別 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6cc52e2b.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6cc52e2b.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6cc52e2b.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6cc52e2b.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6cc52e2b.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6cc52e2b.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6cc52e2b.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6cc52e2b.html><meta property="article:published_time" content="2020-11-14T21:05:10+08:00"><meta property="article:modified_time" content="2020-11-14T21:05:10+08:00"><meta name=Keywords content><meta name=description content="使用機器學習的手寫數字識別"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/6cc52e2b.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>使用機器學習的手寫數字識別</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p>機器學習和深度學習在計算機技術和人工智能中發揮著重要作用。通過深度學習和機器學習，可以減少識別，學習，預測和更多領域的人力。本文介紹了從著名的MNIST數據集識別手寫數字（0到9），比較KNN，PSVM，NN和卷積神經網絡等分類器的性能，準確性，時間，靈敏度，正產生率和特異性，使用不同的參數與分類器。</p><div class=pgc-img><img alt=使用機器學習的手寫數字識別 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b><p class=pgc-img-caption>來自MNIST數據集的樣本數字</p></div><p>無論是機器學習和深度學習的初學者還是已經練習多年的專家，手寫數字識別仍然會獲得非常廣泛的關注。開發這樣的系統包括用於理解手寫數字的圖像並將其分類為10位數（0-9）的機器。MNIST數據庫中的手寫數字在近年來已經在社區中廣為人知，因為使用不同的分類器和參數以及預處理技術降低錯誤率，從線性分類器（1層NN）的12％錯誤率到達到0.23％的誤差具有35個卷積神經網絡層次的速率。本文的範圍是比較不同分類器與不同的參數，並嘗試實現近人類的性能。</p><p><strong>數字識別系統</strong></p><p>數字識別系統是一臺機器的工作，用於訓練自己或識別來自不同來源的數字，如電子郵件，銀行支票，紙張，圖像等，以及在不同的真實場景中，用於在計算機平板電腦或系統上進行在線手寫識別，識別車輛號碼板塊，處理銀行支票金額，手工填寫的表格中的數字條目（比如 - 稅表）等</p><p><strong>手寫數字的問題</strong></p><p>手寫數字並不總是具有相同的大小，寬度，方向和邊距，因為它們與人的寫作方式不同，因此一般的問題是由於數字之間的相似性（例如1和7）對數字進行分類， 5和6,3和8,2和5,2和7等等。當許多人用各種不同的筆跡書寫一個數字時，這個問題就越多。最後，不同個體的筆跡的獨特性和多樣性也影響數字的形成和外觀。</p><p><strong>MNIST數據集</strong></p><p>從MNIST數據集提供的樣本總共有70,000個圖像的手寫數字，其包括訓練集中的60,000個示例和測試集中的10,000個示例，兩者都具有來自10位數（0到9）的標記圖像。這是來自MNIST的一小部分，其中尺寸被標準化以適合20 * 20像素的盒子而不改變長寬比。手寫數字是28 * 28灰度級圖像形式的圖像，表示圖像以及第一列，作為每個圖像的標籤（0到9）。同樣選擇測試集的情況為10,000個圖像，標籤為0到9。</p><p><strong>Yann Lecun，Corinna Cortes</strong>和<strong>Christopher Burges</strong>開發了這個MNIST數據集，用於評估和改進手寫數字分類問題的機器學習模型。MNIST數據集是從NIST的特殊數據集開發的，其中包括特殊數據庫3（美國人口普查局員工）和特殊數據庫1（高中學生），其中包含手寫數字的二進制圖像。早期的SD-3（特殊數據庫-3）被認為是訓練和SD-1（特殊數據庫-1）作為測試集，具有更容易識別的SD-3級別。因此，為了在不同的學習分類器中保持挑戰性，NIST數據集被混淆了。MNIST的劃分來自SD-3的30,000個樣本和來自SD-1的30,000個樣本，其中約250個人參與。來自SD-3和剩餘5個的5,000個樣本，來自SD-1的000個樣本形成不同的測試集。數字圖像取自各種掃描數字，標準化大小並都已經居中對齊。這使其成為評估模型的優秀數據集，並允許機器學習的抱負者專注於深度學習和機器學習，而只需很少的數據清理。</p><p>談到與標準MNIST相似的更新或更多修改版本，2017年出現了EMNIST或擴展MNIST，其中訓練集中的2,40,000個圖像的樣本以及測試集中的40,000個圖像的增量由手寫的數字組成。</p><p><strong>數據集中的可用文件</strong></p><p>因此，在深入探討本主題之前，最好的方法是熟悉所提供的數據集。以下幾點與訓練和測試集以及圖像和標籤文件集相同 -</p><ol><li>像素按行排列，從0到255不等，與RGB顏色代碼一樣。</li><li>背景為白色（RGB值為0），前景為黑色（RGB值為255）。</li><li>數字標籤從0到9分類。</li></ol><p><strong>有4個訓練和測試文件：</strong></p><p>1.訓練集圖像文件（train-images-idx3-ubyte） -</p><p>2.訓練集標籤文件（train-labels-idx1-ubyte） -</p><p>3.測試集圖像文件（t10k-images-idx3-ubyte） -</p><p>4.測試集標籤文件（t10k-labels-idx1-ubyte） -</p><p><strong>更好地理解數據集</strong></p><p>MNIST數據集以IDX格式提供。這種IDX文件格式是一種簡單的格式，在使用不同數值類型的向量和高維矩陣進行操作時非常方便。從文件格式的description列中的數字開始。我們可以將其定義為整數值（比如MSB優先），其中前2個字節總是為零。這給了我們以下信息：</p><ol><li>0000（2個字節）通知文件的開頭。</li><li>08告訴我們第三個字節是無符號字節類型。</li><li>第4個字節0​​3告訴我們矩陣有三個維度，01只用一個維度。</li></ol><p>第3個字節表示數據是整數、浮點數、短整數、長整數還是無符號類型。第4個字節表示向量或矩陣的維數，即行和列的數量。如果它等於1，則它是一個向量，否則它是一個矩陣。items變量的數量也首先被讀作MSB。</p><p><strong>從IDX更改為更簡單的CSV</strong></p><p>由於我們的數據集以IDX格式提供，我們可以通過算法將數據集更改為CSV格式，我們可以用CSV實現MNIST數據集格式。</p><p><strong>為了更好地理解CSV：</strong></p><ol><li>第一列或值是“標籤”，即手寫應該分類的實際真實數字，例如“7”或“9”。這是分類器渴望分類的正確解決方案。</li><li>剩餘值或所有逗號分隔值是手寫數字的像素值強度，從0到255不等。圖像的大小為28乘28，因此標籤的值為784（28 * 28） 。</li></ol><p><strong>分類器</strong></p><p>在本節中，我們將討論機器學習和深度學習的各種算法，以便進行預測和準確性。機器學習中的分類器 -</p><p><strong>KNN（K nearest neighbors（K最近鄰））</strong></p><p>KNN是用於分類和迴歸問題的非參數方法或分類器。這是延遲或晚期學習分類算法，其中所有計算都被導出直到分類的最後階段，以及這是基於實例的學習算法，其中近似是在局部進行的。最簡單和最容易實現，沒有明確的訓練階段，並且算法不執行任何訓練數據的泛化。</p><p><strong>什麼時候用？</strong>直接解決方案是當這些是類之間的非線性決策邊界時，或者當數據量足夠大時。輸入特徵本質上可以是定性的和定量的。而輸出要素可以是分類值，這些值是數據中看到的典型類。</p><p><strong>KNN</strong>使用K個最近鄰的多數選擇來解釋分類值，其中K的值可以不同，因此在改變K的值時，選擇的數值也可以變化。</p><p><strong>假設</strong></p><ol><li>作為非參數，該算法不對基礎數據假設做出任何假設。</li><li>根據數據選擇參數K.</li><li>需要距離度量來定義任意兩個數據點之間的接近度。該距離可以從歐幾里德距離，馬哈拉諾比斯距離，漢明距離等計算得出。</li></ol><p><strong>算法</strong></p><ol><li>計算測試數據點和所有標記數據點之間的距離度量。</li><li>按照距離度量的遞增順序對標記的數據點進行排序。</li><li>選擇前K個標記的數據點並查看類標籤。</li><li>查找大多數這些K標記數據點具有的類標籤，並將其分配給測試數據點。</li></ol><p><strong>需要考慮的事項 -</strong></p><ol><li>參數選擇 - K的最佳選擇取決於數據。較大的K值降低了噪聲對分類的影響，但使得無類別之間的決策邊界不同。較小的K值往往受到噪聲的影響，並且類之間有明顯的分離。</li><li>存在噪聲</li><li>特徵選擇和縮放 - 減少不相關的特徵非常重要。當特徵數量太大並且懷疑是高度冗餘時，將需要提取特徵。如果仔細選擇這些功能，那麼預計分類會更好。</li><li>維度的災難</li></ol><div class=pgc-img><img alt=使用機器學習的手寫數字識別 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a376afabe9a744bb81b15f889dc3c949><p class=pgc-img-caption>案例1（K = 3）和案例2（K = 5）</p></div><p>為了更好地理解，讓我們看看K的不同值。在案例1中，K的值為3。然後，測試數據點的類在紅色和藍色的類中將是紅色。對於情況2中的K = 5，則預測的類將是來自KNN算法的藍色。因此，為了改變K的值，測試數據點的輸出也可以變化。所以有必要明智地選擇K的值。K的較大值可以降低整體噪聲，但不能保證精確度。</p><p><strong>距離函數</strong></p><p>KNN使用的不同距離函數是</p><ol><li>Euclidean function</li><li>Manhattan function</li><li>Minkowski</li><li>Hamming distance</li><li>Mahalanobis distance</li></ol><div class=pgc-img><img alt=使用機器學習的手寫數字識別 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c6c47b55dac845dcae9b7315453eca5e><p class=pgc-img-caption>KNN中的距離函數</p></div><p><strong>執行KNN的命令</strong></p><p>我們可以改變分類器的參數並觀察分類器提取的變化，並比較使用不同參數和超參數的效果和效率。</p><blockquote><p><em>class</em> sklearn.neighbors。 <strong>KNeighborsClassifier</strong>（ <em>n_neighbors = 5</em>， <em>weights ='uniform'</em>， <em>algorithm ='auto'</em>，<em>leaf_size = 30</em>， <em>p = 2</em>， <em>metric ='minkowski'</em>， <em>metric_params = None</em>， <em>n_jobs = 1</em>， <em>** kwargs</em>）</p></blockquote><p><strong>SVM（支持向量機）</strong></p><p>SVM屬於監督學習的範疇，具有分類獎勵和迴歸問題。通常，SVM繪製最佳超平面，其分類為不同的類別。在二維空間中，首先，我們繪製對應於因變量的自變量的數據點。然後，從查看超平面或任何線性或非線性平面開始分類過程，將兩個類別區分開來。</p><p><strong>算法</strong></p><p>首先要理解，在二進制分類的情況下：</p><ol><li>確定正確的超平面，更好地分離這兩個類。</li><li>查找最近數據點（任何類別）和超平面之間的最大距離，距離測量為邊距。因此，尋找具有最大邊距的超平面。具有較高邊距的超平面更加穩健，而低邊距因錯誤分類而發生變化。</li><li>SVM準確選擇分類器以最大化邊距。</li><li>SVM對分類器具有魯棒性，並且具有忽略異常值並嘗試尋找具有最大餘量的超平面的功能。</li></ol><div class=pgc-img><img alt=使用機器學習的手寫數字識別 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5880e0799b884143bc50cb039f401b48><p class=pgc-img-caption>SVM在分類兩個類（紅色和藍色）</p></div><p><strong>調整參數</strong></p><p>1.核（Kernel）：線性代數在線性SVM中轉換超平面學習中起作用。</p><blockquote><p><strong>F（x）= B（0）+ sum（ai *（X，Xi））</strong></p></blockquote><p>2.線性內核（Linear kernel）：點積是內核，顯示為K（x，xi）= sum（x * xi），內核是新數據點和支持向量超平面之間的相似性或距離度量。<strong>核心技巧是通過多項式和指數技巧計算的更高維度的分離線。</strong></p><p>3.多項式內核（Polynomial kernel）：與內核相同但指定了一個度。如果d = 1，則轉換為線性內核。</p><blockquote><p><strong>K（x，xi）= 1 + sum（x * xi）^ d。</strong></p></blockquote><p>4.徑向內核（Radial kernel）：更復雜的內核是徑向內核。</p><blockquote><p><strong>K（x，xi）= exp（-gamma * sum（（x-xi²））</strong></p></blockquote><p>在算法中指定伽馬（γ）的情況下，伽馬的良好考慮值取為0.1，其中伽瑪在0和1之間不同。當徑向核在特徵空間內創建複雜區域時，形成二維中的閉合多邊形。</p><div class=pgc-img><img alt=使用機器學習的手寫數字識別 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f36052da16b84c5a8b043bd6659e650a><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=使用機器學習的手寫數字識別 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/fc40539d98fb48509dc0c228fe88efdb><p class=pgc-img-caption></p></div><p>5.Margin：Margin應保持兩邊等距離。</p><div class=pgc-img><img alt=使用機器學習的手寫數字識別 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0bedbd0b9843488b9ed78ef5acbfb747><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=使用機器學習的手寫數字識別 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/89ed952ae58d42469115181be538fd67><p class=pgc-img-caption></p></div><p><strong>計算命令</strong></p><blockquote><p><em>class</em> sklearn.svm。 <strong>SVC</strong>（ <em>C = 1.0</em>， <em>內核='rbf'</em>， <em>度數= 3</em>， <em>gamma ='auto'</em>， <em>coef0 = 0.0</em>， <em>收縮=真</em>， <em>概率=假</em>， <em>tol = 0.001</em>， <em>cache_size = 200</em>， <em>class_weight =無</em>， <em>詳細=假</em>， <em>max_iter = -1</em>， <em>decision_function_shape =' ovr '</em>，<em>random_state = None</em>）</p></blockquote><p><strong>NN（神經網絡）</strong></p><p>神經網絡模仿我們大腦的工作方式。它們在計算能力不斷提高的時代出現了很多。</p><div class=pgc-img><img alt=使用機器學習的手寫數字識別 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/284da049da6447ac9eccdf924bfa85b1><p class=pgc-img-caption>具有輸入，輸出和隱藏層的神經網絡</p></div><p>深度學習的概念源於人工神經網絡的研究，神經網絡是與多層連接的網絡。這些圖層由節點組成。節點只是一種感知，它接受輸入執行一些計算，然後通過節點的激活函數，以顯示信號進展通過網絡進行上下文分類。</p><p><strong>算法</strong></p><ol><li>隨機初始化權重（並不是將它們保持為零）</li><li>實現向前傳播以實現<em>hθ</em>（<em>x</em>（<em>i</em>））。</li><li>計算成本</li><li>評估反向傳播以計算偏導數並使用梯度檢查來確認反向傳播正常工作。然後禁用梯度檢查。</li><li>使用梯度下降或任何內置優化函數來最小化具有θ權重的成本函數。</li></ol><p><strong>結合</strong></p><p>選擇神經網絡的佈局，包括每層中的多個隱藏單元以及總層數</p><ol><li>特徵Xi的尺寸等於輸入單元的數量。</li><li>輸出單元的數量是類的數量。</li><li>每層隱藏單元的數量通常越多越好（必須與計算成本平衡，因為它隨著更多隱藏單元而增加）。</li><li>默認值：1個隱藏層，如果隱藏層多於1個，則每個隱藏層中的單元數相同。</li></ol><p><strong>分類器命令</strong></p><p>MLP代表多層感知器，在這裡我們使用帶有MLPClassifier的sklearn以及不同的參數。</p><blockquote><p><em>class sklearn.neural_network。</em> <strong><em>MLPClassifier</em></strong> <em>（hidden_​​layer_sizes =（100，），activation ='relu'，solver ='adam'，alpha = 0.0001，batch_size ='auto'，learning_rate ='constant'，learning_rate_init = 0.001，power_t = 0.5，max_iter = 200，shuffle = True，random_state = None，tol = 0.0001，verbose = False，warm_start = False，momentum = 0.9，nesterovs_momentum = True，early_stopping = False，validation_fraction = 0.1，beta_1 = 0.9，beta_2 = 0.999，epsilon = 1e-08）</em></p></blockquote><p><strong>CNN（卷積神經網絡）</strong></p><p>現在讓我們來討論卷積神經網絡，CNN近來已成名。CNN是深度前饋人工神經網絡的一部分，可以在不同的圖像和視頻識別，推薦系統和自然語言處理應用中執行各種任務，比其他分類器具有更好的時間和精度。</p><div class=pgc-img><img alt=使用機器學習的手寫數字識別 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a35bf51b6e5e4b2aabaad71216b8967e><p class=pgc-img-caption>CNN中神經元的排列</p></div><p>隨著Facebook使用神經網絡進行自動標記算法，谷歌搜索亞馬遜的照片搜索產品推薦，Pinterest用於家庭飼料個性化，Instagram用於搜索基礎設施，CNN的使用已經普及。圖像分類或對象識別是一個問題，即將圖像作為參數傳遞並預測條件是否滿足（是貓還是狗），或圖像的概率或最令人滿意的條件。我們能夠快速識別模式，從以前的信息和知識中概括出來。</p><div class=pgc-img><img alt=使用機器學習的手寫數字識別 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b7b7a52e34d247a7bb59a5678b40e460><p class=pgc-img-caption>我們看到的與系統看到的</p></div><p><strong>輸入和輸出</strong></p><p>當計算機或系統拍攝圖像時，它只會看到一組像素值。假設480 * 480 * 3，其中480 * 480是大小，3是指RGB值。這些數字中的每一個都被賦值為0到255，作為該點的像素強度。關鍵點在於，基於將圖像作為輸入，計算機系統預測並作出輸出以用於描述圖像是所述或某個類的概率（對於類1為0.90，對於類2為0.96，對於類3為0.4）。</p><p><strong>算法</strong></p><p>要查看系統預測的執行步驟，我們可以將算法定義為 -</p><ol><li>將圖像分解為小圖像切片 - 與滑動窗口類似，我們可以在整個大圖像上傳遞滑動窗口，並將每個結果保存為單獨的，作為一個大圖像片段作為小圖片圖塊。</li><li>將每個小塊數據送入較小尺寸的神經網絡 - 我們很少用相同的值初始化參數，如果不是這樣，那麼我們將該數據標記為有用。</li><li>將每個小塊數據的結果保存到一個新數組中 - 我們不想錯放原始文件的索引。因此，我們將結果放在與原始圖像相同排列的網格中。</li><li>下采樣 - 為了減小新數組的大小，最大池化使用下采樣。</li></ol><div class=pgc-img><img alt=使用機器學習的手寫數字識別 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/27fb67ad154e454088f4252f2e23df3a><p class=pgc-img-caption>MNIST數據集中的CNN架構</p></div><p><strong>卷積神經網絡層</strong></p><p>這些層的多次出現表明我們的網絡有多深，這種形成被稱為深度神經網絡。</p><ol><li>輸入：原始像素值作為輸入提供。</li><li>卷積層：輸入層轉換神經元層的結果。需要指定要使用的濾波器。每個濾波器只能是一個5 * 5的窗口，可以滑動輸入數據並獲得最大強度的像素。</li><li>線性整流單元[ReLU]層：對作為圖像的數據提供激活函數。在反向傳播的情況下，使用ReLU函數來防止像素值的變化。</li><li>池化層：沿著維度（寬度，高度）在體積中執行下采樣操作。</li><li>完全連接的圖層：聚焦評分等級，並找到輸入數字的最高分數。</li></ol><p>隨著我們在層中越來越深入，複雜性增加了很多。但值得一提的是，準確度可能會提高，但時間消耗也會增加。</p><p><strong>績效措施</strong></p><p>在機器學習和深度學習中，分類器的性能或效率由各種特徵顯示，這些特徵告訴特定分類器的工作情況。</p><p><strong>混淆矩陣</strong></p><p>這也與誤差矩陣相同，通過混淆矩陣可以很容易地證明我們的分類器做出的預測百分比是正確的，並且分類器難以預測實際分類。為了顯示混淆矩陣，最好以表格的形式進行練習。為了為我們的數字創建一個混淆矩陣，我們將面對10個類別，即10行和10列，其中每個數字都將與其他數字進行比較，我們可以很容易地顯示我們的分類器預測錯誤的地方以及它預測正確的地方以及它預測的總次數</p><p><strong>使用的術語是</strong></p><ol><li><strong>TP</strong> = True positive</li><li><strong>TN </strong>= True negative</li><li><strong>FP</strong> = False positive</li><li><strong>FN</strong> = False negative</li></ol><p><strong>TP</strong>模型預測為正的正樣本，<strong>TN</strong>模型預測為負的負樣本，<strong>FP</strong>模型預測為正的負樣本，<strong>FN</strong>模型預測為負的正樣本。</p><p><strong>準確性</strong></p><p>分類器的總體有效性，最好地從總數中定義真實結果的準確性或部分（意味著具有真實的肯定和真實的負面）。</p><blockquote><p>精度=（TP + TN）/ N，其中N是TP，TN，FN，FP的總和。</p></blockquote><p>準確度可以達到的最大值是1.當分類器對兩個組進行精確分類時（即FP = 0和FN = 0），會發生這種情況。請記住，<strong>True positive</strong>的總數是TP + FN。<strong>True negative</strong>總數是TN + FP。</p><p><strong>敏感性</strong></p><p>敏感性可以定義為分類器識別陽性標記的有效性。這也稱為召回。</p><blockquote><p>Sensitivity=（TP）/（TP + FN）</p></blockquote><p><strong>特異性</strong></p><p>這被定義為分類器正確識別陰性標記的有效性。</p><blockquote><p>Specificity=（TN）/（FP + TN）</p></blockquote><p>靈敏度和特異性都在0和1之間，每個都是理想值。我們將平衡準確度計算為平均敏感性和特異性。</p><p><strong>普遍性</strong></p><p>那麼，在我們的樣本中，“是”條件實際發生的頻率是多少？</p><blockquote><p>Prevalence=（TP + FN）/ N.</p></blockquote><p>N是所有條件的總和，即TP，FN，FP，TN。</p><p>正預測值</p><blockquote><p>Positive_predicted_value =（Sensitivity*Prevalence）/（（Sensitivity*Prevalence）+（1-Specificity）*（1-Prevalence））</p></blockquote><p>負預測值</p><blockquote><p>Negative_predicted_values =Specificity*（1 - Prevalence）/（（（1-Sensitivity）*Prevalence）+（Specificity*（1 - Prevalence）））</p></blockquote><p>檢測率</p><p>檢測率是真實陽性除以條件總數。</p><blockquote><p>DR = TP / N.</p></blockquote><p>預期的準確性</p><p>也被視為條件中的隨機機會</p><blockquote><p>Expected_accuracy =（（TP + FN）*（TP + FP）+（FP + TN）*（FN + TN））/ N</p></blockquote><p>其中N是所有條件的總和，即TP，FN，FP和TN。</p><p>Kappa統計</p><p>Kappa統計量（或值）是將觀察到的準確度與預期準確度（比如隨機機會）進行比較的度量。</p><blockquote><p>Kappa =（觀察到的準確度 - expected_accuracy）/（1 - expected_accuracy）</p></blockquote><p>分類器有很多性能指標可以顯示它在這些統計情況下的表現。分類器預測的速度也提高了整體性能，我們對快速分類器進行了分類。通過這些示例，首先保留描述不可見數據集的預測中的錯誤細分。</p><p><strong>結果</strong></p><p>用於研究目的，或將分類器應用於實際場景問題。識別的準確性和速度被認為是更好的衡量標準。現在逐一談論不同的分類器。</p><p>總體比較結果</p><p>顯示訓練集中使用的不同分類器之間的準確性，時間，特異性，敏感性和其他參數比較。</p><div class=pgc-img><img alt=使用機器學習的手寫數字識別 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/bae38362a7fd4767bfb521233e334c92><p class=pgc-img-caption>訓練和測試數據的分類器比較</p></div><p><strong>結論和未來的工作</strong></p><p>由於使用機器學習算法如KNN，SVM，神經網絡以及不同的參數和特徵縮放矢量，我們還看到了分類器之間在精度和時序的最重要特徵方面的不同比較。準確性可以根據訓練和測試數據的分割而改變，如果提供訓練和測試數據的數量，這可以進一步改進。如果數據量增加，總有機會提高準確性。每個分類器都有自己的準確性和時間消耗。我們還可以包括這樣的事實：如果CPU的功率變為GPU，則分類器可以以更好的準確性和更少的時間執行，並且可以觀察到更好的結果。</p><p>分類器的性能可以根據正確識別條件的能力（靈敏度），真實結果的比例（準確性），分類程序中的陽性結果數量作為假陽性（陽性預測）和排除能力來衡量。條件正確（特異性）。在這裡，我們看到了與機器學習和深度學習的分類器的簡要比較。</p><p>到目前為止，深度學習算法在手寫數字識別的應用中表現更好。</p><p><strong>未來的研究</strong>可能會考慮使用卷積網絡的架構，該架構在MNIST數據庫上給出最好的結果，並且所提出的識別系統在手寫數字上實現。這樣的系統可以設計用於手寫字符識別，對象識別，圖像分割，手寫識別，文本語言識別，未來研究也可以考慮在線數字識別系統的硬件實現，具有更高的性能和效率，實時測試結果的實時結果場景。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>數字識別</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3873d795.html alt=金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/694a9289cde541dca807f9a30d291d0d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3873d795.html title=金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例>金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4d0f33bb.html alt=機器學習：什麼是自然語言處理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/734232eb22dd45cfa000a5ed20aa6c78 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4d0f33bb.html title=機器學習：什麼是自然語言處理>機器學習：什麼是自然語言處理</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>