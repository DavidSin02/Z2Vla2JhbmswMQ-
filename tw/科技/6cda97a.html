<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 | 极客快訊</title><meta property="og:title" content="上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/5f757f260bf1489db295e89fa564231d"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6cda97a.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6cda97a.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6cda97a.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6cda97a.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6cda97a.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6cda97a.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/6cda97a.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/6cda97a.html><meta property="article:published_time" content="2020-10-29T21:01:05+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:05+08:00"><meta name=Keywords content><meta name=description content="上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/6cda97a.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p><strong>2020 北京智源大會</strong></p><p>本文屬於2020北京智源大會嘉賓演講的整理報道系列。北京智源大會是北京智源人工智能研究院主辦的年度國際性人工智能高端學術交流活動，以國際性、權威性、專業性和前瞻性的“內行AI大會”為宗旨。2020年6月21日-24日，為期四天的2020北京智源大會在線上圓滿舉辦。來自20多個國家和地區的150多位演講嘉賓，和來自50多個國家、超過50萬名國內外專業觀眾共襄盛會。</p><p><br></p><p>6月24日上午，在第二屆北京智源大會“強化學習專題論壇”上，上海交通大學<strong>張偉楠</strong>副教授做了主題為《<strong>Model-based Reinforcement Learning: Fundamentals and Advances</strong>》的演講。</p><p><br></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5f757f260bf1489db295e89fa564231d><p class=pgc-img-caption></p></div><p></p><p>張偉楠，上海交通大學副教授，主要研究領域為強化學習、深度學習、數據科學、知識圖譜等，2017年獲得上海ACM新星獎，2018年獲得首屆達摩院青橙獎。</p><p></p><p>在本次報告中，張偉楠從無模型強化學習與有模型強化學習的對比開始，結合基於黑盒的有模型強化學習的發展歷史，深入淺出地講解了有模型強化學習諸多算法的基本概念、算法起源、實現原理、理論分析以及實驗結果等，詳細介紹了所在課題組在這一領域的最新工作進展，並對這一領域今後的發展方向進行了前瞻性的總結概述。</p><p></p><p><strong>整理：智源社區 張文聖</strong></p><p><br></p><p><strong>一、基於模型的深度強化學習算法研究背景</strong></p><p></p><p>張偉楠介紹，深度強化學習算法自提出以來，常用於Atari Game、圍棋、DOTA、星際等虛擬場景中。人們通常將這種不建立環境模型，僅依靠實際環境的採樣數據進行訓練學習的強化學習算法稱為無模型強化學習（Model-Free Reinforcement Learning，MFRL）算法，也即是不依賴於環境模型的強化學習算法。這種方法適合應用於深度神經網絡的框架，人們將大量數據以Mini-Batch的形式傳入神經網絡，可以對價值網絡或者策略網絡進行非常高效的訓練。</p><p></p><p>然而，MFRL發展中遇到的一個困境：數據採集效率（Sample Efficiency）太低。在有監督或無監督學習中，人們構建一個目標函數，通過梯度下降（或上升）的方式，不斷趨近理想結果。與有監督/無監督學習不同的是，強化學習屬於一種試錯的學習範式，當前策略的採樣結果如果無法有效幫助當前策略進行提升，則可以認為當前試錯的採樣結果是無效採樣。在MFRL訓練過程中，智能體有大量的交互採樣屬於無效採樣，這些採樣沒有對行動策略的改進產生明顯的影響。為了解決無模型強化學習中的這一數據效率低下的問題，人們開始轉向基於模型強化學習（Model-Based Reinforcement Learning，MBRL）的方法。</p><p></p><p>MBRL的基本思想在於首先建立一個環境的動態模型，然後在建立的環境模型中訓練智能體的行動策略，通過這種方式，實現數據效率的提升。</p><p></p><p>將MBRL與MFRL對比來看，MBRL存在如下特點：</p><p></p><p>1. 環境模型一旦建立起來，便可以採用on-policy的訓練方法，利用當前採樣得到的數據訓練當前的策略，在這種情形下，採樣效率是最高的。</p><p></p><p>2. 建立環境模型後，便可以選擇性地不再與實際場景交互，在模型中進行訓練學習，完成訓練後再在實際場景中投入使用（off-line RL，也稱為batch RL）。</p><p></p><p>3. 相比於MFRL，MBRL數據採樣效率會往往有較大的提升。</p><p></p><p>4. 存在模型與實際環境之間的複合誤差問題（Compounding Error），模型向後推演的幅度越長，推演誤差就會越大，直至模型完全失去作用。</p><p></p><p>而MFRL，則存在如下特點：</p><p></p><p>1. 相比於MBRL，MFRL擁有最好的漸進性能（Asymptotic Performance），當策略與環境交互達到收斂狀態時，相比於MBRL，MFRL下訓練所得策略所最終達到的性能會更好，能夠避免出現複合誤差的問題，因而在實際環境中表現會更為優異。</p><p></p><p>2. MFRL非常適合使用深度學習框架去採樣超大規模的數據，並進行網絡訓練。</p><p></p><p>3. MFRL經常採用Off-Policy訓練方法，這種情況下會有偏差（Bias）導致的訓練效果不穩定（instability）的問題。</p><p></p><p>4. MFRL需要進行超大量的數據採樣，因而需要超高的算力要求，這種算力要求是很多科研院所或者企業所無法負擔的。</p><p></p><p>關於MBRL的進一步分類，其主要包括黑盒模型與白盒模型兩類：</p><p></p><p>黑盒模型中，環境模型的構造是未知的，僅作為數據的採樣來源。由於採樣數據來自於黑盒模型，而不是和真實環境交互得到，因此這些來自模型的採樣數據不計入數據採樣效率的計算中。雖然從計算結果來看MFBL的數據採樣效率較高，但由於訓練過程中使用了大量基於模型採樣的數據，因此從採樣數據總量上來看，實際採樣了更多的數據。常用的基於黑盒模型的MBRL算法包括Dyna-Q、MPC、MBPO等。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/50a6ee66ba71491dad4f990fea05ed5c><p class=pgc-img-caption>圖1：基於黑盒模型的MBRL算法</p></div><p><br></p><p>白盒模型中，環境模型的構造是已知的，可以將模型中狀態的價值函數直接對策略的參數進行求導，從而實現對策略的更新。常用的基於白盒模型的MBRL算法包括MAAC、SVG、PILCO等。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7f2b9a75607146e29e0e51d9389da4ec><p class=pgc-img-caption>圖2：基於白盒模型的MBRL算法</p></div><p><br></p><p><strong>二、基於黑盒模型的Dyna算法</strong></p><p></p><p>接下來，張偉楠主要介紹了基於黑盒模型的MBRL算法。這裡環境模型的構造未知，通過採集更多數據的方法來進行策略訓練。這樣處理的一大優勢在於算法具有更好的普適性，可以和幾乎所有無模型強化學習方法結合。</p><p></p><p>MBRL算法主要流程如下圖所示：</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bee899c5d2db4532900abb573f748222><p class=pgc-img-caption>圖3：基於黑盒模型的MBRL算法流程示意圖</p></div><p><br></p><p>首先，當前的價值函數Q(s, a)以及策略函數π(a | s)與真實環境進行交互，完成交互後採樣出環境反饋的數據Experience{(s, a, r, s’)}。然後通過採樣出的數據來訓練建立的環境模型p(s’, r | s, a )，環境的模型本質上是通過輸入的當前狀態State以及採取的動作Action，來預測產生的Reward以及下一步的狀態State，很多情況下Reward是根據先驗規則或領域知識生成，這時模型只預測下一步的狀態State即可。接下來在當前模型中進行Plannning，也就是通過當前模型進行數據的採樣，通過數據採樣的結果去訓練新一輪的價值函數Q(s, a)以及策略函數π(a | s)。</p><p></p><p>Q-Planning是最簡單的MBRL算法，它通過與真實環境交互的數據來訓練環境模型，然後基於當前的環境模型來進行一步Planning，從而完成Q函數的訓練。其步驟是，首先採集智能體與環境交互的State以及採用的Action，將數據傳入建立的黑盒模型中，採集並得到模型虛擬出來的Reward以及下一步的Next_State，接著將傳入模型的State、Action以及模型虛擬出來的Reward、Next_State進行一步Q-Learning訓練，這樣就完成了一步Q-Planning算法的更新。期間智能體僅僅通過與環境模型交互來進行數據的採樣，得到虛擬的Reward以及下一步的Next_State，並進行策略的訓練和更新，智能體未通過與實際環境交互數據來進行策略的更新。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/38e68fb25b3048018615221bebef825a><p class=pgc-img-caption></p></div><p><br></p><p>更進一步，我們還可以將Q-Planning與Q-Learning結合在一起，形成 Dyna算法。Dyna算法提出於90年代早期，完整的流程示意圖如下所示。可以看到，將中間的direct RL步驟去掉後，就是剛剛討論的Q-Planning算法。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a61da45beea5426196a2f681d79361fd><p class=pgc-img-caption>圖4：Dyna算法流程示意圖</p></div><p><br></p><p>在Dyna算法中，首先通過智能體與實際環境的交互進行一步正常的Q-Learning操作，然後通過與實際環境交互時使用的State、Action，傳入環境模型中進行Q-Planning操作，真實環境中進行一步Q-Learning操作，對應環境模型中進行n步Q-Planning操作。實際環境中的採樣與模型中的採樣具有1：n的關係，通過這種方式來提升訓練過程中的Sample efficiency。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9c9115cfe42342aa8c0080b990d68e02><p class=pgc-img-caption></p></div><p><br></p><p>以一個迷宮問題的場景為例，智能體從起始點S出發，前往目標點G，智能體可以沿上下左右四個方向任意行動，期間不能穿過迷宮的四周牆壁以及深色方塊所示的障礙物。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6bfadd232fbd4ee6b019794d291487a3><p class=pgc-img-caption>圖5：迷宮環境及其動作空間</p></div><p><br></p><p>下圖分別對比了0 planning-steps（只進行強化學習Q-learning）、5 planning-steps（每做1步Q-learning，做5步Q-Planning）以及50 planning-steps（每做1步Q-learning，做50步Q-Planning）情況下的結果。這裡的n planning-steps不代表前向（Forward）推演50步，而是指從頭進行一次State、Action的採樣，從而進行一步Q-Planning，按照這種方式進行n次Q-Planning操作。如下圖所示，橫座標為當前總計完成任務數量，也就是智能體在實際場景中累計從起始點S走到目標點G的次數。縱座標為完成當前輪次任務時智能體從起始點S走到目標點G所使用的Action步數，所使用的步數越低，則當前策略的性能越好。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/770a9b0d5a3943bca481c468dcc57c14><p class=pgc-img-caption>圖6：不同n值情況下n planning-steps的訓練結果</p></div><p><br></p><p>從圖中可以看到，當n值越大，環境模型推演越多時，所需的Episode任務數量越少，Policy達到最優的時間點越靠前，數據採樣效率越高。</p><p></p><p>張偉楠指出，MBRL算法的發展面臨著三個關鍵問題，同時這也是發展的三個思路：</p><p></p><p>1. 環境模型的建立是否真的有助於Sample Efficiency的提高？</p><p></p><p>2. 所建立的模型基本都是基於神經網絡建立，不可避免的會出現泛化性誤差的問題，那麼人們什麼時候可以相信建立的模型並使用模型進行策略的訓練與更新？</p><p></p><p>3. 如何適當地把握模型的使用尺度，來獲得最好的或者至少獲得正向的策略訓練結果？</p><p></p><p><strong></strong></p><p><strong>三、Shooting methods：</strong></p><p><strong>基於隨機採樣的MPC (Model Predictive Control)算法</strong></p><p></p><p>張偉楠介紹，在已經獲得一個有效的環境模型p(s’, r | s, a ) 後，人們可以從初始狀態出發，基於當前模型對後續動作進行序列採樣</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/69fba731cd4e43f28a19ee393bfc0456><p class=pgc-img-caption></p></div><p>的方式，來獲得一個採樣序列</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d9114ca4a0524bafa80513a86d70dedd><p class=pgc-img-caption></p></div><p>，其中動作序列</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/69fba731cd4e43f28a19ee393bfc0456><p class=pgc-img-caption></p></div><p>通過完全隨機採樣的方式獲得，與智能體的State等無關。對於當前狀態為s，a的智能體，可以建立一個動作序列</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b5e39421c47c4c84986206cae921430f><p class=pgc-img-caption></p></div><p>，從當前的狀態出發，在模型中連續向後進行T步推演，可以通過計算得到累計獎勵。共計採樣N次，可以得到N個積累獎勵，對這N個積累獎勵求取平均值，從而得到了在當前狀態s下，採取動作a時，所獲得的價值函數的無偏估計。</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e01cbcdc29d34135a0ed6b7faadb207a><p class=pgc-img-caption></p></div><p>通過對每一個Action的無偏估計的值的計算，可以選取價值函數的無偏估計值最大的Action，作為當前State下選擇Action的策略。</p><p></p><p>在以上整個過程中，幾乎沒有進行策略的學習訓練，而是在基於環境模型的演繹樹上進行搜索。當完成一步Action的選擇後，再在新的狀態的基礎上進行MPC操作即可。</p><p></p><p>張偉楠指出，模型建立之後可能會出現不確定性（uncertainty）的問題，一個解決思路是建立N個不同模型，每個模型都對下一步狀態進行預測，將N個模型的預測結果取平均，作為接下來預測的結果（Probabilistic Ensembles with Trajectory Sampling，PETS）。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fe1b39b3dd404a46911df6f840c7b12b><p class=pgc-img-caption></p></div><p><br></p><p>這裡每一個模型是一個高斯過程，基於當前State、Action來預測下一步的State，通過當前State、Action來得到高斯分佈的中心值和方差值，然後通過模型採樣接下來的狀態。每次採樣時，通過Ensemble中的N個高斯里面採集一個高斯出來，做接下來的推演。通過這樣的方式，可以capture環境模型中的兩部分的Uncertainty。</p><p></p><p>一是認知不確定性（Epistemic Uncertainty），在某些狀態情況下，由於數據缺乏的原因，會導致模型的推演不穩定，方差較大，因此需要通過Ensemble的方式來降低模型的不確定性；</p><p></p><p>二是固有的隨機性（Aleatoric Uncertainty），在每一個模型中，在預測下一步狀態時，會有一個自帶的隨機性，因此通過高斯建模的方式直接將這種隨機性刻畫出來。</p><p></p><p>最終，通過Ensemble的高斯過程模型，首先採樣出模型，得到一個狀態分佈，然後採樣出接下來的狀態，不斷往前（Forward）推演。通過這一過程，再結合使用MPC算法，從而可以得到有效的行動策略。</p><p><br></p><p><strong>四、MBPO算法理論分析與實現方法</strong></p><p></p><p>另一方面，建立的環境模型與真實環境之間一定有偏差（Error），這會導致從虛擬的環境模型中訓練得到的策略，在虛擬環境中交互得到的Value值與在真實環境中交互得到的Value值有所差距，稱為Value Discrepancy Bound（以下簡稱Bound）。人們將相應的差值定義為如下形式，左側</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1b388ab3030e4b3aae0db2d5d214b2af><p class=pgc-img-caption></p></div><p>為真實環境中的Value值，右側</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/88f6ebbd24174e9abc623da25c37f5ce><p class=pgc-img-caption></p></div><p>為環境模型中的Value值及其相應Bound差值。</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/703b6fb98c914aa8b76cc4da93f18833><p class=pgc-img-caption></p></div><p>是策略自身所導致的偏移（Policy Shift），這是由於採集數據時的策略為</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7fd6e581048e4744a2bb4dd80357359e><p class=pgc-img-caption></p></div><p>，而當前採用策略為</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f42fe9a73c9e451d9d7b0072202faf0d><p class=pgc-img-caption></p></div><p>，策略的不同會導致與環境交互產生的數據分佈不同。</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/703b6fb98c914aa8b76cc4da93f18833><p class=pgc-img-caption></p></div><p>為環境模型與真實環境之間的誤差。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/35bd03f2fee041d18c773db80ab14ef5><p class=pgc-img-caption>圖7：真實環境η[π]與環境模型η ̂[π]之間的bound差值</p></div><p><br></p><p>張偉楠介紹，基於以上分析，伯克利研究員提出一種新的基於模型的強化學習算法MBPO（Model-based Policy Optimization）。如下所示，灰色模塊所在的主幹為智能體與真實環境交互時的一條軌跡。中間採樣出來一個State後，由該State所在的Branch Point開始，使用模型進行一個k-steps的分支採樣，如下面的藍色模塊所在的分支所示，並將採樣出來的數據放到一個正常的Model-Free的強化學習算法中進行訓練，從而提高數據採樣效率，並降低模型與真實環境之間的Bound。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/967f76904b5a4b1bb94e5aae5f910dad><p class=pgc-img-caption>圖8：基於模型的k-steps分支採樣</p></div><p><br></p><p>在這個過程中，分叉後的推演步數k的取值起著重要的作用，人們推導了一個Bound的計算方式：</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fe6c1b3024624c55b73fd68183fe3241><p class=pgc-img-caption></p></div><p><br></p><p>左側</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1b388ab3030e4b3aae0db2d5d214b2af><p class=pgc-img-caption></p></div><p>為真實環境中的Value值，右側</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/88f6ebbd24174e9abc623da25c37f5ce><p class=pgc-img-caption></p></div><p>為環境模型中的Value值及其相應Bound差值，根據上式計算，只有當k=0時Bound最小，對應著不使用模型進行推演的情形。為了解決這一問題，人們重新查驗推導過程，發現其中一步的</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/703b6fb98c914aa8b76cc4da93f18833><p class=pgc-img-caption></p></div><p>中使用了基於之前時刻的Policy採樣得到的數據分佈，此時如果換成使用當前時刻的Policy採樣得到的數據分佈，如下所示：</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/edc0e616ef6e4da1af993d3f8a006427><p class=pgc-img-caption></p></div><p><br></p><p>則可以得到一個非常有意義的結果：</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/03a1d4714ae048248716d671cb5378c0><p class=pgc-img-caption></p></div><p><br></p><p>在新的公式中，通過對Bound值的計算會發現，當</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d3eb7ec10e6e421e9162e9b81a4c5568><p class=pgc-img-caption></p></div><p>足夠小，也就是當基於環境模型與真實環境之間的誤差相比策略自身所導致的偏移（shift）足夠小時，則k>0的時候會得到Bound值最小的情形。</p><p></p><p>以Mujoco等環境為例進行分析會發現，</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d3eb7ec10e6e421e9162e9b81a4c5568><p class=pgc-img-caption></p></div><p>的取值通常在</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/26da2e874f2d49c6ac15d2f6e3ab64f3><p class=pgc-img-caption></p></div><p>~</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d862b45c89ca48ac800596a38faa01ca><p class=pgc-img-caption></p></div><p>之間，能夠滿足相應的要求，從而為環境模型的使用提供了理論上的保證。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/19110cfb5f3246d0af1d9b1139272adb><p class=pgc-img-caption>圖9：基於Mujoco等環境進行的</p></div><p><br></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d3eb7ec10e6e421e9162e9b81a4c5568><p class=pgc-img-caption></p></div><p>取值分析</p><p><br></p><p>這一分析結果為人們處理為什麼使用模型、如何使用模型的問題提供了思路。在解決實際問題的時候，可以去尋找一個合適的k值，使得這個k值能夠保證Bound達到最小。</p><p></p><p>如下是MBPO算法的主要流程，通過尋找基於當前模型的最好的Branch Rollout的步數k，使得最後獲得最佳的訓練策略。這一流程基於黑盒模型，最後通過Model-Free的方法來對Branch Rollout的數據進行訓練，比如soft AC等。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f6453a9b5f2b4d28a29d4a8228f1b2e1><p class=pgc-img-caption></p></div><p><br></p><p>下圖是MBPO算法在不同環境下與其它算法的對比實驗，通過MBPO的對比實驗可以看出，MBPO算法在訓練效果上比較明顯的領先於其它算法。</p><p><br></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/063ba9d472e64d3bb69b6278dd877a3b><p class=pgc-img-caption>圖10：MBPO算法與其它算法在不同環境下的訓練效果對比</p></div><p><br></p><p><strong>五、最新的BMPO算法理論分析與實現方法</strong></p><p><br></p><p>張偉楠認為，人們使用環境模型進行前向（Foward）推演的時候，總是基於當前的State去逐步推演後續的State，也就是當前時刻之後的狀態。相應的，人們也可以建立環境模型，基於當前的State去逐步推演之前出現的State，也就是當前時刻之前的狀態。通過建立環境模型去推演到達當前時刻State之前，智能體所經歷的軌跡，這樣的模型被稱為反向模型（Backward Model）。基於此，張偉楠等人在ICML 2020提出了一個新的算法：BMPO（Bidirectional Model-based Policy Optimization）。</p><p></p><p>推演過程中如果使用前向模型（Forward Model），從當前State向當前時刻後續進行推演，由於每一步推演都存在複合誤差（Compounding Error），隨著推演的不斷進行，誤差不斷積累，模型推演出的結果與真實環境實際結果之間的差距將會逐漸增大，直至徹底失去作用。進行反向推演的意義在於，當使用雙向模型（Bidirectional Model）同時進行前向與反向推演時，推演出的軌跡的總長度與使用單向模型一樣的時候，前向模型與後向模型各自所需推演的步數均少於完全使用單向模型的情形，複合誤差的積累要顯著小於完全使用單向模型的情形。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c7d0dd79fbda4fdfba1a8702c8ef0ecc><p class=pgc-img-caption>圖11：前向推演與雙向推演對比示意圖</p></div><p><br></p><p>張偉楠團隊使用PETS方法，利用神經網絡結合高斯過程的方式建模，建立反向推演的策略（Backward Policy），根據當前所在的狀態，通過MLE（Maximum Likelihood Estimation）的方式學習之前State所採用的Action：</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c7e4993ee19842e48fde9ae798b9fc17><p class=pgc-img-caption></p></div><p><br></p><p>也可以通過GAN的方式，來確定之前State下最可能採用的Action：</p><p><br></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fc58f871c19349f892a0b205fb49842c><p class=pgc-img-caption></p></div><p></p><p>通過這兩種方式，便可以學習到反向推演的環境模型。</p><p></p><p>在BMPO的採樣環節做MBPO的Branch Rollout時，根據玻爾茲曼分佈來對選取具有更高價值函數的狀態，以此來保證軌跡中有一個狀態具有比較高的價值函數，從而可以通過雙向模型來學習如何走到當前價值函數比較高的狀態，以及之後選取動作的行動策略，從而最終獲得最好的行動策略。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5735a19a62d545f3895a7aeb95fb57a9><p class=pgc-img-caption></p></div><p><br></p><p>另一方面，我們希望採樣到高價值的狀態。在策略與真實環境中交互過程中，結合當前模型，使用MPC的方式來採樣獲得帶來更好價值函數的動作。</p><p></p><p>BMPO的整個流程如上所述，首先通過MPC的方法來進行與環境交互的高價值數據的收集。通過收集到的數據建立模型，根據模型採樣價值函數較高的狀態作為雙向Rollout的初始狀態。通過雙向模型來進行雙向推演，在獲得的軌跡上使用Model-Free的方法（Soft Actor-critic）進行策略的訓練。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/74c31e7436f94b23a7955107059150cb><p class=pgc-img-caption>圖12：BMPO算法流程示意圖</p></div><p><br></p><p>張偉楠團隊對BMPO與MBPO的結果進行了理論分析。當使用前向推演的單向模型（Forward Model）時，累積的複合誤差為</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/83dd6f8b8210400a93320402854f4714><p class=pgc-img-caption></p></div><p>，而使用雙向模型時，累積的複合誤差為max(</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/435195644c154ec8803b27dbc9c0dd6c><p class=pgc-img-caption></p></div><p>)。由於max(</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/435195644c154ec8803b27dbc9c0dd6c><p class=pgc-img-caption></p></div><p>)一定小於</p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/83dd6f8b8210400a93320402854f4714><p class=pgc-img-caption></p></div><p>，因此雙向模型帶來的複合誤差會更小，訓練效果也會更好。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f9895568a73f490480ae8629b65c92e2><p class=pgc-img-caption>圖13：單向模型與雙向模型複合誤差對比分析</p></div><p><br></p><p>最終基於Mujoco環境的對比實驗結果證明了張偉楠團隊的分析：提出的BMPO算法能夠比之前人們提出的MBPO算法具有更好的數據採樣效率以及收斂效果，</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3da377a8e4f2404ea1f8084f249b9262><p class=pgc-img-caption>圖14：BMPO算法與其它算法在不同環境下的訓練效果對比</p></div><p><br></p><p>張偉楠團隊還對BMPO與MBPO的複合誤差進行了量化的對比計算，結果發現在Rollout Length相同時，雙向模型具有更小的複合誤差，</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/84f9a2c334a6477fba6ddfbc9e81a11f><p class=pgc-img-caption>圖15：BMPO算法與MBPO算法的複合誤差對比計算結果</p></div><p><br></p><p><strong>六、總結</strong></p><p></p><p>張偉楠認為，MBRL將會是接下來幾年強化學習領域的研究熱點，對於真實環境下的問題，我們都需要建立虛擬的環境來進行策略的訓練與測試，之後才能應用於真實環境。MBRL主要分為基於黑盒模型的MBRL以及基於白盒模型的MBRL兩類，這項研究著重分析了基於黑盒模型的MBRL。基於白盒模型的MBRL近期也出現了許多的成果，比如ICLR 2020的MAAC等算法。</p><p></p><div class=pgc-img><img alt=上交張偉楠副教授：基於模型的強化學習算法，基本原理及前沿進展 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a65d43c035e3462c94d809fc3ab15043><p class=pgc-img-caption>圖16：基於黑盒模型與白盒模型的MBRL算法</p></div><p><br></p><p>對於MBRL未來的發展趨勢，張偉楠認為有如下幾點：</p><p></p><p>一是當前研究中所使用的環境比較簡單，比如Mujoco環境等。對於複雜的現實環境中的問題，比如研究電商中用戶購買行為時，所面對的挑戰難度非常大，因為這樣的問題中具有非常多的不確定性，環境的State本身是離散的。人們所採用的高斯過程更適合連續的機械問題的建模，進行離散問題的建模時，一定會有偏差，這時偏差帶來的累計誤差就會非常大，面對複雜問題時，如何實現高質量的環境建模將是未來研究的熱點。</p><p></p><p>二是如何將理論中推導出的Bound使用到具體的算法中，MBPO是一個非常好的實踐，而未來會有更多的發展前景。</p><p></p><p>最後一點，MBRL可以應用於多智能體強化學習（Multi-Agent RL，MARL）中，例如當前滴滴公司使用MARL處理車輛派單問題時，也會建立虛擬的環境模型來進行訓練和測試，然後再在現實環境中進行部署。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>張偉楠</a></li><li><a>強化</a></li><li><a>學習</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/06166af.html alt=強化學習應用於組合優化問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d7490d50d1ae4da4a0c46ff151cb54d2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/06166af.html title=強化學習應用於組合優化問題>強化學習應用於組合優化問題</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e1c0c28.html alt=利用深度強化學習生成測試輸入 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ab2a7fe4123846c38fda0899ec3157c9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e1c0c28.html title=利用深度強化學習生成測試輸入>利用深度強化學習生成測試輸入</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2a4fc7b.html alt=深度強化學習-深度Q網絡（DQN）介紹 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RXfpIwbEJgChY1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2a4fc7b.html title=深度強化學習-深度Q網絡（DQN）介紹>深度強化學習-深度Q網絡（DQN）介紹</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/8efd8d2.html alt=強化學習基礎-對偶梯度上升 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RGPaBiGC9XjT8U style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/8efd8d2.html title=強化學習基礎-對偶梯度上升>強化學習基礎-對偶梯度上升</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html alt=直流鍋爐給水控制學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/eba10edcc8d14d9f8cde6fd5b212d90e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html title=直流鍋爐給水控制學習>直流鍋爐給水控制學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html alt=HTMLCSS學習筆記（六）——元素類型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb5988349894ce9bf568c6418f85b7d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html title=HTMLCSS學習筆記（六）——元素類型>HTMLCSS學習筆記（六）——元素類型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html alt="web前端（從零開始），每天更新學習筆記 HTML5元素分類" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/46d70004fcd55e1ddad3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html title="web前端（從零開始），每天更新學習筆記 HTML5元素分類">web前端（從零開始），每天更新學習筆記 HTML5元素分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html alt="MySQL 學習筆記" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html title="MySQL 學習筆記">MySQL 學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html alt=深入學習MySQL事務：ACID特性的實現原理「轉」 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/cdc702d66d6943499997d11e931425eb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html title=深入學習MySQL事務：ACID特性的實現原理「轉」>深入學習MySQL事務：ACID特性的實現原理「轉」</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html alt=如何學習模擬IC設計？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html title=如何學習模擬IC設計？>如何學習模擬IC設計？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html alt=小猿圈python學習-三大特性之多態 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ad0e8e3777854337abeb7c779ad79a04 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html title=小猿圈python學習-三大特性之多態>小猿圈python學習-三大特性之多態</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/25d2deec.html alt=中基協：強化過程自律，落實信義義務，建設更加高效的私募行業 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/25d2deec.html title=中基協：強化過程自律，落實信義義務，建設更加高效的私募行業>中基協：強化過程自律，落實信義義務，建設更加高效的私募行業</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>