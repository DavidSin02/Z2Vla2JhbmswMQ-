<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>Facebook 爆錘深度度量學習：該領域13年來並無進展！網友：滄海橫流，方顯英雄本色 | 极客快訊</title><meta property="og:title" content="Facebook 爆錘深度度量學習：該領域13年來並無進展！網友：滄海橫流，方顯英雄本色 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/Ryrd4Gp4ZxyFNZ"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/74d90d0.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/74d90d0.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/74d90d0.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/74d90d0.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/74d90d0.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/74d90d0.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/74d90d0.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/74d90d0.html><meta property="article:published_time" content="2020-10-29T21:01:09+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:09+08:00"><meta name=Keywords content><meta name=description content="Facebook 爆錘深度度量學習：該領域13年來並無進展！網友：滄海橫流，方顯英雄本色"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/74d90d0.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>Facebook 爆錘深度度量學習：該領域13年來並無進展！網友：滄海橫流，方顯英雄本色</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><img alt="Facebook 爆錘深度度量學習：該領域13年來並無進展！網友：滄海橫流，方顯英雄本色" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Ryrd4Gp4ZxyFNZ><p>作者 | 蔣寶尚編輯 | 賈偉</p><p>近日，Facebook AI 和 Cornell Tech 的研究人員近期發表研究論文預覽文稿，聲稱近十三年深度度量學習（deep metric learning） 領域的目前研究進展和十三年前的基線方法(Contrastive, Triplet) 比較並無實質提高，近期發表論文中的性能提高主要來自於不公平的實驗比較, 洩露測試集標籤，以及不合理的評價指標。</p><p>也就是說：新出的ArcFace, SoftTriple, CosFace 等十種算法與十三年前的依賴成對或成三元組的損失函數並沒有本質上的區別。</p><p>FB和康奈爾科技此論無疑是對深度度量學習過去十三年研究成果蓋棺定論，斬釘截鐵表示，雖然深度度量學習非常重要，但是學界這些年一直在灌水。</p><p><strong>損失函數對度量學習很重要</strong></p><img alt="Facebook 爆錘深度度量學習：該領域13年來並無進展！網友：滄海橫流，方顯英雄本色" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Ryrd4HaDV44tbn><p>論文下載地址：https://arxiv.org/pdf/2003.08505.pdf</p><p>在綜述論文開頭，FB和康奈爾先肯定了深度度量學習的重要性，他們表示：深度度量學習已成為近年來機器學習最具吸引力的研究領域之一，如何有效的度量物體間的相似性成為問題的關鍵。</p><p>度量學習試圖將數據映射到一個嵌入空間，在這個空間中，相似的數據靠得很近，類型不同的數據離的很遠。而映射的方式可以通過嵌入損失和分類損失實現，這兩種方式各有特點，嵌入損失是根據一批樣本之間的關係來操作，而分類損失包括一個權重矩陣，將嵌入空間轉化為類logits向量。</p><p>在一些適用分類損失的任務下，當任務是信息檢索的某個變體時，通常使用嵌入方法，目標是返回與查詢最相似的數據。例如圖像搜索：輸入是查詢圖像，輸出是數據庫中視覺上最相似的圖像。</p><p>然後，在某些情況下無法使用分類損失。例如，在構建數據集時，為每個樣本分配類別標籤可能很困難或成本很高，並且可能更容易以配對或三元組關係的形式指定樣本之間的相對相似性。另外，樣本對( pair ) 或者樣本三元組（triplets）還可以為現有數據集提供額外的訓練信號。所以在這兩種情況下都沒有顯式標註，因此嵌入損失成為合適的選擇。</p><p>換句話說損失函數在度量學習中起到了非常重要的作用。很多深度度量學習的損失函數構建在樣本對( pair ) 或者樣本三元組 ( triplet ) 之上。隨著深度學習在眾多領域出色表現，逐漸這種方法對度量學習產生了影響，於是度量學習將深度學習方法結合了起來，產生了一個新的領域，即深度度量學習。</p><p>最新的論文也都採用了神經網絡技術，例如構建生成網絡，用生成器作為建模類中心和類內方差的框架的一部分（Lin et al）；使用自適應插值方法，根據模型的強度，產生不同難度的負值（Zheng et al）。除此之外，還有基於注意力的技術、基於聚類和編碼器組合的技術等等。</p><p></p><h2>領域的進展實際並不存在</h2><p>在列舉了一系列技術方法之後，研究員開始針對各種損失函數進行實驗，研究結果表明：通過交叉驗證對超參數進行適當調整時，大多數方法的性能相似。</p><p>研究員一共指出了現有文獻中的三個缺陷：不公平的比較、通過測試集反饋進行訓練、不合理的評價指標不公平的比較：一般大家聲明一個算法性能優於另一個算法，通常需要確保儘可能多的參數不變，而在度量學習的論文中不是如此。另外，一些論文中所提到的精度提高其實只是所選神經網絡帶來的，並不是他們提出的“創新”方法。例如2017年的一篇論文聲稱使用ResNet50 獲得了巨大的性能提升，而實際上他的對比對象是精度較低的GoogleNet。</p><p>通過測試集反饋進行訓練：不僅是度量學習領域，大多數論文都有這一通病：將數據集一半拆分為測試集，一半拆分為訓練集，不設驗證集。在具體訓練的過程中，定期檢查模型的測試集精度，並報告最佳測試集精度，也就是說模型選擇和超參數調優是通過來自測試集的直接反饋來完成的，這顯然會有過擬合的風險。</p><p>不合理的評價指標：為了體現準確性，大多數度量學習論文都會報告Recall@K、歸一化相互信息(NMI)和F1分數。但這些一定是最好的衡量標準嗎？如下圖三個嵌入空間，每一個recall@1指標評價都接近滿分，而事實上，他們之間的特徵並不相同。此外，F1和NMI分數也接近，這在一定程度上說明，其實，這幾個指標並沒帶來啥信息。</p><img alt="Facebook 爆錘深度度量學習：該領域13年來並無進展！網友：滄海橫流，方顯英雄本色" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Ryrd4IR76UaA7i><p>三個 toy示例：不同的精確指標如何評分。</p><p>在指出問題的同時，FB和康奈爾的研究員自然也指出了改進建議，針對上述三個缺點建議進行公平比較和重複性實驗、通過交叉驗證進行超參數搜索、採用更加準確的信息性、準確性度量。</p><p>公平比較和重複性實驗：實驗使用PyTorch；設置相同的輸出嵌入進行預訓練；批次大小設置為32；在訓練期間，使用隨機調整大小的裁剪策略來增強圖像；設置固定的學習率；公開配置文件。</p><p>通過交叉驗證進行超參數搜索：多用交叉驗證；優化超參數，並以最大程度地提高平均驗證準確性；多使用最佳超參數進行訓練，然後報告運行的平均值以及置信區間。</p><p>調整度量指標：建議用MAP@R來做平均精度，因為它結合了MAP（Mean Average Precision）和R精度（ R-precision）。</p><p><strong>勸退？不，滄海橫流，方顯英雄本色！</strong></p><p>深度度量學習，是否有用？是否還值得入坑？！！</p><p>在知乎上迅速激起諸多網友的熱烈討論。</p><img alt="Facebook 爆錘深度度量學習：該領域13年來並無進展！網友：滄海橫流，方顯英雄本色" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Ryrd4J0FbH5fYG><p>一位曾在Deep Metric Learning中單槍匹馬、摸爬滾打過的網友（MS Loss 一作）直言：DML領域大部分方法並無宣稱的那麼大，其“進步”往往來源於：調參、方法本身以及大的Batch Size。</p><blockquote><p>該領域確實水文比例挺大的，有些文章為了中，確實會用特別低的baseline, 用resnet甚至densenet和別人googlenet的結果去比，或者用2048維特徵去比過去方法的128維的結果。</p></blockquote><p>一位網友提到，度量學習在數學上優雅，在可視化上炫酷，在論文結果上漂亮，但在實際應用中卻毫無效果。</p><blockquote><p>Metric learning在數學上推得很優雅，在可視化上很酷炫，在論文結果上表現得很漂亮，但實際應用上卻毫無效果。asoftmax am amm等在某些數據集上確實擬合很好，適合刷指標，但實際在跨數據開集上，還是最簡單的softmax更香。</p></blockquote><p>或者用另一位網友的說法：公式超級多，論文看起來就專業可視化好看，類間離散圖很優雅</p><blockquote><p>當然也有網友認為還是有進展的，就是李飛飛用超大規模數據集，告訴大家堆數據要比算法提升更優秀！基本認同。唯一的進展是李飛飛的超大規模數據集，告訴大家堆數據帶來的提升效果比算法創新還要大。然而這基本只算一個工程經驗，根本也談不上一個'進展'。</p></blockquote><p>不過也有網友認為，發展還是有的，那就是正則化。</p><blockquote><p>那麼metirc learning有沒有發展呢，我認為最主要的發展還是近幾年的normalization，這個東西明確了幾何空間的定義，使得研究人員能夠更好的在幾何層面設計metric，最後無論是訓練的收斂速度還是最後的準確率都有了極大的提升。</p></blockquote><p>最後，我們引用一下知名博主「王晉東不在家」的話（編者注：有刪減）——</p><p>其實大可不必心潮澎湃、攻擊別人、對該領域前途失望。每當一個研究領域出現一些retlinking、revisiting、comprehensive analysis等類型的文章時，往往都說明了幾個現象：</p><p>1、這個領域發展的還可以，出現了很多相關的工作可以參考；</p><p>2、這個領域的文章同質化太嚴重，到了傳說中的“瓶頸期”；</p><p>3、研究人員思考為什麼已經有這麼多好工作，卻好像覺得還差點意思，還“不夠用”、“不好用”、“沒法用”。</p><p>其實這對於研究而言是個好事。我們在一條路上走了太久，卻常常忘記了為什麼出發。此時需要有些人敢於“冒天下之大不睫”，出來給大家頭上澆盆冷水，重新思考一下這個領域出現了什麼問題。哲學上也有“否定之否定”規律。</p><p>回到metric learning的問題上來。其實想想看，機器學習的核心問題之一便是距離。Metric learning這種可以自適應地學習度量的思想，真的是沒用嗎?還是說它只是被目前的方法、實驗手段、評測數據集等等束縛了手腳，陰差陽錯地導致了不好的結果這也是這個問題的提出者原文作者質疑的問題。</p><p>不能經受得住質疑和時間的洗禮的工作，不是好工作。事實上此類事情並不是第一次發生。</p><p>不信你看，作為當下ICML、ICLR、NIPS等頂會的“寵兒”，meta-learning可謂風頭一時無兩。然而，大家都清楚，meta-learning的一大部分工作都是在few-shot的任務上進行算法開發和評測的。從18年開始到今年的ICLR，就已經不斷地有人“質疑”其有效性了。“質疑”的核心問題之一是：用簡單的pretrain network去學習feature embedding，然後再加上簡單的分類器就可以在few-shot那幾個通用任務上，打敗很多“著名”的meta-learning方法。所以到底是meta-learning的這種“學會學習”的思想沒用，還是它只是被不恰當地使用了?或者說，meta-learning的正確用法是什麼?我認為這也是要思考的。我個人是非常喜歡meta的思想的。</p><p>其實在transfer learning領域。我之前也有一篇看似“打臉”的paper:Easy Transfer Learning by Exploiting Intra-domain Structures。我們的實驗同樣證明了僅需簡單pretrain過的ResNet50提取源域和目標域的feature embedding，然後加上簡單的線性規劃分類器，甚至是nearest centroid,就能取得當時（2018年底）幾乎最好的分類結果。但是你能說transfer learning沒用嗎?顯然這並不能掩蓋transfer learning方法的光芒。所以我一直都在質疑自己：肯定是這些數據集不完善、精度不能作為唯一指標、其他方法需要再調參數，等等。</p><p>我覺得這可能是個實驗科學的誤區：我們實驗設定本來就需要完善，並不能因此否定一類方法的有效性。深度學習大部分都是建立在實驗科學的基礎上，因此實驗很關鍵。</p><p>有了廣泛的質疑，才會有更廣泛的討論，於是會有更廣泛的反質疑、新範式、新思想。從整個領域的發展來看，這無疑是好事。</p><p>所以這應是“滄海橫流，方顯英雄本色”的時候了。加油吧！</p><p>（雷鋒網）、（雷鋒網）、（雷鋒網）</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>Facebook</a></li><li><a>爆錘</a></li><li><a>學習</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html alt=直流鍋爐給水控制學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/eba10edcc8d14d9f8cde6fd5b212d90e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html title=直流鍋爐給水控制學習>直流鍋爐給水控制學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html alt=HTMLCSS學習筆記（六）——元素類型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb5988349894ce9bf568c6418f85b7d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html title=HTMLCSS學習筆記（六）——元素類型>HTMLCSS學習筆記（六）——元素類型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html alt="web前端（從零開始），每天更新學習筆記 HTML5元素分類" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/46d70004fcd55e1ddad3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html title="web前端（從零開始），每天更新學習筆記 HTML5元素分類">web前端（從零開始），每天更新學習筆記 HTML5元素分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html alt="MySQL 學習筆記" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html title="MySQL 學習筆記">MySQL 學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html alt=深入學習MySQL事務：ACID特性的實現原理「轉」 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/cdc702d66d6943499997d11e931425eb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html title=深入學習MySQL事務：ACID特性的實現原理「轉」>深入學習MySQL事務：ACID特性的實現原理「轉」</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/813d5912.html alt=關於Facebook像素，這些你瞭解嗎？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RM0p5dK51SH9EU style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/813d5912.html title=關於Facebook像素，這些你瞭解嗎？>關於Facebook像素，這些你瞭解嗎？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html alt=如何學習模擬IC設計？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html title=如何學習模擬IC設計？>如何學習模擬IC設計？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html alt=小猿圈python學習-三大特性之多態 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ad0e8e3777854337abeb7c779ad79a04 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html title=小猿圈python學習-三大特性之多態>小猿圈python學習-三大特性之多態</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/03a295fc.html alt=地理學習5——地球的運動（地球的公轉及其地理意義） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7b2b74c871eb40beb8ee143627d29611 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/03a295fc.html title=地理學習5——地球的運動（地球的公轉及其地理意義）>地理學習5——地球的運動（地球的公轉及其地理意義）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ebad378f.html alt=繼續學習打卡，還真心學不會了，努力，堅持 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/f36d6d47a06840aaaf78138853b9d9d1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ebad378f.html title=繼續學習打卡，還真心學不會了，努力，堅持>繼續學習打卡，還真心學不會了，努力，堅持</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>