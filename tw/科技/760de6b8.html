<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>NLP領域中的遷移學習現狀 | 极客快訊</title><meta property="og:title" content="NLP領域中的遷移學習現狀 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p9.pstatp.com/large/pgc-image/RayMFst8jYuQgG"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/760de6b8.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/760de6b8.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/760de6b8.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/760de6b8.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/760de6b8.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/760de6b8.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/760de6b8.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/760de6b8.html><meta property="article:published_time" content="2020-11-14T21:01:58+08:00"><meta property="article:modified_time" content="2020-11-14T21:01:58+08:00"><meta name=Keywords content><meta name=description content="NLP領域中的遷移學習現狀"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/760de6b8.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>NLP領域中的遷移學習現狀</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>雷鋒網AI科技評論編者按：本文拓展自NAACL 2019教程“NLP領域的遷移學習”，這個教程是由Matthew Peters、Swabha Swayamdipta、Thomas Wolf和我組織策劃的。在這篇文章中，我強調了一些在這個領域中的見解和收穫，並根據最近的工作進展更新了一部分資料。整篇文章的結構如下圖。</p><img alt=NLP領域中的遷移學習現狀 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RayMFst8jYuQgG><p>圖1</p><p></p><h2>一、內容簡介</h2><p>我們在這篇文章中的對遷移學習的主要定義如圖所示，遷移學習是一種從源設置中提取信息並將其應用於不同設定目標的方法。</p><img alt=NLP領域中的遷移學習現狀 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RayMFtAFP4yPly><p>圖2：遷移學習過程的說明</p><p>在過去一年多的時間裡，以預訓練的語言模型形式進行的遷移學習已經在NLP領域中普及，為許多不同的任務帶來了新的最好成績。然而，遷移學習在自然語言處理中並不是最近才出現的。雷鋒網舉個簡單的例子，命名實體識別(NER)任務的進展情況，如下所示。</p><img alt=NLP領域中的遷移學習現狀 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RayMFtMCUqAi96><p>圖3：隨時間推移在CoNLL-2003上的命名實體識別(NER)性能</p><p>縱觀其歷史, 在這項任務中大多數的進步已經是由不同形式的遷移學習推動的:從早期s帶有輔助任務的自監督學習(Ando和Zhang,2005)和短語和單詞集群(Lin和Wu,2009)語言模型嵌入(Peters等人, 2017)和預訓練語言模型(Peters， Akbik， Baevski等人)。</p><p>在當前的自然語言處理領域中，普遍存在著不同類型的遷移學習。它們可以按照三個維度進行分類：1、源設定和目標設定是否處理相同的任務;2、源域和目標域的性質;3、學習任務的順序。這樣就可以把各種遷移學習分為圖中的幾類。</p><img alt=NLP領域中的遷移學習現狀 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RayMFtZ1ARV9fW><p>圖4：NLP中遷移學習的分類(Ruder, 2019)</p><p>序列遷移學習是其中進步最大的那一類。通常的做法是，使用你選擇的方法在一個大型文本語料庫中的未標記數據上進行表徵預訓練，然後通過標註數據把學到的表徵適配到一個有監督任務上。如下圖。</p><img alt=NLP領域中的遷移學習現狀 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RayMFtm3eebCUn><p>圖5： 序列遷移學習的一般過程</p><p></p><h2>二、主要的研究課題</h2><p>在應用這種範式的過程中，我們可以觀察到以下幾個主要的研究課題</p><p></p><h3><strong>1、單詞表徵（如今已經是帶有語境的單詞表徵）</strong></h3><p>隨著時間推移、技術發展，單詞表徵也更多地聯繫上下文的語義。像word2vec這樣的早期方法 (Mikolov 等人， 2013)為每個單詞學習了單獨的表徵，但與上下文沒有聯繫。後來的方法就開始將這些表徵的尺度拓展到了句子和文本級別(Le和Mikolov, 2014;Conneau等人，2017)。當前的方法能夠基於單詞的上下文不同而學到不同的表徵(McCann等人，2017; Peters等人，2018)。</p><p></p><h3><strong>2、LM預訓練</strong></h3><p>許多成功的預訓練方法都是基於語言建模（LM）的變體。 LM的優點是它不需要任何人工註釋，並且許多語言都有足夠的文本資料學出一個不錯的模型d。此外，LM是多用途的，可以通過各種目標函數，學習句子和單詞的表徵。</p><p></p><h3><strong>3、由淺入深</strong></h3><p>在過去的幾年裡，NLP領域中最先進的模型變得越來越深入。僅僅兩年前，大多數任務上表現最佳的模型都還是2-3層的深度BiLSTM，只有機器翻譯領域表現最好的模型是與眾不同的16層(Wu 等人， 2016)。相比之下，目前的模型如BERT-Large和GPT-2由24個Transformer模塊組成，而最新的模型甚至會更深。</p><p></p><h3><strong>4、預訓練vs目標設定</strong></h3><p>預訓練和目標任務的選擇密切相關。例如，句子的表達對於單詞級別的預測並沒有用，而基於詞組的預訓練對於詞組級的預測是重要的。總的來說，為了獲得最佳的目標性能，而選擇類似的預訓練任務是有益的。</p><p></p><h2>三、預訓練</h2><h3><strong>1. 為什麼語言建模會如此有效？</strong></h3><p>預訓練語言模型的取得了驚人的成功。語言建模成功的一個原因可能是，它是一項非常困難的工作，即使對於人類來說也不例外。為了有機會解決這個難題，模型需要學習語法，語義以及某些世界常識。給定足夠的數據，大量參數和足夠的計算能力，模型就可以有不錯的學習成果。根據過往的實驗來看，語言建模比翻譯或自動編碼等其它預訓練工作更有效(Zhang ，Wang等人)。</p><p>最近對人類語言的預測率失真(PRD)的分析(Hahn and Futrell, 2019)研究表明，人類語言和語言建模都具有無窮高的統計複雜性，但語言建模可以在較低層次上模仿逼近人類語言。這一觀察結果有兩個啟示:1)我們可以用相對較小的模型以獲得較為精準的結果;2) 我們的模型有很大的拓展潛力。對於這兩種啟示，我們都有充足證據，我們可以在下一節中看到。</p><p></p><h3><strong>2. 樣本效率</strong></h3><p>預訓練的一個主要好處就是它減少了對有標註數據的需求。 在實際使用中，與非預訓練模型相比，遷移學習模型通常只需要十分之一甚至更少的樣本數量就達到類似的表現，如下圖所示(Howard 和Ruder, 2018)。</p><img alt=NLP領域中的遷移學習現狀 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RayMG6OGQ5fzmP><p>圖6:從零開始訓練的模型（藍色）與分別對標記目標數據(橙色)和未標記目標數據(綠色)進行微調的兩個預訓練模型(紅色)的性能(Howard和Ruder, 2018)。</p><p></p><h3><strong>3、增大預訓練規模</strong></h3><p>通常可以通過同時增加模型參數和預訓練數據量的方式來改善預訓練表徵。但隨著預訓練數據量的增長，回報率開始下降。然而，如下圖所示的當前的性能曲線並不表示我們已達到了穩定狀態。因此，我們期待可以在更多數據基礎上訓練出更大的模型。</p><img alt=NLP領域中的遷移學習現狀 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RayMG6iCRkiZfy><p>圖7：平均GLUE數據與預訓練的爬行數據量對比（Baevski等，2019）。</p><p>最近的例子是ERNIE 2.0，XLNet，GPT-2 8B和RoBERTa。 尤其是後者發現，簡單地對BERT進行更長時間和更多數據的訓練就可以得到更好的結果，而對GPT-2 8B進行更長時間和更多數據的訓練則可以減少語言建模數據集上的困惑度(儘管幅度很小)。</p><p></p><h3><strong>4、跨語言預訓練</strong></h3><p>預訓練的一個主要作用是它可以幫助我們跨越數字語言間鴻溝，使我們能夠為全世界超過6000種語言都學習到NLP模型。（其中大多數語言是缺乏資料的，這既是為每種語言都學習語言模型的難點，也是跨語言預訓練有用的地方）跨語言學習的工作大多集中在訓練不同語言中的單詞嵌入，並學習如何使它們之間匹配度更高(Ruder 等人， 2019)；同樣地，我們也可以對上下文表徵做同樣的事情（Schuster 等人，2019）。另一種常見方法是共享子單詞詞彙表並在多種語言上訓練一個模型（Devlin，Artetxe， Schwenk， Mulcaire， Lample和Conneau等人，2019）。</p><p>雖然這很容易實現，並且是一個很強大的跨語言基線，但它導致資源配置較低的語言代表性不足（Heinzerling和Strube，2019）。特別是多語言BERT已成為備受關注的話題（Pires ，Wu和Dredze等人，2019），雖然它有很好的零樣本學習表現，但專門的單語模型仍然是具有極大競爭力的，同時也更有效率(Eisenschlos 等人， 2019).</p><p></p><h3><strong>5、實際權衡</strong></h3><p>預訓練很費錢。我們在教程中使用的Transformer-XL樣式模型的預訓練在8個V100 gpu上花費了5-20小時(如果是1個V100那麼就要花費幾天的時間)才能達到一個好的效果。因此，共享預訓練的模型非常重要。除了需要為transformer的學習速率預熱外，預訓練對超參數的選擇並不怎麼挑剔。一般來說，如果數據集足夠大，模型就不應該有足夠的容量來進行過度擬合。掩蔽語言建模(如BERT中所述)的訓練速度通常比標準LM慢2-4倍，因為掩蔽一小部分單詞對應著只有一小部分的訓練指導信號。</p><p></p><h2>四、表徵中都有什麼</h2><p>我們已經證明，表徵可以用來預測某些語言現象，如翻譯或句法層次的匹配。使用語法進行預訓練，能夠取得較好的效果;即使沒有明顯地對語法進行編碼，表徵仍然可以學習到一些語法概念(Williams等人，2018)。</p><p>最近的工作進一步表明，當前最先進的模型可以有效地蒸餾出語法知識(Kuncoro等人.， 2019)。網絡架構通常決定了表徵的內容。例如，已經觀察到了BERT能夠捕捉語法(Tenney等人)。對於捕捉到的信息，不同的模型架構會表現出不同的層間變化趨勢。</p><img alt=NLP領域中的遷移學習現狀 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RayMG6vAdSAEOb><p>圖8：探究工作一般用於設置研究語境詞表示中的語言知識(Liu 等人， 2019)</p><p>模型捕獲的信息還取決於你如何看待它:可視化激活或是注意力權重能提供模型知識的總體情況，但只能針對幾個樣本進行分析;如果在學習到的表徵基礎上訓練一個分類器作為探針，用它來預測某些屬性，這種方法可以發現語料庫級別的一些具體特點，但也可能引入探針本身帶來的偏倚;最後，網絡對照試驗對於改進模型非常有用，但得到的結論可能會僅限於當前的任務。</p><p></p><h2>五、適配</h2><p>為了使預訓練模型適應目標任務，我們可以做出若干不同方向上的決策：架構修改，優化方案以及是否要獲得更多的學習信號。</p><p></p><h3><strong>1、架構修改</strong></h3><p>對於架構修改，我們有兩個選項：</p><p><strong>a)保持預訓練的模型內部不變</strong></p><p>簡單的做法可以是在預訓練的模型之上添加一個或多個線性層，通常是在Bert基礎上增加。或者我們也可以使用模型輸出作為另一個模型的輸入。當目標任務需要預訓練的嵌入有一些交互作用但目前的預訓練模型執行不了時，這通常是有幫助的，例如以BERT為預訓練模型但需要詞彙表徵或建立跨句子關係模型。</p><p><strong>b)修改預先訓練的模型內部架構</strong></p><p>我們希望這樣做的原因之一可能是為了適應結構不同的設定目標，例如一個具有多個輸入序列的任務。在這種情況下，我們可以使用預訓練的模型儘可能初始化結構不同的設定模型。我們還可以應用於特定項目的修改，例如添加、跳過或殘餘連接或警示。最後，修改目標任務的參數可以通過在預訓練的模型層之間添加瓶頸模塊(“適配器”)來減少需要精細調整的參數數量(Houlsby，Stickland和Murray等人，2019)。</p><img alt=NLP領域中的遷移學習現狀 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RayMG7CrqgGAE><p>圖9：變壓器(左)中使用的適配器層(右)(Houlsby等人，2019年)</p><p>六、優化方案</p><p>在優化模型方面，我們需要選擇要更新哪些權重，以及何時、如何更新它們</p><p></p><h3><strong>1、要更新哪些權重</strong></h3><p>對於權重更新，我們可以選擇微調或者不微調（預訓練權重）</p><p><strong>a)不要改變預訓練的權重(特徵提取)</strong></p><p>在實際應用中，很多人會在預訓練表徵的基礎上訓練一個線性分類器。如果想要獲得最好的性能，通常就不僅使用頂層的表徵，而要學習層表徵的線性組合 (Peters等人， 2018, Ruder 等人， 2019)。另一個做法是，預訓練的表徵可以作為下游模型中的特性。當添加適配器時，只訓練適配器層。</p><img alt=NLP領域中的遷移學習現狀 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RayMG7RGJB5arq><p>圖10：在單獨的下游模型中使用預:訓練的模型作為特徵</p><p><strong>b)改變預訓練過程中的權重（微調）</strong></p><p>採用預訓練的權值作為下游模型參數的初始化值。然後，在適應階段對整個預訓練架構進行訓練。</p><p></p><h3><strong>2、如何以及何時更新權重</strong></h3><p>選擇順序和如何更新權重的主要目的是要避免覆蓋有用的預訓練的信息並最大化正遷移。與此相關的是災難性遺忘的概念(McCloskey&Cohen，1989；French，1999)，是指一個模型忘記了它最初訓練的任務。在大多數設置中，我們只關心目標任務的性能，但這可能因應用場合的不同而有所不同。</p><p>更新我們模型的參數的一個指導原則是，在時間上、強度上或與預訓練的模型相比，自上而下地逐步更新：</p><p><strong>a)時間上的逐步更新（凍結法）</strong></p><p>對不同分佈和不同任務的數據同時進行所有層的訓練可能會導致性能不穩定，產生的解決方案較差。相反，我們單獨訓練網絡中不同的層，讓它們有時間適應新的任務和數據。這可以追溯到早期深層神經網絡的分層訓練(Hinton等人，2006；Bengio等人，2007)。最近的方法(Felbo等人，2017；Howard和Ruder，2018年；Chronopoulou等，2019年) 的變化主要在於共同訓練的層的選擇不同。 尚未對transformer模型的解凍進行詳細的研究。</p><p><strong>b)強度上的逐步更新(降低的學習率)</strong></p><p>我們希望通過降低學習率，以避免覆蓋有用的信息。較低的學習率在較低層次(因為它們會捕捉到很多的普遍信息)、訓練早期(因為模型仍然需要適應目標分佈)和訓練後期(當模型接近結尾時)尤為重要。為此，我們可以使用判別性微調(Howard和Ruder，2018)，它降低了每一層的學習速度，如下所示。為了在早期的訓練中保持較低的學習率，可以使用三角學習速率計劃，也就是transformer的學習率預熱。（Liu等人，2019)最近指出，在訓練的早期階段，預熱可以減少差異</p><img alt=NLP領域中的遷移學習現狀 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RayMGRQASQ2Xfm><p>圖11：歧視性微調(Howard和Ruder，2018年)</p><p><strong>c)逐步更新vs預訓練模型（正則化）</strong></p><p>最大限度地減少災難性遺忘的一種方法是鼓勵目標模型參數使用正則化項，減小與預訓練模型的參數的差異性(Wiese等人，CoNLL 2017，Kirkpatrick等人，PNAS 2017)。</p><p></p><h2>七、權衡與實際考慮</h2><p>一般來說，需要從頭開始訓練的參數越多訓練速度就越慢。特徵提取需要添加比微調更多的參數(peters等人，2019)，因此訓練速度通常較慢。然而，當一個模型需要適應很多的任務時，特徵提取更節省空間，因為它只需要在內存中存儲一個預先訓練過的模型的副本。適配器通過為每個任務添加少量的附加參數來達到平衡。</p><p>就性能而言，沒有一種適配器方法能在所有設置中都帶來帶來最好的表現。如果源和目標任務不一致，特徵提取似乎更可取(Peters等人，2019)。否則，特徵提取和微調通常執行類似的操作，這取決於用於超參數調整的預算(微調通常需要對超參數進行更廣泛的搜索和嘗試)。據說，transformer比lstms更容易微調(對超參數不那麼敏感)，並且可以通過微調實現更好的性能。</p><p>然而，大型的預訓練模型(如Bert-Large)在對小訓練集的任務進行精細調整時，往往會導致性能退化。通常顯示為“ON-off”：如下圖所示，模型要麼運行良好，要麼根本不起作用。想要了解這種行為的條件和原因，目前來說還是一個等待解決的研究問題。</p><img alt=NLP領域中的遷移學習現狀 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RayMGRi4tXtw8d><p>圖12：BERT（紅色）和BERT的20個隨機重啟的任務分數分佈，在MNLI（綠色）上進行微調，每次任務的微調不超過5k（Phang等，2018）。</p><p></p><h2>八、獲得更多的學習信號</h2><p>目標任務通常都缺少資源.我們通常可以通過組合多種不同的學習信號來提高遷移學習的性能</p><p></p><h3><strong>1、提高樣本效率</strong></h3><p>如果存在相關的任務，我們可以先在相關任務上使用更多數據對模型進行微調，然後再對目標任務進行微調。這尤其有助於數據有限的情況和執行類似任務 (Phunet 等人，2018)，並提高了目標任務的樣本效率(Yogatama等人，2019)。</p><p></p><h3><strong>2、多任務微調</strong></h3><p>或者，我們也可以根據相關任務和目標任務的要求對模型進行共同微調。這個相關的任務也可以是一個無監督的輔助任務。語言建模是一個很好的選擇，並且已經證明即使沒有預訓練也能起到幫助(Rei等人，2017年)。任務比率可以進行選擇，逐漸減小輔助任務的比例，在訓練結束時輔助任務的比例可以減小到0(Chronopoulou等人，NAACL，2019年)。語言模型微調也已經作為一個單獨步驟使用在了ULMFiT中 (Howard和Ruder，2018)。最近，即便有很多個目標任務，多任務微調也可以帶來收益 (Liu等人，2019；Wang等人，2019)。</p><p></p><h3><strong>3、數據集分割</strong></h3><p>我們可以使用只在數據的某個子集上訓練的輔助任務，而不對輔助任務微調。為此，我們首先要分析模型的誤差，採用啟發式方法自動識別訓練數據中的突出子集，然後與目標任務聯合訓練輔助任務。</p><p></p><h3><strong>4、半監督學習</strong></h3><p>我們還可以使用半監督學習方法，通過擾亂未標記的數據來使我們的模型預測更加準確。 干擾方式可以是噪聲，掩蔽（Clark等人，2018）也可以是數據增強，（Xie 等人，2019）。</p><p></p><h3><strong>5、集成模型</strong></h3><p>為了提高性能，可以把不同超參數訓練的模型、不同預訓練模型精細調節得到的模型、乃至不同任務，不同數據集的子集上訓練的模型集成起來</p><p></p><h3><strong>6、蒸餾</strong></h3><p>最後，大型模型或模型組合可能被提煉成小型單一模型。該模型也可以更加簡單化 (Tang等人，2019年)，也可以產生不同的歸納偏差(Kuncoro等人，2019年)。多任務微調也可以與提煉相結合(Clark等人，2019)。</p><p>九、下游應用</p><p>預訓練大型模型在計算和環境影響方面的成本很高(Strubell等人，2019年)。只要有可能，最好使用開源模型。如果您需要訓練自己的模型，請與社區分享您的預訓練模型。</p><p></p><h3><strong>1、框架和函數庫</strong></h3><p>為了共享和使用經過訓練的模型，可以有不同的選擇：</p><p></p><h3><strong>2、Hubs</strong></h3><p>Hubs是中央存儲庫，並能提供訪問預訓練模型的通用API 。最常見的兩個Hub是TensorFlow Hub和PyTorch Hub。Hub通常使用起來很簡單；但是，由於模型的源代碼難以訪問，所以它們更像一個黑匣子。此外，修改預訓練模型架構的內部結構可能會很困難。</p><p></p><h3><strong>3、研究者發佈了檢查點模型</strong></h3><p>檢查點文件通常包含了預訓練模型的所有權重。與Hub相比，仍然需要創建模型圖，需要單獨加載模型權重。因此，檢查點文件比Hub模塊更難使用，但它可以讓你完全控制模型內部。</p><p></p><h3><strong>4、第三方函數庫</strong></h3><p>像是 AllenNLP, fast.ai和 pytorch-transformers這類的第三方函數庫，可以輕鬆訪問預訓練模型。 此類庫通常可實現快速實驗，並涵蓋了許多經典的示例以進行遷移學習。</p><p></p><h2>十、有待解決的問題及未來發展方向</h2><h3><strong>1、預訓練語言模型的缺陷</strong></h3><p>預訓練的語言模型仍然不擅長細粒度的語言任務(Liu等人，2019)、層次句法推理(Kuncoro等人，2019)和常識（Zellers等人，2019)。在自然語言生成方面仍然有所欠缺，特別是在長期的維繫、關係和連貫性方面。微調的時候它們更傾向於過於過度擬合表面形式信息，目前仍然被視為是“效率極高的表面學習者”。</p><p>正如我們前面所指出的，特別是對少量數據進行微調的大型模型很難進行優化，而且存在高度的差異性。目前經過預訓練的語言模型非常龐大。蒸餾和剪枝是處理這一問題的兩種方法。</p><p></p><h3><strong>2、預訓練任務</strong></h3><p>雖然語言建模目標經過試驗正名是有效的，但它也有缺陷。最近，我們看到雙向語境和詞序列的建模尤為重要。也許最重要的是，語言建模鼓勵人們關注語法和詞的共現，並且只提供了用於理解語義和維繫上下文弱信號。我們可以從其他形式的自我監督中汲取靈感。此外，我們還可以設計專門的預訓練任務，學習某些關係(Joshi等人，2019年；Sun等人，2019年)。</p><p>雷鋒網小結，很難能從原始文本中學習某種類型的信息。最新的方法就是將結構化知識(Zhang等人，2019；Logan IV等人，2019年)或多模態(Sun等人，2019；Lu等人，2019)引入，作為緩解這一問題的兩種潛在方式。</p><p>原文鏈接：http://ruder.io/state-of-transfer-learning-in-nlp/</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>NLP</a></li><li><a>領域</a></li><li><a>移學習</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/30177cc2.html alt=NLP中的遷移學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1534906754727755a6a6964 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/30177cc2.html title=NLP中的遷移學習>NLP中的遷移學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0dfb02b.html alt=NLP算法入門系列：不同領域文章的熱詞提取 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/d62c66b1abde415aa1d686c40b8a76c8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0dfb02b.html title=NLP算法入門系列：不同領域文章的熱詞提取>NLP算法入門系列：不同領域文章的熱詞提取</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5e4be374.html alt=鈦及鈦合金鑄件的應用領域​ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/Rca7hqUCttQBq0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5e4be374.html title=鈦及鈦合金鑄件的應用領域​>鈦及鈦合金鑄件的應用領域​</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/10c7d8fd.html alt=神州泰嶽：公司在自然語言處理領域的基礎技術研究和應用落地均走在行業前列 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/10c7d8fd.html title=神州泰嶽：公司在自然語言處理領域的基礎技術研究和應用落地均走在行業前列>神州泰嶽：公司在自然語言處理領域的基礎技術研究和應用落地均走在行業前列</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1700411d.html alt="四大領域“亮眼”表現 勾畫高端製造裝備產業格局" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/78b5000c7833f5cc29af style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1700411d.html title="四大領域“亮眼”表現 勾畫高端製造裝備產業格局">四大領域“亮眼”表現 勾畫高端製造裝備產業格局</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e2e623b5.html alt=工業耐磨陶瓷主要應用在哪些領域 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/e25119604b714ee4bac2538dbb036093 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e2e623b5.html title=工業耐磨陶瓷主要應用在哪些領域>工業耐磨陶瓷主要應用在哪些領域</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ba772a95.html alt=在該領域不再被“卡脖子”，中國成功研出50MW重型燃氣輪機 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/f01ff61edf6d4c44a6ecd1c8529e7597 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ba772a95.html title=在該領域不再被“卡脖子”，中國成功研出50MW重型燃氣輪機>在該領域不再被“卡脖子”，中國成功研出50MW重型燃氣輪機</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9b6a31c5.html alt=早知道就買一個回家過年了：反恐領域CBRN呼吸道防護裝備發展方向 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RoAFZJY2s3oqDd style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9b6a31c5.html title=早知道就買一個回家過年了：反恐領域CBRN呼吸道防護裝備發展方向>早知道就買一個回家過年了：反恐領域CBRN呼吸道防護裝備發展方向</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c078eace.html alt=BIM輕量化圖形引擎領域研發取得重大突破 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ROLNl1UCgFXKU0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c078eace.html title=BIM輕量化圖形引擎領域研發取得重大突破>BIM輕量化圖形引擎領域研發取得重大突破</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/33d04d4d.html alt=2020年各大頂會NLP、ML優質論文分類整理分享 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/fb47112700b049aa88994c8949ec9403 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/33d04d4d.html title=2020年各大頂會NLP、ML優質論文分類整理分享>2020年各大頂會NLP、ML優質論文分類整理分享</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d1e2ced7.html alt="正泰能源物聯網領域新突破 | 電蓄熱調峰項目將正式啟動" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/eb7f0ef4d178485982667d1bc011bc98 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d1e2ced7.html title="正泰能源物聯網領域新突破 | 電蓄熱調峰項目將正式啟動">正泰能源物聯網領域新突破 | 電蓄熱調峰項目將正式啟動</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/acb734b8.html alt=加強重要領域立法以良法保障國家善治 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/acb734b8.html title=加強重要領域立法以良法保障國家善治>加強重要領域立法以良法保障國家善治</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0170174e.html alt="三星立下豪言 2030年成為邏輯芯片領域的世界第一！" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/59d4664a69db4d5f9bbd80fe523064e1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0170174e.html title="三星立下豪言 2030年成為邏輯芯片領域的世界第一！">三星立下豪言 2030年成為邏輯芯片領域的世界第一！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1cf92aa2.html alt=計算機領域三十年的難題，這位華人數學家僅用2頁紙就解決了 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/1b98507605614f07bebec6fb1fc3254e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1cf92aa2.html title=計算機領域三十年的難題，這位華人數學家僅用2頁紙就解決了>計算機領域三十年的難題，這位華人數學家僅用2頁紙就解決了</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8a07cadb.html alt=水晶卷閘門應用領域詳解-深圳祥達電動卷閘門 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8a07cadb.html title=水晶卷閘門應用領域詳解-深圳祥達電動卷閘門>水晶卷閘門應用領域詳解-深圳祥達電動卷閘門</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>