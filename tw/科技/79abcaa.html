<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>廣義線性模型② | 极客快訊</title><meta property="og:title" content="廣義線性模型② - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/7f891d17860b4875a43b94bdd9622312"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79abcaa.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79abcaa.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/79abcaa.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79abcaa.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79abcaa.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/79abcaa.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/79abcaa.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79abcaa.html><meta property="article:published_time" content="2020-10-29T21:01:05+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:05+08:00"><meta name=Keywords content><meta name=description content="廣義線性模型②"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/79abcaa.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>廣義線性模型②</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><h1 class=pgc-h-arrow-right>1.1.10. 貝葉斯迴歸</h1><p style=text-align:start>貝葉斯迴歸可以用於在預估階段的參數正則化: 正則化參數的選擇不是通過人為的選擇，而是通過手動調節數據值來實現。</p><p>上述過程可以通過引入 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">無信息先驗</span> 到模型中的超參數來完成。 在 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">嶺迴歸</span>中使用的</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7f891d17860b4875a43b94bdd9622312><p class=pgc-img-caption></p></div><p>正則項相當於在</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d989ff3f11e45d49df6f0aa4ca47040><p class=pgc-img-caption></p></div><p>為高斯先驗條件，且此先驗的精確度為</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4fec947be31643eca3e3641b7889b167><p class=pgc-img-caption></p></div><p>時，求最大後驗估計。在這裡，我們沒有手工調參數 lambda ，而是讓他作為一個變量，通過數據中估計得到。</p><p>為了得到一個全概率模型，輸出</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9389b34b296e4c67a6e9561c89799486><p class=pgc-img-caption></p></div><p>也被認為是關於</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3bc2793bb7104e77ae2cafe4bbc82eef><p class=pgc-img-caption></p></div><p>的高斯分佈。</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/6fff88982c5e49b6ae66a1e99572f424><p class=pgc-img-caption></p></div><p style=text-align:start>Alpha 在這裡也是作為一個變量，通過數據中估計得到。</p><p style=text-align:start>貝葉斯迴歸有如下幾個優點:</p><ul><li>它能根據已有的數據進行改變。</li><li>它能在估計過程中引入正則項。</li></ul><p style=text-align:start>貝葉斯迴歸有如下缺點:</p><ul><li>它的推斷過程是非常耗時的。</li></ul><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">參考資料</span></strong></p><p><span style="color:#858585;--tt-darkmode-color: #858585">一個對於貝葉斯方法的很好的介紹 C. Bishop: Pattern Recognition and Machine learning詳細介紹原創算法的一本書 Bayesian learning for neural networks by Radford M. Neal</span></p></blockquote><h1 class=pgc-h-arrow-right>1.1.10.1. 貝葉斯嶺迴歸</h1><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">BayesianRidge</span></span> 利用概率模型估算了上述的迴歸問題，其先驗參數</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d989ff3f11e45d49df6f0aa4ca47040><p class=pgc-img-caption></p></div><p>是由以下球面高斯公式得出的：</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7f0545c81efe4d7bb7a9ede236fe8c79><p class=pgc-img-caption></p></div><p>先驗參數</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/40b73f2759314b0fbce47e264b9e127a><p class=pgc-img-caption></p></div><p>和</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d023166191b4df49d76795141085e0a><p class=pgc-img-caption></p></div><p>一般是服從 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">gamma 分佈</span> ，這個分佈與高斯成共軛先驗關係。 得到的模型一般稱為 <strong>貝葉斯嶺迴歸</strong>，並且這個與傳統的 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">Ridge</span></span> 非常相似。</p><p>參數</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d989ff3f11e45d49df6f0aa4ca47040><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/40b73f2759314b0fbce47e264b9e127a><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d023166191b4df49d76795141085e0a><p class=pgc-img-caption></p></div><p>是在模型擬合的時候一起被估算出來的，其中參數</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/40b73f2759314b0fbce47e264b9e127a><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d023166191b4df49d76795141085e0a><p class=pgc-img-caption></p></div><p>通過最大似然估計得到。scikit-learn的實現是基於文獻（Tipping，2001）的附錄A，參數</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/40b73f2759314b0fbce47e264b9e127a><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d023166191b4df49d76795141085e0a><p class=pgc-img-caption></p></div><p>的更新是基於文獻（MacKay，1992）。</p><p>剩下的超參數</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/44bd145ebf8a4802a266f4922344ed2a><p class=pgc-img-caption></p></div><p><br></p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d0a1ceb5a85b4871a9d797bec1339e00><p class=pgc-img-caption></p></div><p><br></p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a86441336667463ebc22c705e0f1b01b><p class=pgc-img-caption></p></div><p>以及</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2132c49b5edf4d6697f7ac08c2a77b75><p class=pgc-img-caption></p></div><p><br></p><p>是關於</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a67a17368688474d8b7242a771d59919><p class=pgc-img-caption></p></div><p>和</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d023166191b4df49d76795141085e0a><p class=pgc-img-caption></p></div><p>的 gamma 分佈的先驗。 它們通常被選擇為 <strong>無信息先驗</strong> 。默認</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/174a6774427c4ebd92be78a298d748ec><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/52e0e61c530644daa0f6665e022ab686><p class=pgc-img-caption></p></div><p style=text-align:start>貝葉斯嶺迴歸用來解決迴歸問題:</p><pre><code>&gt;&gt;&gt; from sklearn import linear_model&gt;&gt;&gt; X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]&gt;&gt;&gt; Y = [0., 1., 2., 3.]&gt;&gt;&gt; reg = linear_model.BayesianRidge()&gt;&gt;&gt; reg.fit(X, Y)BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300, normalize=False, tol=0.001, verbose=False)Copy</code></pre><p style=text-align:start>在模型訓練完成後，可以用來預測新值:</p><pre><code>&gt;&gt;&gt; reg.predict ([[1, 0.]])array([ 0.50000013])Copy</code></pre><p>權值</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d989ff3f11e45d49df6f0aa4ca47040><p class=pgc-img-caption></p></div><p>可以被這樣訪問:</p><pre><code>&gt;&gt;&gt; reg.coef_array([ 0.49999993,  0.49999993])Copy</code></pre><p style=text-align:start>由於貝葉斯框架的緣故，權值與 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">普通最小二乘法</span> 產生的不太一樣。 但是，貝葉斯嶺迴歸對病態問題（ill-posed）的魯棒性要更好。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">示例</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">:</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">貝葉斯嶺迴歸</span></p><p><strong>參考資料</strong></p><p>Section 3.3 in Christopher M. Bishop: Pattern Recognition and Machine Learning, 2006David J. C. MacKay, <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Bayesian Interpolation</span>, 1992.Michael E. Tipping, <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Sparse Bayesian Learning and the Relevance Vector Machine</span>, 2001.</p></blockquote><h1 class=pgc-h-arrow-right>1.1.10.2. 主動相關決策理論 - ARD</h1><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">ARDRegression</span></span> （主動相關決策理論）和 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">Bayesian Ridge Regression</span></span> 非常相似，但是會導致一個更加稀疏的權重w<span style="color:#4183c4;--tt-darkmode-color: #4183C4">[1][2]</span> 。</p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">ARDRegression</span></span> 提出了一個不同的</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d989ff3f11e45d49df6f0aa4ca47040><p class=pgc-img-caption></p></div><p>的先驗假設。具體來說，就是弱化了高斯分佈為球形的假設。</p><p>它採用</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d989ff3f11e45d49df6f0aa4ca47040><p class=pgc-img-caption></p></div><p>分佈是與軸平行的橢圓高斯分佈。</p><p>也就是說，每個權值</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1a0dd4578ec045a2a27d80888cc57b7f><p class=pgc-img-caption></p></div><p>從一箇中心在 0 點，精度為</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/913bf994b7b34a7aad8f49e1f9355a54><p class=pgc-img-caption></p></div><p>的高斯分佈中採樣得到的。</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b07c725118fd4356afb3360e90cc20de><p class=pgc-img-caption></p></div><p>並且</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a9aafdf578984780a1a7a68f1813485e><p class=pgc-img-caption></p></div><p>.</p><p>與 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">Bayesian Ridge Regression</span></span><span style="color:#4183c4;--tt-darkmode-color: #4183C4">_</span> 不同， 每個</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7c7e8bf3b8e640a7ac0216fa2fea3368><p class=pgc-img-caption></p></div><p>都有一個標準差</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/735deea541154c8591dd9cbbbc0a2236><p class=pgc-img-caption></p></div><p>。所有</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/735deea541154c8591dd9cbbbc0a2236><p class=pgc-img-caption></p></div><p>的先驗分佈 由超參數</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/51c6ae64e9bd46de948d042ad32c76aa><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0849346be1134bcbbb6f4720ea16116c><p class=pgc-img-caption></p></div><p>確定的相同的 gamma 分佈確定。</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dfe3fd43d46b4591b85ac14733714a16><p class=pgc-img-caption></p></div><p style=text-align:start>ARD 也被稱為 <strong>稀疏貝葉斯學習</strong> 或 <strong>相關向量機</strong> [3][4]。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">示例</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">:</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">Automatic Relevance Determination Regression (ARD)</span></p><p><strong>參考資料</strong>:</p><p>[1] Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1[2] David Wipf and Srikantan Nagarajan: <span style="color:#4183c4;--tt-darkmode-color: #4183C4">A new view of automatic relevance determination</span>[3] Michael E. Tipping: <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Sparse Bayesian Learning and the Relevance Vector Machine</span>[4] Tristan Fletcher: <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Relevance Vector Machines explained</span></p></blockquote><h1 class=pgc-h-arrow-right>1.1.11. logistic 迴歸</h1><p style=text-align:start>logistic 迴歸，雖然名字裡有 “迴歸” 二字，但實際上是解決分類問題的一類線性模型。在某些文獻中，logistic 迴歸又被稱作 logit 迴歸，maximum-entropy classification（MaxEnt，最大熵分類），或 log-linear classifier（對數線性分類器）。該模型利用函數 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">logistic function</span> 將單次試驗（single trial）的可能結果輸出為概率。</p><p style=text-align:start>scikit-learn 中 logistic 迴歸在 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LogisticRegression</span></span> 類中實現了二分類（binary）、一對多分類（one-vs-rest）及多項式 logistic 迴歸，並帶有可選的 L1 和 L2 正則化。</p><blockquote><p><span style="color:#858585;--tt-darkmode-color: #858585">注意，scikit-learn的邏輯迴歸在默認情況下使用L2正則化，這樣的方式在機器學習領域是常見的，在統計分析領域是不常見的。正則化的另一優勢是提升數值穩定性。scikit-learn通過將C設置為很大的值實現無正則化。</span></p></blockquote><p style=text-align:start>作為優化問題，帶 L2罰項的二分類 logistic 迴歸要最小化以下代價函數（cost function）：</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8286139a0e2b420aa65682a72eceb9e9><p class=pgc-img-caption></p></div><p style=text-align:start>類似地，帶 L1 正則的 logistic 迴歸解決的是如下優化問題：</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/558a8665edef4757b4c7768b7590a510><p class=pgc-img-caption></p></div><p style=text-align:start><br></p><p style=text-align:start>Elastic-Net正則化是L1 和 L2的組合，來使如下代價函數最小:</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c50b8563ffb640fa93094d477c6bb2e0><p class=pgc-img-caption></p></div><p style=text-align:start>其中ρ控制正則化L1與正則化L2的強度(對應於<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">l1_ratio</span>參數)。</p><p style=text-align:start>注意，在這個表示法中，假定目標y_i在測試時應屬於集合[-1,1]。我們可以發現Elastic-Net在ρ=1時與L1罰項等價,在ρ=0時與L2罰項等價</p><p style=text-align:start>在 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LogisticRegression</span></span> 類中實現了這些優化算法: <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">liblinear</span>， <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">newton-cg</span>， <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">lbfgs</span>， <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">sag</span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">saga</span>。</p><p style=text-align:start><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">liblinear</span>應用了座標下降算法（Coordinate Descent, CD），並基於 scikit-learn 內附的高性能 C++ 庫 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">LIBLINEAR library</span> 實現。不過 CD 算法訓練的模型不是真正意義上的多分類模型，而是基於 “one-vs-rest” 思想分解了這個優化問題，為每個類別都訓練了一個二元分類器。因為實現在底層使用該求解器的 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LogisticRegression</span></span> 實例對象表面上看是一個多元分類器。 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">sklearn.svm.l1_min_c</span></span> 可以計算使用 L1時 C 的下界，以避免模型為空（即全部特徵分量的權重為零）。</p><p style=text-align:start><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">lbfgs</span>, <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">sag</span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">newton-cg</span> 求解器只支持 L2罰項以及無罰項，對某些高維數據收斂更快。這些求解器的參數 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">multi_class</span>設為 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">multinomial</span> 即可訓練一個真正的多項式 logistic 迴歸 [5] ，其預測的概率比默認的 “<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">one-vs-rest</span>” 設定更為準確。</p><p style=text-align:start><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">sag</span> 求解器基於平均隨機梯度下降算法（Stochastic Average Gradient descent） [6]。在大數據集上的表現更快，大數據集指樣本量大且特徵數多。</p><p style=text-align:start><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">saga</span> 求解器 [7] 是 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">sag</span> 的一類變體，它支持非平滑（non-smooth）的 L1 正則選項 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">penalty="l1"</span> 。因此對於稀疏多項式 logistic 迴歸 ，往往選用該求解器。<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">saga</span>求解器是唯一支持彈性網絡正則選項的求解器。</p><p style=text-align:start><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">lbfgs</span>是一種近似於Broyden–Fletcher–Goldfarb–Shanno算法[8]的優化算法，屬於準牛頓法。<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">lbfgs</span>求解器推薦用於較小的數據集，對於較大的數據集，它的性能會受到影響。[9]</p><p style=text-align:start>總的來說，各求解器特點如下:</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fda3d048a91549dfbc09e693f9cc3e80><p class=pgc-img-caption></p></div><p style=text-align:start>默認情況下，<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">lbfgs</span>求解器魯棒性佔優。對於大型數據集，<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">saga</span>求解器通常更快。對於大數據集，還可以用 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">SGDClassifier</span></span> ，並使用對數損失（<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">log</span> loss）這可能更快，但需要更多的調優。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">示例</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">：</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">Logistic迴歸中的L1罰項和稀疏係數L1罰項-logistic迴歸的路徑多項式和OVR的Logistic迴歸newgroups20上的多類稀疏Logistic迴歸使用多項式Logistic迴歸和L1進行MNIST數據集的分類</span></p><p><strong>與 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">liblinear</span> 的區別:</strong></p><p>當 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">fit_intercept=False</span> 擬合得到的 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">coef_</span> 或者待預測的數據為零時，用 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">solver=liblinear</span> 的 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LogisticRegression</span></span> 或 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LinearSVC</span> 與直接使用外部 liblinear 庫預測得分會有差異。這是因為， 對於 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">decision_function</span> 為零的樣本， <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LogisticRegression</span></span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LinearSVC</span> 將預測為負類，而 liblinear 預測為正類。 注意，設定了 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">fit_intercept=False</span> ，又有很多樣本使得 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">decision_function</span> 為零的模型，很可能會欠擬合，其表現往往比較差。建議您設置 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">fit_intercept=True</span> 並增大 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">intercept_scaling</span> 。</p><p><strong>注意:利用稀疏 logistic 迴歸進行特徵選擇</strong></p><p>帶 L1罰項的 logistic 迴歸 將得到稀疏模型（sparse model），相當於進行了特徵選擇（feature selection），詳情參見 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">基於 L1 的特徵選取</span>。</p></blockquote><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">LogisticRegressionCV</span></span> 對 logistic 迴歸 的實現內置了交叉驗證（cross-validation），可以找出最優的 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">C</span>和<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">l1_ratio</span>參數 。<span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">newton-cg</span>， <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">sag</span>， <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">saga</span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">lbfgs</span> 在高維數據上更快，這是因為採用了熱啟動（warm-starting）。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">參考資料</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">：</span></p><p><span style="color:#858585;--tt-darkmode-color: #858585">[5] Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 4.3.4[6] Mark Schmidt, Nicolas Le Roux, and Francis Bach: </span><span style="color:#4183c4;--tt-darkmode-color: #4183C4">Minimizing Finite Sums with the Stochastic Average Gradient.</span>[7] Aaron Defazio, Francis Bach, Simon Lacoste-Julien: <span style="color:#4183c4;--tt-darkmode-color: #4183C4">SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives.</span>[8]<span style="color:#4183c4;--tt-darkmode-color: #4183C4">https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm</span>[9] <span style="color:#4183c4;--tt-darkmode-color: #4183C4">“Performance Evaluation of Lbfgs vs other solvers”</span></p></blockquote><h1 class=pgc-h-arrow-right>1.1.12. 隨機梯度下降， SGD</h1><p style=text-align:start>隨機梯度下降是擬合線性模型的一個簡單而高效的方法。在樣本量（和特徵數）很大時尤為有用。 方法 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">partial_fit</span> 可用於 online learning （在線學習）或基於 out-of-core learning （外存的學習）</p><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">SGDClassifier</span></span> 和 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">SGDRegressor</span></span> 分別用於擬合分類問題和迴歸問題的線性模型，可使用不同的（凸）損失函數，支持不同的罰項。 例如，設定 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">loss="log"</span> ，則 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">SGDClassifier</span></span> 擬合一個邏輯斯蒂迴歸模型，而 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">loss="hinge"</span> 擬合線性支持向量機（SVM）。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">參考資料</span></strong></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">隨機梯度下降</span></p></blockquote><h1 class=pgc-h-arrow-right>1.1.13. Perceptron（感知器）</h1><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">Perceptron</span></span> 是適用於大規模學習的一種簡單算法。默認情況下：</p><ul><li>不需要設置學習率（learning rate）。</li><li>不需要正則化處理。</li><li>僅使用錯誤樣本更新模型。</li></ul><p style=text-align:start>最後一點表明使用合頁損失（hinge loss）的感知機比 SGD 略快，所得模型更稀疏。</p><h1 class=pgc-h-arrow-right>1.1.14. Passive Aggressive Algorithms（被動攻擊算法）</h1><p style=text-align:start>被動攻擊算法是大規模學習的一類算法。和感知機類似，它也不需要設置學習率，不過比感知機多出一個正則化參數 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">C</span> 。</p><p style=text-align:start>對於分類問題， <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">PassiveAggressiveClassifier</span></span> 可設定 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">loss='hinge'</span> （PA-I）或 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">loss='squared_hinge'</span>（PA-II）。對於迴歸問題， <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">PassiveAggressiveRegressor</span></span> 可設置 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">loss='epsilon_insensitive'</span> （PA-I）或 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">loss='squared_epsilon_insensitive'</span> （PA-II）。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">參考資料</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">：</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">“Online Passive-Aggressive Algorithms”</span> K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR 7 (2006)</p></blockquote><h1 class=pgc-h-arrow-right>1.1.15. 穩健迴歸（Robustness regression）: 處理離群點（outliers）和模型錯誤</h1><p style=text-align:start>穩健迴歸（robust regression）特別適用於迴歸模型包含損壞數據（corrupt data）的情況，如離群點或模型中的錯誤。</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4d3415e71155405586f6f8f9510108c7><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>1.1.15.1. 各種使用場景與相關概念</h1><p style=text-align:start>處理包含離群點的數據時牢記以下幾點:</p><ul><li><strong>離群值在 X 上還是在 y 方向上</strong>?離群值在 y 方向上離群值在 X 方向上</li></ul><ul><li><strong>離群點的比例 vs. 錯誤的量級（amplitude）</strong>離群點的數量很重要，離群程度也同樣重要。低離群點的數量高離群點的數量</li></ul><p style=text-align:start>穩健擬合（robust fitting）的一個重要概念是崩潰點（breakdown point），即擬合模型（仍準確預測）所能承受的離群值最大比例。</p><p style=text-align:start>注意，在高維數據條件下（ <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">n_features</span>大），一般而言很難完成穩健擬合，很可能完全不起作用。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">尋找平衡： 預測器的選擇</span></strong></p><p><span style="color:#858585;--tt-darkmode-color: #858585">Scikit-learn提供了三種穩健迴歸的預測器（estimator）: </span><span style="color:#4183c4;--tt-darkmode-color: #4183C4">RANSAC</span> ， <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Theil Sen</span> 和 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">HuberRegressor</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">HuberRegressor</span> 一般快於 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">RANSAC</span> 和 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Theil Sen</span> ，除非樣本數很大，即 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">n_samples</span> >> <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">n_features</span> 。 這是因為 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">RANSAC</span> 和 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Theil Sen</span> 都是基於數據的較小子集進行擬合。但使用默認參數時， <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Theil Sen</span> 和 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">RANSAC</span> 可能不如 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">HuberRegressor</span> 魯棒。</p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">RANSAC</span> 比 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Theil Sen</span> 更快，在樣本數量上的伸縮性（適應性）更好。<span style="color:#4183c4;--tt-darkmode-color: #4183C4">RANSAC</span> 能更好地處理y方向的大值離群點（通常情況下）。<span style="color:#4183c4;--tt-darkmode-color: #4183C4">Theil Sen</span> 能更好地處理x方向中等大小的離群點，但在高維情況下無法保證這一特點。 實在決定不了的話，請使用 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">RANSAC</span></p></blockquote><h1 class=pgc-h-arrow-right>1.1.15.2. RANSAC： 隨機抽樣一致性算法（RANdom SAmple Consensus）</h1><p style=text-align:start>隨機抽樣一致性算法（RANdom SAmple Consensus， RANSAC）利用全體數據中局內點（inliers）的一個隨機子集擬合模型。</p><p style=text-align:start>RANSAC 是一種非確定性算法，以一定概率輸出一個可能的合理結果，依賴於迭代次數（參數 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">max_trials</span>）。這種算法主要解決線性或非線性迴歸問題，在計算機視覺攝影測繪領域尤為流行。</p><p style=text-align:start>算法從全體樣本輸入中分出一個局內點集合，全體樣本可能由於測量錯誤或對數據的假設錯誤而含有噪點、離群點。最終的模型僅從這個局內點集合中得出。</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/582cb03c289f45ecb6f92aebd2093241><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>1.1.15.2.1. 算法細節</h1><p style=text-align:start>每輪迭代執行以下步驟:</p><ol start=1><li>從原始數據中抽樣 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">min_samples</span> 數量的隨機樣本，檢查數據是否合法（見 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">is_data_valid</span> ）。</li><li>用一個隨機子集擬合模型（ <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">base_estimator.fit</span> ）。檢查模型是否合法（見 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">is_model_valid</span> ）。</li><li>計算預測模型的殘差（residual），將全體數據分成局內點和離群點（ <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">base_estimator.predict(X) - y</span>）。絕對殘差小於 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">residual_threshold</span> 的全體數據認為是局內點。</li><li>若局內點樣本數最大，保存當前模型為最佳模型。以免當前模型離群點數量恰好相等（而出現未定義情況），規定僅當數值大於當前最值時認為是最佳模型。</li></ol><p style=text-align:start>上述步驟或者迭代到最大次數（ <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">max_trials</span> ），或者某些終止條件滿足時停下（見 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">stop_n_inliers</span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">stop_score</span> )。最終模型由之前確定的最佳模型的局內點樣本（一致性集合，consensus set）預測。</p><p style=text-align:start>函數 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">is_data_valid</span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">is_model_valid</span> 可以識別出隨機樣本子集中的退化組合（degenerate combinations）並予以丟棄（reject）。即便不需要考慮退化情況，也會使用 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">is_data_valid</span> ，因為在擬合模型之前調用它能得到更高的計算性能。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">示例</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">：</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">基於RANSAC的穩健線性模型估計穩健線性估計擬合</span></p><p><strong>參考資料</strong>：</p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">https://en.wikipedia.org/wiki/RANSAC“Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography”</span> Martin A. Fischler and Robert C. Bolles - SRI International (1981)<span style="color:#4183c4;--tt-darkmode-color: #4183C4">“Performance Evaluation of RANSAC Family”</span> Sunglok Choi, Taemin Kim and Wonpil Yu - BMVC (2009)</p></blockquote><h1 class=pgc-h-arrow-right>1.1.15.3. Theil-Sen 預估器: 廣義中值估計器（generalized-median-based estimator）</h1><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">TheilSenRegressor</span></span> 估計器：使用中位數在多個維度泛化，對多元異常值更具有魯棒性，但問題是，隨著維數的增加，估計器的準確性在迅速下降。準確性的丟失，導致在高維上的估計值比不上普通的最小二乘法。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">示例</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">:</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">廣義中值估計器迴歸穩健線性估計擬合</span></p><p><strong>參考資料</strong>:</p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator</span></p></blockquote><h1 class=pgc-h-arrow-right>1.1.15.3.1. 算法理論細節</h1><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">TheilSenRegressor</span></span> 在漸近效率和無偏估計方面足以媲美 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Ordinary Least Squares (OLS)</span> （普通最小二乘法（OLS））。與 OLS 不同的是， Theil-Sen 是一種非參數方法，這意味著它沒有對底層數據的分佈假設。由於 Theil-Sen 是基於中值的估計，它更適合於損壞的數據即離群值。 在單變量的設置中，Theil-Sen 在簡單的線性迴歸的情況下，其崩潰點大約 29.3% ，這意味著它可以容忍任意損壞的數據高達 29.3% 。</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4d3415e71155405586f6f8f9510108c7><p class=pgc-img-caption></p></div><p style=text-align:start>scikit-learn 中實現的 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">TheilSenRegressor</span></span> 是多元線性迴歸模型的推廣 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">[8]</span> ，利用了空間中值方法，它是多維中值的推廣 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">[9]</span> 。</p><p style=text-align:start>關於時間複雜度和空間複雜度，Theil-Sen 的尺度根據</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cb6c6fd1756843ca96a312158150661d><p class=pgc-img-caption></p></div><p style=text-align:start>這使得它不適用於大量樣本和特徵的問題。因此，可以選擇一個亞群的大小來限制時間和空間複雜度，只考慮所有可能組合的隨機子集。</p><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">示例</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">:</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">廣義中值估計器迴歸</span></p><p><strong>參考資料</strong>:</p><p>[10] Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Theil-Sen Estimators in a Multiple Linear Regression Model.</span> |[11] Kärkkäinen and S. Äyrämö: <span style="color:#4183c4;--tt-darkmode-color: #4183C4">On Computation of Spatial Median for Robust Data Mining.</span></p></blockquote><h1 class=pgc-h-arrow-right>1.1.15.4. Huber 迴歸</h1><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">HuberRegressor</span></span> 與 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">Ridge</span></span> 不同，因為它對於被分為異常值的樣本應用了一個線性損失。如果這個樣品的絕對誤差小於某一閾值，樣品就被分為內圍值。 它不同於 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">TheilSenRegressor</span></span> 和 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">RANSACRegressor</span></span> ，因為它沒有忽略異常值的影響，並分配給它們較小的權重。</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a5d884276c584c159a1e4a221c45aca5><p class=pgc-img-caption></p></div><p style=text-align:start>這個 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">HuberRegressor</span></span> 最小化的損失函數是：</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/da62517d44a2478da467443e7f74fb96><p class=pgc-img-caption></p></div><p style=text-align:start>其中</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9b51492aff884fa787a9fe7dbe6694ab><p class=pgc-img-caption></p></div><p style=text-align:start>建議設置參數 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">epsilon</span> 為 1.35 以實現 95% 統計效率。</p><h1 class=pgc-h-arrow-right>1.1.15.5. 注意</h1><p style=text-align:start><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">HuberRegressor</span></span> 與將損失設置為 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">huber</span>的 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">SGDRegressor</span></span> 並不相同，體現在以下方面的使用方式上。</p><ul><li><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">HuberRegressor</span></span> 是標度不變性的. 一旦設置了 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">epsilon</span> ， 通過不同的值向上或向下縮放 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">X</span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">y</span> ，就會跟以前一樣對異常值產生同樣的魯棒性。相比 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">SGDRegressor</span></span> 其中 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">epsilon</span> 在 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">X</span> 和 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">y</span> 被縮放的時候必須再次設置。</li><li><span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">HuberRegressor</span></span> 應該更有效地使用在小樣本數據，同時 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">SGDRegressor</span></span> 需要一些訓練數據的 passes 來產生一致的魯棒性。</li></ul><blockquote><p><strong><span style="color:#858585;--tt-darkmode-color: #858585">示例</span></strong><span style="color:#858585;--tt-darkmode-color: #858585">:</span></p><p><span style="color:#4183c4;--tt-darkmode-color: #4183C4">強</span><span style="color:#4183c4;--tt-darkmode-color: #4183C4">異常數據集上的huberregression與 Ridge</span></p><p><strong>參考資料</strong>:</p><p>Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale estimates, pg 172</p></blockquote><p style=text-align:start>另外，這個估計是不同於 R 實現的 Robust Regression (<span style="color:#4183c4;--tt-darkmode-color: #4183C4">http://www.ats.ucla.edu/stat/r/dae/rreg.htm</span>) ，因為 R 實現加權最小二乘，權重考慮到每個樣本並基於殘差大於某一閾值的量。</p><h1 class=pgc-h-arrow-right>1.1.16. 多項式迴歸：用基函數展開線性模型</h1><p style=text-align:start>機器學習中一種常見的模式，是使用線性模型訓練數據的非線性函數。這種方法保持了一般快速的線性方法的性能，同時允許它們適應更廣泛的數據範圍。</p><p style=text-align:start>例如，可以通過構造係數的 <strong>polynomial features</strong> 來擴展一個簡單的線性迴歸。在標準線性迴歸的情況下，你可能有一個類似於二維數據的模型:</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/22f0755614784fd799ccff85e5deb319><p class=pgc-img-caption></p></div><p style=text-align:start>如果我們想把拋物面擬合成數據而不是平面，我們可以結合二階多項式的特徵，使模型看起來像這樣:</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/78d856f2a8d24ed3ad550e7e996077c7><p class=pgc-img-caption></p></div><p style=text-align:start>觀察到這 <em>還是一個線性模型</em> （這有時候是令人驚訝的）: 看到這個，想象創造一個新的變量</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d41b7573b9bb48b78caefae54a9d5370><p class=pgc-img-caption></p></div><p style=text-align:start>有了這些重新標記的數據，我們可以將問題寫成</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5654a29f26f5448bba6bbd58dd4ee659><p class=pgc-img-caption></p></div><p>我們看到，所得的 <em>polynomial regression</em> 與我們上文所述線性模型是同一類（即關於</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6d989ff3f11e45d49df6f0aa4ca47040><p class=pgc-img-caption></p></div><p>是線性的），因此可以用同樣的方法解決。通過用這些基函數建立的高維空間中的線性擬合，該模型具有靈活性，可以適應更廣泛的數據範圍。</p><p style=text-align:start>這裡是一個示例，使用不同程度的多項式特徵將這個想法應用於一維數據:</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1875b0d429454d87ad3db365d055b36e><p class=pgc-img-caption></p></div><p style=text-align:start>這個圖是使用 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">PolynomialFeatures</span></span> 預創建。該預處理器將輸入數據矩陣轉換為給定度的新數據矩陣。使用方法如下:</p><pre><code>&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; X = np.arange(6).reshape(3, 2)&gt;&gt;&gt; Xarray([[0, 1], [2, 3], [4, 5]])&gt;&gt;&gt; poly = PolynomialFeatures(degree=2)&gt;&gt;&gt; poly.fit_transform(X)array([[  1.,   0.,   1.,   0.,   0.,   1.], [  1.,   2.,   3.,   4.,   6.,   9.], [  1.,   4.,   5.,  16.,  20.,  25.]])Copy</code></pre><p><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">X</span> 的特徵已經從</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6a66a12f73b541a1984debde8ad11e4d><p class=pgc-img-caption></p></div><p>轉換到</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/245df5305fe14b3c89ceb4ae622fa77a><p class=pgc-img-caption></p></div><p>, 並且現在可以用在任何線性模型。</p><p style=text-align:start>這種預處理可以通過 <span style="color:#4183c4;--tt-darkmode-color: #4183C4">Pipeline</span> 工具進行簡化。可以創建一個表示簡單多項式迴歸的單個對象，使用方法如下所示:</p><pre><code>&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; from sklearn.linear_model import LinearRegression&gt;&gt;&gt; from sklearn.pipeline import Pipeline&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; model = Pipeline([('poly', PolynomialFeatures(degree=3)),...                   ('linear', LinearRegression(fit_intercept=False))])&gt;&gt;&gt; # fit to an order-3 polynomial data&gt;&gt;&gt; x = np.arange(5)&gt;&gt;&gt; y = 3 - 2 * x + x ** 2 - x ** 3&gt;&gt;&gt; model = model.fit(x[:, np.newaxis], y)&gt;&gt;&gt; model.named_steps['linear'].coef_array([ 3., -2.,  1., -1.])Copy</code></pre><p style=text-align:start>利用多項式特徵訓練的線性模型能夠準確地恢復輸入多項式係數。</p><p>在某些情況下，沒有必要包含任何單個特徵的更高的冪，只需要相乘最多</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/66a5cddd27b14b1088a524c58d9a7364><p class=pgc-img-caption></p></div><p>個不同的特徵即可，所謂 <em>interaction features（交互特徵）</em> 。這些可通過設定 <span style="color:#4183c4;--tt-darkmode-color: #4183C4"><span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">PolynomialFeatures</span></span> 的 <span style="background-color:#f7f7f7;--tt-darkmode-bgcolor: #1A1A1A">interaction_only=True</span> 得到。</p><p>例如，當處理布爾屬性，對於所有</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3cb406b96b60424a8f82adbc5a9f37d0><p class=pgc-img-caption></p></div><p></p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/12abc4634a82402f8a4edc34d0b56a7e><p class=pgc-img-caption></p></div><p>，因此是無用的；但</p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bf939892acf945a88cab56a882084922><p class=pgc-img-caption></p></div><p>代表兩布爾結合。這樣我們就可以用線性分類器解決異或問題:</p><pre><code>&gt;&gt;&gt; from sklearn.linear_model import Perceptron&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])&gt;&gt;&gt; y = X[:, 0] ^ X[:, 1]&gt;&gt;&gt; yarray([0, 1, 1, 0])&gt;&gt;&gt; X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)&gt;&gt;&gt; Xarray([[1, 0, 0, 0], [1, 0, 1, 0], [1, 1, 0, 0], [1, 1, 1, 1]])&gt;&gt;&gt; clf = Perceptron(fit_intercept=False, max_iter=10, tol=None,...                  shuffle=False).fit(X, y)Copy</code></pre><p style=text-align:start>分類器的 “predictions” 是完美的:</p><pre><code>&gt;&gt;&gt; clf.predict(X)array([0, 1, 1, 0])&gt;&gt;&gt; clf.score(X, y)1.0Copy</code></pre><p style=text-align:start><br></p><hr><p>一直在努力!<br></p><div class=pgc-img><img alt=廣義線性模型② onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a6020647f68344fda2e5d786f154a01a><p class=pgc-img-caption></p></div><p><br></p><p>最後，小編想說：我是一名python開發工程師，</p><p>整理了一套最新的python系統學習教程，</p><p>想要這些資料的可以關注私信小編“1或者6”即可（免費分享哦）希望能對你有所幫助.</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>廣義線性</a></li><li><a>模型</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/e1360dd4.html alt=「Linux」高併發服務器模型（多進程和多線程實例模型） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/170e1596c32348f39d6ace1f327e45d5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e1360dd4.html title=「Linux」高併發服務器模型（多進程和多線程實例模型）>「Linux」高併發服務器模型（多進程和多線程實例模型）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4f2fe4db.html alt=全等三角形八大模型——邊邊角模型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/66b90004d37cea41ea93 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4f2fe4db.html title=全等三角形八大模型——邊邊角模型>全等三角形八大模型——邊邊角模型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b7d18c2c.html alt=「思維模型」#生物學：6.複製 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3ea4498d170b4e65b4c5c8b113bf9924 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b7d18c2c.html title=「思維模型」#生物學：6.複製>「思維模型」#生物學：6.複製</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/52db9979.html alt="基於 Python 的時序模型——AMIRA模型" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1531801339038572a99eb47 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/52db9979.html title="基於 Python 的時序模型——AMIRA模型">基於 Python 的時序模型——AMIRA模型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/86431765.html alt=時間序列模型怎麼畫？乾貨分享高顏值模型圖軟件 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/24e3eb9f18f746d2ae3a4e98a1fbaa60 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/86431765.html title=時間序列模型怎麼畫？乾貨分享高顏值模型圖軟件>時間序列模型怎麼畫？乾貨分享高顏值模型圖軟件</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a35add78.html alt=RFM模型是什麼？輕鬆製作好看模型圖軟件 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7217744eab464b748da50bb57645caa1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a35add78.html title=RFM模型是什麼？輕鬆製作好看模型圖軟件>RFM模型是什麼？輕鬆製作好看模型圖軟件</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/23af2adf.html alt=中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RcGwzyuCkrlJ0I style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/23af2adf.html title=中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍>中文預訓練ALBERT模型來了：小模型登頂GLUE，Base版模型小10倍、速度快1倍</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c677e605.html alt=軟件的質量模型（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8aaa2a8120704809af7e859613470b4b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c677e605.html title=軟件的質量模型（一）>軟件的質量模型（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a16de757.html alt=44思維模型：三重心智模型一為什麼聰明人也會做蠢事 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/6476a6326d854588bb6686c54f4242f1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a16de757.html title=44思維模型：三重心智模型一為什麼聰明人也會做蠢事>44思維模型：三重心智模型一為什麼聰明人也會做蠢事</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/6fd71411.html alt=“專業塑造，品質模型”長沙市模型公司優秀企業推薦 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/493a856786464f46a4c4da2226bca508 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/6fd71411.html title=“專業塑造，品質模型”長沙市模型公司優秀企業推薦>“專業塑造，品質模型”長沙市模型公司優秀企業推薦</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/d3d5884b.html alt=商之道：利用飛輪模型來推演現代商業系統模型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3236576184584329aba7ce3bc377fdb4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/d3d5884b.html title=商之道：利用飛輪模型來推演現代商業系統模型>商之道：利用飛輪模型來推演現代商業系統模型</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/31276977.html alt="疾病模型國際研討會在粵召開 跨領域探討疾病模型發展" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2b26db4c9e8141ff9346c9c6e5fc64a4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/31276977.html title="疾病模型國際研討會在粵召開 跨領域探討疾病模型發展">疾病模型國際研討會在粵召開 跨領域探討疾病模型發展</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/979ea406.html alt=現代計算機模型-J.U.C併發系列（1） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/0a2ce00c253f402cbcd3c7a9969d2543 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/979ea406.html title=現代計算機模型-J.U.C併發系列（1）>現代計算機模型-J.U.C併發系列（1）</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/6c823316.html alt=初中必會幾何模型（口訣突破）：手拉手模型（或旋轉型） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/af3d6fa7183a47a98414eb4410772850 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/6c823316.html title=初中必會幾何模型（口訣突破）：手拉手模型（或旋轉型）>初中必會幾何模型（口訣突破）：手拉手模型（或旋轉型）</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/748d2c3e.html alt="34套恐龍模型合集 Dinosaurs with Rig恐龍模型" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/23af0714027345c4b03b00080d9e7408 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/748d2c3e.html title="34套恐龍模型合集 Dinosaurs with Rig恐龍模型">34套恐龍模型合集 Dinosaurs with Rig恐龍模型</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>