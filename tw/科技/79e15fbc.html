<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>如何使用pytorch自動求梯度 | 极客快訊</title><meta property="og:title" content="如何使用pytorch自動求梯度 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/81815a9554cb45cba6a49188c07e579c"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79e15fbc.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79e15fbc.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/79e15fbc.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79e15fbc.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79e15fbc.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/79e15fbc.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/79e15fbc.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/79e15fbc.html><meta property="article:published_time" content="2020-11-14T21:08:21+08:00"><meta property="article:modified_time" content="2020-11-14T21:08:21+08:00"><meta name=Keywords content><meta name=description content="如何使用pytorch自動求梯度"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/79e15fbc.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>如何使用pytorch自動求梯度</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>構建深度學習模型的基本流程就是：搭建計算圖，求得損失函數，然後計算損失函數對模型參數的導數，再利用梯度下降法等方法來更新參數。搭建計算圖的過程，稱為“正向傳播”，這個是需要我們自己動手的，因為我們需要設計我們模型的結構。由損失函數求導的過程，稱為“反向傳播”，求導是件辛苦事兒，所以自動求導基本上是各種深度學習框架的基本功能和最重要的功能之一，PyTorch也不例外。</p><h1 class=pgc-h-arrow-right><strong>一、pytorch自動求導初步認識</strong></h1><p>比如有一個函數，y=x的平方（y=x2）,在x=3的時候它的導數為6，我們通過代碼來演示這樣一個過程。</p><pre><code>x=torch.tensor(3.0,requires_grad=True)y=torch.pow(x,2)#判斷x,y是否是可以求導的print(x.requires_grad)print(y.requires_grad)#求導，通過backward函數來實現y.backward()#查看導數，也即所謂的梯度print(x.grad)</code></pre><p>最終的運行結果為：</p><pre><code>TrueTruetensor(6.) #這和我們自己算的是一模一樣的。</code></pre><p>這裡有一些關鍵點</p><h1 class=pgc-h-arrow-right><strong>1.1 tensor的創建與屬性設置</strong></h1><p>先來看一下tensor的定義：</p><p>tensor(data, dtype=None, device=None, requires_grad=False) -> Tensor</p><pre><code>參數:data： (array_like): tensor的初始值. 可以是列表，元組，numpy數組，標量等;dtype： tensor元素的數據類型device： 指定CPU或者是GPU設備，默認是Nonerequires_grad：是否可以求導，即求梯度，默認是False，即不可導的</code></pre><p><strong>（1）tensor對象的requires_grad屬性</strong></p><p>每一個tensor都有一個requires_grad屬性，表示這個tensor是否是可求導的，如果是true則可以求導，否則不能求導，語法格式為：</p><p>x.requires_grad 判斷一個tensor是否可以求導，返回布爾值</p><p>需要注意的是，只有當所有的“葉子變量”，即所謂的leaf variable都是不可求導的，那函數y才是不能求導的，什麼是leaf variable呢？這其實涉及到“計算圖”相關的知識，但是我們通過下面的例子一下就能明白了，如下：</p><pre><code>#創建一個二元函數，即z=f(x,y)=x2+y2，x可求導，y設置不可求導x=torch.tensor(3.0,requires_grad=True)y=torch.tensor(4.0,requires_grad=False)z=torch.pow(x,2)+torch.pow(y,2)#判斷x,y是否是可以求導的print(x.requires_grad)print(y.requires_grad)print(z.requires_grad)#求導，通過backward函數來實現z.backward()#查看導數，也即所謂的梯度print(x.grad)print(y.grad)</code></pre><p>運行結果為：</p><pre><code>True # x是可導的False # y是不可導的True # z是可導的，因為它有一個 leaf variable 是可導的，即x可導tensor(6.) # x的導數None # 因為y不可導，所以是none</code></pre><p>如果是上面的 leaf variable變量x也設置為不可導的，那麼z也不可導，因為x、y均不可導，那麼z自然不可導了。</p><p><strong>（2）leaf variable（也是tensor）的requires_grad_()方法</strong></p><p>如果某一個葉子變量，開始時不可導的，後面想設置它可導，或者反過來，該怎麼辦呢？tensor提供了一個方法，即</p><p>x.requires_grad_(True/False) 設置tensor的可導與不可導，注意後面有一個下劃線哦！</p><p>但是需要注意的是，我只能夠設置葉子變量，即leaf variable的這個方法，否則會出現以下錯誤：</p><p>RuntimeError: you can only change requires_grad flags of leaf variables.</p><h1 class=pgc-h-arrow-right><strong>1.2 函數的求導方法——y.backward()方法</strong></h1><p>上面只演示了簡單函數的求導法則，</p><p>需要注意的是：如果出現了複合函數，比如 y是x的函數，z是y的函數，f是z的函數，那麼在求導的時候，會使用 f.backwrad()只會默認求f對於葉子變量leaf variable的導數值，而對於中間變量y、z的導數值是不知道的，直接通過x.grad是知道的、y.grad、z.grad的值為none。</p><p>下面來看一下這個函數backward的定義：</p><pre><code>backward(gradient=None, retain_graph=None, create_graph=False)</code></pre><p>它的三個參數都是可選的，上面的示例中還沒有用到任何一個參數，關於這三個參數，我後面會詳細說到，這裡先跳過。</p><p><strong>1.3 查看求得的導數的值——x.grad屬性</strong></p><p>通過tensor的grad屬性查看所求得的梯度值。</p><p>總結：</p><p>（1）torch.tensor()設置requires_grad關鍵字參數</p><p>（2）查看tensor是否可導，x.requires_grad 屬性</p><p>（3）設置葉子變量 leaf variable的可導性，x.requires_grad_()方法</p><p>（4）自動求導方法 y.backward() ，直接調用backward()方法，只會計算對計算圖葉節點的導數。</p><p>（5）查看求得的到數值， x.grad 屬性</p><p>易錯點：</p><p>為什麼上面的標量x的值是3.0和4.0，而不是整數呢？這是因為，要想使x支持求導，必須讓x為浮點類型，也就是我們給初始值的時候要加個點：“.”。不然的話，就會報錯。 即，不能定義[1,2,3]，而應該定義成[1.,2.,3.]，前者是整數，後者才是浮點數，浮點數才能求導。</p><h1 class=pgc-h-arrow-right><strong>二、求導的核心函數——backwrad函數詳解</strong></h1><h1 class=pgc-h-arrow-right><strong>2.1 默認的求導規則</strong></h1><p>在pytorch裡面，默認：只能是【標量】對【標量】，或者【標量】對向【量/矩陣】求導！這個很關鍵，很重要！</p><p><strong>（1）標量對標量求導</strong></p><p>參見上面的例子，x,y,z都是標量，所以求導過程也很簡單，不再贅述。</p><p><strong>（2）標量對向量/矩陣求導</strong></p><p>為什麼標量對於向量/矩陣是默認的呢？因為在深度學習中，我們一般在求導的時候是對損失函數求導，損失函數一般都是一個標量，即將所有項的損失加起來，但是參數又往往是向量或者是矩陣，所以這就是默認的了。看下面的例子。</p><p>比如有一個輸入層為3節點的輸入層，輸出層為一個節點的輸出層，這樣一個簡單的神經網絡，針對以組樣本而言，有</p><p>X=（x1,x2,x3）=(1.5,2.5,3.5)，X是（1,3）維的，輸出層的權值矩陣為W=（w1,w2,w3）T=(0.2,0.4,0.6)T，這裡表示初始化的權值矩陣，T表示轉置，則W表示的是（3,1）維度，偏置項為b=0.1,是一個標量，則可以構建一個模型如下：</p><p>Y=XW+b，其中W,b就是要求倒數的變量，這裡Y是一個標量，W是向量，b是標量，W,b是葉節點，leaf variable，</p><p>將上面展開得到：</p><p>Y=x1<em>w1+x2</em>w2<em>x3</em>w3+b （這裡的1,2,3是下標，不是次方哦！難得用公式截圖）</p><p>自己手動計算得到，</p><p>Y對w1的導數為1.5</p><p>Y對w2的導數為2.5</p><p>Y對w3的導數為3.5</p><p>Y對b的導數為1</p><p>下面我們來驗證一下：</p><pre><code>#創建一個多元函數，即Y=XW+b=Y=x1*w1+x2*w2*x3*w3+b，x不可求導，W,b設置可求導X=torch.tensor([1.5,2.5,3.5],requires_grad=False)W=torch.tensor([0.2,0.4,0.6],requires_grad=True)b=torch.tensor(0.1,requires_grad=True)Y=torch.add(torch.dot(X,W),b)#判斷每個tensor是否是可以求導的print(X.requires_grad)print(W.requires_grad)print(b.requires_grad)print(Y.requires_grad)#求導，通過backward函數來實現Y.backward()#查看導數，也即所謂的梯度print(W.grad)print(b.grad)</code></pre><p>運行結果為：</p><pre><code>FalseTrueTrueTruetensor([1.5000, 2.5000, 3.5000])tensor(1.)</code></pre><p>我們發現這和我們自己算的結果是一樣的。</p><p><strong>（3）標量對向量/矩陣求導的進一步理解</strong></p><p>比如有下面的一個複合函數，而且是矩陣，定義如下：</p><p>x 是一個（2,3）的矩陣，設置為可導，是葉節點，即leaf variable y 是中間變量,由於x可導，所以y可導 z 是中間變量,由於x，y可導，所以z可導 f 是一個求和函數，最終得到的是一個標量scaler</p><pre><code>x = torch.tensor([[1.,2.,3.],[4.,5.,6.]],requires_grad=True)y = torch.add(x,1)z = 2*torch.pow(y,2)f = torch.mean(z)</code></pre><p>則x,y,z,f實際上的函數關係如下：</p><p>為：</p><p>可見現在我麼自己都可以手動求出函數f對於x11,x12,x13,x21,x22,x23的導數了，那我們通過torch來試一試。</p><pre><code>print(x.requires_grad)print(y.requires_grad)print(z.requires_grad)print(f.requires_grad)print('===================================')f.backward()print(x.grad)</code></pre><p>運行結果為：</p><pre><code>TrueTrueTrueTrue===================================tensor([[1.3333, 2.0000, 2.6667],[3.3333, 4.0000, 4.6667]])</code></pre><p>現在我們是不是更加了解自動求導的規則了呢？</p><p>標量如何對標量、向量、矩陣求導數！！！</p><h1 class=pgc-h-arrow-right><strong>2.2 向量/矩陣 對 向量/矩陣求導——通過backward的第一個參數gradient來實現</strong></h1><p><strong>（1）求導的一個規則</strong></p><p>比如有下面的例子：</p><p>x 是一個（2,3）的矩陣，設置為可導，是葉節點，即leaf variable y 也是一個（2,3）的矩陣，即 y=x2+x (x的平方加x) 實際上，就是要y的各個元素對相對應的x求導</p><pre><code>x = torch.tensor([[1.,2.,3.],[4.,5.,6.]],requires_grad=True)y = torch.add(torch.pow(x,2),x)gradient=torch.tensor([[1.0,1.0,1.0],[1.0,1.0,1.0]])y.backward(gradient)print(x.grad)</code></pre><p>運行結果為：</p><pre><code>tensor([[ 3., 5., 7.],[ 9., 11., 13.]])</code></pre><p>這其實跟我們自己算的是一樣的，</p><p>相較於上面的標量對於向量或者是矩陣求導，關鍵是backward（）函數的第一個參數gradient，那麼這個參數是什麼意思呢？</p><p>為了搞清楚傳入的這個gradient參數到底做了什麼工作，我們進一步做一個實驗，有下面的一個向量對向量的求導，即</p><pre><code>x = torch.tensor([1.,2.,3.],requires_grad=True)y = torch.pow(x,2)gradient=torch.tensor([1.0,1.0,1.0])y.backward(gradient)print(x.grad)</code></pre><p>得到的結果：</p><pre><code>tensor([2., 4., 6.]) 這和我們期望的是一樣的</code></pre><p>因為這裡的gradient參數全部是1，所以看不出差別，現在更改一下gradient的值，如下：</p><pre><code>gradient=torch.tensor([1.0,0.1,0.01])</code></pre><p>輸出為：</p><pre><code>tensor([2.0000, 0.4000, 0.0600])</code></pre><p>從結果上來看，就是第二個導數縮小了十倍，第三個導數縮小了100倍，這個倍數和gradient裡面的數字是息息相關的。</p><p>如果你想讓不同的分量有不同的權重，從效果上來理解確實是這樣子的，比如我是三個loss，loss1，loss2，loss3，它們的權重可能是不一樣的，我們就可以通過它來設置，即</p><pre><code>dy/dx=0.1*dy1/dx+1.0*dy2/dx+0.0001*dy3/dx。</code></pre><p>需要注意的是，gradient的維度是和最終的需要求導的那個y的維度是一樣的，從上面的兩個例子也可以看出來。</p><p>總結：gradient參數的維度與最終的函數y保持一樣的形狀，每一個元素表示當前這個元素所對應的權重</p><h1 class=pgc-h-arrow-right><strong>2.3 自動求導函數backward的第二、第三個參數</strong></h1><p><strong>（1）保留運算圖——retain_graph</strong></p><p>在構建函數關係的時候，特別是多個複合函數的時候，會有一個運算圖，比如下面：</p><div class=pgc-img><img alt=如何使用pytorch自動求梯度 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/81815a9554cb45cba6a49188c07e579c><p class=pgc-img-caption></p></div><p>則有如下一些函數關係：</p><pre><code>p=f(y)——&gt;y=f(x)q=f(z)——&gt;z=f(x)</code></pre><p>一個計算圖在進行反向求導之後，為了節省內存，這個計算圖就銷燬了。 如果你想再次求導，就會報錯。</p><p>就比如這裡的例子而言，</p><p>你先求p求導，那麼這個過程就是反向的p對y求導，y對x求導。 求導完畢之後，這三個節點構成的計算子圖就會被釋放：</p><div class=pgc-img><img alt=如何使用pytorch自動求梯度 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b8296d30547d471cacfed5a816cecdf7><p class=pgc-img-caption></p></div><p>那麼計算圖就只剩下z、q了，已經不完整，無法求導了。 所以這個時候，無論你是想再次運行p.backward()還是q.backward()，都無法進行，因為x已經被銷燬了，報錯如下：</p><p>RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.</p><p>那怎麼辦呢？遇到這種問題，我們可以通過設置 retain_graph=True 來保留計算圖，</p><p>即更改你的backward函數，添加參數retain_graph=True，重新進行backward，這個時候你的計算圖就被保留了，不會報錯。 但是這樣會吃內存！，尤其是，你在大量迭代進行參數更新的時候，很快就會內存不足，所以這個參數在絕大部分情況下是不要去使用的。</p><p><strong>（2）高階導數——create_graph</strong></p><p>create_graph參數的資料現在很少，我也還沒有搜尋到一些更詳細的用法，它的官方描述是這樣的：</p><p>更高層次的計算圖會創建出來，允許計算高階導數，如二階導數、三階導數等等，下面有一個簡單的小例子：</p><pre><code>x = torch.tensor(5.0,requires_grad=True)y = torch.pow(x,3)grad_x = torch.autograd.grad(y, x, create_graph=True)print(grad_x) # dy/dx = 3 * x^2，即75grad_grad_x = torch.autograd.grad(grad_x[0],x)print(grad_grad_x) # 二階導數 d(2x)/dx = 30</code></pre><p>運行結果為：</p><pre><code>(tensor(75., grad_fn=&lt;MulBackward0&gt;),)(tensor(30.),)</code></pre><h1 class=pgc-h-arrow-right><strong>三、關於向量對向量求導的解釋</strong></h1><p>補充說明：關於向量對向量求梯度的進一步繞論：</p><p>比如說下面一個三維向量求梯度：</p><p>然後，要計算z關於x或者y的梯度，需要將一個外部梯度傳遞給z.backward()函數，如下所示：</p><pre><code>z.backward(torch.FloatTensor([1.0, 1.0, 1.0])</code></pre><p>反向函數傳遞的張量就像梯度加權輸出的權值。從數學上講，這是一個向量乘以非標量張量的雅可比矩陣(本文將進一步討論)，因此它幾乎總是一個維度的單位張量，與 backward張量相同，除非需要計算加權輸出。</p><p>注意 ：向後圖是由autograd類在向前傳遞過程中自動動態創建的。Backward()只是通過將其參數傳遞給已經生成的反向圖來計算梯度。</p><p><strong>數學—雅克比矩陣和向量</strong></p><p>從數學上講，autograd類只是一個雅可比向量積計算引擎。雅可比矩陣是一個非常簡單的單詞，它表示兩個向量所有可能的偏導數。它是一個向量相對於另一個向量的梯度。</p><p>注意：在這個過程中，PyTorch從不顯式地構造整個雅可比矩陣。直接計算JVP (Jacobian vector product)通常更簡單、更有效。</p><p>如果一個向量X = [x1, x2，…xn]通過f(X) = [f1, f2，…fn]來計算其他向量，則雅可比矩陣(J)包含以下所有偏導組合：</p><p>注意：雅可比矩陣實現的是 n維向量 到 m 維向量的映射。</p><p>雅克比矩陣</p><p>上面的矩陣表示f(X)相對於X的梯度。</p><p>假設一個啟用PyTorch梯度的張量X：</p><p>X = <strong>x1,x2,…,xn</strong></p><p>X經過一些運算形成一個向量Y</p><p>Y = f(X) = [y1, y2，…,ym]</p><p>然後使用Y計算標量損失l。假設向量v恰好是標量損失l關於向量Y的梯度，如下：（注意體會這句話，這個很重要！）</p><p>向量v稱為grad_tensor（梯度張量），並作為參數傳遞給backward() 函數。</p><p>為了得到損失的梯度l關於權重X的梯度，雅可比矩陣J是向量乘以向量v</p><p>這種計算雅可比矩陣並將其與向量v相乘的方法使PyTorch能夠輕鬆地為非標量輸出提供外部梯度。</p><h1 class=pgc-h-arrow-right><strong>四、求導的另外兩種方法</strong></h1><h1 class=pgc-h-arrow-right><strong>方法一：通過 torch.autograd.backward(）求導</strong></h1><p>前面介紹的求導的基本公式為：</p><pre><code>y.backward(grad_tensors=None, retain_graph=None, create_graph=False),這三個參數我在前面已經說了，</code></pre><p>反向求導它等價於：</p><pre><code>torch.autograd.backward(tensors,grad_tensors=None, retain_graph=None, create_graph=False), 這裡的tensors參數就相當於是y,</code></pre><p>所以：</p><pre><code>y.backward(） #標量y 等價於torch.autograd.backward(y)。</code></pre><p>需要注意的是，這個函數只是提供求導功能，並不返回值，返回的總是None，如下例子：</p><pre><code>import torchx=torch.tensor([1.0,2.0,3.0],requires_grad=True)y=torch.tensor([4.0,5.0,6.0],requires_grad=True)z=torch.sum(torch.pow(x,2)+torch.pow(y,3)) # z=x2+y3torch.autograd.backward([z]) # 求導，等價於z.backward()print(x.grad) # 獲取求導的結果print(y.grad)</code></pre><p>輸出</p><pre><code>tensor([2., 4., 6.])tensor([ 48., 75., 108.])</code></pre><p>注意事項：</p><p>（1）該方法只負責求導，返回的總是None，</p><p>（2）當向量對向量求導的時候，需要傳遞參數grad_tensor，這個參數的含義其實和前一篇文章的y.backward()裡面的那個是一個含義；</p><p>（3）retain_graph=None, create_graph=False 也和前面的含義是一樣的</p><h1 class=pgc-h-arrow-right><strong>方法二：通過torch.autograd.grad(）來求導</strong></h1><p>除了前面的兩種方法來求導以外，即</p><pre><code>y.backward()torch.autograd.backward(y) 這兩種方法</code></pre><p>還有一種方法，即通過torch.autograd.grad(）來求導，先來看一下這個函數的定義。</p><pre><code>def grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False,only_inputs=True, allow_unused=False):</code></pre><p>outputs : 函數的因變量，即需要求導的那個函數，在本例子中，為z，當然，他可以是一個tensor，也可以是幾個tensor，如[tensor1,tensor2,tensor3...] inputs : 函數的自變量，在本例中，即對應的是[x,y]，他可以是一個tensor，也可以是幾個tensor，如[tensor1,tensor2,tensor3...] grad_output : 這個參數和前面兩種方法中的grad_tensors是同樣的含義，當出現向量對向量求導的時候需要指定該參數</p><p>依然以這個例子而言，來看一下怎麼做：</p><pre><code>import torchx=torch.tensor([1.0,2.0,3.0],requires_grad=True)y=torch.tensor([4.0,5.0,6.0],requires_grad=True)z=torch.sum(torch.pow(x,2)+torch.pow(y,3)) # z=x2+y3print(torch.autograd.grad(z,[x,y])) # 求導，並且返回值</code></pre><p>輸出</p><pre><code>(tensor([2., 4., 6.]), tensor([ 48., 75., 108.]))</code></pre><p>注意事項：</p><p>該函數會自動完成求導過程，而且會自動返回對於每一個自變量求導的結果。這是和前面不一樣的地方。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>pytorch</a></li><li><a>自動求</a></li><li><a>梯度</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/e9cbcc2d.html alt=什麼是“志願梯度”？為什麼填報志願時要注意“志願梯度”？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1524050959957fa1dbd0e0d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e9cbcc2d.html title=什麼是“志願梯度”？為什麼填報志願時要注意“志願梯度”？>什麼是“志願梯度”？為什麼填報志願時要注意“志願梯度”？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/087a8932.html alt=如何真正理解梯度的含義 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d4719d4d74574ed1a033aefbd26cee65 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/087a8932.html title=如何真正理解梯度的含義>如何真正理解梯度的含義</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6c932f22.html alt=策略梯度的簡明介紹 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1527998215130569eb83799 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6c932f22.html title=策略梯度的簡明介紹>策略梯度的簡明介紹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/65d2605f.html alt=什麼是梯度：用形象的語言解讀梯度的本質原理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/557c7b377c5449bb885df35b2d354e03 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/65d2605f.html title=什麼是梯度：用形象的語言解讀梯度的本質原理>什麼是梯度：用形象的語言解讀梯度的本質原理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e087ca41.html alt=偏導數和函數的梯度 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/9d20a1e4cbff42a094d57df057fe9597 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e087ca41.html title=偏導數和函數的梯度>偏導數和函數的梯度</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5fc113b1.html alt=梯度原理：梯度在每一點上都指向函數增長最快的方向 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/cdb8db41d5024f38a2e490e66baebdb4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5fc113b1.html title=梯度原理：梯度在每一點上都指向函數增長最快的方向>梯度原理：梯度在每一點上都指向函數增長最快的方向</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9451cf29.html alt=梯度組織工程用生物材料的製備 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/11e3fbe804ad4ef09d8f41415c48a879 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9451cf29.html title=梯度組織工程用生物材料的製備>梯度組織工程用生物材料的製備</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/931a18f4.html alt=高考志願填報院校梯度、專業梯度、批次梯度如何設置？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/931a18f4.html title=高考志願填報院校梯度、專業梯度、批次梯度如何設置？>高考志願填報院校梯度、專業梯度、批次梯度如何設置？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f547bc70.html alt=理解梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/27c4a533219b46df901c0c34ad0937bd style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f547bc70.html title=理解梯度下降>理解梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cab0681.html alt=Day167:pytorch獲取模型某一層參數名及參數值 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/f62ec372c5204f35ad9d3b8aa96bd3e2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cab0681.html title=Day167:pytorch獲取模型某一層參數名及參數值>Day167:pytorch獲取模型某一層參數名及參數值</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/3ac236dd.html alt=太赫茲梯度超表面綜述 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/RVcfyUC2rgoFuj style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/3ac236dd.html title=太赫茲梯度超表面綜述>太赫茲梯度超表面綜述</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/18fbd3c.html alt=梯度、散度和旋度 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/f9b525da30474bfe82499f7bebdeaaf4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/18fbd3c.html title=梯度、散度和旋度>梯度、散度和旋度</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/444bc9c.html alt=梯度，旋度，與散度 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5af5d9d298394df4a95749beedaecdcc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/444bc9c.html title=梯度，旋度，與散度>梯度，旋度，與散度</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>