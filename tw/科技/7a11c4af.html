<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>圖解BERT（NLP中的遷移學習） | 极客快訊</title><meta property="og:title" content="圖解BERT（NLP中的遷移學習） - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/b29b82aef73748bd9fc0a049212fba09"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/7a11c4af.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/7a11c4af.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/7a11c4af.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/7a11c4af.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/7a11c4af.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/7a11c4af.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/7a11c4af.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/7a11c4af.html><meta property="article:published_time" content="2020-11-14T21:03:16+08:00"><meta property="article:modified_time" content="2020-11-14T21:03:16+08:00"><meta name=Keywords content><meta name=description content="圖解BERT（NLP中的遷移學習）"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/7a11c4af.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>圖解BERT（NLP中的遷移學習）</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p><em>本文翻譯自Jay Alammar的博客The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</em></p><p>2018年是機器學習模型處理文本(更準確地說，是自然語言處理或簡稱NLP)的轉折點。我們對以何種方式捕捉潛在語義和關係的來表示單詞和句子這一問題的理解也在迅速發展。此外，NLP社區中也有人分享了許多了非常強大的模型供你免費下載並在自己的模型和pipeline中使用(它被稱為NLP的ImageNet moment，參考了多年前計算機視覺的快速發展)。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b29b82aef73748bd9fc0a049212fba09><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>其中一個最新的里程碑就是BERT的發佈，這被人們看作是NLP新時代的開始。BERT是一個打破了許多基於語言的任務中的記錄。在論文發佈後不久，該模型的團隊還開源了模型的代碼，並提供了模型的下載版本，這些模型已經在大數據集上進行過了預訓練。這是一個重大的進步，因為它使任何想要構建自然語言處理的機器學習模型的人都可以將這個強大的預訓練好的模型作為一個隨時可用的組件使用——從而節省了從頭開始訓練模型所需的時間、精力、知識和資源。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c562cd452eff4dd3a47169bb76e65ea8><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>使用BERT的兩個步驟。第一步：下載預訓練好的模型；第二步：在特定任務上進行微調</p><p>BERT也是基於最近在NLP社區中湧現的許多聰明的想法，包括 Semi-supervised Sequence Learning (by Andrew Dai 和 Quoc Le), ELMo (by Matthew Peters 和來自 AI2 and UW CSE的研究人員), ULMFiT (by fast.ai 創始人 Jeremy Howard 和大牛 Sebastian Ruder), OpenAI transformer (by OpenAI 研究員Radford, Narasimhan, Salimans, and Sutskever), 以及Transformer (Vaswani et al)等.</p><p>要正確理BERT是什麼，我們需要了解許多概念。倒不如先看看BERT有哪些用途。</p><p><strong>一、例子:句子分類</strong></p><p>最直接的想法就是使用BERT進行單個文本的分類。這個模型看起來是這樣的:</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c35cf39d97014e699df91d183f8db5e1><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>要訓練這樣的模型，您需要重點訓練一個分類器，在訓練階段對BERT模型的改動非常小。這種訓練過程稱為微調（fine-tuning），其根源在於 Semi-supervised Sequence Learning 和ULMFiT。</p><p>對於不熟悉這個概念的人來說，由於我們討論的是分類器，所以在這個任務中涉及到的是機器學習中的監督學習。這意味著我們需要一個標記好的數據集來訓練這樣的模型。以垃圾郵件分類為例，標記的數據集將是一個電子郵件消息列表和一個標籤（標註“垃圾郵件”或“非垃圾郵件”）。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a64c708106504c47a93d6cd4692890f6><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>類似的任務場景還有:</p><ul><li>情感分析</li><li class=ql-indent-1>輸入: 一條影評/商品評價。</li><li class=ql-indent-1>輸出: 正面評價還是負面評價?</li><li class=ql-indent-1>數據集如: SST</li><li>事實核查</li><li class=ql-indent-1>輸入:一個句子。</li><li class=ql-indent-1>輸出: 是不是一個斷言</li><li>更難的任務:</li><li class=ql-indent-2>輸入: 一句斷言。</li><li class=ql-indent-1>輸出: 真的還是假的</li><li class=ql-indent-1>Full Fact 組織構建了一個自動事實核查工具。這個工具的流程中包含了一個分類器，這個分類器讀取新聞文章來檢測斷言.</li><li class=ql-indent-1>視頻: Sentence embeddings for automated factchecking - Lev Konstantinovskiy.</li></ul><p><strong>二、模型架構</strong></p><p>現在您已經有了一個BERT的用例，接下來讓我們進一步瞭解它是如何工作的。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bd0fc02760644c23a4650a977fccd55d><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>Google在論文中提到了兩個不同模型規模的BERT:</p><ul><li>BERT BASE –和OpenAI Transformer模型的規模差不多，方便與其進行性能比較</li><li>BERT LARGE – 一個達到目前多個benchmark的SOTA的巨大的模型</li></ul><p>BERT基本上就是一個訓練好的Transformer編碼器棧。關於Transformer的內容可以看看 圖解Transformer這篇博文。Transformer是BERT的基礎，下面我們也會涉及到這個概念。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6e6d3947587b4c2bb6e3a9e994070cd8><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>兩種規模的BERT模型都有許多編碼器層 (在論文中稱為“Transformer塊”) – BERT Base有12個這樣的結構，BERT Large有24個。編碼器中也有前饋網絡 (BERT Base中的是768個隱層神經元，BERT Large中的是1024個隱層神經元)， 以及注意力層中使用了比Transformer那篇論文中更多的“頭” （BERT Base有12個“頭”，BERT Large中有16個）。</p><p><strong>模型的輸入</strong></p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5a3d0f36d0834da2bbf0bec51f3805d4><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>輸入序列的第一個token是一個特殊的符號[CLS]，這裡的CLS代表class。</p><p>就像Transformer的編碼器一樣，BERT以一串單詞作為輸入，這些單詞不斷地想編碼器棧上層流動。每一層都要經過自注意力層和前饋網絡，然後在將其交給下一個編碼器。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a88e119914a74709b755ec30ac4aa0f7><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>在體系結構方面，到目前為止，還是與Transformer是相同的（除了一些超參數之外）。接下來在輸出端，我們會看到其和Transformer的不同之處。</p><p><strong>模型的輸出</strong></p><p>每個位置對應地輸出一個維度為<em>hidden_size</em>(BERT Base中為768)的向量。對於之前提到的句子分類的例子，我們只關注第一個位置的輸出（也就是被我們用[CLS]符號代替的位置）。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/70b078ed603042b8b25f22396e9abd58><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>輸出的這個向量現在可以用作我們選擇的分類器的輸入。論文利用一個單層神經網絡作為分類器，就能取得較好的分類效果。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/26e226c811244e69bcef0e2225a0febd><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>如果你有更多的標籤（例如，如果你是一個電子郵件服務提供商，你需要將電子郵件標記為“垃圾郵件”、“非垃圾郵件”、“社交”和“促銷”等等），你只需調整分類器這部分的網絡，使其具有更多的輸出神經元，然後通過softmax。</p><p><strong>三、與卷積網絡並行</strong></p><p>對於有CV背景的人來說，這種向量傳遞應該讓人想起像VGGNet這樣的網絡的卷積部分和網絡結構最後的全連接層之間發生的事情。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a7010a579a8f4af6a360a72665deefa0><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p><strong>四、嵌入表示的新時代</strong></p><p>這些新的探索帶來了文本編碼方式的新轉變。到目前為止，在絕大多數的NLP模型中，詞嵌入一直是一個主要的文本表示方法。Word2Vec、Glove等方法已廣泛應用於此類任務。下面先讓我們回顧一下如何使用它們。</p><p><strong>回顧一下詞嵌入</strong></p><p>為了要讓機器學習模型能夠處理單詞，我們需要以數字的形式表示文本，以便模型在計算中使用。通過使用Word2Vec，我們可以用一個向量來代表單詞，而這一向量還捕捉了一定的語義信息（如“斯德哥爾摩”和“瑞典”的關係相當於“開羅”與“埃及”的關係)以及語法信息，或基於語法的關係（例如，“had”和“has”的關係與“was”和“is”的關係是一樣的）。</p><p>人們很快意識到，使用大量文本數據進行預訓練學習詞嵌入是一個好主意，而不是在小數據集上模型從零開始訓練。你可以下載預訓練的Word2Vec或GloVe。下面是GloVe訓練得到的“stick”對應的向量表示(嵌入向量維度為200）。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/21964dcadf7e42ea973d4e785e4dba48><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>由於維度很大，在後面的文章中會用下面這種形狀代表向量：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3a2abebdce914f76883600c07dc2fb49><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p><strong>ELMo: 語境的重要性</strong></p><p>如果我們是使用GloVe訓練好的向量作為一個詞，比如“stick”的表示，那麼不管在什麼上下文中，這個表示都是一樣的。在一些研究中 (Peters et. al., 2017, McCann et. al., 2017, Peters et. al., 2018 in the ELMo paper )，研究人員認為像“<em>stick</em>”這樣的詞其實有很多意思，具體是什麼意思取決於在什麼語境中用它。那麼為什麼不基於其上下文語境來學習一個詞的嵌入表示呢？也就是即學習到這個詞的上下文的語義，有學習到其他的語境信息。就此，語境化的詞嵌入模型應運而生。</p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d9486cfaaf034cffb605811a79ee7dee><p class=pgc-img-caption></p></div><p class=ql-align-center>語境化的詞嵌入模型能夠基於一個單詞的上下文的意思給出單詞的向量表示[RIP Robin Williams](https://www.youtube.com/watch?v=OwwdgsN9wF8)</p><p>ELMo沒有為每個單詞使用固定的嵌入，而是在為每個單詞分配嵌入之前查看整個句子。它使用針對特定任務的雙向LSTM來創建這些嵌入。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/934d9a90a203416fa57f900ce960115e><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>ELMo為在語境中進行預訓練提供了重要的思路。ELMo LSTM能夠在大數據集上進行訓練，然後作為其他模型的一個部分處理其他的自然語言任務。</p><p><strong>ELMo的祕訣是什麼?</strong></p><p>ELMo通過訓練預測單詞序列中的下一個單詞來理解語言——這項任務被稱為<strong>語言建模</strong>。這很方便，因為我們有的是大量的文本數據，這樣的模型可以從這些數據中學習，而不需要額外的標籤。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8fe9e3855bd14f03a869e213995e8dae><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>ELMo進行預訓練的一個步驟：給定輸入“Let’s stick to”， 預測接下來一個詞，這就是語言模型的任務。當模型在大語料上進行預訓練，他就會學習其中的語言模式。它不太可能準確地直接地猜出這個例子中的下一個單詞。更實際一點說，在“hang”這樣的單詞之後，它將為“out”這樣的單詞分配更高的概率(組成 “hang out”) 而不是給“camera”分配更高的概率。</p><p>我們可以看到每個LSTM時間步的隱狀態從ELMo的“頭部”後面探出來。這些向量會在預訓練結束後的嵌入過程中會派上用場。</p><p>ELMo實際上更進一步，訓練了一個雙向的LSTM——這樣它的語言模型不僅能預測下一個詞，還有預測上一個詞。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4d56ae7eb8cb4e41a443ff8819fcdbe3><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>[Great slides](https://www.slideshare.net/shuntaroy/a-review-of-deep-contextualized-word-representations-peters-2018) on ELMo</p><p>ELMo通過將隱藏狀態(和初始嵌入)以某種方式(拼接之後加權求和)組合在一起，提出了語境化的詞嵌入。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4560ddbf41254e58baf776cafad08d27><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p><strong>五、ULM-FiT：搞懂NLP中的遷移學習</strong></p><p>ULM-FiT引入了一些方法來有效地利用模型在預訓練中學到的東西——不僅僅是嵌入，還有語境化的嵌入表示。ULM-FiT引入了一個語言模型和一套針對各種任務有效地對語言模型進行微調的流程。</p><p>NLP終於找到了一種方法，可以像CV那樣進行遷移學習了。</p><p><strong>六、Transformer：超越LSTM</strong></p><p>Transformer的論文和代碼的發佈，以及它在機器翻譯等任務上取得的成果，開始使一些業內人士認為它是LSTM的替代品。Transformer比LSTM更能處理長期依賴。</p><p>Transformer的編碼器-譯碼器結構使其成為機器翻譯的理想模型。但是你會如何使用它來進行句子分類呢？你將如何針對其他特定任務對語言模型進行微調呢？</p><p><strong>七、OpenAI　Transformer：為語言建模預訓練一個Transformer解碼器</strong></p><p>事實證明，我們可以不用一個完整的Transformer來遷移學習並進行微調。我們可以只用Transformer的解碼器就可以了。解碼器是一個很好的選擇，因為它能屏蔽掉後來的詞（當進行逐詞生成翻譯時，這是一個很有用的特性）。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/76b81c619e8b4f49bcd3d8be40345798><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>The OpenAI Transformer 是由Transformer的解碼器棧組成的</p><p>這個模型堆疊了12個解碼器層。由於在這種設計中沒有編碼器，因此這些解碼器層也不會有Transformer原文中的那種編碼器-解碼器注意力子層。但是，仍然還是有自注意力層。</p><p>有了這種結構，我們可以繼續在相同的語言建模任務上進行訓練模型：使用大量(未標記的)文本來預測下一個單詞。只是，把7000本書的文本扔給模型，讓它學習！書籍非常適合這類任務，因為它允許模型學習相關聯的信息，而當您使用tweet或文章進行訓練時，您無法獲得這些信息。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/87c8760012aa40eeb99ce32c40427fc1><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>The OpenAI Transformer現在已經準備好被訓練成能夠預測下一個單詞了</p><p><strong>八、在下游任務中使用遷移學習</strong></p><p>既然OpenAI　Transformer已經經過了預訓練，而且它的各個層也經過了調整，我們就可以開始在下游任務中使用它了。讓我們先來看看句子分類（將郵件信息分為“垃圾郵件”或“非垃圾郵件”）:</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/08a4d09417c6469eb8b1d54443d892f5><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>How to use a pre-trained OpenAI transformer to do sentence clasification</p><p>OpenAI的論文列出了許多用於處理不同類型任務輸入的輸入變換。下圖顯示了模型的結構和執行不同任務時的輸入變換。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3ee683ec34bc4a8ba8f8ec2485b36788><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p><strong>九、BERT：從解碼器到編碼器</strong></p><p>openAI Transformer為我們提供了一個基於Transformer的可微調的預訓練的模型。但是把LSTM換成Transformer還是讓有些東西丟失了。ELMo的語言模型是雙向的，而openAI Transformer則只訓練一個從左到右的語言模型。那麼我們能否建立一個既能從左到右預測又能從右到左預測（同時受上、下文的制約)的基於Transformer的模型呢？</p><p><strong>MLM語言模型</strong></p><p><em>“我們將使用Transformer編碼器”</em>，BERT說。</p><p><em>“這太瘋狂了”</em>，有人說，“<em>每個人都知道雙向條件作用會讓每個詞在多層次的語境中間接地看到自己。</em>”</p><p>“<em>我們將使用掩碼</em>”，BERT自信地說。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d5c7bd4427614483ad5fa3d7e8603ae2><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>BERT遮罩住15%輸入序列中15%的token，然後讓模型預測這些遮罩住的位置是什麼單詞</p><p>找到合適的任務來訓練Transformer的編碼器棧是一個複雜的問題，BERT採用了早期文獻(完形填空任務)中的“帶掩碼的語言模型”概念來解決這個問題。</p><p>除了屏蔽15%的輸入，BERT還混入一些東西，以改進模型的微調方式。有時它會隨機地將一個單詞替換成另一個單詞，並讓模型預測該位置的正確單詞。</p><p><strong>兩個句子的任務</strong></p><p>如果你還記得OpenAI Transformer處理不同任務時所做的輸入變換，你會注意到一些任務需要模型處理關於兩個句子的信息（例如，一個句子是否是另一個句子的複述；再例如假設一個維基百科條目作為輸入，一個關於這個條目的問題作為另一個輸入，我們能回答這個問題嗎？）</p><p>為了讓BERT更好地處理多個句子之間的關係，預訓練的過程還有一個額外的任務：給定兩個句子（A和B）， B可能是接在A後面出現的句子嗎？</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0871af37e1d6430fb22929ba2d2ff4dd><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>BERT預訓練的第二個任務是兩個句子的分類任務。</p><p><strong>解決特定任務的模型</strong></p><p>BERT論文展示了BERT在不同任務上的應用。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fcd6227f4b3d43599621457332b8a7ac><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p><strong>用於特徵提取的BERT</strong></p><p>微調的方法並不是使用BERT的唯一方法。就像ELMo一樣，你也可以使用預訓練好的BERT來創建語境化的詞嵌入。然後，您可以將這些嵌入表示餵給現有的模型——論文中也提到，在NER這類任務中，這種用法的最終效果也沒有比用微調的方法的結果差很多。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2d6d35f18b7649c5bad7144eefc9b6e2><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>哪種向量作為語境化嵌入的效果最好？我認為這取決於具體任務。論文比較了6中選擇（與微調後的96.4分模型相比):</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=圖解BERT（NLP中的遷移學習） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1df73c5b5ced4865b2ee2e22001583d5><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p><strong>十、把BERT牽出來遛一遛</strong></p><p>試用BERT的最好方法是通過在谷歌Colab上託管的BERT FineTuning with Cloud TPUs notebook。如果你之前從未使用過Cloud TPU，那麼這也是嘗試它們的一個很好的開始，而且BERT代碼也可以在TPU、CPU和GPU上工作。</p><p>下一步可以看看 BERT代碼實現:</p><ul><li>模型在 modeling.py (class BertModel)中定義，而且和原生的Transformer encoder非常相似。</li><li>run_classifier.py 是一個微調過程的例子。其中構造了一個分類層。如果你想構建自己的分類器，可以看看文件中的 create_model()方法。</li><li>一些預訓練模型可供下載。其中包括了BERT Base和 BERT Large，以及在中文、英文等102種語言的維基百科上預訓練得到的模型。</li><li>BERT並不是直接處理單詞，而是把 WordPieces作為token。 tokenization.py 是能夠將單詞轉換成wordPieces的腳本。</li></ul><p>你也可以參考 BERT的PyTorch實現。AllenNLP 用這個代碼讓其他模型也能夠用BERT的嵌入表示</p><p>轉自：https://www.cnblogs.com/d0main/p/10165671.html</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>圖解</a></li><li><a>BERT</a></li><li><a>NLP</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/aa6ebdaa.html alt=圖解光纜、終端盒、尾纖的作用和接法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1533878533643f382d474be style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/aa6ebdaa.html title=圖解光纜、終端盒、尾纖的作用和接法>圖解光纜、終端盒、尾纖的作用和接法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8809067a.html alt=圖解：中國首款量產的兩檔電驅動橋，專為SUV電動化而來！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/153809842787775889e4159 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8809067a.html title=圖解：中國首款量產的兩檔電驅動橋，專為SUV電動化而來！>圖解：中國首款量產的兩檔電驅動橋，專為SUV電動化而來！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/156d2a92.html alt="圖解MySQL | 「原理解析」 MySQL組提交(group commit)" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/e5444e9094614d6b88bd3fc8ac0524fb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/156d2a92.html title="圖解MySQL | 「原理解析」 MySQL組提交(group commit)">圖解MySQL | 「原理解析」 MySQL組提交(group commit)</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/070f12d8.html alt=施工工藝管理最新圖解 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RHC49E98HS0oHf style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/070f12d8.html title=施工工藝管理最新圖解>施工工藝管理最新圖解</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/bcc93b8c.html alt=詳細圖解地球自轉與公轉的黃赤交角如何形成四季更換 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/6acd8115d8fe4cf1b1b1bbce7e353cea style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bcc93b8c.html title=詳細圖解地球自轉與公轉的黃赤交角如何形成四季更換>詳細圖解地球自轉與公轉的黃赤交角如何形成四季更換</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8eeec4c4.html alt=深度剖析“買入分歧賣出一致”的買賣精髓（圖解） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9fca4cdd669d4862b254f9f2447baa06 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8eeec4c4.html title=深度剖析“買入分歧賣出一致”的買賣精髓（圖解）>深度剖析“買入分歧賣出一致”的買賣精髓（圖解）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/73ba27bb.html alt=老股民總結的賣出技巧實戰圖解 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/7575000bb7b8e57fa260 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/73ba27bb.html title=老股民總結的賣出技巧實戰圖解>老股民總結的賣出技巧實戰圖解</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/22408815.html alt=股票交易指南：實戰圖解經典的賣出技巧 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/15305260751844061be70e0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/22408815.html title=股票交易指南：實戰圖解經典的賣出技巧>股票交易指南：實戰圖解經典的賣出技巧</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0488f1da.html alt=339頁工程造價全能圖解，全方位講解，造價小白也能輕鬆搞定 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/c14ba2845d1d432fbf4cc8a534775e30 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0488f1da.html title=339頁工程造價全能圖解，全方位講解，造價小白也能輕鬆搞定>339頁工程造價全能圖解，全方位講解，造價小白也能輕鬆搞定</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/64ba00ea.html alt=麒麟和獬豸傻傻分不清？圖解~ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/c64d3d8c00804f93b4093eb4a4358758 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/64ba00ea.html title=麒麟和獬豸傻傻分不清？圖解~>麒麟和獬豸傻傻分不清？圖解~</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b06da4c5.html alt=圖解功率MOS管的參數 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1940284f671c4b559a1db4ef0e83976c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b06da4c5.html title=圖解功率MOS管的參數>圖解功率MOS管的參數</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f42c2f55.html alt=圖解｜看懂芯片原來這麼簡單：什麼是集成電路？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/71da3ec689e0497d9669ee59566d03fb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f42c2f55.html title=圖解｜看懂芯片原來這麼簡單：什麼是集成電路？>圖解｜看懂芯片原來這麼簡單：什麼是集成電路？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1a5783a2.html alt=史上最全圖解汽車構造與原理系列（九）活塞連桿組圖解 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/37d7000172f8074b6abc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1a5783a2.html title=史上最全圖解汽車構造與原理系列（九）活塞連桿組圖解>史上最全圖解汽車構造與原理系列（九）活塞連桿組圖解</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/64bb3b85.html alt=發動機各零部件詳細圖解（一）曲柄連桿機構 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/889cecc2ee9e4b3abc9c8f3b5feac06d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/64bb3b85.html title=發動機各零部件詳細圖解（一）曲柄連桿機構>發動機各零部件詳細圖解（一）曲柄連桿機構</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/89cb505d.html alt=圖解如何做一名優秀的項目經理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/f4ccb1d70aeb4ae9abdfdcb43c49e3d1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/89cb505d.html title=圖解如何做一名優秀的項目經理>圖解如何做一名優秀的項目經理</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>