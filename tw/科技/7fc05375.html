<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>思必馳俞凱：端到端與半監督語音識別的技術進展 | CCF-GAIR 2020 | 极客快訊</title><meta property="og:title" content="思必馳俞凱：端到端與半監督語音識別的技術進展 | CCF-GAIR 2020 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/S7QC3IpFC8ZCY8"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/7fc05375.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/7fc05375.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/7fc05375.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/7fc05375.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/7fc05375.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/7fc05375.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/7fc05375.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/7fc05375.html><meta property="article:published_time" content="2020-11-14T21:03:41+08:00"><meta property="article:modified_time" content="2020-11-14T21:03:41+08:00"><meta name=Keywords content><meta name=description content="思必馳俞凱：端到端與半監督語音識別的技術進展 | CCF-GAIR 2020"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/7fc05375.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>思必馳俞凱：端到端與半監督語音識別的技術進展 | CCF-GAIR 2020</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>雷鋒網按：如何低成本高效率地利用少量帶標註的數據，挖掘大量語音數據中的有效信息，半監督學習正成為當下研究趨勢之一。在這種趨勢背景下，端到端的訓練方法也正嘗試結合預訓練或先驗知識，投入在語音識別網絡的探索中。</p><p>在8月8日的CCF-GAIR 2020全球人工智能與機器人大會·前沿語音技術專場上，俞凱教授分享了端到端和半監督學習技術在語音識別問題中的最新研究思路及進展。</p><img alt="思必馳俞凱：端到端與半監督語音識別的技術進展 | CCF-GAIR 2020" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/S7QC3IpFC8ZCY8><blockquote><p>簡介：俞凱，上海交通大學計算機系教授，思必馳創始人之一、首席科學家。俞教授長期從事對話式人工智能的研究和產業化工作，獲得過多個國際期刊及會議優秀論文獎和研究評測冠軍，還入選為國家級人才項目、優青，上海市“東方學者”特聘教授。</p></blockquote><p>俞教授指出，在深度學習發展以來，語音識別研究領域現在所面臨的問題，除了在工程技巧和數據對接上做一些工作之外，最重要的事情是長尾的非配合語音識別。</p><p>其中，具備高效率的高精度系統和高質量的精準大數據構建是兩個比較重要的進展和趨勢。</p><p>一是具備高效率的高精度系統。高精度語音識別在前幾年已經超過人的識別，但是高精度語音識別在長尾上仍有很多工作值得研究。很重要的一點是，現在大家的關注點由一般意義的高精度語音識別變成高效率的語音識別。如何在保持高精度的同時，還要保證系統構建和複雜度、響應速度、規模化能力和靈活性都必須同等提高，這是目前端到端語音識別引起很大興趣的原因。</p><p>二是高質量精準大數據。大數據很有用，但有了大數據，精度就能提高嗎？其實並非如此，越來越多的人發現真正的大數據應該是結構上的大，而不僅僅是數量上的大，也就是要在聲學因素的分佈、監督信號獲取和識別系統適配方面，有高質量的精準數據。於是，也就出現了很多半監督、生成式的數據擴充方法。</p><p><strong>何為端到端，以及為什麼需要端到端？</strong></p><p>在俞教授看來，傳統的識別框架是結構不同的模型模塊組成，稱之為異構識別系統。首先，它本質上不是統一的參數化模型，中間需要WFST解碼器，對各個模塊分別建模訓練；其次，聲學、語言、字典等模型的類型和結構本質上完全不同，且解碼器是不可缺少的連接模塊信息的核心，需要構建複雜的搜索網絡。</p><p>端到端識別框架不同之處在於，在大數據的背景下，能通過完整神經網絡實現聲學信號到識別結果的直接映射，各個組成部分是“同構”的。今天報告中談到的端到端更多指的是，直接或簡單轉換後輸出結果是“詞序列”。</p><p>從優勢上講，端到端能夠降低複雜度，減少參數量（不是聲學上的減少，有神經網絡語言模型的參數來描述語言空間），從而使得訓練的流水線得以簡化。其次，大數據資源的使用更為簡單，數據驅動更為友好；此外，搜索解碼速度加快，但是否真的需要構建搜索網絡，俞教授指出，這項研究目前存在爭議。</p><p><strong>端到端的定義與分類</strong></p><p>端到端主要分為兩類，一類是同步框架，另一類是異步框架，主要解決語音識別的兩個基本問題：分類與對齊。解決“對齊”問題通常採用的思路包括：馬爾可夫模型（HMM）、標籤填充、序列解碼網絡等方法。其中，後兩種是端到端中比較常用的方法。</p><p>同步端到端框架採用的是，與輸入同步逐幀輸出，通過引入blank標籤實現變長序列對齊；異步端到端框架採用的是，輸入與輸出使用兩個網絡分別處理，使用attention（注意力機制）解決對齊問題。</p><p>同步端到端框架最典型的就是CTC和RNN-T：前者通過引入相應的標籤填充，同時在條件獨立性假設上，每一幀輸出之間條件獨立，而後者沒有條件獨立性的假設。</p><p>隨後，俞教授詳細討論了異步端到端存在的研究價值和爭議。</p><p>俞教授表示，異步端到端最大的特點是輸出與輸入沒有統一的時鐘，是兩個不同的網絡。</p><p>在encoder-decoder架構上，encoder對整體輸入序列提取所有信息，然後根據輸出的要求再進行輸出，時鐘和輸出標籤是逐詞進行的。這時，會通過attention的方式處理對齊。一般情況下，輸出序列的個數會遠遠小於時間幀的個數，這種情況下，輸出序列信息速率會遠低於輸入信息速率，beam搜索效率會變得很高。</p><p>不少研究指出，異步端到端的識別精度會優於同步端到端模型（上文講到的CTC 、RNN-T），但這目前也是存在爭議的。</p><p><strong>端到端的問題與挑戰</strong></p><p>即便端到端存在一定優勢，但問題在於，類似於encoder-decoder這樣的架構，實時響應遲延可能會變長；同時，端到端的提出主要是在聲學數據上的訓練，對語言數據使用的討論不夠充分，直到最近才有一些新的工作。那麼，端到端具體會有怎樣的挑戰？</p><ul><li><p>一是在線編碼的遲延問題。</p></li></ul><p>這種情況下雙向的神經網絡無法使用，只能用單向網絡，這就造成輸入的信息變少。這時，如果通過注意力機制進行在線化解碼，從而得到即時的、短遲延識別結果，就會變得非常有挑戰性。</p><ul><li><p>二是文本資源及語言空間的約束問題。端到端模型需要有標註語音數據，而最開始研究端到端時，用到的是聲學模型的數據，並沒有用到大規模文本語料。</p></li></ul><p>為此，俞教授指出，當下解決端到端的在線解碼遲延問題，已有的思路主要有三類：一是固定短時窗口預測（Neural Transducer）；二是基於單幀觸發的變長窗口方法（MoChA，Triggered Attention）；三是基於多幀累計觸發閾值的方法（Adaptive Computing Steps）。其本質都是隻用歷史信息或非常小的前探信息。</p><p>再回來上文所提到的，早期的端到端模型是融合聲學語料文本的超大聲學模型，它並不包括語言模型，那麼海量的文本數據如何使用？</p><p>當前端到端框架下的文本數據使用的解題思路主要有三種：一是模型融合（Fusion）——將文本數據訓練的神經網絡LM，在decoder輸出層進行插值融合； 二是語言模型模塊嵌入——將端到端系統的部分網絡作為LM建模，允許額外文本數據訓練更新； 三是半監督訓練——利用文本數據做端到端模型訓練的數據擴充（無顯示的語言空間建模）。</p><p><strong>從海量數據到高質量精準大數據</strong></p><p>想要從海量數據中提取到高質量、精準的大數據，最大的挑戰在於沒有監督信號、標註起來也很難。解決該問題主要會運用到三個思想：一是自監督預訓練，二是半監督訓練，三是使用生成數據訓練。</p><p>首先是自監督預訓練，這種思路下數據自身就是標註，不需要額外標註，這與自然語言處理使用詞序列作為標註，設計一些訓練任務使得能夠提取比較好的預訓練特徵是比較一致的方法。比較典型的是wav2vec或結合了預訓練模型BERT的方法，以及重構任務DecoAR。</p><p>其次是半監督訓練，可以是海量無標註音頻或海量文本加適量有標註音頻的方式。大體思路也有三種：置信度選擇、先驗知識蒸餾、音頻文本一致性訓練。</p><p>在報告最後，俞教授還表達了對精準的環境數據擴充及語音合成研究方向的看好。對於語音合成，俞教授認為合成語音數據的難點在於，不同於語音識別，語音合成是一個信息增加的過程，這個過程需要解決的問題會更為複雜，往往這種“無中生有”的過程基本上是通過引入生成模型進行解決。比方說，在低資源數據下使用VAE建模說話人空間，或者不使用句子級的VAE，而是通過逐個phone的音頻提取隱變量序列z。這些都是當下比較主流的解決問題的思路。</p><p>（雷鋒網雷鋒網）</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>思必馳</a></li><li><a>俞凱</a></li><li><a>端到</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/521cfbb3.html alt=端到端一致性，流系統Spark/Flink/Kafka/DataFlow對比總結 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/e21d09c33a9447aba53883736c891f10 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/521cfbb3.html title=端到端一致性，流系統Spark/Flink/Kafka/DataFlow對比總結>端到端一致性，流系統Spark/Flink/Kafka/DataFlow對比總結</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/01fc532d.html alt=快速理解kafka端到端的延遲 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/c9eba460f46e4874bf959581369bd364 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/01fc532d.html title=快速理解kafka端到端的延遲>快速理解kafka端到端的延遲</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/19ff5abe.html alt=流程規劃：“端到端”的思想已經不適用了？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/15307042024657fb4fb9e48 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/19ff5abe.html title=流程規劃：“端到端”的思想已經不適用了？>流程規劃：“端到端”的思想已經不適用了？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3139c00d.html alt=翻譯：端到端的神經網絡圖像序列識別及其在場景文本識別中的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/b43c051705ae41f18cf033aafc0a9ab8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3139c00d.html title=翻譯：端到端的神經網絡圖像序列識別及其在場景文本識別中的應用>翻譯：端到端的神經網絡圖像序列識別及其在場景文本識別中的應用</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>