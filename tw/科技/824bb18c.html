<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>自然語言的語義表示學習方法與應用 | 极客快訊</title><meta property="og:title" content="自然語言的語義表示學習方法與應用 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/521cf0b197594fb3957739696fc08bc7"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/824bb18c.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/824bb18c.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/824bb18c.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/824bb18c.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/824bb18c.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/824bb18c.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/824bb18c.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/824bb18c.html><meta property="article:published_time" content="2020-11-14T21:01:41+08:00"><meta property="article:modified_time" content="2020-11-14T21:01:41+08:00"><meta name=Keywords content><meta name=description content="自然語言的語義表示學習方法與應用"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/824bb18c.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>自然語言的語義表示學習方法與應用</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p class=ql-align-justify><br></p><hr><p class=ql-align-justify>聲明：本文轉自網絡，僅供學習之用，無任何商業用途，如果有侵權，請聯繫易思博遠立即刪除</p><hr class=ql-align-justify><p class=ql-align-justify><br></p><div class=pgc-img><img alt=自然語言的語義表示學習方法與應用 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/521cf0b197594fb3957739696fc08bc7><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>引言</p><p class=ql-align-justify>近年來，以深度學習為代表的表示學習技術在語音識別、圖像分析和自然語言處理（NLP）領域獲得了廣泛關注。表示學習旨在將研究對象的語義信息表示為低維稠密實值向量。表示學習得到的低維向量表示是一種分佈式表示，孤立地看向量中的每一維，都沒有明確對應的含義；而綜合各維形成一個向量，則能夠表示對象的語義信息。與更簡單的獨熱（one-hot）表示方法相比，表示學習的向量維度較低，有助於提高計算效率，同時能夠充分利用對象間的語義信息，從而有效緩解數據稀疏問題。由於表示學習的這些優點，最近出現了大量關於單詞、短語、實體、句子、文檔和社會網絡的表示學習研究。</p><p class=ql-align-justify>1自然語言的詞表示方法</p><p class=ql-align-justify>在NLP 中，文本表示是一個極為關鍵的問題。最初，詞袋模型是最常用的文本表示模型之一。隨著深度神經網絡的興起，人們提出了一種新的獲得詞向量的詞嵌入（Word Embedding）方法[1-3]，以解決詞彙表過大帶來的“維度炸”問題。詞和句子的嵌入已成為所有基於深度學習的NLP系統的重要組成部分，它們在固定長度的稠密向量中編碼單詞和句子，從而大幅度提高神經網絡處理文本數據的能力。詞向量的獲取方式可以大體分為基於統計的方法（例如基於共現矩陣、SVD）和基於語言模型[4-5] 的方法兩類。2013 年，Google 團隊發表了基於語言模型獲取詞向量的word2vec工具[6]。它的核心思想是通過詞的上下文得到詞的向量化表示，包括CBOW（通過附近詞預測中心詞）和Skip-gram（通過中心詞預測附近詞）兩種方法，以及負採樣和層次softmax 兩種近似訓練法。word2vec 的詞向量可以較好地表達不同詞之間的相似和類比關係，自提出後被廣泛應用在NLP任務中。進一步地，由於word2vec 的詞向量是固定不變的，不能有效地解決多義詞的問題，產生了根據上下文隨時變化詞向量的ELMO 模型[7]。該模型從深層的雙向語言模型的內部狀態學習得到詞的表示，能夠處理單詞用法中的複雜特性，以及這些用法在不同的語言上下文中的變化，從而解決了多義詞的問題。</p><p class=ql-align-justify>2 自然語言的結構表示方法</p><p class=ql-align-justify>在獲取句子或文檔的語義表示時，一段話的語義由其各組成部分的語義，以及它們之間的組合方法所確定[8]。由此，一些工作開始嘗試根據輸入的結構設計模型的結構。比如卷積神經網絡（CNN）以n-gram作為基本單位建立句子表示[9-10]。而遞歸神經網絡（Recursive Neural Network） 則根據輸入的樹結構構建句子的表示[11-12]。此外，循環神經網絡（RNN）及各種改進（如長短時記憶網絡（LSTM））被證明是有效的句子級別表示方法[13]。在此基礎上，一些更為優越的結構增強型LSTM 和之前模型的各種組合的方法也在之後被提出。事實上，LSTM 引入一個近似線性依賴的記憶單元來存儲遠距離的信息，以解決簡單RNN 的長期依賴問題。記憶單元的存儲能力和其大小有關，增加記憶單元的大小將導致網絡參數的增加。針對這種情況，產生了注意力機制和外部記憶的改進方法。其中注意力機制[14] 是近年來在NLP 任務中被廣泛應用的一種十分有效的技術，在諸多領域都展示出了其優越性。進一步地，產生了一種只基於注意力機制對序列進行表示的Transformer 結構[15]。它摒棄了固有的定式，沒有使用任何CNN 或者RNN 的結構。Transformer 可以綜合考慮句子兩個方向的信息，而且有很好的並行性質，可以大大減少訓練時間。</p><p class=ql-align-justify>3 預訓練在NLP 中的應用</p><p class=ql-align-justify>值得一提的是，很多自然語言特徵表示方法及詞表示方法都採用一種兩階段的訓練方法，即首先在無標記數據上通過預訓練學習特徵或者詞的表示；再以這些表示作為特徵，在標記數據上進行監督訓練。前文所提到的word2vec 和ELMO 方法就經常被用於詞向量的預訓練。隨著深度學習在表示學習領域成為主流方法，以及Transformer等序列表示模型的發展，自然語言的表示學習從特徵和詞的粒度被推廣到了更大的粒度，如短語和句子。這些深度學習模型也同樣受益於這種兩階段的訓練方法。在ELMO 之後，新的語言表徵預訓練模型GPT 使用Transformer 來編碼[16]， 克服了ELMO 使用LSTM 作為語言模型而帶來的並行計算能力差的缺點。而BERT 模型在採用Transformer 進行編碼的同時雙向綜合地考慮上下文特徵來對詞進行預測[17]。與word2vec 和ELMO 不同，GPT 和BERT 在進行第一階段的預訓練之後只需要根據第二階段的任務對模型結構進行改造，精加工（fine-tuning）模型進行監督訓練，使之適用於具體的任務。BERT 具有很強的普適性，幾乎所有 NLP 任務都可以套用這種兩階段解決思路，並且獲得效果的明顯提升。</p><p class=ql-align-justify>4 其他NLP 表示學習方法與應用</p><p class=ql-align-justify>除了上文中通用的NLP 表示學習方法，自然語言仍存在很多性質需要進行深入研究。例如，漢語具有部首共享和漢字共享的特殊性質，即幾個漢字共同的部首通常是它們之間的核心語義關聯；相應地，一個漢語詞的意思可以通過其包含的漢字來表達。如圖1 所示，基於部首感知和注意</p><div class=pgc-img><img alt=自然語言的語義表示學習方法與應用 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/2a1f999b686242e1a4b26af903e1bfcc><p class=pgc-img-caption>圖1 RAFG 獲得中文文本四個粒度特徵的說明</p></div><p class=ql-align-justify>力機制的四粒度模型RAFG[18] 對這兩種性質加以挖掘和利用，並將這些特徵系統地融入到中文文本分類的任務中，從而實現對中文文本更為準確的語義表示。</p><p class=ql-align-justify>此外，語言所處的環境信息（如圖像）會對語言的語義產生影響。進一步地，圖像所包含的信息可能與句子語義的不同的粒度表示有關聯。為此，如圖2 所示，圖像增強的層次化句子語義表示網絡IEMLRN[19]利用圖像信息從不同粒度來增強句子的語義理解與表示，實現了更為準確的句子語義表示，以及句子對的語義關係分類。</p><div class=pgc-img><img alt=自然語言的語義表示學習方法與應用 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a79d66a55e5e4c9d93dc92748f859398><p class=pgc-img-caption>圖2 圖像增強的層次化句子語義表示網絡IEMLRN 結構</p></div><p class=ql-align-justify><br></p><p class=ql-align-justify></p><p class=ql-align-justify>最後，語義表示技術的發展使得多媒體信息的有效建模與語義表示成為可能，進而為推薦、檢索等實際應用場景提供支撐。近年來，多媒體共享平臺取得了突飛猛進的發展。其中一種叫做“彈幕”的視頻實時評論愈發流行。為了有效理解視頻片段的內容，如圖3 所示，基於深度神經網絡的彈幕語義表徵方法[20] 通過利用彈幕與視頻情節之間的關聯性，對彈幕進行表示學習，實現了對視頻片段的標註。這種方法突破了常規視頻推薦/ 檢索系統只關注整段視頻的侷限性，可以滿足細粒度的要求。</p><div class=pgc-img><img alt=自然語言的語義表示學習方法與應用 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5d83d740c47043e783a81fb77cc8e01f><p class=pgc-img-caption>圖3 基於彈幕語義表徵的視頻片段標註框架會對語言的語義產生影響</p></div><p class=ql-align-justify>5 結束語</p><p class=ql-align-justify>自然語言的語義表示學習方法的發展為各種NLP 任務帶來了更多的可能性。新型網絡結構的出現使我們可以得到更加有效的語義表徵。而兩階段的預訓練方法可以把大量的無標註文本利用起來，對大量的通用語言學知識進行抽取與表示，從而提升NLP 下游任務的效果。</p><p class=ql-align-justify>自然語言的語義表示學習方法取得了令人矚目的成就，但在很多方面都仍值得繼續研究。無論是更強的特徵抽取器還是引入大量數據中包含的語言學知識，對更加精確的語義表示都有著重要作用。儘管現有的很多NLP 任務還無法達到人類的水平， 但相信對自然語言語義表徵的不斷研究、新技術的不斷出現，會創造出更豐富的成果。</p><hr class=ql-align-justify><p class=ql-align-justify>參考文獻</p><p class=ql-align-justify>[1] Turian, J.; Ratinov, L.; and Bengio, Y. 2010. Word representations: a simple and general method for semisupervised learning[C]. In ACL2010.</p><p class=ql-align-justify>[2] Melamud O, Levy O, Dagan I, et al. A simple word embedding model for lexical substitution[C].Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing. 2015: 1-7.</p><p class=ql-align-justify>[3] Palangi H, Deng L, Shen Y, et al. Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval[J]. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP),2016, 24(4): 694-707.</p><p class=ql-align-justify>[4] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model[C]. In NIPS2001.</p><p class=ql-align-justify>[5] Yoshua Bengio, Réjean Ducharme, et al.. A Neural Probabilistic Language Model[J]. The Journal of Machine Learning Research, 3:1137–1155, 2003.</p><p class=ql-align-justify>[6] Tomas Mikolov, Ilya Sutskever, et al. Distributed representations of words and phrases and their compositionality[C]. In NIPS2013.</p><p class=ql-align-justify>[7] Matthew E. Peters, Mark Neumann, et al. Deep contextualized word representations[C]. In NAACL2018.</p><p class=ql-align-justify>[8] Gottlob Frege. On Sense and Reference. Function - Term - Meaning, 1892.</p><p class=ql-align-justify>[9] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences[C]. In ACL2014.</p><p class=ql-align-justify>[10] Yoon Kim. Convolutional neural networks for sentence classification[C]. In EMNLP2014.</p><p class=ql-align-justify>[11] Richard Socher, Eric H Huang, et al.. Dynamic pooling and unfolding recursive autoencoders for paraphrase</p><p class=ql-align-justify>detection[C]. In NIPS2011.</p><p class=ql-align-justify>[12] Richard Socher, Alex Perelygin, et al. Recursive deep models for semantic compositionality over a sentiment treebank[C]. In EMNLP13.</p><p class=ql-align-justify>[13] Ilya Sutskever, Oriol Vinyals, Quoc V. Le, Sequence to Sequence Learning with Neural Networks[C] In NIPS2014.</p><p class=ql-align-justify>[14] Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate[C]. In ICLR2015.</p><p class=ql-align-justify>[15] Ashish Vaswani, Noam Shazeer, et al. Attention is All You Need[C]. In NIPS2017.</p><p class=ql-align-justify>[16] Alec Radford, Karthik Narasimhan, et al. Improving Language Understanding by Generative Pre-Training.</p><p class=ql-align-justify>[17] Jacob Devlin, Ming-Wei Chang, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805, 2018.</p><p class=ql-align-justify>[18] Hanqing Tao, Shiwei Tong, et al. A Radical-aware Attention-based Model for Chinese Text Classification[C], In AAAI2019.</p><p class=ql-align-justify>[19] Kun Zhang, Guangyi Lv, et al. Image-Enhanced Multi-Level Sentence Representation Net for Natural Language Inference[C], In ICDM2018.</p><p class=ql-align-justify>[20] Guangyi Lv, Tong Xu, et al. Reading the Videos: Temporal Labeling for Crowdsourced Time-Sync Videos based on Semantic Embedding[C], In AAAI2016.</p><p class=ql-align-justify><br></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>語言</a></li><li><a>語義</a></li><li><a>學習</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/f4c5c93c.html alt=谷歌推出自然語言框架語義解析器SLING，但沒說有沒有用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/46ea0001172cab9535dc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f4c5c93c.html title=谷歌推出自然語言框架語義解析器SLING，但沒說有沒有用>谷歌推出自然語言框架語義解析器SLING，但沒說有沒有用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1b9aa0e9.html alt=最有效的易語言命令學習方法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/aa37fb5eda8c4ae280dd0cacc0f60944 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1b9aa0e9.html title=最有效的易語言命令學習方法>最有效的易語言命令學習方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d212db72.html alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/585f4091ce744a8081779ea42cdd1c58 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d212db72.html title=「語義搜索」愛奇藝深度語義表示學習的探索與實踐>「語義搜索」愛奇藝深度語義表示學習的探索與實踐</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a895615c.html alt=語義分割的經典學習方法和深度學習方法綜述 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bb5a9fd3cedf4dc29238f1faf98d704f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a895615c.html title=語義分割的經典學習方法和深度學習方法綜述>語義分割的經典學習方法和深度學習方法綜述</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e0b5c472.html alt=自然語言處理中的深度學習：評析與展望 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3738e409cd4648ef9d28084a94faaade style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e0b5c472.html title=自然語言處理中的深度學習：評析與展望>自然語言處理中的深度學習：評析與展望</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b0abef72.html alt="深度學習自然語言處理模型實現大集合（精簡版<100行）" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d1461dcb71974b569c9b1ae64e150139 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b0abef72.html title="深度學習自然語言處理模型實現大集合（精簡版<100行）">深度學習自然語言處理模型實現大集合（精簡版&lt;100行）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3245ae5d.html alt=終於有人把自然語言處理、機器學習、深度學習和人工智能講明白了 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/abfc5dbee1af460aaca1a249b49bd56b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3245ae5d.html title=終於有人把自然語言處理、機器學習、深度學習和人工智能講明白了>終於有人把自然語言處理、機器學習、深度學習和人工智能講明白了</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b78b3729.html alt=0基礎學習C語言第五章：輸入與輸出 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/cc585bf3c5cb46c4b21788d65c61d0e5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b78b3729.html title=0基礎學習C語言第五章：輸入與輸出>0基礎學習C語言第五章：輸入與輸出</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b97d10e3.html alt=C語言基礎學習基本數據類型-變量的輸出與輸入 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/69aecd7fa7264d59ba61a9ff18f40725 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b97d10e3.html title=C語言基礎學習基本數據類型-變量的輸出與輸入>C語言基礎學習基本數據類型-變量的輸出與輸入</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4cf6eb3.html alt="學習Python語言（第八章 異常處理1 增強開發效率和容錯性）" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/9a2a5486919b4ed59cc79eb13875e1d0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4cf6eb3.html title="學習Python語言（第八章 異常處理1 增強開發效率和容錯性）">學習Python語言（第八章 異常處理1 增強開發效率和容錯性）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b0bbebe.html alt=學習C語言，要掌握操作系統中進程和內存的概念 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b0bbebe.html title=學習C語言，要掌握操作系統中進程和內存的概念>學習C語言，要掌握操作系統中進程和內存的概念</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc4ffcd.html alt=C語言函數學習-函數的嵌套調用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/09285ad7210842488cb51542b16aff06 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc4ffcd.html title=C語言函數學習-函數的嵌套調用>C語言函數學習-函數的嵌套調用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>