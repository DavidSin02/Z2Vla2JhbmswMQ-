<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習西瓜書簡明筆記（5）神經網絡 | 极客快訊</title><meta property="og:title" content="機器學習西瓜書簡明筆記（5）神經網絡 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/409546226ff447c88abbc570a41e75da"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/843e90b.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/843e90b.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/843e90b.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/843e90b.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/843e90b.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/843e90b.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/843e90b.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/843e90b.html><meta property="article:published_time" content="2020-10-29T21:05:34+08:00"><meta property="article:modified_time" content="2020-10-29T21:05:34+08:00"><meta name=Keywords content><meta name=description content="機器學習西瓜書簡明筆記（5）神經網絡"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/843e90b.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習西瓜書簡明筆記（5）神經網絡</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/409546226ff447c88abbc570a41e75da><p class=pgc-img-caption></p></div><blockquote><p style=text-align:start>上篇主要討論了決策樹算法。首先從決策樹的基本概念出發，引出決策樹基於樹形結構進行決策，進一步介紹了構造決策樹的遞歸流程以及其遞歸終止條件，在遞歸的過程中，劃分屬性的選擇起到了關鍵作用，因此緊接著討論了三種評估屬性劃分效果的經典算法，介紹了剪枝策略來解決原生決策樹容易產生的過擬合問題，最後簡述了屬性連續值/缺失值的處理方法。本篇將討論現階段十分熱門的另一個經典監督學習算法--神經網絡（neural network）。</p></blockquote><p style=text-align:start><strong>5、神經網絡</strong></p><p style=text-align:start>在機器學習中，神經網絡一般指的是“神經網絡學習”，是機器學習與神經網絡兩個學科的交叉部分。所謂神經網絡，目前用得最廣泛的一個定義是“神經網絡是由具有適應性的簡單單元組成的廣泛並行互連的網絡，它的組織能夠模擬生物神經系統對真實世界物體所做出的交互反應”。</p><p style=text-align:start><strong>5.1 神經元模型</strong></p><p style=text-align:start>神經網絡中最基本的單元是神經元模型（neuron）。在生物神經網絡的原始機制中，每個神經元通常都有多個樹突（dendrite），一個軸突（axon）和一個細胞體（cell body），樹突短而多分支，軸突長而只有一個；在功能上，樹突用於傳入其它神經元傳遞的神經衝動，而軸突用於將神經衝動傳出到其它神經元，當樹突或細胞體傳入的神經衝動使得神經元興奮時，該神經元就會通過軸突向其它神經元傳遞興奮。神經元的生物學結構如下圖所示，不得不說高中的生化知識大學忘得可是真乾淨...</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/58040fa8578c44b58a025d1751ed952d><p class=pgc-img-caption></p></div><p style=text-align:start>一直沿用至今的“M-P神經元模型”正是對這一結構進行了抽象，也稱“閾值邏輯單元“，其中樹突對應於輸入部分，每個神經元收到n個其他神經元傳遞過來的輸入信號，這些信號通過帶權重的連接傳遞給細胞體，這些權重又稱為連接權（connection weight）。細胞體分為兩部分，前一部分計算總輸入值（即輸入信號的加權和，或者說累積電平），後一部分先計算總輸入值與該神經元閾值的差值，然後通過激活函數（activation function）的處理，產生輸出從軸突傳送給其它神經元。M-P神經元模型如下圖所示：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3fba05c868574accb6a20395e71d22f9><p class=pgc-img-caption></p></div><p style=text-align:start>與線性分類十分相似，神經元模型最理想的激活函數也是階躍函數，即將神經元輸入值與閾值的差值映射為輸出值1或0，若差值大於零輸出1，對應興奮；若差值小於零則輸出0，對應抑制。但階躍函數不連續，不光滑，故在M-P神經元模型中，也採用Sigmoid函數來近似， Sigmoid函數將較大範圍內變化的輸入值擠壓到 (0,1) 輸出值範圍內，所以也稱為擠壓函數（squashing function）。</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c1b3124abb71461ea2d627e3637f4a8c><p class=pgc-img-caption></p></div><p style=text-align:start>將多個神經元按一定的層次結構連接起來，就得到了神經網絡。它是一種包含多個參數的模型，比方說10個神經元兩兩連接，則有100個參數需要學習（每個神經元有9個連接權以及1個閾值），若將每個神經元都看作一個函數，則整個神經網絡就是由這些函數相互嵌套而成。</p><p style=text-align:start><strong>5.2 感知機與多層網絡</strong></p><p style=text-align:start>感知機（Perceptron）是由兩層神經元組成的一個簡單模型，但只有輸出層是M-P神經元，即只有輸出層神經元進行激活函數處理，也稱為功能神經元（functional neuron）；輸入層只是接受外界信號（樣本屬性）並傳遞給輸出層（輸入層的神經元個數等於樣本的屬性數目），而沒有激活函數。這樣一來，感知機與之前線性模型中的對數機率迴歸的思想基本是一樣的，都是通過對屬性加權與另一個常數求和，再使用sigmoid函數將這個輸出值壓縮到0-1之間，從而解決分類問題。不同的是感知機的輸出層應該可以有多個神經元，從而可以實現多分類問題，同時兩個模型所用的參數估計方法十分不同。</p><p style=text-align:start>給定訓練集，則感知機的n+1個參數（n個權重+1個閾值）都可以通過學習得到。閾值Θ可以看作一個輸入值固定為-1的啞結點的權重ωn+1，即假設有一個固定輸入xn+1=-1的輸入層神經元，其對應的權重為ωn+1，這樣就把權重和閾值統一為權重的學習了。簡單感知機的結構如下圖所示：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/9abb5bf079914508a770894328a85d5b><p class=pgc-img-caption></p></div><p style=text-align:start>感知機權重的學習規則如下：對於訓練樣本（x，y），當該樣本進入感知機學習後，會產生一個輸出值，若該輸出值與樣本的真實標記不一致，則感知機會對權重進行調整，若激活函數為階躍函數，則調整的方法為（基於梯度下降法）：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8016a41108974da4b5dfa9e33371f8a7><p class=pgc-img-caption></p></div><p style=text-align:start>其中 η∈（0，1）稱為學習率，可以看出感知機是通過逐個樣本輸入來更新權重，首先設定好初始權重（一般為隨機），逐個地輸入樣本數據，若輸出值與真實標記相同則繼續輸入下一個樣本，若不一致則更新權重，然後再重新逐個檢驗，直到每個樣本數據的輸出值都與真實標記相同。容易看出：感知機模型總是能將訓練數據的每一個樣本都預測正確，和決策樹模型總是能將所有訓練數據都分開一樣，感知機模型很容易產生過擬合問題。</p><p style=text-align:start>由於感知機模型只有一層功能神經元，因此其功能十分有限，只能處理線性可分的問題，對於這類問題，感知機的學習過程一定會收斂（converge），因此總是可以求出適當的權值。但是對於像書上提到的異或問題，只通過一層功能神經元往往不能解決，因此要解決非線性可分問題，需要考慮使用多層功能神經元，即神經網絡。多層神經網絡的拓撲結構如下圖所示：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a7ef0b81507849e5ba4b76cd4f4be30d><p class=pgc-img-caption></p></div><p style=text-align:start>在神經網絡中，輸入層與輸出層之間的層稱為隱含層或隱層（hidden layer），隱層和輸出層的神經元都是具有激活函數的功能神經元。只需包含一個隱層便可以稱為多層神經網絡，常用的神經網絡稱為“多層前饋神經網絡”（multi-layer feedforward neural network），該結構滿足以下幾個特點：</p><pre><code>* 每層神經元與下一層神經元之間完全互連* 神經元之間不存在同層連接* 神經元之間不存在跨層連接</code></pre><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/25762fd914bd4bc7b94aef123af72a10><p class=pgc-img-caption></p></div><p style=text-align:start>根據上面的特點可以得知：這裡的“前饋”指的是網絡拓撲結構中不存在環或迴路，而不是指該網絡只能向前傳播而不能向後傳播（下節中的BP神經網絡正是基於前饋神經網絡而增加了反饋調節機制）。神經網絡的學習過程就是根據訓練數據來調整神經元之間的“連接權”以及每個神經元的閾值，換句話說：神經網絡所學習到的東西都蘊含在網絡的連接權與閾值中。</p><p style=text-align:start><strong>5.3 BP神經網絡算法</strong></p><p style=text-align:start>由上面可以得知：神經網絡的學習主要蘊含在權重和閾值中，多層網絡使用上面簡單感知機的權重調整規則顯然不夠用了，BP神經網絡算法即誤差逆傳播算法（error BackPropagation）正是為學習多層前饋神經網絡而設計，BP神經網絡算法是迄今為止最成功的的神經網絡學習算法。</p><p style=text-align:start>一般而言，只需包含一個足夠多神經元的隱層，就能以任意精度逼近任意複雜度的連續函數[Hornik et al.,1989]，故下面以訓練單隱層的前饋神經網絡為例，介紹BP神經網絡的算法思想。</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/db65e97977de4373bac7009b02e2a116><p class=pgc-img-caption></p></div><p style=text-align:start>上圖為一個單隱層前饋神經網絡的拓撲結構，BP神經網絡算法也使用梯度下降法（gradient descent），以單個樣本的均方誤差的負梯度方向對權重進行調節。可以看出：BP算法首先將誤差反向傳播給隱層神經元，調節隱層到輸出層的連接權重與輸出層神經元的閾值；接著根據隱含層神經元的均方誤差，來調節輸入層到隱含層的連接權值與隱含層神經元的閾值。BP算法基本的推導過程與感知機的推導過程原理是相同的，下面給出調整隱含層到輸出層的權重調整規則的推導過程：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f610958388894c458ebd59028ab6a470><p class=pgc-img-caption></p></div><p style=text-align:start>學習率η∈（0，1）控制著沿反梯度方向下降的步長，若步長太大則下降太快容易產生震盪，若步長太小則收斂速度太慢，一般的，常把η設置為0.1，有時更新權重時會將輸出層與隱含層設置為不同的學習率。BP算法的基本流程如下所示：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fa3681ebe0f34b249e618a1b18e8f1f3><p class=pgc-img-caption></p></div><p style=text-align:start>BP算法的更新規則是基於每個樣本的預測值與真實類標的均方誤差來進行權值調節，即BP算法每次更新只針對於單個樣例。需要注意的是：BP算法的最終目標是要最小化整個訓練集D上的累積誤差，即：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e06183d65ee84c888a657e9f55a2cfc0><p class=pgc-img-caption></p></div><p style=text-align:start>如果基於累積誤差最小化的更新規則，則得到了累積誤差逆傳播算法（accumulated error backpropagation），即每次讀取全部的數據集一遍，進行一輪學習，從而基於當前的累積誤差進行權值調整，因此參數更新的頻率相比標準BP算法低了很多，但在很多任務中，尤其是在數據量很大的時候，往往標準BP算法會獲得較好的結果。另外對於如何設置隱層神經元個數的問題，至今仍然沒有好的解決方案，常使用“試錯法”進行調整。</p><p style=text-align:start>前面提到，BP神經網絡強大的學習能力常常容易造成過擬合問題，有以下兩種策略來緩解BP網絡的過擬合問題：</p><ul><li>早停：將數據分為訓練集與測試集，訓練集用於學習，測試集用於評估性能，若在訓練過程中，訓練集的累積誤差降低，而測試集的累積誤差升高，則停止訓練。</li><li>引入正則化（regularization）：基本思想是在累積誤差函數中增加一個用於描述網絡複雜度的部分，例如所有權值與閾值的平方和，其中λ∈（0,1）用於對累積經驗誤差與網絡複雜度這兩項進行折中，常通過交叉驗證法來估計。</li></ul><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8f3a70f41471452eb3eb2ccb72f2a91d><p class=pgc-img-caption></p></div><p style=text-align:start><strong>5.4 全局最小與局部最小</strong></p><p style=text-align:start>模型學習的過程實質上就是一個尋找最優參數的過程，例如BP算法試圖通過最速下降來尋找使得累積經驗誤差最小的權值與閾值，在談到最優時，一般會提到局部極小（local minimum）和全局最小（global minimum）。</p><pre><code>* 局部極小解：參數空間中的某個點，其鄰域點的誤差函數值均不小於該點的誤差函數值。* 全局最小解：參數空間中的某個點，所有其他點的誤差函數值均不小於該點的誤差函數值。</code></pre><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4993771db7664d6db68fa2830e12cb64><p class=pgc-img-caption></p></div><p style=text-align:start>要成為局部極小點，只要滿足該點在參數空間中的梯度為零。局部極小可以有多個，而全局最小隻有一個。全局最小一定是局部極小，但局部最小卻不一定是全局最小。顯然在很多機器學習算法中，都試圖找到目標函數的全局最小。梯度下降法的主要思想就是沿著負梯度方向去搜索最優解，負梯度方向是函數值下降最快的方向，若迭代到某處的梯度為0，則表示達到一個局部最小，參數更新停止。因此在現實任務中，通常使用以下策略儘可能地去接近全局最小。</p><pre><code>* 以多組不同參數值初始化多個神經網絡，按標準方法訓練，迭代停止後，取其中誤差最小的解作為最終參數。* 使用“模擬退火”技術，這裡不做具體介紹。* 使用隨機梯度下降，即在計算梯度時加入了隨機因素，使得在局部最小時，計算的梯度仍可能不為0，從而迭代可以繼續進行。</code></pre><p style=text-align:start><strong>5.5 深度學習</strong></p><p style=text-align:start>理論上，參數越多，模型複雜度就越高，容量（capability）就越大，從而能完成更復雜的學習任務。深度學習（deep learning）正是一種極其複雜而強大的模型。</p><p style=text-align:start>怎麼增大模型複雜度呢？兩個辦法，一是增加隱層的數目，二是增加隱層神經元的數目。前者更有效一些，因為它不僅增加了功能神經元的數量，還增加了激活函數嵌套的層數。但是對於多隱層神經網絡，經典算法如標準BP算法往往會在誤差逆傳播時發散（diverge），無法收斂達到穩定狀態。</p><p style=text-align:start>那要怎麼有效地訓練多隱層神經網絡呢？一般來說有以下兩種方法：</p><ul><li>無監督逐層訓練（unsupervised layer-wise training）：每次訓練一層隱節點，把上一層隱節點的輸出當作輸入來訓練，本層隱結點訓練好後，輸出再作為下一層的輸入來訓練，這稱為預訓練（pre-training）。全部預訓練完成後，再對整個網絡進行微調（fine-tuning）訓練。一個典型例子就是深度信念網絡（deep belief network，簡稱DBN）。這種做法其實可以視為把大量的參數進行分組，先找出每組較好的設置，再基於這些局部最優的結果來訓練全局最優。</li><li>權共享（weight sharing）：令同一層神經元使用完全相同的連接權，典型的例子是卷積神經網絡（Convolutional Neural Network，簡稱CNN）。這樣做可以大大減少需要訓練的參數數目。</li></ul><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（5）神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a514d35e622e4a2fbd820a55464534e5><p class=pgc-img-caption></p></div><p style=text-align:start>深度學習可以理解為一種特徵學習（feature learning）或者表示學習（representation learning），無論是DBN還是CNN，都是通過多個隱層來把與輸出目標聯繫不大的初始輸入轉化為與輸出目標更加密切的表示，使原來只通過單層映射難以完成的任務變為可能。即通過多層處理，逐漸將初始的“低層”特徵表示轉化為“高層”特徵表示，從而使得最後可以用簡單的模型來完成複雜的學習任務。</p><p style=text-align:start>傳統任務中，樣本的特徵需要人類專家來設計，這稱為特徵工程（feature engineering）。特徵好壞對泛化性能有至關重要的影響。而深度學習為全自動數據分析帶來了可能，可以自動產生更好的特徵。</p><p>相關人工智能與異構計算的知識分享，歡迎關注我的公眾號【<strong>AI異構】</strong></p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>簡明</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/8d200025.html alt=機器學習西瓜書簡明筆記（15）概率圖模型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/ae46374e32bb4e80ab4f93a219a0c41e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8d200025.html title=機器學習西瓜書簡明筆記（15）概率圖模型>機器學習西瓜書簡明筆記（15）概率圖模型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>