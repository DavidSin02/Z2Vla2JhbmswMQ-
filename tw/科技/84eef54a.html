<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>有關自然語言處理的深度學習知識有哪些？ | 极客快訊</title><meta property="og:title" content="有關自然語言處理的深度學習知識有哪些？ - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/b6e6ac2f1c1948158c7edbe790f52b66"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/84eef54a.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/84eef54a.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/84eef54a.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/84eef54a.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/84eef54a.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/84eef54a.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/84eef54a.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/84eef54a.html><meta property="article:published_time" content="2020-11-14T21:00:19+08:00"><meta property="article:modified_time" content="2020-11-14T21:00:19+08:00"><meta name=Keywords content><meta name=description content="有關自然語言處理的深度學習知識有哪些？"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/84eef54a.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>有關自然語言處理的深度學習知識有哪些？</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p style="margin:0 0 8px"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">偏置是神經元中常用的輸入項。和其他輸入元素一樣，神經元會給偏置一個權重，該權重與其他權重用同樣的方式來訓練。在關於神經網絡的各種文獻中，偏置有兩種表示形式。一種表示形式是將其表示為輸入向量，例如對於</span><em><span style="color:#666;--tt-darkmode-color: #666666">n</span></em><span style="color:#666;--tt-darkmode-color: #666666">維向量的輸入，在向量的開頭或結尾處增加一個元素，構成一個</span><em><span style="color:#666;--tt-darkmode-color: #666666">n </span></em><span style="color:#666;--tt-darkmode-color: #666666">+ 1維的向量。1的位置與網絡無關，只要在所有樣本中保持一致即可。另一種表示形式是，首先假定存在一個偏置項，將其獨立於輸入之外，其對應一個獨立的權重，將該權重乘以1，然後與樣本輸入值及其相關權重的點積進行加和。這兩者實際上是一樣的，只不過分別是兩種常見的表示形式而已。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">設置偏置權重的原因是神經元需要對全0的輸入具有彈性。網絡需要學習在輸入全為0的情況下輸出仍然為0，但它可能做不到這一點。如果沒有偏置項，神經元對初始或學習的任意權重都會輸出0 × 權重 = 0。而有了偏置項之後，就不會有這個問題了。如果神經元需要學習輸出0，在這種情況下，神經元可以學會減小與偏置相關的權重，使點積保持在閾值以下即可。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">圖5-3用可視化方法對生物的大腦神經元的信號與深度學習人工神經元的信號進行了類比，如果想要做更深入的瞭解，可以思考一下你是如何使用生物神經元來閱讀本書並學習有關自然語言處理的深度學習知識的[5]。</span></span></p><div class=pgc-img><img alt=有關自然語言處理的深度學習知識有哪些？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b6e6ac2f1c1948158c7edbe790f52b66><p class=pgc-img-caption></p></div><p style="text-align:center;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">圖5-3　感知機與生物神經元</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">用數學術語來說，感知機的輸出表示為</span><em><span style="color:#666;--tt-darkmode-color: #666666">f </span></em><span style="color:#666;--tt-darkmode-color: #666666">(</span><em><span style="color:#666;--tt-darkmode-color: #666666">x</span></em><span style="color:#666;--tt-darkmode-color: #666666">)，如下：</span></span></p><div class=pgc-img><img alt=有關自然語言處理的深度學習知識有哪些？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3d70e409ac5e42f699be3f62f3464401><p class=pgc-img-caption></p></div><p style="text-align:center;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">公式5-1　閾值激活函數</span></span></p><blockquote><p style="margin:0 0 8px"><span style=font-size:.667em><strong>提示 </strong><span style="color:#666;--tt-darkmode-color: #666666">　</span></span></p><p style="margin:8px 0 0"><span style=font-size:.667em>輸入向量（<em><strong>X</strong></em>）與權重向量（<em><strong>W</strong></em>）兩兩相乘後的加和就是這兩個向量的點積。這是<strong>線性代數</strong>在神經網絡中最基礎的應用，對神經網絡的發展影響巨大。另外，通過現代計算機GPU對線性代數操作的性能優化來完成感知機的矩陣乘法運算，使得實現的神經網絡變得極為高效。</span></p></blockquote><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">此時的感知機並未學到任何東西，不過大家已經獲得了非常重要的結果，我們已經向模型輸入數據並且得到輸出。當然這個輸出可能是錯誤的，因為還沒有告訴感知機如何獲得權重，而這正是最有趣的地方所在。</span></span></p><blockquote><p style="margin:0 0 8px"><span style=font-size:.667em><strong>提示 </strong><span style="color:#666;--tt-darkmode-color: #666666">　</span></span></p><p style="margin:8px 0 0"><span style=font-size:.667em>所有神經網絡的基本單位都是神經元，基本感知機是廣義神經元的一個特例，從現在開始，我們將感知機稱為一個神經元。</span></p></blockquote><h1 class=pgc-h-arrow-right>1．Python版神經元</h1><p style="margin:0 0 8px"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">在Python中，計算神經元的輸出是很簡單的。大家可以用numpy的dot函數將兩個向量相乘：</span></span></p><pre><code>&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; example_input = [1, .2, .1, .05, .2]&gt;&gt;&gt; example_weights = [.2, .12, .4, .6, .90]&gt;&gt;&gt; input_vector = np.array(example_input)&gt;&gt;&gt; weights = np.array(example_weights)&gt;&gt;&gt; bias_weight = .2&gt;&gt;&gt; activation_level = np.dot(input_vector, weights) +\...     (bias_weight * 1) 　　⇽---　這裡bias_weight * 1只是為了強調bias_weight和其他權重一樣：權重與輸入值相乘，區別只是bias_weight的輸入特徵值總是1&gt;&gt;&gt; activation_level0.674</code></pre><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">接下來，假設我們選擇一個簡單的閾值激活函數，並選擇0.5作為閾值，結果如下：</span></span></p><pre><code>&gt;&gt;&gt; threshold = 0.5&gt;&gt;&gt; if activation_level &gt;= threshold:...    perceptron_output = 1... else:...    perceptron_output = 0&gt;&gt;&gt; perceptron_output)1</code></pre><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">對於給定的輸入樣本</span><span style="color:#d14;--tt-darkmode-color: #DD1144"><span style="background-color:#f7f7f9;--tt-darkmode-bgcolor: #BEBEBF">example_input</span></span><span style="color:#666;--tt-darkmode-color: #666666">和權重，這個感知機將會輸出1。如果有許多</span><span style="color:#d14;--tt-darkmode-color: #DD1144"><span style="background-color:#f7f7f9;--tt-darkmode-bgcolor: #BEBEBF">example_input</span></span><span style="color:#666;--tt-darkmode-color: #666666">向量，輸出將會是一個標籤集合，大家可以檢查每次感知機的預測是否正確。</span></span></p><h1 class=pgc-h-arrow-right>2．課堂時間</h1><p style="margin:0 0 8px"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">大家已經構建了一個基於數據進行預測的方法，它為機器學習創造了條件。到目前為止，權重都作為任意值而被我們忽略了。實際上，它們是整個架構的關鍵，現在我們需要一種算法，基於給定樣本的預測結果來調整權重值的大小。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">感知機將權重的調整看成是給定輸入下預測系統正確性的一個函數，從而</span><strong>學習</strong><span style="color:#666;--tt-darkmode-color: #666666">這些權重。但是這一切從何開始呢？未經訓練的神經元的權重一開始是隨機的！通常是從正態分佈中選取趨近於零的隨機值。在前面的例子中，大家可以看到從零開始的權重（包括偏置權重）為何會導致輸出全部為零。但是通過設置微小的變化，無須提供給神經元太多的能力，神經元便能以此為依據判斷結果何時為對何時為錯。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">然後就可以開始學習過程了。通過向系統輸入許多不同的樣本，並根據神經元的輸出是否是我們想要的結果來對權重進行微小的調整。當有足夠的樣本（且在正確的條件下），誤差</span><strong>應該</strong><span style="color:#666;--tt-darkmode-color: #666666">逐漸趨於零，系統就經過了</span><strong>學習</strong><span style="color:#666;--tt-darkmode-color: #666666">。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">其中最關鍵的一個訣竅是，每個權重都是根據它對結果誤差的貢獻程度來進行調整。權重越大（對結果影響越大），那麼該權重對給定輸入的感知機輸出的正確性/錯誤性就負有越大的責任。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">假設之前的輸入</span><span style="color:#d14;--tt-darkmode-color: #DD1144"><span style="background-color:#f7f7f9;--tt-darkmode-bgcolor: #BEBEBF">example_input</span></span><span style="color:#666;--tt-darkmode-color: #666666">對應的結果是0：</span></span></p><pre><code>&gt;&gt;&gt; expected_output = 0&gt;&gt;&gt; new_weights = []&gt;&gt;&gt; for i, x in enumerate(example_input):...     new_weights.append(weights[i] + (expected_output -\...         perceptron_output) * x) 　　⇽---　例如，在上述的第一次計算中，new_weight = 0.2 + (0 - 1) × 1 = −0.8 &gt;&gt;&gt; weights = np.array(new_weights)&gt;&gt;&gt; example_weights　　⇽---　初始權重[0.2, 0.12, 0.4, 0.6, 0.9]&gt;&gt;&gt; weights　　⇽---　新的權重[-0.8  -0.08  0.3  0.55  0.7]</code></pre><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">這個處理方法將同一份訓練集反覆輸入網絡中，在適當的情況下，即使是對於之前沒見過的數據，感知機也能做出正確的預測。</span></span></p><h1 class=pgc-h-arrow-right>3．有趣的邏輯學習問題</h1><p style="margin:0 0 8px"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">上面使用了一些隨機數字做例子。我們把這個方法應用到一個具體問題上，來看看如何通過僅向計算機展示一些標記樣本來教它學會一個概念。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">接下來，我們將讓計算機理解邏輯或（OR）。如果一個表達式的一邊或另一邊為真（或兩邊都為真），則邏輯或語句的結果為真。這個邏輯非常簡單。對於以下這個問題，我們可以手動構造所有可能的樣本（在現實中很少出現這種情況），每個樣本由兩個信號組成，其中每個信號都為真（1）或假（0），如代碼清單5-1所示。</span></span></p><p style="text-align:right;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">代碼清單5-1　邏輯或問題</span></span></p><pre><code>&gt;&gt;&gt; sample_data = [[0, 0],  # False, False...                [0, 1],  # False, True...                [1, 0],  # True, False...                [1, 1]]  # True, True&gt;&gt;&gt; expected_results = [0,  # (False OR False) gives False...                     1,  # (False OR True ) gives True...                     1,  # (True  OR False) gives True...                     1]  # (True  OR True ) gives True&gt;&gt;&gt; activation_threshold = 0.5</code></pre><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">我們需要一些工具，numpy可以用來做向量（數組）乘法，</span><span style="color:#d14;--tt-darkmode-color: #DD1144"><span style="background-color:#f7f7f9;--tt-darkmode-bgcolor: #BEBEBF">random</span></span><span style="color:#666;--tt-darkmode-color: #666666">用來初始化權重：</span></span></p><pre><code>&gt;&gt;&gt; from random import random&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; weights = np.random.random(2)/1000  # Small random float 0 &lt; w &lt; .001&gt;&gt;&gt; weights[5.62332144e-04 7.69468028e-05]</code></pre><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">這裡還需要一個偏置：</span></span></p><pre><code>&gt;&gt;&gt; bias_weight = np.random.random() / 1000&gt;&gt;&gt; bias_weight0.0009984699077277136</code></pre><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">然後將其傳遞到流水線中，計算得到4個樣本的預測結果，如代碼清單5-2所示。</span></span></p><p style="text-align:right;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">代碼清單5-2　感知機隨機預測</span></span></p><pre><code>&gt;&gt;&gt; for idx, sample in enumerate(sample_data):...     input_vector = np.array(sample)...     activation_level = np.dot(input_vector, weights) +\...         (bias_weight * 1)...     if activation_level &gt; activation_threshold:...         perceptron_output = 1...     else:...         perceptron_output = 0...     print('Predicted {}'.format(perceptron_output))...     print('Expected: {}'.format(expected_results[idx]))...     print()Predicted 0Expected: 0Predicted 0Expected: 1Predicted 0Expected: 1Predicted 0Expected: 1</code></pre><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">隨機的權重值對這個神經元沒有多大幫助，我們得到1個正確、3個錯誤的預測結果。接下來我們讓網絡繼續學習，並在每次迭代中不只是打印1或0，而是同時更新權重值，如代碼清單5-3所示。</span></span></p><p style="text-align:right;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">代碼清單5-3　感知機學習</span></span></p><pre><code>&gt;&gt;&gt; for iteration_num in range(5):...     correct_answers = 0...     for idx, sample in enumerate(sample_data):...         input_vector = np.array(sample)...         weights = np.array(weights)...         activation_level = np.dot(input_vector, weights) +\...             (bias_weight * 1)...         if activation_level &gt; activation_threshold:...             perceptron_output = 1...         else:...             perceptron_output = 0...         if perceptron_output == expected_results[idx]:...             correct_answers += 1...         new_weights = []...         for i, x in enumerate(sample): 　　⇽---　這就是使用魔法的地方。當然還有一些更高效的方法來實現，不過我們還是通過循環來強調每個權重是由其輸入（xi）更新的。如果輸入數據很小或為零，則無論誤差大小，該輸入對該權重的影響都將會很小。相反，如果輸入數據很大，則影響會很大...             new_weights.append(weights[i] + (expected_results[idx] -\...                  perceptron_output) * x)...         bias_weight = bias_weight + ((expected_results[idx] -\...             perceptron_output) * 1) 　　⇽---　偏置權重也會隨著輸入一起更新...         weights = np.array(new_weights)...     print('{} correct answers out of 4, for iteration {}'\...         .format(correct_answers, iteration_num))3 correct answers out of 4, for iteration 02 correct answers out of 4, for iteration 13 correct answers out of 4, for iteration 24 correct answers out of 4, for iteration 34 correct answers out of 4, for iteration 4</code></pre><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">哈哈！這個感知機真是個好學生。通過內部循環更新權重，感知機從數據集中學習了經驗。在第一次迭代後，它比隨機猜測（正確率為1/4）多得到了兩個正確結果（正確率為3/4）。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">在第二次迭代中，它過度修正了權重（更改了太多），然後通過調整權重來回溯結果。當第四次迭代完成後，它已經完美地學習了這些關係。隨後的迭代將不再更新網絡，因為每個樣本的誤差為0，所以不會再對權重做調整。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">這就是所謂的</span><strong>收斂</strong><span style="color:#666;--tt-darkmode-color: #666666">。當一個模型的誤差函數達到了最小值，或者穩定在一個值上，該模型就被稱為收斂。有時候可能沒有這麼幸運。有時神經網絡在尋找最優權值時不斷波動以滿足一批數據的相互關係，但無法收斂。在5.8節中，大家將看到</span><strong>目標函數</strong><span style="color:#666;--tt-darkmode-color: #666666">（objective function）或</span><strong>損失函數</strong><span style="color:#666;--tt-darkmode-color: #666666">（loss function）如何影響神經網絡對最優權重的選擇。</span></span></p><h1 class=pgc-h-arrow-right>4．下一步</h1><p style="margin:0 0 8px"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">基本感知機有一個固有缺陷，那就是，如果數據不是線性可分的，或者數據之間的關係不能用線性關係來描述，模型將無法收斂，也將不具有任何有效預測的能力，因為它無法準確地預測目標變量。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">早期的實驗在僅基於樣本圖像及其類別來進行圖像分類的學習上取得了成功。這個概念在早期很激動人心，但很快受到了來自明斯基（Minsky）和佩珀特（Papert）的考驗[6]，他們指出感知機在分類方面有嚴重的侷限性，他們證明了如果數據樣本不能線性可分為獨立的組，那麼感知機將無法學習如何對輸入數據進行分類。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">線性可分的數據點（如圖5-4所示）對感知機來說是沒有問題的，而存在類別交叉的數據將導致單神經元感知機原地踏步，學習預測的結果將不比隨機猜測好，表現得就像是在隨機拋硬幣。在圖5-5中我們就無法在兩個類（分別用點和叉表示）之間畫一條分割線。</span></span></p><div class=pgc-img><img alt=有關自然語言處理的深度學習知識有哪些？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/080f576d604048bc89b39d871414b970><p class=pgc-img-caption></p></div><p style="text-align:center;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">圖5-4　線性可分的數據</span></span></p><div class=pgc-img><img alt=有關自然語言處理的深度學習知識有哪些？ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/27f59e70da6c40f38b2aea52b9c0caf8><p class=pgc-img-caption></p></div><p style="text-align:center;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">圖5-5　非線性可分的數據</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">感知機會用線性方程來描述數據集的特徵與數據集中的目標變量之間的關係，這就是線性迴歸，但是感知機無法描述非線性方程或者非線性關係。</span></span></p><p style="margin:8px 0"><br></p><hr><p><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666"><br></span></span></p><p style="margin:8px 0"><br></p><p style="margin:8px 0"><span style=font-size:.667em><strong>局部極小值與全局極小值</strong></span></p><p style="margin:8px 0"><span style=font-size:.667em>當一個感知機收斂時，可以說它找到了一個描述數據與目標變量之間關係的線性方程。然而，這並不能說明這個描述性線性方程有多好，或者說代價有多“小”。如果有多個解決方案，即存在著多個可能的極小代價，它只會確定一個由權重初始值決定的、特定的極小值。這被稱為<strong>局部極小值</strong>，因為它是在權重開始的地方附近找到的最優值（最小的代價）。它可能不是全局極小值，因為全局極小值需要搜索所有可能的權重值。在大多數情況下，無法確定是否找到了全局極小值。</span></p><p style="margin:8px 0"><br></p><hr><p><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666"><br></span></span></p><p style="margin:8px 0"><br></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">很多數據值之間的關係不是線性的，也沒有好的線性迴歸或線性方程能夠描述這些關係。許多數據集不能用直線或平面來線性分割。因為世界上的大多數數據不能由直線或平面來清楚地分開，明斯基和佩珀特發表的“證明”讓感知機被束之高閣。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">但是有關感知機的想法並不會就此消亡。Rumelhardt McClelland的合作成果[7]（Geoffrey Hinton也參與其中）展示了可以使用多層感知機的組合來解決異或（XOR）問題[8]，此後感知機再次浮出水面。之前，大家使用單層感知機解決的或（OR）問題屬於比較簡單的問題，也沒有用到多層反向傳播。Rumelhardt McClelland的關鍵突破是發現了一種方法，該方法可以為每個感知機適當地分配誤差。他們使用的是一種叫反向傳播的傳統思想，通過這種跨越多個神經元層的反向傳播思想，第一個現代神經網絡誕生了。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">基本感知機有一個固有的缺陷，即如果數據不是線性可分的，則模型不會收斂到具有有效預測能力的解。</span></span></p><blockquote><p style="margin:0 0 8px"><span style=font-size:.667em><strong>注意</strong><span style="color:#666;--tt-darkmode-color: #666666">　</span></span></p><p style="margin:8px 0 0"><span style=font-size:.667em>代碼清單5-3中的代碼通過一個單層感知機解決了或問題。代碼清單5-1代碼中感知機學到的0和1組成的表格是二元邏輯或的輸出結果。異或問題稍微改變了一下該數據表，以此來教感知機如何模擬一個<strong>獨佔</strong>的邏輯或門。如果改變一下最後一個示例的正確結果，將<span style="color:#d14;--tt-darkmode-color: #DD1144"><span style="background-color:#f7f7f9;--tt-darkmode-bgcolor: #BEBEBF">1</span></span>（真）改為<span style="color:#d14;--tt-darkmode-color: #DD1144"><span style="background-color:#f7f7f9;--tt-darkmode-bgcolor: #BEBEBF">0</span></span>（假），從而將其轉換為邏輯異或，這個問題就會變得困難許多。在不向神經網絡添加額外神經元的情況下，每類（0或1）樣本將是非線性可分的。在二維特徵向量空間中，這些類彼此呈對角線分佈（類似於圖5-5），所以無法通過畫一條直線來區分出數據樣本1（邏輯真）和0（邏輯假）。</span></p></blockquote><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">儘管神經網絡可以解決複雜的非線性問題，但是那時計算成本太高，用兩個感知機和一堆花哨的反向傳播數學算法來解決異或問題被認為是對寶貴計算資源的極大浪費，因為這個問題只需要用一個邏輯門或一行代碼就能解決。事實證明，它們不適合被廣泛使用，所以只能再次回到學術界和超級計算機實驗的角落。由此開啟了第二次“人工智能寒冬”，這種情況從1990年持續到2010年左右[9]。後來，隨著計算能力、反向傳播算法和原始數據（例如貓和狗的標註圖像[10]）的發展，昂貴的計算算法和有限的數據集都不再是障礙。第三次神經網絡時代開始了。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">下面我們來看看他們發現了什麼。</span></span></p><h1 class=pgc-h-arrow-right>5．第二次人工智能寒冬的出現</h1><p style="margin:0 0 8px"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">和大多數偉大的思想一樣，好的思想最終都會浮出水面。人們發現只要對感知機背後的基本思想進行擴展，就可以克服之前的那些限制。將多個感知機集合到一起，並將數據輸入一個（或多個）感知機中，並且以這些感知機的輸出作為輸入，傳遞到更多的感知機，最後將輸出與期望值進行比較，這個系統（神經網絡）便可以學習更復雜的模式，克服了類的線性不可分的挑戰，如異或問題。其中的關鍵是：如何更新前面各層中的權重？</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">接下來我們暫停一下，將這個過程的一個重要部分形式化。到目前為止，我們已經討論了誤差和感知機的預測結果與真實結果的偏離程度。測量這個誤差是由</span><strong>代價函數</strong><span style="color:#666;--tt-darkmode-color: #666666">或</span><strong>損失函數</strong><span style="color:#666;--tt-darkmode-color: #666666">來完成的。正如我們看到的，代價函數量化了對於輸入“問題”（</span><em><span style="color:#666;--tt-darkmode-color: #666666">x</span></em><span style="color:#666;--tt-darkmode-color: #666666">）網絡應該輸出的正確答案與實際輸出值（</span><em><span style="color:#666;--tt-darkmode-color: #666666">y</span></em><span style="color:#666;--tt-darkmode-color: #666666">）之間的差距。損失函數則表示網絡輸出錯誤答案的次數以及錯誤總量。公式5-2是代價函數的一個例子，表示真實值與模型預測值之間的誤差：</span></span></p><div class=pgc-img><img alt=有關自然語言處理的深度學習知識有哪些？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/189f97306fc24f0b9d2843a25c702a92><p class=pgc-img-caption></p></div><p style="text-align:center;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">公式5-2　真實值與預測值之間的誤差</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">訓練感知機或者神經網絡的目標是最小化所有輸入樣本數據的代價函數。</span></span></p><div class=pgc-img><img alt=有關自然語言處理的深度學習知識有哪些？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/016a68f7d0de497eb33af2cf59f4399b><p class=pgc-img-caption></p></div><p style="text-align:center;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">公式5-3　希望最小化的代價函數</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">接下來大家會看到還有一些其他種類的代價函數，如均方誤差，這些通常已經在神經網絡框架中定義好了，大家無須自己去決定哪些是最好的代價函數。需要牢記的是，最終目標是將數據集上的代價函數最小化，這樣此處給出的其他概念才有意義。</span></span></p><h1 class=pgc-h-arrow-right>6．反向傳播算法</h1><p style="margin:0 0 8px"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">辛頓（Hinton）和他的同事提出一種用多層感知機同時處理一個目標的方法。這個方法可以解決線性不可分問題。通過該方法，他們可以像擬合線性函數那樣去擬合非線性函數。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">但是如何更新這些不同感知機的權重呢？造成誤差的原因是什麼？假設兩個感知機彼此相鄰，並接收相同的輸入，無論怎樣處理輸出（連接、求和、相乘），當我們試圖將誤差傳播回到初始權重的時候，它們（輸出）都將是輸入的函數（兩邊是相同的），所以它們每一步的更新量都是一樣的，感知機不會有不同的結果。這裡的多個感知機將是冗餘的。它們的權重一樣，神經網絡也不會學到更多東西。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">大家再來想象一下，如果將一個感知機作為第二個感知機的輸入，是不是更令人困惑了，這到底是在做什麼？</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">反向傳播可以解決這個問題，但首先需要稍微調整一下感知機。記住，權重是根據它們對整體誤差的貢獻來更新的。但是如果權重對應的輸出成為另一個感知機的輸入，那麼從第二個感知機開始，我們對誤差的認識就變得有些模糊了。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">如圖5-6所示，權重</span><em><span style="color:#666;--tt-darkmode-color: #666666">w</span></em><span style="color:#666;--tt-darkmode-color: #666666">1</span><em><span style="color:#666;--tt-darkmode-color: #666666">i</span></em><span style="color:#666;--tt-darkmode-color: #666666">通過下一層的權重（</span><em><span style="color:#666;--tt-darkmode-color: #666666">w</span></em><span style="color:#666;--tt-darkmode-color: #666666">1</span><em><span style="color:#666;--tt-darkmode-color: #666666">j</span></em><span style="color:#666;--tt-darkmode-color: #666666">）和（</span><em><span style="color:#666;--tt-darkmode-color: #666666">w</span></em><span style="color:#666;--tt-darkmode-color: #666666">2</span><em><span style="color:#666;--tt-darkmode-color: #666666">j</span></em><span style="color:#666;--tt-darkmode-color: #666666">）來影響誤差，因此我們需要一種方法來計算</span><em><span style="color:#666;--tt-darkmode-color: #666666">w</span></em><span style="color:#666;--tt-darkmode-color: #666666">1</span><em><span style="color:#666;--tt-darkmode-color: #666666">i</span></em><span style="color:#666;--tt-darkmode-color: #666666">對誤差的貢獻，這個方法就是</span><strong>反向傳播</strong><span style="color:#666;--tt-darkmode-color: #666666">。</span></span></p><div class=pgc-img><img alt=有關自然語言處理的深度學習知識有哪些？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1da9f4fe14604fd3a7c5e0ff810e77b4><p class=pgc-img-caption></p></div><p style="text-align:center;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">圖5-6　包含隱藏權重的神經網絡</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">現在是時候停止使用“感知機”這個術語了，因為之後大家將改變每個神經元權重的更新方式。從這裡開始，我們提到的</span><strong>神經元</strong><span style="color:#666;--tt-darkmode-color: #666666">將更通用也更強大，它包含了感知機。在很多文獻中神經元也被稱為單元或節點，在大多數情況下，這些術語是可以互換的。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">所有類型的神經網絡都是由一組神經元和神經元之間的連接組成的。我們經常把它們組織成層級結構，不過這不是必需的。如果在神經網絡的結構中，將一個神經元的輸出作為另一個神經元的輸入，就意味著出現了</span><strong>隱藏</strong><span style="color:#666;--tt-darkmode-color: #666666">神經元或者隱藏層，而不再只是單純的</span><strong>輸入</strong><span style="color:#666;--tt-darkmode-color: #666666">層、</span><strong>輸出</strong><span style="color:#666;--tt-darkmode-color: #666666">層。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">圖5-7中展示的是一個</span><strong>全連接</strong><span style="color:#666;--tt-darkmode-color: #666666">網絡，圖中沒有展示出所有的連接，在全連接網絡中，每個輸入元素都與下一層的</span><strong>各個</strong><span style="color:#666;--tt-darkmode-color: #666666">神經元相連，每個連接都有相應的權重。因此，在一個以四維向量為輸入、有5個神經元的全連接神經網絡中，一共有20個權重（5個神經元各連接4個權重）。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">感知機的每個輸入都有一個權重，第二層神經元的權重不是分配給原始輸入的，而是分配給來自第一層的各個輸出。從這裡我們可以看到計算第一層權重對總體誤差的影響的難度。第一層權重對誤差的影響並不是只來自某個單獨權重，而是通過下一層中每個神經元的權重來產生的。雖然反向傳播算法本身的推導和數學細節非常有趣，但超出了本書的範圍，我們對此只做一個簡單的概述，使大家不至於對神經網絡這個黑盒一無所知。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">反向傳播是誤差反向傳播的縮寫，描述瞭如何根據輸入、輸出和期望值來更新權重。</span><strong>傳播</strong><span style="color:#666;--tt-darkmode-color: #666666">，或者說前向傳播，是指輸入數據通過網絡“向前”流動，並以此計算出輸入對應的輸出。要進行反向傳播，首先需要將感知機的激活函數更改為稍微複雜一點兒的函數。</span></span></p><div class=pgc-img><img alt=有關自然語言處理的深度學習知識有哪些？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4722a4cf42ba4f29940244c60e481198><p class=pgc-img-caption></p></div><p style="text-align:center;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">圖5-7　全連接神經網絡</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">到目前為止，大家一直在使用階躍函數作為人工神經元的</span><strong>激活函數</strong><span style="color:#666;--tt-darkmode-color: #666666">。但是接下來會發現，反向傳播需要一個非線性連續可微的激活函數[11]，如公式5-4中常用的sigmoid函數所示，現在每個神經元都會輸出</span><strong>介於</strong><span style="color:#666;--tt-darkmode-color: #666666">兩個值（如0和1）之間的值：</span></span></p><div class=pgc-img><img alt=有關自然語言處理的深度學習知識有哪些？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/18f0d3031f2f4633b11943e411c3c7c6><p class=pgc-img-caption></p></div><p style="text-align:center;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">公式5-4　sigmoid函數</span></span></p><p style="margin:8px 0"><br></p><hr><p><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666"><br></span></span></p><p style="margin:8px 0"><br></p><p style="margin:8px 0"><span style=font-size:.667em><strong>為什麼激活函數需是非線性的</strong></span></p><p style="margin:8px 0"><span style=font-size:.667em>因為需要讓神經元能夠模擬特徵向量和目標變量之間的非線性關係。如果神經元只是將輸入與權重相乘然後做加和，那麼輸出必然是輸入的線性函數，這個模型連最簡單的非線性關係都無法表示。</span></p><p style="margin:8px 0"><span style=font-size:.667em>之前使用的神經元閾值函數是一個非線性階躍函數。所以理論上只要有足夠多的神經元就可以用來訓練非線性關係模型。</span></p><p style="margin:8px 0"><span style=font-size:.667em>這就是非線性激活函數的優勢，它使神經網絡可以建立非線性關係模型。一個連續可微的非線性函數，如sigmoid，可以將誤差平滑地反向傳播到多層神經元上，從而加速訓練進程。sigmoid神經元的學習速度很快。</span></p><p style="margin:8px 0"><br></p><hr><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">還有許多其他的激活函數，如</span><strong>雙曲正切</strong><span style="color:#666;--tt-darkmode-color: #666666">函數和</span><strong>修正線性單元</strong><span style="color:#666;--tt-darkmode-color: #666666">函數，它們各有優劣，適用於不同的神經網絡結構，大家將在之後的章節中學習。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">為什麼要求可微呢？如果能夠計算出這個函數的導數，就能對函數中的各個變量求偏導數。這裡的關鍵是“各個變量”，這樣就能通過接收到的輸入來更新權重！</span></span></p><h1 class=pgc-h-arrow-right>7．求導</h1><p style="margin:0 0 8px"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">首先用</span><strong>平方誤差</strong><span style="color:#666;--tt-darkmode-color: #666666">作為代價函數來計算網絡的誤差，如公式5-5所示[12]：</span></span></p><div class=pgc-img><img alt=有關自然語言處理的深度學習知識有哪些？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9c5cb94b144142dc9c5db91d02f3280c><p class=pgc-img-caption></p></div><p style="text-align:center;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">公式5-5　均方誤差</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">然後利用微積分</span><strong>鏈式法則</strong><span style="color:#666;--tt-darkmode-color: #666666">計算複合函數的導數，如公式5-6所示。網絡本身只不過是函數的複合（點積之後的非線性激活函數）。</span></span></p><div class=pgc-img><img alt=有關自然語言處理的深度學習知識有哪些？ onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a1d5a4de64e64597a95fdc4f377dd041><p class=pgc-img-caption></p></div><p style="text-align:center;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">公式5-6　鏈式法則</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">接下來可以用這個公式計算每個神經元上激活函數的導數。通過這個方法可以計算出各個權重對最終誤差的貢獻，從而進行適當的調整。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">如果該層是輸出層，藉助於可微的激活函數，權重的更新比較簡單，對於第</span><em><span style="color:#666;--tt-darkmode-color: #666666">j</span></em><span style="color:#666;--tt-darkmode-color: #666666">個輸出，誤差的導數如下[13]：</span></span></p><div class=pgc-img><img alt=有關自然語言處理的深度學習知識有哪些？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0ba14cf35e31410f99eeec4bfe06a14f><p class=pgc-img-caption></p></div><p style="text-align:center;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">公式5-7　誤差導數</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">如果要更新隱藏層的權重，則會稍微複雜一點兒，如公式5-8所示：</span></span></p><div class=pgc-img><img alt=有關自然語言處理的深度學習知識有哪些？ onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8d6f120671dc4e1d841265c789679520><p class=pgc-img-caption></p></div><p style="text-align:center;margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">公式5-8　前一層的導數</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">在公式5-7中，函數</span><em><span style="color:#666;--tt-darkmode-color: #666666">f</span></em><span style="color:#666;--tt-darkmode-color: #666666">(</span><em><span style="color:#666;--tt-darkmode-color: #666666">x</span></em><span style="color:#666;--tt-darkmode-color: #666666">)表示實際結果向量，</span><em><span style="color:#666;--tt-darkmode-color: #666666">f</span></em><span style="color:#666;--tt-darkmode-color: #666666">(</span><em><span style="color:#666;--tt-darkmode-color: #666666">x</span></em><span style="color:#666;--tt-darkmode-color: #666666">)</span><em><span style="color:#666;--tt-darkmode-color: #666666">j</span></em><span style="color:#666;--tt-darkmode-color: #666666">表示該向量第</span><em><span style="color:#666;--tt-darkmode-color: #666666">j</span></em><span style="color:#666;--tt-darkmode-color: #666666">個位置上的值，</span><em><span style="color:#666;--tt-darkmode-color: #666666">yi</span></em><span style="color:#666;--tt-darkmode-color: #666666">、</span><em><span style="color:#666;--tt-darkmode-color: #666666">yj</span></em><span style="color:#666;--tt-darkmode-color: #666666">是倒數第二層第</span><em><span style="color:#666;--tt-darkmode-color: #666666">i</span></em><span style="color:#666;--tt-darkmode-color: #666666">個節點和輸出第</span><em><span style="color:#666;--tt-darkmode-color: #666666">j</span></em><span style="color:#666;--tt-darkmode-color: #666666">個節點的輸出，連接這兩個節點的權重為</span><em><span style="color:#666;--tt-darkmode-color: #666666">wij</span></em><span style="color:#666;--tt-darkmode-color: #666666">，誤差代價函數對</span><em><span style="color:#666;--tt-darkmode-color: #666666">wij</span></em><span style="color:#666;--tt-darkmode-color: #666666">求導的結果相當於用</span><em><span style="color:#666;--tt-darkmode-color: #666666">α</span></em><span style="color:#666;--tt-darkmode-color: #666666">（學習率）乘以前一層的輸出再乘以後一層代價函數的導數。公式5-8中</span><em><span style="color:#666;--tt-darkmode-color: #666666">δl</span></em><span style="color:#666;--tt-darkmode-color: #666666">表示</span><em><span style="color:#666;--tt-darkmode-color: #666666">L</span></em><span style="color:#666;--tt-darkmode-color: #666666">層第</span><em><span style="color:#666;--tt-darkmode-color: #666666">l</span></em><span style="color:#666;--tt-darkmode-color: #666666">個節點上的誤差項，前一層第</span><em><span style="color:#666;--tt-darkmode-color: #666666">j</span></em><span style="color:#666;--tt-darkmode-color: #666666">個節點到</span><em><span style="color:#666;--tt-darkmode-color: #666666">L</span></em><span style="color:#666;--tt-darkmode-color: #666666">層所有的節點進行加權求和[14]。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">重要的是要明確何時更新權重。在計算每一層中權重的更新時，需要依賴網絡在前向傳播中的當前狀態。一旦計算出誤差，我們就可以得到網絡中各個權重的更新值，但仍然需要回到網絡的起始節點才能去做更新。否則，如果在網絡末端更新權重，前面計算的導數將不再是對於本輸入的正確的梯度。另外，也可以將權重在每個訓練樣本上的變化值記錄下來，其間不做任何更新，等訓練結束後再一起更新，我們將在5.1.6節中討論這項內容。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">接下來將全部數據輸入網絡中進行訓練，得到每個輸入對應的誤差，然後將這些誤差反向傳播至每個權重，根據誤差的總體變化來更新每個權重。當網絡處理完全部的訓練數據後，誤差的反向傳播也隨之完成，我們將這個過程稱為神經網絡的一個訓練</span><strong>週期</strong><span style="color:#666;--tt-darkmode-color: #666666">（epoch）。我們可以將數據集一遍又一遍地輸入網絡來優化權重。但是要注意，網絡可能會對訓練集過擬合，導致對訓練集外部的新數據無法做出有效的預測。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">在公式5-7和公式5-8中，</span><em><span style="color:#666;--tt-darkmode-color: #666666">α</span></em><span style="color:#666;--tt-darkmode-color: #666666">表示</span><strong>學習率</strong><span style="color:#666;--tt-darkmode-color: #666666">。它決定了在一個訓練週期或一批數據中權重中誤差的修正量。通常在一個訓練週期內</span><em><span style="color:#666;--tt-darkmode-color: #666666">α</span></em><span style="color:#666;--tt-darkmode-color: #666666">保持不變，但也有一些複雜的訓練算法會對</span><em><span style="color:#666;--tt-darkmode-color: #666666">α</span></em><span style="color:#666;--tt-darkmode-color: #666666">進行自適應調整，以加快訓練速度並確保收斂。如果</span><em><span style="color:#666;--tt-darkmode-color: #666666">α</span></em><span style="color:#666;--tt-darkmode-color: #666666">過大，很可能會矯枉過正，使下一次誤差變得更大，導致權重離目標更遠。如果</span><em><span style="color:#666;--tt-darkmode-color: #666666">α</span></em><span style="color:#666;--tt-darkmode-color: #666666">設置太小會使模型收斂過慢，更糟糕的是，可能會陷入</span><strong>誤差曲面</strong><span style="color:#666;--tt-darkmode-color: #666666">的局部極小值。</span></span></p><p style="margin:8px 0"><span style=font-size:.833em><span style="color:#666;--tt-darkmode-color: #666666">本文摘自《</span>自然語言處理實戰 利用Python理解、分析和生成文本<span style="color:#666;--tt-darkmode-color: #666666">》</span></span></p><div class=pgc-img><img alt=有關自然語言處理的深度學習知識有哪些？ onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b28249174b7b464688bd1ad0d985c96b><p class=pgc-img-caption></p></div><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">在本書中，讀者不僅會學習這些系統的內部工作原理，還會學習相關的理論和實踐技能，並創建自己的算法或模型。基本計算機科學概念無縫地轉換為方法和實踐的堅實基礎。從一些久經考驗的經典方法（如TF-IDF）開始，再深入到NLP相關的深層神經網絡，作者帶領讀者對於自然語言處理的核心方法開啟了一段清晰的體驗之旅。</span></span></p><p style="margin:8px 0"><span style=font-size:.667em><span style="color:#666;--tt-darkmode-color: #666666">語言是人類建立共識的基礎。人們之間交流的不僅有事實，還有情感。通過語言，人們獲得了經驗領域之外的知識，並通過分享這些經驗來構建理解的過程。通過本書，大家將會深入理解自然語言處理技術的原理，有朝一日可能創建出能通過語言來了解人類的系統。</span></span></p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>有關</a></li><li><a>語言</a></li><li><a>處理</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/10c7d8fd.html alt=神州泰嶽：公司在自然語言處理領域的基礎技術研究和應用落地均走在行業前列 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/10c7d8fd.html title=神州泰嶽：公司在自然語言處理領域的基礎技術研究和應用落地均走在行業前列>神州泰嶽：公司在自然語言處理領域的基礎技術研究和應用落地均走在行業前列</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/45b722bf.html alt=第12屆自然語言處理和知識工程國際會議將在西華大學舉行 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/45b722bf.html title=第12屆自然語言處理和知識工程國際會議將在西華大學舉行>第12屆自然語言處理和知識工程國際會議將在西華大學舉行</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/49bb3bbd.html alt=第12屆自然語言處理與知識工程國際學術會議在西華大學舉行 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/4e62000034a58600d55e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/49bb3bbd.html title=第12屆自然語言處理與知識工程國際學術會議在西華大學舉行>第12屆自然語言處理與知識工程國際學術會議在西華大學舉行</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/91a0fd9b.html alt=自然語言處理中的遷移學習(上) class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RfRw76K9qI7Kdu style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/91a0fd9b.html title=自然語言處理中的遷移學習(上)>自然語言處理中的遷移學習(上)</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d3668904.html alt=自然語言處理（NLP）常用庫整理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/235e94cda81a4858a3000bb62b4f970d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d3668904.html title=自然語言處理（NLP）常用庫整理>自然語言處理（NLP）常用庫整理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e8ff9c12.html alt=有關城市汙水處理的問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/db1e3fe9c7504300875cc46349cbf1fe style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e8ff9c12.html title=有關城市汙水處理的問題>有關城市汙水處理的問題</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/eabb9fa9.html alt=你對自然語言處理了解多少呢？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/150674bcc0e44efcae3427c70ad2f072 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/eabb9fa9.html title=你對自然語言處理了解多少呢？>你對自然語言處理了解多少呢？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e0b5c472.html alt=自然語言處理中的深度學習：評析與展望 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3738e409cd4648ef9d28084a94faaade style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e0b5c472.html title=自然語言處理中的深度學習：評析與展望>自然語言處理中的深度學習：評析與展望</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/49d71ab7.html alt=自然語言處理中的語言模型簡介 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/0da5799ae4d94824b62b9e71c6e07aa3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/49d71ab7.html title=自然語言處理中的語言模型簡介>自然語言處理中的語言模型簡介</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/316cbcad.html alt=自然語言處理的十大應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/dea6cbd6fbef4e9c935b6f56cb9b0097 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/316cbcad.html title=自然語言處理的十大應用>自然語言處理的十大應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2070e90b.html alt=一文看懂自然語言處理-NLP（4個典型應用+5個難點+6個實現步驟） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d1504f3b2d614621bd4081a64ef145ca style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2070e90b.html title=一文看懂自然語言處理-NLP（4個典型應用+5個難點+6個實現步驟）>一文看懂自然語言處理-NLP（4個典型應用+5個難點+6個實現步驟）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/49c42cc2.html alt=人工智能之自然語言處理初探 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/S4bjUwAFhO20v style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/49c42cc2.html title=人工智能之自然語言處理初探>人工智能之自然語言處理初探</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ca1cc7d7.html alt=人工智能的研究熱點：自然語言處理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/5fdd13a7-6c6d-45d6-9fcd-2829793b5dd3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ca1cc7d7.html title=人工智能的研究熱點：自然語言處理>人工智能的研究熱點：自然語言處理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cb76338d.html alt=什麼是自然語言處理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/64bcc3b1fb8a4f9ca59d452035ca25cb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cb76338d.html title=什麼是自然語言處理>什麼是自然語言處理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1f73ce58.html alt=復旦大學黃萱菁：自然語言處理中的表示學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/77184e60d8e74b9da944a638e38aedfa style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1f73ce58.html title=復旦大學黃萱菁：自然語言處理中的表示學習>復旦大學黃萱菁：自然語言處理中的表示學習</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>