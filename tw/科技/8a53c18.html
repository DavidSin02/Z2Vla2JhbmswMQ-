<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>開源神經網絡框架Caffe2全介紹 | 极客快訊</title><meta property="og:title" content="開源神經網絡框架Caffe2全介紹 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/4d0500045936717d3255"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8a53c18.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8a53c18.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/8a53c18.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8a53c18.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8a53c18.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/8a53c18.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/8a53c18.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8a53c18.html><meta property="article:published_time" content="2020-10-29T21:00:26+08:00"><meta property="article:modified_time" content="2020-10-29T21:00:26+08:00"><meta name=Keywords content><meta name=description content="開源神經網絡框架Caffe2全介紹"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/8a53c18.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>開源神經網絡框架Caffe2全介紹</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>雷鋒網按：本文作者吳逸鳴，整理自作者在GTC China 2017大會上的演講，首發於作者的知乎文章，雷鋒網獲其授權發佈。</p><p>我個人認為這是一份很值得分享的資料，因為：</p><blockquote><ul><li><p>這應該是第一次使用全中文來講解Caffe2和FB的AI應用的演講</p></li><li><p>觀看這次演講不需要機器學習／神經網絡，甚至計算機科學的基礎。它適合每一個願意瞭解人工智能、神經網絡和Caffe2的人。</p></li><li><p>我準備了很久！（這才是主要原因哈哈哈，但第一次上臺還蠻緊張</p></li></ul></blockquote><p>在觀看視頻前你可能需要注意以下幾點：</p><blockquote><ul><li><p>該視頻所有權解釋權各種權全都歸<strong>英偉達</strong>所有</p></li><li><p><strong>此次演講只是談論了我自己的一些看法，和FB無關</strong></p></li><li><p>我的演講稿和視頻裡說的可能有一些出入。有的地方為了更好地讓大家理解，我做了自己的解釋。演講稿中有可能有的地方我懶得寫但是我在演講中說了。有的地方是因為我理解不夠直接是錯誤的。還有各種錯別字。這些都歡迎大家指正，互相學習。</p></li></ul></blockquote><p></p><h3>以下是演講原文：</h3><p>大家好 我叫吳逸鳴。我來自Facebook的Caffe2組。</p><p>演講開始前，想先給大家看一個demo。這是一個自動把圖片和視頻轉換成大師美術風格的神經網絡算法。在大約兩年前剛被髮表的時候，在服務器上處理單張照片需要秒級別的時間。到現在，我們已經成功做到實時的視頻風格轉換。不僅實時，在我們同事的努力下，我們還做到了在手機移動端本地實施演算。這相較於之前，已經有好幾個量級的效率提升。</p><p>今天，我想給大家介紹一下讓這一切變成現實，以及將各種AI算法部署到產品中的神經網絡框架：Caffe2。</p><p><strong>Caffe2是：</strong></p><blockquote><ul><li><p>一個輕量化的深度學習算法框架</p></li><li><p>caffe2 主要為產品級別的深度學習算法設計</p></li><li><p>為移動端實時計算做了很多優化</p></li><li><p>同時支持大規模的分佈式計算</p></li><li><p>Caffe2是一個跨平臺的框架</p><p>支持移動端iOS, Android, 服務器端Linux, Mac, Windows, 甚至一些物聯網設備如Raspberry Pi, NVIDIA Jetson TX2等平臺部署</p></li></ul></blockquote><p>說到這裡，我們需要暫停一下。深度學習框架是什麼？若放在五年前，你甚至很難用這六個字造句。Caffe2是什麼？你們會不會給咖啡拉花？這些問題別說外行，很多科技企業的內行包括一些我的學長、別的部門同事也會有這些疑惑。</p><p>簡單來說，從我個人的理解，深度學習，作為機器學習的一個分支，是一個尋找理想中的函數fx的過程。這個函數代表了從數據輸入x到期待輸出y的某種映射。在深度學習裡， 這個函數是多層有疊加的。在這裡你的輸入x可以是一些圖片，可以是音頻，可以是一些高緯度的向量用。函數的輸出y可以是推薦系統的排位、另一種語言的翻譯或者是無人車對下一秒操作的決策。</p><p>深度學習，和別的AI的算法，要為社會創造價值，還得落到產品中去。在工業界，我們訓練和部署深度學習算法的時候，通常有以下幾個環節</p><blockquote><ul><li><p>你有數據</p></li><li><p>你有模型</p></li><li><p>你想要找到那個神器的函數fx。這個不斷嘗試和逼進的過程，我們稱為訓練</p></li><li><p>你可能需要在移動端／服務器端／物聯網設備／嵌入式系統上部署你的神經網絡算法</p></li></ul></blockquote><p>那麼Caffe2作為一個神經網絡框架，為你提供了模型搭建、訓練、和跨平臺的部署。簡而言之，全包辦了。</p><p>在設計開發Caffe2中，我們認為一個好的經的起業界規模考驗的神經網絡框架需要具備：</p><blockquote><ul><li><p>支持最新的計算模型</p></li><li><p>分佈式訓練</p></li><li><p>高模塊化</p></li><li><p>跨平臺的支持</p></li><li><p>高效率</p></li></ul></blockquote><p>今天我想大家展示一些Caffe2的例子。談一下在FB我們如何用Caffe2來搭建我們的AI產品。</p><p>在FB，我們使用Caffe2來搭建全套的AI產品和功能，其中包括</p><blockquote><ul><li><p>計算機視覺相關</p></li><li><p>機器翻譯</p></li><li><p>語音識別</p></li><li><p>推薦系統</p></li></ul></blockquote><p>首先是移動端的Caffe2</p><p>從設計之初，Caffe2就十分重視在移動端部署神經網絡。我們一直在優化Caffe2在移動端的性能，並保證我們能支持各類移動計算框架。現在Caffe2主要為15年以後的機型優化，但也支持13年以後的機型。</p><p>為了在手機上這裡是一個使用OpenGL在iOS和安卓設備上加速的例子。通過我們在移動端的努力，原本在CPU上每秒只能處理4幀的算法，利用手機端的GPU，我們實現了24+fps每秒的效果，實現了實時計算修改。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4d0500045936717d3255><p>我們和高通合作一起在搭載了高通芯片的移動端設備上加速神經網絡。這是一個移動端圖像識別的例子，可以看到在移動端，不僅是神經網絡風格變換算法，圖像分類算法也可以做到超過實時的演算。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4d070000abee94e23783><p>在這些移動端的樣例背後是我們在各種設備上的優化。在蘋果設備上，我們是第一個完全把Metal，也就是iOS的GPU API完全融合到後端的框架。如果你是一個移動開發者，準備好自己的模型後，通過簡單幾行調用，你就可以讓你的神經網絡在蘋果設備上用GPU運行起來。</p><p>使用Caffe2移動端的GPU實現可以給你的網絡帶來實質性的加速和能耗上的優化，這一切都是開源的。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4d0800002049a20c5116><p>在安卓設備上 我們用OpenGL來使用GPU加速神經網絡的執行。這之中我們有一整套基於OpenGL的GPU底層運算實現。CPU只需調度，無需處理數據。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4d0300081cd0b9d07355><p>在部署產品的時候，有時用戶終端設備只有CPU的硬件支持。我們對arm架構的CPU有Neon(一個並發指令集的拓展)的實現。我們也維護開發調用自己的開源CPU高性能運算庫NNPack。我們也會做Caffe2編譯文件的壓縮來保證把AI添加到你的產品不會成為空間上的負擔。</p><p>總而言之，Caffe2提供了從入門機到旗艦機的一整套移動部署解決方案。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4d080000204ac3f221c6><p>另一個我想分享的Caffe2案例是我們的機器翻譯系統。</p><p>在我們的產品中，我們有來自世界各地的超過20億用戶。為他們提供高質量的翻譯，是一個一個很複雜的問題。左邊的圖上是一個從土耳其語翻譯到英文的例子。上半部分翻譯的不好，在英語裡是不流暢的。下半部分翻譯的好。可見翻譯的質量、支持語言類型的數量至關重要。在FB，如果中文到英文和英文到中文算兩種翻譯的話，每天我們需要提供2000多種翻譯。處理45億條翻譯請求。今天，我們可以很驕傲地說，這一切底下的算法和服務，都是由Caffe2支持的。</p><p>在這個項目中我們使用了seq2seq LSTM model with attention。這是現在業界主流的翻譯模型。</p><p>我們使用Caffe2做大規模的訓練，並在GPU/CPU上都對相應的計算做了優化。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4d0300081cd1e9e9b669><p>在這些優化中，最值得一提的是我們對循環神經網絡的內存優化。循環神經網絡的單元，英文裡稱之為Cell，往往需要把自己的輸出，當作自己的輸入，循環執行。所以，我們稱之為循環。在真實實現的過程中，這就好比把同樣的單元展開很多遍。這種計算的模式，使得循環神經網絡的計算量很大。要處理2000+種翻譯方向，計算量更是有量級上的增長。</p><p>優化循環網絡的計算顯得極為重要。Caffe2為循環神經網絡提供了：</p><blockquote><ul><li><p>反向運算時的展開單元內存重用</p></li><li><p>替換內存中部分參數的空間，當需要的時候再重新計算參數</p></li><li><p>專門正向推導模式提供進一步的內存空間優化</p></li><li><p>對於多層的循環神經網絡單元，他們展開後會在計算圖中形成一個運算單元的陣列。我們儘可能地依照對角線的方式執行這些運算來儘可能達到最大的並行化。</p></li></ul></blockquote><p>在這些優化之後，我們把我們產品中的循環神經網絡效率提高了2.5倍。達到了20億用戶級別的訓練和部署要求。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4d040004c70c0ccad640><p>更值得興奮的是，我們從上個月開始開源了我們的循環神經網絡支持。開源社區和在座的每一位也可以開始用Caffe2來優化你的循環神經網絡。從框架本身，調用Caffe2的循環網絡引擎對性能幾乎沒有影響。Caffe2支持主流的循環神經網絡單元包括LSTM，Multiplicative Integration LSTM（這是一種在單元裡面加上乘法的更復雜的LSTM單元）和帶attention的模型等。之前我提到的內存和正向推導時的優化，都已經開源。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4d0300081cd28c346a9e><p>和翻譯相關的一個例子是語音識別，在fb，和語音相關的應用有以下這些。</p><p><strong>1.自動語音識別</strong></p><p>顧名思義，這項任務中輸入是一段音頻，輸出可以是對應的文字，可以是一連串的音標等。Caffe2提供了自動語音識別中非常常用的雙向LSTM單元，Caffe2支持CE和 CTC這兩種算法，並且對他們做了速度上的優化。這兩種是現在ASR，自動語音識別中主流的輸出和真實標註（真實句子）之間差異性的方法。</p><p><strong>2.語音合成</strong></p><p>剛才的自動語音識別是從音頻到文字。語音合成，是從文字，合成一段聽起來像人講話的音頻。一般語音合成現在有用多層神經網絡實現的，有用LSTM實現的、有用卷積神經網絡實現的。我們生成的模型回去模擬聲音的音長，頻率，峰值和聲音的間歇性。</p><p>我們還有一些別的聲音上的應用，在這裡由於時間的原因，不做過多的展開了。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4d040004c70d0bf1213a><p>那麼，到現在，我們已經通過在FB的例子講述了Caffe2在手機端的部署，包括移動端CPU，移動端GPU的優化，這樣來支持從低到高各種類型的機型。</p><p>我們介紹了我們在自然語言處理領域， 也就是我們機器翻譯的成果。Caffe2對循環神經網絡做了許多優化，使得我們訓練的模型經受起了45億級別的單日訪問量的考驗</p><p>我們也介紹了在語音領域，包括ASR、TTS實現的各種模型，說明Caffe2對於不同模型都有良好的支持。</p><p>但是接下來，我想給大家分享的，是任何一個工業級的機器學習框架都需要解決的應用場景——分佈式訓練。</p><p>這項工作的名稱，叫做 ImageNet in 1 hr，中文叫做1小時完成ImageNet挑戰。這項工作是由我們以下的同事一同完成的。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4d05000459379c8acbee><p>ImageNet是一個由斯坦佛大學主導的大型開源圖像數據集。ImageNet挑戰是一個經典的計算機視覺挑戰。在這項任務中，我們的模型需要做圖像分類，也就是說，給它一張圖片，他需要告訴我這是貓還是狗還是飛機還是船還是汽車。</p><p>如果羅列一些簡單的數據：</p><blockquote><p>1. 使用當前主流的圖像識別模型，單張圖片大概要做80億次浮點數計算</p><p>2. 在ImageNet數據集中大約有120萬張圖像</p><p>3. 為了訓練出一個現在學界認可的模型，我們大約需要把這120萬張圖片給我們的模型看100遍</p><p>4. 如果做一下簡單的計算，我們完成這項任務需要用到1個exa個浮點數計算。exa有多少呢？大約是10的18次方，是Giga往上走3級。這個任務的計算量，真的很大。</p></blockquote><p>在這項任務裡，我們把本來需要好幾天的訓練量，用1個小時的時間訓練完了。而且我們用的是全開源的軟件框架，深度學習用Caffe2，網絡調度同步用我們開源的gloo，硬件就是你也能買得到的英偉達的GPU。</p><p>好，那我們來自習地介紹一下這項任務。我們要在ImgeNet-1k數據集上做圖像分類。這其中ImageNet-1k 的圖像有一千種類別。每一張圖片只有一個類別訓練數據集，像我之前說的，有120萬張圖片。驗證集中有5萬張圖片。我們在機器學習中，我們會單獨拿出一些數據作為驗證集，來調整一些模型的參數，或者監控一下模型訓練的好不好。我們在訓練的過程中也需要跑這5萬張圖片的正向推導。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4d040004c70f8fd7dfe5><p>輸入的格式是NCHW -》輸出是1 hot label的形式。現在學術屆在這個數據集上最領先的是我們FB的同事 kaiming大神的工作：rextnet，大概達到了22%的訓練錯誤率。也就是說，給模型看100張來自ImgeNet訓練集的圖片，他期望上能說對其中78張圖片的類別。相較於2012年AlexNet橫空出世時的36-40的top1 error rate，我們深度學習的研究者已經有了很大的進步。</p><p>在這項任務中，我們把單次處理的數據量上升到8192張圖片。這在機器學習的層面會帶來一些挑戰，在接下來的ppt中我會提到。另外，我們在單次訓練中我們同時使用了256塊GPU，搭建這樣一套系統也充滿了挑戰。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4d02000d5aab2f414866><p>在訓練神經網絡的過程中我們關注訓練上的錯誤率。這裡是一張訓練錯誤率隨著訓練的數據增加而下降的圖。我們可以看到y軸是訓練的錯誤率，這個數值越低說明我們的模型能夠在訓練集數據上分類得更準確。X軸是數據的期的數量。一期（Epoch）數據指的是我們的模型把訓練數據完全過一遍，在imagenet-1k的任務中大概100萬張圖片。我們可以看到，橘黃色的線是一個標準的訓練resnet50曲線。大概經過了90期數據後，訓練精度達到了20出頭。這條橘黃的線可以算一個基準，我們的目標就是去逼近這條黃線。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4d060000bbc7ec238973><p>因為我們期待能夠在分佈式的機群上做數據並行的訓練，意味著我們單次處理的數據量要遠大於標準的256張圖片。那麼假設我們直接把單次處理的數據量增加到8192張，會發生什麼呢？我們就得到了藍色的訓練曲線。在大約90期數據後，模型的訓練錯誤率大約停在41%左右。這兩者之間的差距很大。藍色的訓練曲線完全不達標。（但值得一提的是，在一個1000類的分類任務中，如果我們只有一個隨機分類器，我們的訓練錯誤率大概在99.9%左右。這個模型雖然和我們心中標準差了很多，但他顯然是具備一點關於這些圖像上分類的知識的）</p><p>之前有一些工作，包括李沐學長的文章裡，都提出過通過同比例放大學習效率（learning rate）來應對大批量的數據的想法。我們也嘗試了相應的做法，得到了新的藍線。新的訓練參數中，因為我們把同批數據量從256放大了32倍到8192，所以我們把學習效率從原來的0.1放大到3.2 。可以看到訓練錯誤率有了很不錯的提升。從之前的41左右的錯誤率下降到24.84左右。這個結果雖好，但和我們期待的目標還有大約1%的差距。不熟悉圖像分類的人可能會問，這1%的差距也追求，會不會太苛刻了？其實在1000類的圖像分類裡，如果把分類結果打印出來，這1%的差距有時在有些類別上會顯得特別明顯。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4d0500045938db7973f5><p>那麼經過我們同事的一番探索，在一番努力後，我們找到了一種循序增大學習效率的辦法，使得分佈式的訓練曲線和我們目標的標準訓練曲線收斂在一樣的訓練錯誤率上。我們總結了一系列在大規模分佈式上訓練神經網絡的經驗，更多的內容可以到我們的論文中查看。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4d070000abf162cc1be2><p>說完了機器學習理論上的困難和解決方案後，我們來說一說這項工作中在系統方面的挑戰。之前提到過，在一塊英偉達Tesla P100上ResNet-50的訓練數據吞吐量大約是230張每秒。</p><p>為了訓練出一個比較理想的模型，之前的收斂圖表也顯示了，我們大概需要把整個數據集處理100遍。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4d080000204f3699190a><p>那麼120萬張圖片過100次是1億2千萬張圖片，按照230張圖片每秒的速度，訓練一個resnet-50模型需要6天。</p><p>這時我們就不得不開始使用分佈式訓練。也就是用大規模機群來完成訓練算法。</p><p>這裡給大家展示了一張分佈式訓練的簡易說明圖。每一個虛線的方框代表一張GPU或者一臺機器。方框之間用PCIe或者網線連接通訊。在這項工作裡我們使用數據並行的訓練方法。數據並行指的是每個機器／GPU同時處理不同的數據，並且在完成後做運算結果的總和同步。在這張圖裡，每個batch代表了小批的數據。我們把batch 1到k都放在k個機器上分別處理。在真實的例子裡，一小批數據有32張圖片，k=256，這樣就做到一次處理8192張圖片，也就是我們之前理論得到的單次大批數據量。說完了圖片中的輸入Input，我們把輸入放到方程中，也就是我們用Caffe2搭建的ResNet-50模型。注意在這k臺機器上每一臺都有函數參數的一套拷貝。我們同時在這些GPU／機器上調用模型方程，得到分類器的輸出，以及關於真實數據標註的邏輯迴歸損失。在這張圖裡就是結果Result。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4d070000abf2a5fa725e><p>上一部我在每臺機器上得到了模型在所有數據上的邏輯迴歸損失。那麼利用Caffe2提供的自動求導功能，我們可以針對本地的數據算出我們的模型在這些數據上的本地導數， local grad。那麼下一步呢？下一步如果我們像普通的機器學習訓練算法直接把導數加到本地的參數上，那麼我們就變成了同時訓練k個模型了。這和我們的目標不符合，我們的目標是訓練一個好的模型，這個模型需要把所有數據看100遍。所以我們需要先同步(synchronize)所有機器上算出來的導數，得到一個統一的導數，再作用到每分本地參數拷貝上。</p><p>在數據並行的分佈式訓練中，同步一直是最重要的一環。在我們的這次訓練中，我們使用的是一種叫做全相加的同步方式。也就是把每個機器算出來的導數，全都加在一起。這裡用P來代表，也可以用G，指代更加清楚。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/4d0800002051d32d1138><p>一個信息：ResNet-50一共有2千5百萬個參數需要算導數並同步。那麼在計算機裡，大約是100MB的大小。這在今天這個時代，聽起來不是什麼海量數據。</p><p>這裡需要提出一個概念：同步效率。這個指標代表了同步給系統帶來時間延遲有多少。</p><p>他的分子上是兩項時間的相加。Tf代表在神經網絡正向運算的時候消耗的時間，Tb代表反向運算的時間。這兩項時間相加代表了在假象的完美的0同步時間系統中訓練需要的時間。分母上第一項依然是正向時間Tf，第二項是反向時間和同步時間的最大值。同步時間由我們需要傳輸的數據量M除以系統帶寬B得到。之所以在返鄉時間和同步時間取最大值是因為神經網絡參數反向運算是有序的。在算完了反向第一層的參數導數後我們就能開始同步。</p><p>理論上在這個效率是100%的時候，同步不會對訓練系統帶來任何延遲。效率越高，同步給系統帶來的延遲最少。</p><p>如果我們把實際的數據帶入到上一頁的公式中。在Tesla P100上resnet單次處理32張圖片，正向時間大概是0.06秒，60ms，反向時間120ms。那麼如果我們這個系統想要達到50%的同步效率，也就是理論上起碼一半的時間在做主要的訓練工作，我們必須得在200ms中完成256臺機器的本地導數全相加同步。這時候，100MB看起來就很多了。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/4d040004c71346e5e8c1><p>針對同步我們做了很多優化。比如我們做了多層次的同步。我們會在第一步先在機器本地把GPU們的數據做一次全相加。在圖裡比如左上方，就是把四張GPU卡的數據合併。第二步我們通過互相拷貝做機器間的全相加同步。第三步我們把計算出的統一數據再在機器內部分發到所有GPU上。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4d02000d5aaf43f899f9><p>還有一個值得一提的優化。我們研究了resnet中出現的參數的大小。 這張圖是一張resnet中參數大小及其數量的圖表。橫軸是參數大小，縱軸是參數的數量。我們可以看到左半邊是小參數，右半邊是大參數。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4d050004593b89ec3fbe><p>我們發現大的參數和小的參數在同步的時候表現出不同的性質。小參數我們稱為latency bound, 延遲限制的參數。這類參數的同步過程中，同步的時間很短，主要的時間在等待別的機器的運算結果。針對這類參數，我們實現了樹形的參數同步算法，能夠最快地同步這些參數。大的參數通常是bandwidth bound，帶寬限制的參數。這類參數的同步時間很大。我們針對這類參數實現了環狀的同步算法。儘可能地減少同步次數並儘早開始同步過程，更充分利用帶寬。</p><p>在這項工作中我們還做了很多別的如數據讀入，GPU通訊的優化。我們也開源了我們的分佈式訓練通訊框架Gloo。由於時間的原因，在這裡我們不能更多展開了。有興趣的朋友可以來參考我們的論文。</p><p>今天的演講講了很多，總結一下。</p><p>Caffe2是一個工業級的高拓展性高性能的神經網絡框架。在CPU端，我們調用我們開發的高性能CPU庫NNPACK。在移動端，我們有包括iOS Metal，OpenGL等一套底層實現。在分佈式訓練端，我們可以加載分佈式通訊庫Gloo並完成大規模分佈式訓練。另外，Caffe2完全開源，大家可以添加自己想要的更快的訂製底層實現。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4d040004c7161ec1b2b0><p>在開發Caffe2的過程中，我們經常被問到這個問題：你們和原來的Caffe有什麼區別？總結下來有以下這些區別。</p><img alt=開源神經網絡框架Caffe2全介紹 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/4d040004c717c4ea5817><p>1.Caffe2有更好的規模拓展性。今天的演講裡也給大家加少了很多大規模訓練的例子，比如翻譯，比如imagenet-1k in an hour。這些大規模訓練的例子都已經在Facebook級別的數據量和系統上經過了實戰檢驗。</p><p>2.Caffe2對手機端移動部署神經網絡有一整套支持。比如今天我們提到的手機端實時風格變換。這是原來的Caffe做不到的</p><p>3.是模塊化。Caffe2是一個高模塊化的神經網絡框架。我今天也展示了我們和Gloo，和NNPACK，和Metal等一系列其他代碼／業務邏輯的整合樣例。Caffe2可以更好的融入到業務邏輯中去。</p><p>總而言之，Caffe2是一個跨平臺的新型工業級神經網絡框架。我們在移動端，服務器端，物聯網設備，嵌入式系統都能部署Caffe2訓練的模型。希望在不久的將來，Caffe2可以幫助大家在各種各樣的設備上部署新的人工智能算法。</p><p>（完）</p><p><strong>AI研習社注：</strong>文中插圖截取自作者在GTC China 2017大會演講視頻，點擊鏈接即可觀看。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>開源</a></li><li><a>神經</a></li><li><a>網絡</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html alt=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/65c4000bda98898dcdbb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html title=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼>谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2bc1496a.html alt=為了更好的深度神經網絡視覺，只需添加反饋（循環） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/17fccfd7096d44eeb3921bbd0dc29a13 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2bc1496a.html title=為了更好的深度神經網絡視覺，只需添加反饋（循環）>為了更好的深度神經網絡視覺，只需添加反饋（循環）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fd4c22a3.html alt=你還不知道神經網絡是啥？十分鐘教你跟上人工智能熱潮 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/470f0001d893b2ad09e2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fd4c22a3.html title=你還不知道神經網絡是啥？十分鐘教你跟上人工智能熱潮>你還不知道神經網絡是啥？十分鐘教你跟上人工智能熱潮</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5d2a6211.html alt=神經網絡與圖靈機的複雜度博弈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/4af200040ff1f5233c1c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5d2a6211.html title=神經網絡與圖靈機的複雜度博弈>神經網絡與圖靈機的複雜度博弈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cc9d1be9.html alt=基於二維材料、用於人工神經網絡的高密度憶阻陣列的晶圓級集成 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/3c0b503678da4b15be05f6f56c0d213f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cc9d1be9.html title=基於二維材料、用於人工神經網絡的高密度憶阻陣列的晶圓級集成>基於二維材料、用於人工神經網絡的高密度憶阻陣列的晶圓級集成</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/6062a4c0.html alt=BP神經網絡的線性本質的理解和剖析-卷積小白的隨機世界 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/6d474536ff3d4b1fba0cbfc85968ff6f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/6062a4c0.html title=BP神經網絡的線性本質的理解和剖析-卷積小白的隨機世界>BP神經網絡的線性本質的理解和剖析-卷積小白的隨機世界</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/f149efd9.html alt=用於調整深度神經網絡的簡單參考指南 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/15379529924702cde52ac04 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/f149efd9.html title=用於調整深度神經網絡的簡單參考指南>用於調整深度神經網絡的簡單參考指南</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0396dba3.html alt=貝葉斯神經網絡(系列)：第二篇 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RKYlnth9DPo8ac style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0396dba3.html title=貝葉斯神經網絡(系列)：第二篇>貝葉斯神經網絡(系列)：第二篇</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f406d4a3.html alt=七大開源網絡監控工具系統分析 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/cd61fff3f6de44f38b9eb7330f46aba7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f406d4a3.html title=七大開源網絡監控工具系統分析>七大開源網絡監控工具系統分析</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a4bbdd29.html alt=針對深度神經網絡的簡單黑盒對抗攻擊 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/b9ec712cd33442338496141ebfcecb45 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a4bbdd29.html title=針對深度神經網絡的簡單黑盒對抗攻擊>針對深度神經網絡的簡單黑盒對抗攻擊</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cafcc06.html alt=模式識別與神經網絡的發展 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1523254283784d3d276a90f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cafcc06.html title=模式識別與神經網絡的發展>模式識別與神經網絡的發展</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fcf9e89.html alt=BP神經網絡學習筆記 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/fc5cec456c184c48b1ee22a233b9ee0b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fcf9e89.html title=BP神經網絡學習筆記>BP神經網絡學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d7196c1.html alt=手工打造神經網絡：透視分析 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6ee200033390f3f6b2ca style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d7196c1.html title=手工打造神經網絡：透視分析>手工打造神經網絡：透視分析</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9f3924a.html alt=機器學習：神經網絡學習之多層前饋神經網絡（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a0a4cd0f7d9244a6a12da3c0af6893a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9f3924a.html title=機器學習：神經網絡學習之多層前饋神經網絡（一）>機器學習：神經網絡學習之多層前饋神經網絡（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/556321d.html alt=機器學習：神經網絡學習之多層前饋神經網絡（二） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/2d53a815-ab09-4da3-94a2-5b6843366e3a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/556321d.html title=機器學習：神經網絡學習之多層前饋神經網絡（二）>機器學習：神經網絡學習之多層前饋神經網絡（二）</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>