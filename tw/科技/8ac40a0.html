<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選 | 极客快訊</title><meta property="og:title" content="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/Rlqx6xj3cbnsKY"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8ac40a0.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8ac40a0.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/8ac40a0.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8ac40a0.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8ac40a0.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/8ac40a0.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/8ac40a0.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8ac40a0.html><meta property="article:published_time" content="2020-10-29T20:52:24+08:00"><meta property="article:modified_time" content="2020-10-29T20:52:24+08:00"><meta name=Keywords content><meta name=description content="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/8ac40a0.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>作者 | Eastmount</p><p>來源 | CSDN博文精選</p><p><strong class=highlight-text toutiao-origin=span>(*點擊閱讀原文，查看作者更多精彩文章）</strong></p><p>本篇文章將分享gensim詞向量Word2Vec安裝、基礎用法，並實現《慶餘年》中文短文本相似度計算及多個案例。本專欄主要結合作者之前的博客、AI經驗和相關文章及論文介紹，後面隨著深入會講解更多的Python人工智能案例及應用。</p><p>基礎性文章，希望對您有所幫助，如果文章中存在錯誤或不足之處，還請海涵~作者作為人工智能的菜鳥，希望大家能與我在這一筆一劃的博客中成長起來。寫了這麼多年博客，嘗試第一個付費專欄，但更多博客尤其基礎性文章，還是會繼續免費分享，但該專欄也會用心撰寫，望對得起讀者，共勉！</p><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rlqx6xj3cbnsKY><p>https://github.com/eastmountyxz/AI-for-TensorFlow</p><p><strong>文章目錄</strong></p><p>一、Word2Vec原理</p><ul><li><p>統計語言模型</p></li><li><p>神經網絡概率語言模型</p></li><li><p>詞向量</p></li><li><p>Word2vec</p></li></ul><p>二、Word2Vec安裝及入門</p><ul><li><p>安裝</p></li><li><p>基礎用法</p></li></ul><p>三、Word2Vec+DBSCAN短文本聚類及可視化</p><p>四、Word2Vec計算《慶餘年》中文短文本相似度</p><p>五、Word2Vec寫詩詳解</p><p>六、總結</p><p><strong class=highlight-text toutiao-origin=span>一、Word2Vec原理</strong></p><p>作者第一次接觸Word2Vec是2015年左右，那時候LDA+Word2Vec系列的論文都比較火，現在圖書情報領域應用也挺多的，而自然語言處理領域BiLSTM+CNN+Attention相對流行。這個系列專欄不只是講解TensorFlow知識，也會圍繞NLP和AI前沿技術及論文、圖像識別、語義識別、數據挖掘、惡意代碼識別、情報分析等案例進行補充，本文就帶領大家走進Word2Vec的世界，希望能幫助到大家！</p><p>參考前文：word2vec詞向量訓練及中文文本相似度計算（https://blog.csdn.net/eastmount/article/details/50637476）</p><p><strong class=highlight-text toutiao-origin=span>1.統計語言模型</strong></p><p>統計語言模型的一般形式是給定已知的一組詞，求解下一個詞的條件概率。形式如下：</p><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Rlqx6yPDNqQPXD><p>統計語言模型的一般形式直觀、準確，n-gram模型中假設在不改變詞語在上下文中的順序前提下，距離相近的詞語關係越近，距離較遠的關聯度越遠，當距離足夠遠時，詞語之間則沒有關聯度。</p><p>但該模型沒有完全利用語料的信息：</p><p>(1) 沒有考慮距離更遠的詞語與當前詞的關係，即超出範圍n的詞被忽略了，而這兩者很可能有關係的。</p><p>例如，“華盛頓是美國的首都”是當前語句，隔了大於n個詞的地方又出現了“北京是中國的首都”，在n元模型中“華盛頓”和“北京”是沒有關係的，然而這兩個句子卻隱含了語法及語義關係，即”華盛頓“和“北京”都是名詞，並且分別是美國和中國的首都。</p><p>(2) 忽略了詞語之間的相似性，即上述模型無法考慮詞語的語法關係。</p><p>例如，語料中的“魚在水中游”應該能夠幫助我們產生“馬在草原上跑”這樣的句子，因為兩個句子中“魚”和“馬”、“水”和“草原”、“遊”和“跑”、“中”和“上”具有相同的語法特性。</p><p>而在神經網絡概率語言模型中，這兩種信息將充分利用到。</p><p><strong class=highlight-text toutiao-origin=span>2.神經網絡概率語言模型</strong></p><p>神經網絡概率語言模型是一種新興的自然語言處理算法，該模型通過學習訓練語料獲取詞向量和概率密度函數，詞向量是多維實數向量，向量中包含了自然語言中的語義和語法關係，詞向量之間餘弦距離的大小代表了詞語之間關係的遠近，詞向量的加減運算則是計算機在"遣詞造句"。</p><p>神經網絡概率語言模型經歷了很長的發展階段，由Bengio等人2003年提出的神經網絡語言模型NNLM（Neural network language model）最為知名，以後的發展工作都參照此模型進行。歷經十餘年的研究，神經網絡概率語言模型有了很大發展。如今在架構方面有比NNLM更簡單的CBOW模型、Skip-gram模型；其次在訓練方面，出現了Hierarchical Softmax算法、負採樣算法（Negative Sampling），以及為了減小頻繁詞對結果準確性和訓練速度的影響而引入的欠採樣（Subsumpling）技術。</p><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rlqx6ysCFeLMnr><p>上圖是基於三層神經網絡的自然語言估計模型NNLM(Neural Network Language Model)。NNLM可以計算某一個上下文的下一個詞為wi的概率，即(wi=i|context)，詞向量是其訓練的副產物，NNLM根據語料庫C生成對應的詞彙表V。</p><p>近年來，神經網絡概率語言模型發展迅速，Word2vec是最新技術理論的合集。Word2vec是Google公司在2013年開放的一款用於訓練詞向量的軟件工具。所以，在講述word2vec之前，先給大家介紹詞向量的概念。</p><p><strong class=highlight-text toutiao-origin=span>3.詞向量</strong></p><p>這裡推薦licstar大神的NLP文章：Deep Learning in NLP （一）詞向量和語言模型（http://licstar.net/archives/328）</p><blockquote><div><p>正如作者所說：Deep Learning 算法已經在圖像和音頻領域取得了驚人的成果，但是在 NLP 領域中尚未見到如此激動人心的結果。有一種說法是，語言（詞、句子、篇章等）屬於人類認知過程中產生的高層認知抽象實體，而語音和圖像屬於較為底層的原始輸入信號，所以後兩者更適合做deep learning來學習特徵。</p><p>但是將詞用“詞向量”的方式表示可謂是將Deep Learning算法引入NLP領域的一個核心技術。自然語言理解問題轉化為機器學習問題的第一步都是通過一種方法把這些符號數學化。</p></div></blockquote><p>詞向量具有良好的語義特性，是表示詞語特徵的常用方式。詞向量的每一維的值代表一個具有一定的語義和語法上解釋的特徵。故可以將詞向量的每一維稱為一個詞語特徵。詞向量用Distributed Representation表示，一種低維實數向量。</p><p>例如，NLP中最直觀、最常用的詞表示方法是One-hot Representation。每個詞用一個很長的向量表示，向量的維度表示詞表大小，絕大多數是0，只有一個維度是1，代表當前詞。“話筒”表示為 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 …] 即從0開始話筒記為3。</p><p>但這種One-hot Representation採用稀疏矩陣的方式表示詞，在解決某些任務時會造成維數災難，而使用低維的詞向量就很好的解決了該問題。同時從實踐上看，高維的特徵如果要套用Deep Learning，其複雜度幾乎是難以接受的，因此低維的詞向量在這裡也飽受追捧。Distributed Representation低維實數向量，如：[0.792, −0.177, −0.107, 0.109, −0.542, …]。它讓相似或相關的詞在距離上更加接近。</p><p>總之，Distributed Representation是一個稠密、低維的實數限量，它的每一維表示詞語的一個潛在特徵，該特徵捕獲了有用的句法和語義特徵。其特點是將詞語的不同句法和語義特徵分佈到它的每一個維度上去表示。</p><p><strong class=highlight-text toutiao-origin=span>4.Word2vec</strong></p><p>Word2vec是Google公司在2013年開放的一款用於訓練詞向量的軟件工具。它根據給定的語料庫，通過優化後的訓練模型快速有效的將一個詞語表達成向量形式，其核心架構包括CBOW和Skip-gram。</p><p>在開始之前，引入模型複雜度，定義如下：</p><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rlqx6zOF4ZGywS><p>其中，E表示訓練的次數，T表示訓練語料中詞的個數，Q因模型而異。E值不是我們關心的內容，T與訓練語料有關，其值越大模型就越準確，Q在下面講述具體模型是討論。</p><p>NNLM模型是神經網絡概率語言模型的基礎模型。在NNLM模型中，從隱含層到輸出層的計算是主要影響訓練效率的地方，CBOW和Skip-gram模型考慮去掉隱含層。實踐證明新訓練的詞向量的精確度可能不如NNLM模型（具有隱含層），但可以通過增加訓練語料的方法來完善。</p><p>Word2vec包含兩種訓練模型，分別是CBOW和Skip_gram(輸入層、發射層、輸出層)，如下圖所示：</p><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rlqx7TkJH4ou7W><p><strong>CBOW模型：</strong></p><p>理解為上下文決定當前詞出現的概率。在CBOW模型中，上下文所有的詞對當前詞出現概率的影響的權重是一樣的，因此叫CBOW(continuous bag-of-words model)模型。如在袋子中取詞，取出數量足夠的詞就可以了，至於取出的先後順序是無關緊要的。</p><p><strong>Skip-gram模型：</strong></p><p>Skip-gram模型是一個簡單實用的模型。為什麼會提出該模型呢？</p><blockquote><div><p>在NLP中，語料的選取是一個相當重要的問題。</p><p>首先，語料必須充分。一方面詞典的詞量要足夠大，另一方面儘可能地包含反映詞語之間關係的句子，如“魚在水中游”這種句式在語料中儘可能地多，模型才能學習到該句中的語義和語法關係，這和人類學習自然語言是一個道理，重複次數多了，也就會模型了。</p><p>其次，語料必須準確。所選取的語料能夠正確反映該語言的語義和語法關係，如中文的《人民日報》比較準確。但更多時候不是語料選取引發準確性問題，而是處理的方法。</p><p>由於窗口大小的限制，這會導致超出窗口的詞語與當前詞之間的關係不能正確地反映到模型中，如果單純擴大窗口大小會增加訓練的複雜度。Skip-gram模型的提出很好解決了這些問題。</p></div></blockquote><p>Skip-gram表示“跳過某些符號”。例如句子“中國足球踢得真是太爛了”有4個3元詞組，分別是“中國足球踢得”、“足球踢得真是”、“踢得真是太爛”、“真是太爛了”，句子的本意都是“中國足球太爛”，可是上面4個3元組並不能反映出這個信息。</p><p>此時，使用Skip-gram模型允許某些詞被跳過，因此可組成“中國足球太爛”這個3元詞組。如果允許跳過2個詞，即2-Skip-gram，那麼上句話組成的3元詞組為：</p><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rlqx7UL9Xubocb><p>由上表可知：一方面Skip-gram反映了句子的真實意思，在新組成的這18個3元詞組中，有8個詞組能夠正確反映例句中的真實意思；另一方面，擴大了語料，3元詞組由原來的4個擴展到了18個。語料的擴展能夠提高訓練的準確度，獲得的詞向量更能反映真實的文本含義。</p><p><strong class=highlight-text toutiao-origin=span>Word2Vec總結</strong></p><p>Word2Vec是通過深度學習將詞表徵為數值型向量的工具。它把文本內容簡化處理，把詞當做特徵，Word2Vec將特徵映射到K維向量空間，為文本數據尋求更加深層次的特徵表示。Word2Vec獲得的詞向量可被用於聚類、找同義詞、詞性分析等。通過詞之間的距離（如cosine相似度、歐氏距離等）來判斷它們之間的語義相似度，採用一個三層的神經網絡 “輸入層-隱層-輸出層”。Word2Vec有個核心的技術是根據詞頻用Huffman編碼 ，使得所有詞頻相似的詞隱藏層激活的內容基本一致，出現頻率越高的詞語，他們激活的隱藏層數目越少，這樣有效的降低了計算的複雜度。</p><p>Word2Vec優點如下：</p><ul><li><p>高效、考慮詞語上下文的語義關係</p></li><li><p>與潛在語義分析（Latent Semantic Index, LSI）、潛在狄立克雷分配（Latent Dirichlet Allocation，LDA）的經典過程相比，Word2vec利用了詞的上下文，語義信息更加地豐富</p></li><li><p>在醫療項目（診斷報告）、短信輿情分析、社交網絡評價分析等領域，短文本很常見，因此word2vec會達到很好的語義表徵效果</p></li></ul><p>PS：這裡推薦大家閱讀熊富林等老師論文《Word2vec的核心架構及其應用》。</p><p><strong class=highlight-text toutiao-origin=span>二.Word2Vec安裝及入門</strong></p><p>Word2Vec原理知識講解完畢之後，我們開始嘗試在Anaconda的TensorFlow環境中安裝該擴展包。</p><p><strong class=highlight-text toutiao-origin=span>1.安裝</strong></p><p>第一步，打開Anaconda程序，並選擇已經安裝好的“TensorFlow”環境，運行Spyder。</p><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Rlqx7UoA3AcFqv><p>第二步，我們需要在TensorFlow環境中安裝gensim擴展包，否則會提示錯誤“ModuleNotFoundError: No module named ‘gensim’”。調用Anaconda Prompt安裝即可，如下圖所示：</p><pre><div><p>activate tensorflow</p><p>conda install gensim</p></div></pre><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rlqx7VKGizo4vn><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rlqx7VkHDmWVv7><p>但是，由於anaconda的.org服務器在國外，下載速度很慢，提示錯誤“Anaconda An HTTP error occurred when trying to retrieve this URL.HTTP errors are often intermittent”。</p><p>解決方法一：從國內清華的鏡像下載，參考文章：配置清華PIP鏡像（https://blog.csdn.net/weixin_44479045/article/details/89817549）</p><p>解決方法二：從PYPI網站（https://pypi.org/project/gensim/#files）下載對應版本的gensim，在再安裝本地下載的.whl文件。</p><p>由於第一種方法一直失敗，這裡推薦讀者嘗試第二種方法，同時作者會將完整的實驗環境上傳供大家直接使用。</p><p>第三步，調用PIP安裝本地gensim擴展包。</p><pre><div><p>pip install C:\Usersiuzhang\Desktop\TensorFlow\gensim-3.8.1-cp36-cp36m-win_amd64.whl</p></div></pre><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rlqx8BMFBp5I53><p>安裝成功之後，開始編寫我們的代碼吧！</p><p><strong class=highlight-text toutiao-origin=span>2.基礎用法</strong></p><p>第一步，調用random.choice隨機生成數據集。</p><p>初始數據集為三個方向“人工智能”、“網絡安全”和“網站開發”，然後調用choice函數隨機選擇某行數據，再從該行數據中4個詞語隨機挑選，最終形成1000行的數據集。</p><pre><div><p>from random import choice</p><p>from gensim.models import Word2Vec</p><p>#----------------------------------數據集生成----------------------------------</p><p># 定義初始數據集</p><p>ls_of_ls = [['Python', '大數據', '人工智能', 'Tensorflow'], </p><p>['網絡安全', 'Web滲透', 'SQLMAP', 'Burpsuite'],</p><p>['網站開發', 'Java', 'MySQL', 'HTML5']]</p><p># 真實項目中為數據集中文分詞(jieba.cut)後的詞列表</p><p>ls_of_words =  </p><p>for i in range(1000):</p><p>ls = choice(ls_of_ls) #隨機選擇某行數據集</p><p>ls_of_words.append([choice(ls) for _ in range(9, 15)])</p><p>print(ls_of_words)</p></div></pre><p>輸出部分結果如下所示，真實項目中會將中文預料分詞，然後構建特徵預料。</p><pre><div><p>[['Web滲透', 'SQLMAP', 'Burpsuite', 'SQLMAP', 'Web滲透', '網絡安全'], </p><p>['HTML5', 'MySQL', 'Java', 'MySQL', 'HTML5', 'MySQL'], </p><p>['Tensorflow', '大數據', '人工智能', 'Python', 'Tensorflow', '大數據'], </p><p>['Tensorflow', '人工智能', '大數據', 'Python', 'Python', '大數據'], </p><p>['網絡安全', 'Burpsuite', '網絡安全', 'SQLMAP', 'Burpsuite', '網絡安全'], </p><p>['Java', 'Java', '網站開發', '網站開發', 'Java', 'HTML5'], </p><p>....</p><p>['人工智能', '人工智能', 'Python', 'Python', 'Tensorflow', 'Tensorflow'], </p><p>['HTML5', 'Java', '網站開發', '網站開發', 'MySQL', '網站開發'], </p><p>['Python', '大數據', 'Python', 'Tensorflow', 'Python', '大數據'], </p><p>['MySQL', 'MySQL', 'HTML5', '網站開發', 'MySQL', 'HTML5'], </p><p>['HTML5', 'MySQL', 'MySQL', 'Java', 'Java', '網站開發']</p></div></pre><p>第二步，調用Word2Vec進行詞向量訓練，這裡僅一個參數，即傳入的數據集序列。</p><pre><div><p># 訓練</p><p>model = Word2Vec(ls_of_words)</p><p>print(model)</p></div></pre><p>其輸出結果為模型，即Word2Vec(vocab=12, size=100, alpha=0.025)。參數含義如下：</p><ul><li><p>sentences：傳入的數據集序列（list of lists of tokens），默認值為None</p></li><li><p>size：詞向量維數，默認值為100</p></li><li><p>window：同句中當前詞和預測詞的最大距離，默認值為5</p></li><li><p>min_count：最低詞頻過濾，默認值為5</p></li><li><p>workers：線程數，默認值為3</p></li><li><p>sg：模型參數，其值為0表示CBOW，值為1表示skip-gram，默認值為0</p></li><li><p>hs：模型參數，其值為0表示負例採樣，值為1表示層次softmax，默認值為0</p></li><li><p>negative：負例樣本數，默認值為5</p></li><li><p>ns_exponent：用於形成負例樣本的指數，默認值為0.75</p></li><li><p>cbow_mean：上下文詞向量參數，其值為0表示上下文詞向量求和值，值為1表示上下文詞向量平均值，默認值為1</p></li><li><p>alpha：初始學習率，默認值為0.025</p></li><li><p>min_alpha：最小學習率，默認值為0.0001</p></li></ul><p>第三步，計算兩個詞語之間的相似度、某個詞語與所有詞語的相似度。</p><pre><div><p>print(model.wv.similar_by_word('Tensorflow'))</p><p>print(model.wv.similarity('Web滲透', 'SQLMAP'))</p></div></pre><p>輸出結果如下圖所示，可以發現“TensorFlow”最相關的詞語包括“人工智能”、“Python”、“大數據”等，然後才是其他關鍵詞，這就是所謂的基於上下文語義信息的相似度計算。在真實的數據集中，某個詞如“人工智能”，經常和“CNN”同時出現，其相似度就較高；反之較低。通過該方法可以進行主題提取、文本分類、實體消歧等研究，比如將“體育”、“音樂”、“娛樂”、“學術”等不同主題新聞進行分類。</p><pre><div><p>[('人工智能', 0.989766538143158), </p><p>('Python', 0.9859075546264648), </p><p>('大數據', 0.9857005476951599), </p><p>('網絡安全', 0.9730607271194458), </p><p>('Java', 0.9701968431472778), </p><p>('HTML5', 0.9696391820907593), </p><p>('Web滲透', 0.9695969820022583), </p><p>('SQLMAP', 0.9685726165771484), </p><p>('Burpsuite', 0.9682992696762085), </p><p>('網站開發', 0.9662492871284485)]</p><p>0.9881845</p></div></pre><p>第四步，顯示詞語並計算詞向量矩陣及相似度。</p><pre><div><p>#-------------------------------詞矩陣計算------------------------------</p><p># 顯示詞</p><p>print("【顯示詞語】")</p><p>print(model.wv.index2word)</p><p># 顯示詞向量矩陣</p><p>print("【詞向量矩陣】")</p><p>vectors = model.wv.vectors</p><p>print(vectors)</p><p>print(vectors.shape)</p><p># 顯示四個詞語最相關的相似度</p><p>print("【詞向量相似度】")</p><p>for i in range(4):</p><p>print(model.wv.similar_by_vector(vectors[i]))</p></div></pre><p>輸出結果如下所示，可以看到我們的12個詞語，每個詞語會表示成一個100維的詞向量，最終形狀為（12,100）。接著計算四個詞語的相似度，第一個詞語是“TensorFlow”，它與自身的相似度為1，與“人工智能”、“大數據”、“Python”等高度相似。</p><pre><div><p>【顯示詞語】</p><p>['Tensorflow', 'MySQL', '人工智能', '大數據', 'Python', 'SQLMAP', 'HTML5', '網站開發', 'Burpsuite', 'Java', 'Web滲透', '網絡安全']</p><p>【詞向量矩陣】</p><p>[[-2.95917643e-03 -2.66699842e-03 -2.78887269e-03 ... -1.96327344e-02</p><p>-8.24492238e-03 1.00784563e-02]</p><p>[-1.68114714e-03 -1.09694153e-02 -5.14865573e-03 ... -1.93562061e-02</p><p>-8.51074420e-03 2.57991627e-02]</p><p>[-2.47456809e-03 -5.95887068e-05 -4.99962037e-03 ... -1.30442847e-02</p><p>-1.27229355e-02 9.84096527e-03]</p><p><strong class=highlight-text toutiao-origin=span>...</strong></p><p>[ 6.16406498e-04 -2.90613738e-03 3.06741858e-04 ... -9.37678479e-03</p><p>-5.20517444e-03 1.89058483e-02]</p><p>[-1.10806311e-02 -1.09268585e-02 -5.63549530e-03 ... -8.65904987e-03</p><p>-4.51919530e-03 2.01134961e-02]</p><p>[-1.34018352e-02 -1.60384644e-03 2.59250798e-03 ... -1.38302138e-02</p><p>-1.92157237e-03 1.76613722e-02]]</p><p>(12, 100)</p><p>【詞向量相似度】</p><p>[('Tensorflow', 1.0), ('人工智能', 0.9916591644287109), ('大數據', 0.9904486536979675), ('Python', 0.9859318137168884), ...]</p><p>[('MySQL', 1.0), ('網站開發', 0.9906301498413086), ('HTML5', 0.9899565577507019), ('Java', 0.9862704277038574), ...]</p><p>[('人工智能', 0.9999999403953552), ('Tensorflow', 0.9916592836380005), ('大數據', 0.9901371598243713), ('Python', 0.984708309173584), ...]</p><p>[('大數據', 1.0), ('Tensorflow', 0.9904486536979675), ('人工智能', 0.9901370406150818), ('Python', 0.9863395690917969), ...]</p></div></pre><p>第五步，調用predict_output_word函數預測新詞，並計算器概率總和。</p><pre><div><p>print("【預測新詞】")</p><p>print(model.predict_output_word(['人工智能']))</p><p>total = sum(i[1] for i in model.predict_output_word(['人工智能']))</p><p>print('概率總和為%.2f' % total)</p></div></pre><p>輸出結果如下所示：</p><pre><div><p>【預測新詞】</p><p>[('網站開發', 0.08391691), ('網絡安全', 0.08386225), ('SQLMAP', 0.08367824), </p><p>('HTML5', 0.08365326), ('Burpsuite', 0.083473735), ('Web滲透', 0.08326004), </p><p>('Tensorflow', 0.08323507), ('Java', 0.08317445), ('MySQL', 0.08317307), </p><p>('Python', 0.08307663)]</p><p>概率總和為0.83</p></div></pre><p>完整代碼為：</p><pre><div><p># -*- coding: utf-8 -*-</p><p>"""</p><p>Created on Mon Dec 23 17:48:50 2019</p><p>@author: xiuzhang Eastmount CSDN</p><p>"""</p><p>from random import choice</p><p>from gensim.models import Word2Vec</p><p>#----------------------------------數據集生成----------------------------------</p><p># 定義初始數據集</p><p>ls_of_ls = [['Python', '大數據', '人工智能', 'Tensorflow'], </p><p>['網絡安全', 'Web滲透', 'SQLMAP', 'Burpsuite'],</p><p>['網站開發', 'Java', 'MySQL', 'HTML5']]</p><p># 真實項目中為數據集中文分詞(jieba.cut)後的詞列表</p><p>ls_of_words =  </p><p>for i in range(1000):</p><p>ls = choice(ls_of_ls) #隨機選擇某行數據集</p><p>ls_of_words.append([choice(ls) for _ in range(9, 15)])</p><p>print(ls_of_words)</p><p>#----------------------------------詞向量訓練----------------------------------</p><p># 訓練</p><p>model = Word2Vec(ls_of_words)</p><p>print(model)</p><p>#-------------------------------計算詞語之間相似度------------------------------</p><p>print(model.wv.similar_by_word('Tensorflow'))</p><p>print(model.wv.similarity('Web滲透', 'SQLMAP'))</p><p>#-------------------------------詞矩陣計算------------------------------</p><p># 顯示詞</p><p>print("【顯示詞語】")</p><p>print(model.wv.index2word)</p><p># 顯示詞向量矩陣</p><p>print("【詞向量矩陣】")</p><p>vectors = model.wv.vectors</p><p>print(vectors)</p><p>print(vectors.shape)</p><p># 顯示四個詞語最相關的相似度</p><p>print("【詞向量相似度】")</p><p>for i in range(4):</p><p>print(model.wv.similar_by_vector(vectors[i]))</p><p>#-------------------------------預測新詞------------------------------</p><p>print("【預測新詞】")</p><p>print(model.predict_output_word(['人工智能']))</p><p>total = sum(i[1] for i in model.predict_output_word(['人工智能']))</p><p>print('概率總和為%.2f' % total)</p></div></pre><p>寫到這裡，Word2Vec基礎用法介紹完畢，接下來我們將結合具體案例分享中文短文本相似度的分析。同時，本文實驗參考了很多yellow_python大神的文章，真心推薦大家學習，他的地址為：https://blog.csdn.net/Yellow_python。</p><p><strong class=highlight-text toutiao-origin=span>三、Word2Vec+DBSCAN短文本聚類及可視化</strong></p><p>首先我們用Word2Vec結合DBSCAN對上一部分的案例進行聚類分析及可視化展示，預測的類標為：[0 0 0 1 0 1 1 1 2 2 2 2]，對應的圖如下所示，其顯示的結果挺好的。</p><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Rlqx8CB72jwBnJ><p>接著我們來看看完整的代碼，詳細過程見註釋。</p><pre><div><p># -*- coding: utf-8 -*-</p><p>"""</p><p>Created on Mon Dec 23 17:48:50 2019</p><p>@author: xiuzhang Eastmount CSDN</p><p>"""</p><p>from random import choice</p><p>from gensim.models import Word2Vec</p><p>from sklearn.cluster import DBSCAN</p><p>import matplotlib</p><p>from mpl_toolkits import mplot3d</p><p>import matplotlib.pyplot as plt</p><p>#----------------------------------數據集生成----------------------------------</p><p># 定義初始數據集</p><p>ls_of_ls = [['Python', '大數據', '人工智能', 'Tensorflow'], </p><p>['網絡安全', 'Web滲透', 'SQLMAP', 'Burpsuite'],</p><p>['網站開發', 'Java', 'MySQL', 'HTML5']]</p><p># 真實項目中為數據集中文分詞(jieba.cut)後的詞列表</p><p>ls_of_words =  </p><p>for i in range(2000):</p><p>ls = choice(ls_of_ls) #隨機選擇某行數據集</p><p>ls_of_words.append([choice(ls) for _ in range(9, 15)])</p><p>print(ls_of_words)</p><p>#----------------------------------詞向量訓練----------------------------------</p><p># 訓練 size詞向量維數3 window預測距離7</p><p>model = Word2Vec(ls_of_words, size=3, window=7)</p><p>print(model)</p><p># 提取詞向量</p><p>vectors = [model[word] for word in model.wv.index2word]</p><p>#----------------------------------詞向量聚類----------------------------------</p><p># 基於密度的DBSCAN聚類</p><p>labels = DBSCAN(eps=0.24, min_samples=3).fit(vectors).labels_</p><p>print(labels)</p><p>#----------------------------------可視化顯示----------------------------------</p><p>plt.rcParams['font.sans-serif'] = ['SimHei'] # 顯示中文</p><p>matplotlib.rcParams['axes.unicode_minus'] = False # 顯示負號</p><p>fig = plt.figure</p><p>ax = mplot3d.Axes3D(fig) # 創建3d座標軸</p><p>colors = ['red', 'blue', 'green', 'black']</p><p># 繪製散點圖 詞語 詞向量 類標(顏色)</p><p>for word, vector, label in zip(model.wv.index2word, vectors, labels):</p><p>ax.scatter(vector[0], vector[1], vector[2], c=colors[label], s=500, alpha=0.4)</p><p>ax.text(vector[0], vector[1], vector[2], word, ha='center', va='center')</p><p>plt.show</p></div></pre><p>假設我們從中國知網抓取了四個主題論文的關鍵詞，包括數據挖掘、民族、數學、文學。每個主題包括220篇論文，其關鍵詞均空格連接（已分詞），如下圖所示。</p><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rlqx8Ck1shUzc7><p>接著通過讀取數據集，並轉換成詞向量序列進行聚類，最終輸出結果如下圖所示，效果不是很理想，後續深入分析。</p><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rlqx8DN1ZV0WtF><p>完整代碼如下圖所示：</p><pre><div><p># -*- coding: utf-8 -*-</p><p>"""</p><p>Created on Mon Dec 23 17:48:50 2019</p><p>@author: xiuzhang Eastmount CSDN</p><p>"""</p><p>from gensim.models import Word2Vec</p><p>from sklearn.cluster import KMeans</p><p>import matplotlib</p><p>from mpl_toolkits import mplot3d</p><p>import matplotlib.pyplot as plt</p><p>#----------------------------------加載語料----------------------------------</p><p>file_name = "data_fc.txt"</p><p>ls_of_words =  </p><p>f = open(file_name, encoding='utf-8')</p><p>for lines in f.readlines: </p><p>words = lines.strip.split(" ")</p><p>#print(words)</p><p>ls_of_words.append(words)</p><p>print(ls_of_words)</p><p>#----------------------------------詞向量訓練----------------------------------</p><p># 訓練 size詞向量維數100 vocab單詞200</p><p>model = Word2Vec(sentences=ls_of_words, size=100, window=10)</p><p>print(model)</p><p># 提取詞向量</p><p>vectors = [model[word] for word in model.wv.index2word]</p><p>print(len(model.wv.index2word))</p><p>#----------------------------------詞向量聚類----------------------------------</p><p># 基於KMeans聚類</p><p>labels = KMeans(n_clusters=4).fit(vectors).labels_</p><p>print(labels)</p><p>#----------------------------------可視化顯示----------------------------------</p><p>plt.rcParams['font.sans-serif'] = ['SimHei'] # 顯示中文</p><p>matplotlib.rcParams['axes.unicode_minus'] = False # 顯示負號</p><p>fig = plt.figure</p><p>ax = mplot3d.Axes3D(fig) # 創建3d座標軸</p><p>colors = ['red', 'blue', 'green', 'black']</p><p># 繪製散點圖 詞語 詞向量 類標(顏色)</p><p>for word, vector, label in zip(model.wv.index2word, vectors, labels):</p><p>ax.scatter(vector[0], vector[1], vector[2], c=colors[label], s=500, alpha=0.4)</p><p>ax.text(vector[0], vector[1], vector[2], word, ha='center', va='center')</p><p>plt.show</p></div></pre><p><strong class=highlight-text toutiao-origin=span>四、Word2Vec計算《慶餘年》中文短文本相似度</strong></p><p>第一步，我們下載《慶餘年》TXT小說，編碼方式設置為“UTF-8”。</p><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rlqx8ER8iuJbk9><p>第二步，調用Jieba工具進行中文分詞，安裝調用“pip install jieba”。</p><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rlqx9D18ezrGuE><p>中文分詞代碼如下所示：</p><pre><div><p>import os</p><p>import jieba</p><p>from gensim.models import Word2Vec</p><p>from gensim.models import word2vec</p><p>#----------------------------------中文分詞----------------------------------</p><p># 定義中文分詞後文件名</p><p>file_name = "慶餘年.txt"</p><p>cut_file = "慶餘年_cut.txt"</p><p># 文件讀取操作</p><p>f = open(file_name, 'r', encoding='utf-8')</p><p>text = f.read</p><p># Jieba分詞</p><p>new_text = jieba.cut(text, cut_all=False) #精確模式</p><p># 過濾標點符號</p><p>str_out = ' '.join(new_text).replace('，', '').replace('。', '').replace('？', '').replace('！', '') \</p><p>.replace('“', '').replace('”', '').replace('：', '').replace('…', '').replace('（', '').replace('）', '') \</p><p>.replace('—', '').replace('《', '').replace('》', '').replace('、', '').replace('‘', '') \</p><p>.replace('’', '') </p><p># 輸出文件</p><p>fo = open(cut_file, 'w', encoding='utf-8')</p><p># 寫入操作</p><p>fo.write(str_out)</p><p>f.close</p><p>fo.close</p></div></pre><p>第三步，調用Word2vec進行上下文語義相似度計算，代碼如下。</p><p>這裡的亮點是代碼中判斷是否訓練，如果存在訓練模型文件，則直接調用</p><pre><div><p>#----------------------------------訓練模型---------------------------------- </p><p>save_model_name = '慶餘年.model'</p><p># 判斷訓練的模型文件是否存在</p><p>if not os.path.exists(save_model_name): # 模型訓練 </p><p>sentences = word2vec.Text8Corpus(cut_file) # 加載語料</p><p>model = Word2Vec(sentences, size=200) # 訓練skip-gram模型</p><p>model.save(save_model_name)</p><p># 二進制類型保存模型 後續直接使用</p><p>model.wv.save_word2vec_format(save_model_name + ".bin", binary=True) </p><p>else:</p><p>print('此訓練模型已經存在，不用再次訓練')</p><p>#----------------------------------預測結果---------------------------------- </p><p># 加載已訓練好的模型</p><p>model = Word2Vec.load(save_model_name)</p><p># 計算兩個詞的相似度/相關程度</p><p>res1 = model.wv.similarity("範閒", "林婉兒")</p><p>print(u"範閒和林婉兒的相似度為：", res1, "\n")</p><p># 計算某個詞的相關詞列表</p><p>res2 = model.wv.most_similar("範閒", topn=10) # 10個最相關的</p><p>print(u"和 [範閒] 最相關的詞有：\n")</p><p>for item in res2:</p><p>print(item[0], item[1])</p><p>print("-------------------------------\n")</p><p># 計算某個詞的相關詞列表</p><p>res3 = model.wv.most_similar("五竹", topn=10) # 10個最相關的</p><p>print(u"和 [五竹] 最相關的詞有：\n")</p><p>for item in res3:</p><p>print(item[0], item[1])</p><p>print("-------------------------------\n")</p></div></pre><p>最終輸出結果如下所示：</p><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Rlqx9Dp7G8L1RB><pre><div><p>此訓練模型已經存在，不用再次訓練</p><p>範閒和林婉兒的相似度為： 0.76498425 </p><p>和 [範閒] 最相關的詞有：</p><p>他 0.8427201509475708</p><p>婉兒 0.7699953317642212</p><p>林婉兒 0.7649842500686646</p><p>洪竹 0.7614920139312744</p><p>她 0.7607567310333252</p><p>夏棲飛 0.7595056891441345</p><p>言冰雲 0.7514396905899048</p><p>楊萬里 0.7514225840568542</p><p>明青達 0.7475594282150269</p><p>五竹 0.7399575710296631</p><p><strong class=highlight-text toutiao-origin=span>-------------------------------</strong></p><p>和 [五竹] 最相關的詞有：</p><p>肖恩 0.7810727953910828</p><p>言冰雲 0.7754340171813965</p><p>他 0.7447681427001953</p><p>範閒 0.7399576902389526</p><p>她 0.7292275428771973</p><p>婉兒 0.7235128879547119</p><p>海棠 0.721315860748291</p><p>洪竹 0.7139796018600464</p><p>陳萍萍 0.7094859480857849</p><p>林婉兒 0.6981754302978516</p><p><strong class=highlight-text toutiao-origin=span>-------------------------------</strong></p></div></pre><p>注：該部分思路參考了笑傲蒼穹老師的思想，推薦大家學習：中文word2vec的python實現（https://blog.csdn.net/sinat_29694963/article/details/79177832）</p><p><strong class=highlight-text toutiao-origin=span>五、Word2Vec寫詩詳解</strong></p><p>最後補充一段Yellow_python大神的寫詩博客，這裡強烈推薦大家學習他的論文，AI和NLP系列每篇都是精華。博客地址：Python程序寫詩【訓練1分鐘】古詩生成 - Yellow_python（https://blog.csdn.net/Yellow_python/article/details/86726619）</p><p>基本流程如下：</p><ul><li><p>首先，存在一個“古詩詞.txt”數據集，包含了43101首詩，通過該數據集進行訓練。</p></li><li><p>接著，讀取古詩詞數據，通過 ls_of_ls_of_c = [list(line.strip()) for line in f] 代碼將古詩詞拆分成單個字，每首詩句裡面的字都有上下文。</p></li><li><p>然後，調用Word2Vec進行訓練，轉換成詞向量並保存模型。</p></li><li><p>最後，輸入標題進行補全，根據標題四個字進行語義查找，計算每個字的相似度，最終形成對應的詩句。核心代碼為：model.predict_output_word(poem[-self.window:], max(self.topn, len(poem) + 1))</p></li></ul><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Rlqx9E82y1IuQX><p>完整代碼如下：</p><pre><div><p># -*- coding: utf-8 -*-</p><p>"""</p><p>Created on Mon Dec 23 17:48:50 2019</p><p>@author: xiuzhang Eastmount CSDN</p><p>博客原地址：https://blog.csdn.net/Yellow_python/article/details/86726619</p><p>推薦大家學習Yellow_python大神的文章</p><p>"""</p><p>from gensim.models import Word2Vec # 詞向量</p><p>from random import choice</p><p>from os.path import exists</p><p>class CONF:</p><p>path = '古詩詞.txt'</p><p>window = 16 # 滑窗大小</p><p>min_count = 60 # 過濾低頻字</p><p>size = 125 # 詞向量維度</p><p>topn = 14 # 生成詩詞的開放度</p><p>model_path = 'word2vec'</p><p># 定義模型</p><p>class Model:</p><p><strong class=highlight-text toutiao-origin=span>def __init__(self, window, topn, model):</strong></p><p>self.window = window</p><p>self.topn = topn</p><p>self.model = model # 詞向量模型</p><p>self.chr_dict = model.wv.index2word # 字典</p><p>"""模型初始化"""</p><p>@classmethod</p><p><strong class=highlight-text toutiao-origin=span>def initialize(cls, config):</strong></p><p>if exists(config.model_path):</p><p># 模型讀取</p><p>model = Word2Vec.load(config.model_path)</p><p>else:</p><p># 語料讀取</p><p>with open(config.path, encoding='utf-8') as f:</p><p>ls_of_ls_of_c = [list(line.strip()) for line in f]</p><p># 模型訓練和保存</p><p>model = Word2Vec(sentences=ls_of_ls_of_c, size=config.size, window=config.window, min_count=config.min_count)</p><p>model.save(config.model_path)</p><p>return cls(config.window, config.topn, model)</p><p>"""古詩詞生成"""</p><p><strong class=highlight-text toutiao-origin=span>def poem_generator(self, title, form):</strong></p><p>filter = lambda lst: [t[0] for t in lst if t[0] not in ['，', '。']]</p><p># 標題補全</p><p>if len(title) &lt; 4:</p><p>if not title:</p><p>title += choice(self.chr_dict)</p><p>for _ in range(4 - len(title)):</p><p>similar_chr = self.model.similar_by_word(title[-1], self.topn // 2)</p><p>similar_chr = filter(similar_chr)</p><p>char = choice([c for c in similar_chr if c not in title])</p><p>title += char</p><p># 文本生成</p><p>poem = list(title)</p><p>for i in range(form[0]):</p><p>for _ in range(form[1]):</p><p>predict_chr = self.model.predict_output_word(poem[-self.window:], max(self.topn, len(poem) + 1))</p><p>predict_chr = filter(predict_chr)</p><p>char = choice([c for c in predict_chr if c not in poem[len(title):]])</p><p>poem.append(char)</p><p>poem.append('，' if i % 2 == 0 else '。')</p><p>length = form[0] * (form[1] + 1)</p><p>return '《%s》' % ''.join(poem[:-length]) + '\n' + ''.join(poem[-length:])</p><p><strong class=highlight-text toutiao-origin=span>def main(config=CONF):</strong></p><p>form = {'五言絕句': (4, 5), '七言絕句': (4, 7), '對聯': (2, 9)}</p><p>m = Model.initialize(config)</p><p>while True:</p><p>title = input('輸入標題：').strip</p><p>try:</p><p>poem = m.poem_generator(title, form['五言絕句'])</p><p>print('%s' % poem) # red</p><p>poem = m.poem_generator(title, form['七言絕句'])</p><p>print('%s' % poem) # yellow</p><p>poem = m.poem_generator(title, form['對聯'])</p><p>print('%s' % poem) # purple</p><p>print</p><p>except:</p><p><strong class=highlight-text toutiao-origin=span>pass</strong></p><p>if __name__ == '__main__':</p><p>main</p></div></pre><p>輸出結果如下圖所示：</p><img alt="gensim詞向量Word2Vec安裝及《慶餘年》中文短文本相似度計算 | CSDN博文精選" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Rlqx9ErCX924xX><p>比如輸入“武大”，輸出的五言絕句、七嚴絕句如下所示，注意“勳旗旌虜戰”這句話在我們的“古詩詞.txt”中是找不到的，而是因為“勳”和“旗”存在相似度語義所致。</p><pre><div><p>輸入標題：武大</p><p>《武大震甲》</p><p>勳旗旌虜戰，戎陣勇軍旆。營鼓兵獵烽，嫖卒戍蕃馬。</p><p>《武大運德》</p><p>皇德聖功昭謨臣，勳堯宣祀祚肅禋。舜慶睿禮雍昌承，爰啟旒穆恭崇嘉。</p><p>《武大震建》</p><p>王宣昌帝昭皇祀穆聖，臣堯謨德恭孝禮祚肅。</p></div></pre><p><strong class=highlight-text toutiao-origin=span>六、總結</strong></p><p>寫到這裡，這篇文章就講解完畢，不知道您是否體會到了Word2Vec的強大功能。但如果您從事論文研究，您會發現它真是無處不在，從數據挖掘、自然語言處理大數據、人工智能，再到情感分析、圖書情報、醫學本體識別等，甚至惡意代碼識別也有它的身影。但它寫得詩也有辭藻堆砌的感覺，後續我們學到強化學習的時候，再對比它們的效果。</p><p>本文詳細介紹了Word2Vec的基本原理，通過案例代碼詳細介紹了Word2Vec的用法。最後，希望這篇基礎性文章對您有所幫助，如果文章中存在錯誤或不足之處，還請海涵~作為人工智能的菜鳥，我希望自己能不斷進步並深入，後續將它應用於圖像識別、網絡安全、對抗樣本等領域，指導大家撰寫簡單的學術論文，一起加油！</p><p>PS：這是作者的第一個付費專欄，會非常用心的去撰寫，希望能對得起讀者的9塊錢。本來只想設置1快的，但CSDN固定了價格。寫了八年的免費文章，這也算知識付費的一個簡單嘗試吧！畢竟讀博也不易，寫文章也花費時間和精力，但作者更多的文章會免費分享。如果您購買了該專欄，有Python數據分析、圖像處理、人工智能、網絡安全的問題，我們都可以深入探討，尤其是做研究的同學，共同進步~</p><p>參考文獻</p><p class=pgc-end-literature>[1] word2vec詞向量訓練及中文文本相似度計算 - 作者的文章</p><p>https://blog.csdn.net/eastmount/article/details/50637476</p><p>[2] 《Word2vec的核心架構及其應用》· 熊富林，鄧怡豪，唐曉晟 · 北郵2015年</p><p>[3] 《Word2vec的工作原理及應用探究》 · 周練 · 西安電子科技大學2014年</p><p>[4] word2vec——高效word特徵求取 - 推薦Rachel-Zhang大神文章</p><p>https://blog.csdn.net/abcjennifer/article/details/46397829</p><p>[5] Deep Learning in NLP （一）詞向量和語言模型 - licstar大神</p><p>http://licstar.net/archives/328</p><p>[6] gensim詞向量Word2Vec - Yellow_python（強推）</p><p>https://blog.csdn.net/Yellow_python/article/details/84347878</p><p>[7] word2vec函數參數 - 冥更</p><p>https://blog.csdn.net/qq_24852439/article/details/85302172</p><p>[8] NLP之詞向量：利用word2vec對20類新聞文本數據集進行詞向量訓練、測試(某個單詞的相關詞彙) - 一個處女座的程序猿</p><p>https://blog.csdn.net/qq_41185868/article/details/88344822</p><p>[9]gensim中word2vec python源碼理解（一）初始化構建單詞表 - ForcedOverflow</p><p>https://blog.csdn.net/u014568072/article/details/79071116</p><p>[10] https://github.com/AryeYellow/PyProjects</p><p>[11] Python程序寫詩【訓練1分鐘】古詩生成 - Yellow_python</p><p>https://blog.csdn.net/Yellow_python/article/details/86726619</p><p>[12] 機器學習100問|Word2Vec是如何工作的？它和LDA有什麼區別與聯繫？</p><p>https://cloud.tencent.com/developer/article/1464953</p><p>[13] https://github.com/eastmountyxz/AI-for-TensorFlow</p><p>[14] windows下使用word2vec訓練維基百科中文語料全攻略！（三）- 文哥的學習日記</p><p>https://www.jianshu.com/p/83b742994946</p><p>[15] NLP之——Word2Vec詳解 - 郭耀華</p><p>https://www.cnblogs.com/guoyaohua/p/9240336.html</p><p>[16] 中文word2vec的python實現 - 笑傲蒼穹0</p><p>https://blog.csdn.net/sinat_29694963/article/details/79177832</p><p class=pgc-end-source>作者theano人工智能系列：</p><p class=pgc-end-source>[Python人工智能] 一.神經網絡入門及theano基礎代碼講解</p><p class=pgc-end-source>https://blog.csdn.net/Eastmount/article/details/80363106</p><p class=pgc-end-source>[Python人工智能] 二.theano實現迴歸神經網絡分析</p><p class=pgc-end-source>https://blog.csdn.net/Eastmount/article/details/80390462</p><p class=pgc-end-source>[Python人工智能] 三.theano實現分類神經網絡及機器學習基礎</p><p class=pgc-end-source>https://blog.csdn.net/Eastmount/article/details/80432844</p><p class=pgc-end-source>[Python人工智能] 四.神經網絡和深度學習入門知識</p><p class=pgc-end-source>https://blog.csdn.net/Eastmount/article/details/80499259</p><p class=pgc-end-source>[Python人工智能] 五.theano實現神經網絡正規化Regularization處理</p><p class=pgc-end-source>https://blog.csdn.net/Eastmount/article/details/80536725</p><p class=pgc-end-source>[Python人工智能] 六.神經網絡的評價指標、特徵標準化和特徵選擇</p><p class=pgc-end-source>https://blog.csdn.net/Eastmount/article/details/80650980</p><p class=pgc-end-source>[Python人工智能] 七.加速神經網絡、激勵函數和過擬合</p><p class=pgc-end-source>https://blog.csdn.net/Eastmount/article/details/80757556</p><p class=pgc-end-source>技術的道路一個人走著極為艱難？</p><p class=pgc-end-source>一身的本領得不施展？</p><p class=pgc-end-source>優質的文章得不到曝光？</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>gensim</a></li><li><a>Word2Vec</a></li><li><a>安裝及</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/a87504d6.html alt=「原創」PHP7.X安裝及配置教程 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/1e34aa66-9562-4e41-8b8e-9b8f54359223 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a87504d6.html title=「原創」PHP7.X安裝及配置教程>「原創」PHP7.X安裝及配置教程</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e3caf773.html alt=換熱器（管殼式換熱器）的安裝及應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/27a5846a672a4384b4ee68c79d366210 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e3caf773.html title=換熱器（管殼式換熱器）的安裝及應用>換熱器（管殼式換熱器）的安裝及應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/869e483d.html alt=液壓系統各元件的安裝及要求 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/c3ca9ca8d987496988a14e69a33dff32 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/869e483d.html title=液壓系統各元件的安裝及要求>液壓系統各元件的安裝及要求</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/06e9f84f.html alt=定位銷的安裝及維護保養過程 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7782b0abdf9f48f9a0fa17dd2c3d7920 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/06e9f84f.html title=定位銷的安裝及維護保養過程>定位銷的安裝及維護保養過程</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6e025cf8.html alt=抗震支架設計、技術、安裝及管理相關要求 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1c33fc6346bb4c829128f464575b1b6d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6e025cf8.html title=抗震支架設計、技術、安裝及管理相關要求>抗震支架設計、技術、安裝及管理相關要求</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0fa560e.html alt=接地裝置的安裝及定額套用技巧 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1755b16edc574edca670558fb3c87e72 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0fa560e.html title=接地裝置的安裝及定額套用技巧>接地裝置的安裝及定額套用技巧</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6fffe0c.html alt=砂輪使用技巧：砂輪的安裝及調整需要注意哪些問題？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8f9c476a706f4b7dad67b9b4553d0be1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6fffe0c.html title=砂輪使用技巧：砂輪的安裝及調整需要注意哪些問題？>砂輪使用技巧：砂輪的安裝及調整需要注意哪些問題？</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>