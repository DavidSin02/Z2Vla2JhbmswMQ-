<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習西瓜書簡明筆記（15）概率圖模型 | 极客快訊</title><meta property="og:title" content="機器學習西瓜書簡明筆記（15）概率圖模型 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p9.pstatp.com/large/pgc-image/ae46374e32bb4e80ab4f93a219a0c41e"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8d200025.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8d200025.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/8d200025.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8d200025.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8d200025.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/8d200025.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/8d200025.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/8d200025.html><meta property="article:published_time" content="2020-10-29T21:12:20+08:00"><meta property="article:modified_time" content="2020-10-29T21:12:20+08:00"><meta name=Keywords content><meta name=description content="機器學習西瓜書簡明筆記（15）概率圖模型"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/8d200025.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習西瓜書簡明筆記（15）概率圖模型</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/ae46374e32bb4e80ab4f93a219a0c41e><p class=pgc-img-caption></p></div><blockquote><p style=text-align:start>上篇主要介紹了半監督學習，首先從如何利用未標記樣本所蘊含的分佈信息出發，引入了半監督學習的基本概念，即訓練數據同時包含有標記樣本和未標記樣本的學習方法；接著分別介紹了幾種常見的半監督學習方法：生成式方法基於對數據分佈的假設，利用未標記樣本隱含的分佈信息，使得對模型參數的估計更加準確；TSVM給未標記樣本賦予偽標記，並通過不斷調整易出錯樣本的標記得到最終輸出；基於分歧的方法結合了集成學習的思想，通過多個學習器在不同視圖上的協作，有效利用了未標記樣本數據 ；最後半監督聚類則是藉助已有的監督信息來輔助聚類的過程，帶約束k-均值算法需檢測當前樣本劃分是否滿足約束關係，帶標記k-均值算法則利用有標記樣本指定初始類中心。本篇將討論一種基於圖的學習算法--概率圖模型。</p></blockquote><p style=text-align:start><strong>15、概率圖模型</strong></p><p style=text-align:start>現在再來談談機器學習的核心價值觀，可以更通俗地理解為：<strong>根據一些已觀察到的證據來推斷未知</strong>，更具哲學性地可以闡述為：未來的發展總是遵循著歷史的規律。其中<strong>基於概率的模型將學習任務歸結為計算變量的概率分佈</strong>，正如之前已經提到的：生成式模型先對聯合分佈進行建模，從而再來求解後驗概率，例如：貝葉斯分類器先對聯合分佈進行最大似然估計，從而便可以計算類條件概率；判別式模型則是直接對條件分佈進行建模。</p><p style=text-align:start><strong>概率圖模型</strong>（probabilistic graphical model）是一類用<strong>圖結構</strong>來表達各屬性之間相關關係的概率模型，一般而言：<strong>圖中的一個結點表示一個或一組隨機變量，結點之間的邊則表示變量間的相關關係</strong>，從而形成了一張“<strong>變量關係圖</strong>”。若使用有向的邊來表達變量之間的依賴關係，這樣的有向關係圖稱為<strong>貝葉斯網</strong>（Bayesian nerwork）或有向圖模型；若使用無向邊，則稱為<strong>馬爾可夫網</strong>（Markov network）或無向圖模型。</p><p style=text-align:start><strong>15.1 隱馬爾可夫模型(HMM)</strong></p><p style=text-align:start>隱馬爾可夫模型（Hidden Markov Model，簡稱HMM）是結構最簡單的一種貝葉斯網，在語音識別與自然語言處理領域上有著廣泛的應用。HMM中的變量分為兩組：<strong>狀態變量</strong>與<strong>觀測變量</strong>，其中狀態變量一般是未知的，因此又稱為“<strong>隱變量</strong>”，觀測變量則是已知的輸出值。在隱馬爾可夫模型中，變量之間的依賴關係遵循如下兩個規則：</p><ul><li><strong>1. 觀測變量的取值僅依賴於狀態變量</strong>；</li><li><strong>2. 下一個狀態的取值僅依賴於當前狀態</strong>，通俗來講：<strong>現在決定未來，未來與過去無關</strong>，這就是著名的<strong>馬爾可夫性</strong>。</li></ul><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3c7ea95225214b09b204ee542cdfef20><p class=pgc-img-caption></p></div><p style=text-align:start>基於上述變量之間的依賴關係，我們很容易寫出隱馬爾可夫模型中所有變量的聯合概率分佈：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/690c41eda03e4892bc9ad29d12a3fe8e><p class=pgc-img-caption></p></div><p style=text-align:start>易知：<strong>欲確定一個HMM模型需要以下三組參數</strong>：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a784b0bbcaa242f2b18763dc64971bd3><p class=pgc-img-caption></p></div><p style=text-align:start>當確定了一個HMM模型的三個參數後，便按照下面的規則來生成觀測值序列：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e2f6863066b1402ea645586943c8ee7a><p class=pgc-img-caption></p></div><p style=text-align:start>在實際應用中，HMM模型的發力點主要體現在下述三個問題上：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3bc99cda874a4b6d979acf721f61765e><p class=pgc-img-caption></p></div><p style=text-align:start><strong>15.1.1 HMM評估問題</strong></p><p style=text-align:start>HMM評估問題指的是：<strong>給定了模型的三個參數與觀測值序列，求該觀測值序列出現的概率</strong>。例如：對於賭場問題，便可以依據骰子擲出的結果序列來計算該結果序列出現的可能性，若小概率的事件發生了則可認為賭場的骰子有作弊的可能。解決該問題使用的是<strong>前向算法</strong>，即步步為營，自底向上的方式逐步增加序列的長度，直到獲得目標概率值。在前向算法中，定義了一個<strong>前向變量</strong>，即給定觀察值序列且t時刻的狀態為Si的概率：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8feaee7b2e464adea5362115272b818d><p class=pgc-img-caption></p></div><p style=text-align:start>基於前向變量，很容易得到該問題的遞推關係及終止條件：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5410ba7355dc42daae31053af8152717><p class=pgc-img-caption></p></div><p style=text-align:start>因此可使用動態規劃法，從最小的子問題開始，通過填表格的形式一步一步計算出目標結果。</p><p style=text-align:start><strong>15.1.2 HMM解碼問題</strong></p><p style=text-align:start>HMM解碼問題指的是：<strong>給定了模型的三個參數與觀測值序列，求可能性最大的狀態序列</strong>。例如：在語音識別問題中，人說話形成的數字信號對應著觀測值序列，對應的具體文字則是狀態序列，從數字信號轉化為文字正是對應著根據觀測值序列推斷最有可能的狀態值序列。解決該問題使用的是<strong>Viterbi算法</strong>，與前向算法十分類似的，Viterbi算法定義了一個<strong>Viterbi變量</strong>，也是採用動態規劃的方法，自底向上逐步求解。</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3ca61630a22e4b8ab072638a4e55bfbf><p class=pgc-img-caption></p></div><p style=text-align:start><strong>15.1.3 HMM學習問題</strong></p><p style=text-align:start>HMM學習問題指的是：<strong>給定觀測值序列，如何調整模型的參數使得該序列出現的概率最大</strong>。這便轉化成了機器學習問題，即從給定的觀測值序列中學習出一個HMM模型，<strong>該問題正是EM算法的經典案例之一</strong>。其思想也十分簡單：對於給定的觀測值序列，如果我們能夠按照該序列潛在的規律來調整模型的三個參數，則可以使得該序列出現的可能性最大。假設狀態值序列也已知，則很容易計算出與該序列最契合的模型參數：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1fd28722f1d845179cd48404592d50d8><p class=pgc-img-caption></p></div><p style=text-align:start>但一般狀態值序列都是不可觀測的，且<strong>即使給定觀測值序列與模型參數，狀態序列仍然遭遇組合爆炸</strong>。因此上面這種簡單的統計方法就行不通了，若將狀態值序列看作為隱變量，這時便可以考慮使用EM算法來對該問題進行求解：</p><ul><li>首先對HMM模型的三個參數進行隨機初始化；</li><li>根據模型的參數與觀測值序列，計算t時刻狀態為i且t+1時刻狀態為j的概率以及t時刻狀態為i的概率。</li></ul><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2dd9a19bbe8d4c5db73e257ca4ce3b79><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/add7021a44e64185b6556f2fb6983026><p class=pgc-img-caption></p></div><ul><li>接著便可以對模型的三個參數進行重新估計：</li></ul><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/19e9adfbe83b4779ae92143519905bbb><p class=pgc-img-caption></p></div><ul><li>重複步驟2-3，直至三個參數值收斂，便得到了最終的HMM模型。</li></ul><p style=text-align:start><strong>15.2 馬爾可夫隨機場（MRF）</strong></p><p style=text-align:start>馬爾可夫隨機場（Markov Random Field）是一種典型的馬爾可夫網，即使用無向邊來表達變量間的依賴關係。在馬爾可夫隨機場中，對於關係圖中的一個子集，<strong>若任意兩結點間都有邊連接，則稱該子集為一個團；若再加一個結點便不能形成團，則稱該子集為極大團</strong>。MRF使用<strong>勢函數</strong>來定義多個變量的概率分佈函數，其中<strong>每個（極大）團對應一個勢函數</strong>，一般團中的變量關係也體現在它所對應的極大團中，因此常常基於極大團來定義變量的聯合概率分佈函數。具體而言，若所有變量構成的極大團的集合為C，則MRF的聯合概率函數可以定義為：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fcf7fd064d9d4985890c1e661e570ca3><p class=pgc-img-caption></p></div><p style=text-align:start>對於條件獨立性，<strong>馬爾可夫隨機場通過分離集來實現條件獨立</strong>，若A結點集必須經過C結點集才能到達B結點集，則稱C為分離集。書上給出了一個簡單情形下的條件獨立證明過程，十分貼切易懂，此處不再展開。基於分離集的概念，得到了MRF的三個性質：</p><ul><li><strong>全局馬爾可夫性</strong>：給定兩個變量子集的分離集，則這兩個變量子集條件獨立。</li><li><strong>局部馬爾可夫性</strong>：給定某變量的鄰接變量，則該變量與其它變量條件獨立。</li><li><strong>成對馬爾可夫性</strong>：給定所有其他變量，兩個非鄰接變量條件獨立。</li></ul><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6ab9ed70bb8d4a96a6aea8f37511329c><p class=pgc-img-caption></p></div><p style=text-align:start>對於MRF中的勢函數，勢函數主要用於描述團中變量之間的相關關係，且要求為非負函數，直觀來看：勢函數需要在偏好的變量取值上函數值較大，例如：若x1與x2成正相關，則需要將這種關係反映在勢函數的函數值中。一般我們常使用指數函數來定義勢函數：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b3fdf943970b40f59ef14de55e9306c0><p class=pgc-img-caption></p></div><p style=text-align:start><strong>15.3 條件隨機場（CRF）</strong></p><p style=text-align:start>前面所講到的<strong>隱馬爾可夫模型和馬爾可夫隨機場都屬於生成式模型，即對聯合概率進行建模，條件隨機場則是對條件分佈進行建模</strong>。CRF試圖在給定觀測值序列後，對狀態序列的概率分佈進行建模，即P(y | x)。直觀上看：CRF與HMM的解碼問題十分類似，都是在給定觀測值序列後，研究狀態序列可能的取值。CRF可以有多種結構，只需保證狀態序列滿足馬爾可夫性即可，一般我們常使用的是<strong>鏈式條件隨機場</strong>：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/57708012c2b04f5b841d74562f254981><p class=pgc-img-caption></p></div><p style=text-align:start>與馬爾可夫隨機場定義聯合概率類似地，CRF也通過團以及勢函數的概念來定義條件概率P(y | x)。在給定觀測值序列的條件下，鏈式條件隨機場主要包含兩種團結構：單個狀態團及相鄰狀態團，通過引入兩類特徵函數便可以定義出目標條件概率：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b31696bee4b848d4926b988a17c5976a><p class=pgc-img-caption></p></div><p style=text-align:start>以詞性標註為例，如何判斷給出的一個標註序列靠譜不靠譜呢？<strong>轉移特徵函數主要判定兩個相鄰的標註是否合理</strong>，例如：動詞+動詞顯然語法不通；<strong>狀態特徵函數則判定觀測值與對應的標註是否合理</strong>，例如： ly結尾的詞-->副詞較合理。因此我們可以定義一個特徵函數集合，用這個特徵函數集合來為一個標註序列打分，並據此選出最靠譜的標註序列。也就是說，每一個特徵函數（對應一種規則）都可以用來為一個標註序列評分，把集合中所有特徵函數對同一個標註序列的評分綜合起來，就是這個標註序列最終的評分值。可以看出：<strong>特徵函數是一些經驗的特性</strong>。</p><p style=text-align:start><strong>15.4 學習與推斷</strong></p><p style=text-align:start>對於生成式模型，通常我們都是先對變量的聯合概率分佈進行建模，接著再求出目標變量的<strong>邊際分佈</strong>（marginal distribution），那如何從聯合概率得到邊際分佈呢？這便是學習與推斷。下面主要介紹兩種精確推斷的方法：<strong>變量消去</strong>與<strong>信念傳播</strong>。</p><p style=text-align:start><strong>15.4.1 變量消去</strong></p><p style=text-align:start>變量消去利用條件獨立性來消減計算目標概率值所需的計算量，它通過運用<strong>乘法與加法的分配率</strong>，將對變量的積的求和問題轉化為對部分變量交替進行求積與求和的問題，從而將每次的<strong>運算控制在局部</strong>，達到簡化運算的目的。</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7d24195c18ba429394d234718236c79a><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e515704738614fd18c4e62b09cc03efb><p class=pgc-img-caption></p></div><p style=text-align:start><strong>15.4.2 信念傳播</strong></p><p style=text-align:start>若將變量求和操作看作是一種消息的傳遞過程，信息傳播可以理解成：<strong>一個節點在接收到所有其它節點的消息後才向另一個節點發送消息</strong>，同時當前節點的邊際概率正比於他所接收的消息的乘積：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b51e3ec76954444d8ec47f8d24464896><p class=pgc-img-caption></p></div><p style=text-align:start>因此只需要經過下面兩個步驟，便可以完成所有的消息傳遞過程。利用動態規劃法的思想記錄傳遞過程中的所有消息，當計算某個結點的邊際概率分佈時，只需直接取出傳到該結點的消息即可，從而避免了計算多個邊際分佈時的冗餘計算問題。</p><ul><li>1.指定一個根節點，從所有的葉節點開始向根節點傳遞消息，直到根節點收到所有鄰接結點的消息<strong>（從葉到根</strong>）；</li><li>2.從根節點開始向葉節點傳遞消息，直到所有葉節點均收到消息<strong>（從根到葉）</strong>。</li></ul><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ce98c2009fe14bd2adb9db982c19e0c6><p class=pgc-img-caption></p></div><p style=text-align:start><strong>15.5 LDA話題模型</strong></p><p style=text-align:start>話題模型主要用於處理文本類數據，其中<strong>隱狄利克雷分配模型</strong>（Latent Dirichlet Allocation，簡稱LDA）是話題模型的傑出代表。在話題模型中，有以下幾個基本概念：詞（word）、文檔（document）、話題（topic）。</p><ul><li><strong>詞</strong>：最基本的離散單元；</li><li><strong>文檔</strong>：由一組詞組成，詞在文檔中不計順序；</li><li><strong>話題</strong>：由一組特定的詞組成，這組詞具有較強的相關關係。</li></ul><p style=text-align:start>在現實任務中，一般我們可以得出一個文檔的詞頻分佈，但不知道該文檔對應著哪些話題，LDA話題模型正是為了解決這個問題。具體來說：<strong>LDA認為每篇文檔包含多個話題，且其中每一個詞都對應著一個話題</strong>。因此可以假設文檔是通過如下方式生成：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/c6503ac1a02841a086ec9bc43b905f80><p class=pgc-img-caption></p></div><p style=text-align:start>這樣一個文檔中的所有詞都可以認為是通過話題模型來生成的，當已知一個文檔的詞頻分佈後（即一個N維向量，N為詞庫大小），則可以認為：<strong>每一個詞頻元素都對應著一個話題，而話題對應的詞頻分佈則影響著該詞頻元素的大小</strong>。因此很容易寫出LDA模型對應的聯合概率函數：</p><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/64d11e84b5ec43b384d8d9ac5282ff3e><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=機器學習西瓜書簡明筆記（15）概率圖模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f17f1587c23b4e589b3a6309fbf39276><p class=pgc-img-caption></p></div><p style=text-align:start>從上圖可以看出，LDA的三個表示層被三種顏色表示出來：</p><ul><li><strong>corpus-level（紅色）：</strong> α和β表示語料級別的參數，也就是每個文檔都一樣，因此生成過程只採樣一次。</li><li><strong>document-level（橙色）：</strong> θ是文檔級別的變量，每個文檔對應一個θ。</li><li><strong>word-level（綠色）：</strong> z和w都是單詞級別變量，z由θ生成，w由z和β共同生成，一個單詞w對應一個主題z。</li></ul><p style=text-align:start>通過上面對LDA生成模型的討論，可以知道<strong>LDA模型主要是想從給定的輸入語料中學習訓練出兩個控制參數α和β</strong>，當學習出了這兩個控制參數就確定了模型，便可以用來生成文檔。其中α和β分別對應以下各個信息：</p><ul><li><strong>α</strong>：分佈p(θ)需要一個向量參數，即Dirichlet分佈的參數，用於生成一個主題θ向量；</li><li><strong>β</strong>：各個主題對應的單詞概率分佈矩陣p(w|z)。</li></ul><p style=text-align:start>把w當做觀察變量，θ和z當做隱藏變量，就可以通過EM算法學習出α和β，求解過程中遇到後驗概率p(θ,z|w)無法直接求解，需要找一個似然函數下界來近似求解，原作者使用基於分解（factorization）假設的變分法（varialtional inference）進行計算，用到了EM算法。每次E-step輸入α和β，計算似然函數，M-step最大化這個似然函數，算出α和β，不斷迭代直到收斂。</p><p>相關人工智能與異構計算的知識分享，歡迎關注我的公眾號【<strong>AI異構】</strong></p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>簡明</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/843e90b.html alt=機器學習西瓜書簡明筆記（5）神經網絡 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/409546226ff447c88abbc570a41e75da style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/843e90b.html title=機器學習西瓜書簡明筆記（5）神經網絡>機器學習西瓜書簡明筆記（5）神經網絡</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>