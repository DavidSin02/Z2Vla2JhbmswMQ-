<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>自然語言處理中的遷移學習(上) | 极客快訊</title><meta property="og:title" content="自然語言處理中的遷移學習(上) - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RfRw76K9qI7Kdu"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/91a0fd9b.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/91a0fd9b.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/91a0fd9b.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/91a0fd9b.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/91a0fd9b.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/91a0fd9b.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/91a0fd9b.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/91a0fd9b.html><meta property="article:published_time" content="2020-11-14T21:01:59+08:00"><meta property="article:modified_time" content="2020-11-14T21:01:59+08:00"><meta name=Keywords content><meta name=description content="自然語言處理中的遷移學習(上)"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/91a0fd9b.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>自然語言處理中的遷移學習(上)</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw76K9qI7Kdu><p>作者 | 哈工大SCIR 徐嘯</p><p>編輯 | 唐裡</p><p toutiao-origin=p>本文轉載自<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-3">公眾號</i>「哈工大SCIR」（<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-4">微信</i><i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">ID：</i>HIt_SCIR），該<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-3">公眾號</i>為哈爾濱工業大學社會計算與信息檢索研究中心（劉挺教授為中心<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">主任</i>）的師生的信息分享平臺，本文作者為哈工大SCIR 徐嘯。</p><p toutiao-origin=p>來源：Transfer Learning in Natural Language Processing Tutorial (NAACL 2019)</p><p toutiao-origin=p>作者：Sebastian Ruder, Matthew Peters, Swabha Swayamdipta, Thomas Wolf</p><p>相關資源：</p><ul><li><p><strong toutiao-origin=p>Slides: tiny.cc/NAACLTransfer</strong></p></li><li><p><strong toutiao-origin=p>Colab: tiny.cc/NAACLTransferCo</strong></p></li><li><p><strong toutiao-origin=p>Code: github<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-4">.com</i>/huggingface/</strong></p></li><li><blockquote toutiao-origin=p>Ruder 對教程進行了一定擴展的博文 The State of Transfer Learning in NLP，AI科技評論 已對此文進行了翻譯</blockquote></li><li><blockquote toutiao-origin=p>Sebastian Ruder: Transfer Learning in Open-Source Natural Language Processing (spaCy IRL 2019)</blockquote></li></ul><blockquote toutiao-origin=p>本文小結：本文為教程的第一篇，包含教程的 0-2 部分。主要是對遷移學習的介紹以及預訓練方面的經典方法。</blockquote><p></p><h1 toutiao-origin=h2>提綱</h1><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw76fAlAwY3S><ol><li><p>介紹：本節將介紹本教程的主題：遷移學習當前在自然語言處理中的應用。在不同的遷移學習領域中，我們主要定位於順序遷移學習 sequential transfer learning 。</p></li><li><p>預訓練：我們將討論無監督、監督和遠程監督的預訓練方法。</p></li><li><p>表示捕獲了什麼：在討論如何在下游任務中使用預訓練的表示之前，我們將討論分析表示的方法，以及觀察到它們捕獲了哪些內容。</p></li><li><p>調整：在這個部分，我們將介紹幾種調整這些表示的方法，包括特徵提取和微調。我們將討論諸如學習率安排、架構修改等的實際考慮。</p></li><li><p>下游應用程序：本節，我們將重點介紹預訓練的表示是如何被用在不同的下游任務中的，例如文本分類、自然語言生成、結構化預測等等。</p></li><li><p>開放問題和方向：在最後一節中，我們將提出對未來的展望。我們將突出待解決的問題以及未來的研究方向。</p></li></ol><p></p><h1 toutiao-origin=h2>0. 前言</h1><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RfRw76rAzOgtla><p>A Survey on Transfer Learning, Pan and Yang (2010) 摘要：在許多機器學習和數據挖掘算法中，一個主要的假設是訓練和未來的數據必須在相同的特徵空間中，並且具有相同的分佈。然而，在許多實際應用程序中，這種假設可能不成立。例如，我們有時在一個感興趣的領域中有一個分類任務，但是我們只在另一個感興趣的領域中有足夠的訓練數據，並且後者可能位於不同的特徵空間或遵循不同的數據分佈。在這種情況下，如果成功地進行知識遷移，就可以避免昂貴的數據標記工作，從而<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-5">大大</i>提高學習性能。近年來，遷移學習作為一種新的學習框架應運而生。本研究的重點是分類和回顧目前遷移學習在分類、迴歸和聚類問題上的進展。在本研究中，我們討論了遷移學習與其他相關機器學習技術的關係，如領域適應、多任務學習、樣本選擇偏差以及協變量偏移。同時，我們也探討了遷移學習研究中一些潛在的未來問題。</p><p><strong>為什麼要在自然語言處理任務中使用遷移學習 ?</strong></p><ul><li><p>許多 NLP 任務共享關於語言的常識 (例如語言表示、結構相似性)</p></li><ul><li><p>跨任務共享的、不同層次的，含義和結構的表示</p></li></ul><li><p>任務之間可以互通有無——例如語法和語義</p></li><li><p>帶註釋的數據很少，應當儘可能多地利用其進行監督學習</p></li><li><p>經驗上看，遷移學習促成了許多有監督的 NLP 任務的 SOTA (如分類、信息提取、問答等)。</p></li></ul><p><strong>為什麼要在自然語言處理任務中使用遷移學習 ?（經驗之談）</strong></p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw775H4cZu96><p><strong>自然語言處理任務中的遷移學習種類</strong></p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw77S9fBIpyI><p>Ruder (2019)</p><p>遷移學習包括 Transductive 與 Inductive 兩種：</p><ul><li><p>Transductive：相同的任務；但只有原領域的標註數據</p></li><ul><li><p>不同的領域：領域適應</p></li><li><p>不同的語言：跨語種學習</p></li></ul><li><p>Inductive：不同的任務；只有目標領域的標註數據</p></li><ul><li><p>同時學習任務：多任務學習</p></li><li><p>順序學習任務：順序遷移學習（教程重點）</p></li></ul></ul><p><strong>這篇tutorial與什麼有關？</strong></p><ul><li><p>目標：提供NLP中遷移方法的廣泛概述，重點介紹截至目前(2019年年中)最成功的經驗方法。</p></li><li><p>提供實用的動手<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">建議</i>→在教程結束時，每個人都有能力將最新的進展應用到文本分類任務</p></li><li><p>這不是：全面的(不可能在一個教程中涵蓋所有相關的論文！)</p></li><li><p>(Bender Rule：本教程主要針對用英語完成的工作，對其他語言的可擴展性取決於數據和資源的可用性。)</p></li></ul><p><strong>1. 介紹</strong></p><p>Sequential transfer learning 指的是在一個任務/數據集上學習後遷移到其他任務/數據集上。</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw7Wz9A8M36f><p><strong>預訓練的任務和數據集</strong></p><ul><li><p>Unlabeled data and self-supervision</p></li><ul><li><p>很容易收集非常大的語料庫：維基百科，新聞，網頁抓取，社交媒體等。</p></li><li><p>利用分佈假設進行訓練：“You shall know a word by the company it keeps”(Firth, 1957)，通常形式化為訓練某種語言模型的變體</p></li><li><p><i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">關注</i>使用有效的算法以利用豐富的數據</p></li></ul><li><p>Supervised pretraining</p></li><ul><li><p>在視覺領域很常見，但由於缺乏大型監督數據集，在NLP中較少</p></li><li><p>機器翻譯</p></li><li><p>NLI 用於學習句子表示</p></li><li><p>任務特定——從一個問答數據集遷移到另一個</p></li></ul></ul><p><strong>目標任務和數據集</strong></p><ul><li><p>目標任務通常是監督的，並跨越一系列常見的NLP任務:</p></li><ul><li><p>句子或文檔分類(例如情感分類)</p></li><li><p>句子對分類(如NLI, paraphrase)</p></li><li><p>單詞級別(如序列標註、抽取問答)</p></li><li><p>結構化預測(例如解析)</p></li><li><p>生成(例如對話、總結)</p></li></ul></ul><p><strong>從單詞到文本中的單詞</strong></p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw7XG6Z6A0vC><p><strong>語言模型預訓練</strong></p><ul><li><p>許多成功的預訓練方法都是基於語言模型的</p></li><li><p>非正式地，語言模型學習</p></li><li><p>不需要人工註釋</p></li><li><p>許多語言都有足夠的文本來學習高容量模型</p></li><li><p>語言模型是“多才多藝”的——可以學習句子和單詞的表示，具有多種目標函數</p></li></ul><p><strong>由淺到深</strong></p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw7Xf1ORHXnD><p>Bengio et al 2003: A Neural Probabilistic Language Model Devlin et al 2019: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p><p><strong>預訓練任務 vs 目標任務</strong></p><p>預訓練任務的選擇和目標任務是耦合的</p><ul><li><p>句子/文檔表示對單詞級別的預測沒有幫助</p></li><li><p>詞向量可以跨上下文進行池化，但通常會被其他方法超越 (Attention)</p></li><li><p>在語境詞向量中，雙向語境非常重要</p></li></ul><p>通常：相似的預訓練和目標任務 → 最好結果</p><p></p><h1 toutiao-origin=h2>2. 預訓練</h1><p><strong>語言模型預訓練</strong></p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw7YMDmfNwDT><p><strong>詞向量</strong></p><p>為什麼要詞嵌入？</p><ul><li><p>詞嵌入是可以學習的參數</p></li><li><p>在不同任務中共享表示</p></li><li><p>低維空間更好計算——難以處理稀疏向量</p></li></ul><p>無監督預訓練 : 神經網絡之前</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw7rSAeadGLy><p>詞向量預訓練</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw7rjAzIgQPy><p>word2vec</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw7rw3wjvUpT><p>以 CBOW 為例，輸入層將目標詞語境 c 中的每一個詞向量簡單求和（也可求平均）後得到語境向量，然後直接與目標詞的輸出向量求點積，目標函數也就是要讓這個與目標詞向量的點積取得最大值，對應的與非目標詞的點積儘量取得最小值。</p><ul><li><p>word2vec 的出現，極大促進了 NLP 的發展，尤其是促進了深度學習在 NLP 中的應用（不過word2vec 算法本身其實並不是一個深度模型，它只有兩層全連接），利用預訓練好的詞向量來初始化網絡結構的第一層幾乎已經成了標配，尤其是在只有少量監督數據的情況下，如果不拿預訓練的 embe<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-5">dd</i>ing 初始化第一層，幾乎可以被認為是在蠻幹。</p></li></ul><p>Word2vec 與 NNLM 都是語言模型，但不同的是 word2vec 的重點是通過訓練語言模型從而得到詞向量，因此以詞向量為重點對 NNLM 進行了改進優化，並且使用 CBOW 和 Skip-gram 兩種方式學習詞向量，GloVe 則通過構建共現矩陣，不通過傳統的 SVD 進行計算複雜度較高的矩陣分解，而是使用平方誤差促使點積儘可能得接近共現概率的對數，因為如果使向量點積等於共現概率的對數，向量差異就會成為共現概率的比率即單詞 j 出現在單詞 i 的上下文中的概率，這一比值蘊含了語義信息。</p><p>相比word2vec，GloVe卻更加充分的利用了詞的共現信息，word2vec中則是直接粗暴的讓兩個向量的點乘相比其他詞的點乘最大，至少在表面上看來似乎是沒有用到詞的共現信息，不像GloVe這裡明確的就是擬合詞對的共現頻率。</p><p>fastText 則是利用帶有監督標記的文本分類數據完成訓練，框架和 CBOW 一致，不過輸入數據不再是 bag-of-words 的信息，還<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-5">加上</i>了 ngram 信息，這就加入了語序信息，而且輸出的是當前輸入文本的類別。此外還引入 subword 來處理長詞，處理 OOV 問題。</p><p><strong>句子和文檔向量</strong></p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw7s86Sa0NET><p>無監督篇章嵌入 (Le & Mikolov, 2014)</p><p>PV-DM的全稱是 Distributed Memory Model of Paragraph Vectors，和CBOW類似，也是通過上下文預測下一個詞，不過在輸入層的時候，同時也維護了一個文檔ID映射到一個向量的look-up table，模型的目的便是將當前文檔的向量以及上下文向量聯合輸入模型，並讓模型預測下一個詞，訓練結束後，對於現有的文檔，便可以直接通過查表的方式快速得到該文檔的向量，而對於新的一篇文檔，那麼則需要將已有的look-up table添加相應的列，然後重新走一遍訓練流程，只不過此時固定好其他的參數，只調整look-up table，收斂後便可以得到新文檔對應的向量了。PV-DBOW的全稱則是Distributed Bag of Words version of Paragraph Vector，和Skip-gram類似，通過文檔來預測文檔內的詞，訓練的時候，隨機採樣一些文本片段，然後再從這個片段中採樣一個詞，讓PV-DBOW模型來預測這個詞，以此分類任務作為訓練方法，說白了，本質上和Skip-gram是一樣的。這個方法有個致命的弱點，就是為了獲取新文檔的向量，還得繼續走一遍訓練流程，並且由於模型主要是針對文檔向量預測詞向量的過程進行建模，其實很難去表徵詞語之間的更豐富的語義結構，所以這兩種獲取文檔向量的方法都未能大規模應用開來。</p><p>Skip-Thought</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw7sP8FP6ZxE><p>借鑑 Skip-gram 的思想，Skip-thoughts 直接在句子間進行預測，也就是將 Skip-gram 中以詞為基本單位，替換成了以句子為基本單位，具體做法就是選定一個窗口，遍歷其中的句子，然後分別利用當前句子去預測和輸出它的上一句和下一句。</p><p>不過和普通框架不一樣的是，Skip-thoughts有兩個Decoder。在今天看來，這個框架還有很多不完善或者可以改進的地方（作者也在論文中分別提到了這些future works），比如輸入的Encoder可以引入attention機制，從而讓Decoder的輸入不再只是依賴Encoder最後一個時刻的輸出；Encoder和Decoder可以利用更深層的結構；Decoder也可以繼續擴大，可以預測上下文中更多的句子；RNN也不是唯一的選擇，諸如CNN以及2017年穀歌提出的Transformer的結構也可以利用進來，後來果不其然，谷歌的BERT便借鑑了這一思路。</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw8Kh5LKfKh8><p>Quick-thoughts 在此基礎上進一步改進，將生成任務改為分類任務。具體說來就是把同一個上下文窗口中的句子對標記為正例，把不是出現在同一個上下文窗口中的句子對標記為負例，並將這些句子對輸入模型，讓模型判斷這些句子對是否是同一個上下文窗口中，很明顯，這是一個分類任務。可以說，僅僅幾個月之後的BERT正是利用的這種思路。而這些方法都和Skip-thoughts一脈相承。</p><p>自編碼器預訓練</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw8KtERiuEvC><p>半監督序列學習</p><p>論文提出了兩個方法，用無標籤數據進行無監督訓練的參數來初始化有監督學習的模型：一種是用基礎的語言模型，另一種是用seq2seq自編碼模型(sequence autoencoder, SA-LSTM)，encoder 輸入為WXYZ，decoder輸出為依然為WXYZ。和普通的seq2seq模型相比不同的是，這裡的encoder和decoder隱層是共享的。有種提前讓循環神經網絡學會句子的表達，再之後根據標籤去學習分類的能力的思想。</p><p>有監督的句子嵌入</p><ul><li><p>Paragram-phrase: uses paraphrase database for supervision, best for paraphrase and semantic similarity (Wieting et al.2016)</p></li><li><p>InferSent: bi-LSTM trained on SNLI + MNLI (Conneau et al.2017)</p></li><li><p>GenSen: multitask training(skip-thought, machine translation, NLI, parsing) (Subramanian et al. 2018)</p></li></ul><p>InferSent 是在斯坦福的SNLI（Stanford Natural Language Inference）數據集上訓練的模型，而後將訓練好的模型當做特徵提取器，以此來獲得一個句子的向量表示，再將這個句子的表示應用在新的分類任務上，來評估句子向量的優劣。框架結構如下圖所示</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw8LH5SRw5Q3><p>這個框架最底層是一個 Encoder，也就是最終要獲取的句子向量提取器，然後將得到的句子向量通過一些向量操作後得到句子對的混合語義特徵，最後接上全連接層並做 SNLI 上的三分類任務。</p><p><strong>上下文相關詞向量</strong></p><p>動機：詞向量將所有的上下文都壓縮到一個單一向量中</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw8LT9nLdF5X><p>關鍵想法：不是每個單詞學習一個向量，而是學習依賴於上下文的向量。</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw8Lf1xmAaJx><p><strong>context2vec</strong></p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw8nVB8i4XCS><p>基於 CBOW 框架，為了捕捉句子語境的本質，使用雙向 LSTM 提取特徵。</p><p><strong>TagLM</strong></p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw8nsIPdLf4U><p>TagLM workflow</p><ul><li><p>與上文無關的單詞嵌入 + RNN model 得到的 hi<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-5">dd</i>en states 作為特徵輸入</p></li></ul><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw8oAIxJkFEF><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw8oMHGQLemC><ul><li><p>Char CNN / RNN + Token Embe<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-5">dd</i>ing 作為 bi-LSTM 的輸入</p></li><li><p>得到的 hi<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-5">dd</i>en states 與 Pre-trained bi-LM（凍結的） 的 hi<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-5">dd</i>en states 連接起來輸入到第二層的 bi-LSTM 中</p></li></ul><p>Seq2Seq的無監督預訓練</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw8oa3HGabw8><p>提出一種通用的提高seq2seq模型的無監督訓練方法。seq2seq模型的encoder和decoder的權重用兩個預訓練語言模型初始化然後微調。</p><p>seq2seq模型的缺點：監督學習的語料有限，容易過擬合。本文提出了改善seq2seq效果的無監督訓練方法。在微調階段，訓練任務為語言模型任務和seq2seq的聯合任務。(開始fune-tuning可能導致災難性的遺忘：模型在語言模型上的性能急劇下降，可能損害模型的泛化能力。為保證模型不在有監督語料上過擬合，在fine-tuning階段繼續單語言語言模型任務，seq2seq和語言模型任務的損失相加作為最終損失)</p><p>此外還用了殘差連接，Encoder 和 Decoder 之間也用了 Attention。</p><p><strong>CoVe</strong></p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw9FQCMwLrrw><p>CoVe更側重於如何將現有數據上預訓練得到的表徵遷移到新任務場景中，而之前的句子級任務中大多數都只把遷移過程當做一個評估他們表徵效果的手段，因此觀念上有所不同</p><ul><li><p>也有使用訓練好的序列模型為其他NLP模型提供上下文的想法</p></li><li><p>想法：機器翻譯是為了保存意思，所以這也許是個好目標？</p></li><li><p>使用seq2seq + attention NMT system中的Encoder，即 2層 bi-LSTM ，作為上下文提供者</p></li><li><p>所得到的 CoVe 向量在各種任務上都優於 GloVe 向量</p></li><li><p>但是，結果並不像其他更簡單的NLM培訓那麼好，所以似乎被放棄了</p></li><li><p>也許NMT只是比語言建模更難？</p></li><li><p>或許有一天這個想法會回來？</p></li></ul><p><strong>ELMo</strong></p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw9FkBlvMOtx><p>使用長上下文而不是上下文窗口學習 word token 向量(這裡，整個句子可能更長)</p><p>學習深度Bi-NLM，並在預測中使用它的所有層</p><p>訓練一個雙向LM</p><p>目標是 performant 但LM不要太大</p><p>使用2個biLSTM層</p><p>這兩個biLSTM NLM層有不同的用途/含義</p><p>低層更適合低級語法，例如</p><ul><li><p>詞性標註(part-of-speech tagging)、句法依賴(syntactic dependency)、NER</p></li></ul><p>高層更適合更高級別的語義</p><ul><li><p>情緒、Semantic role labeling 語義角色標記 、question answering、SNLI</p></li></ul><p>目標函數為</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw9G77rA9P7W><p>(僅)使用字符CNN構建初始單詞表示</p><p>如下圖所示，在輸入層和輸出層均使用瞭如下CNN結構，減少了參數規模，解決了 OOV 問題，並且每一個詞向量的計算可以預先做好，更能夠減輕inference階段的計算壓力</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw9GLJ9tF0Ie><p>2048 個 char n-gram filters 和 2 個 highway layers，512 維的 projection</p><p>4096 dim hi<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-5">dd</i>en/cell LSTM狀態，使用 512 dim的對下一個輸入的投影</p><p>使用殘差連接</p><p>綁定 token 的輸入和輸出的參數(softmax)，並將這些參數綁定到正向和反向LMs之間</p><p>ELMo學習biLM表示的特定任務組合</p><p>這是一個創新，TagLM 中僅僅使用堆疊LSTM的頂層，ELMo 認為BiLSTM所有層都是有用的</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw9GYITFlHTw><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw9f88wWEPj9><p>衡量ELMo對任務的總體有用性，是為特定任務學習的全局比例因子</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw9fM6NfoDo6><p>是 softmax 歸一化的混合模型權重，是 BiLSTM 的加權平均值的權重，對不同的任務是不同的</p><p>對於每一個詞，可以根據下面的式子得到它的向量，其中 γ 是一個scale因子，加入這個因子主要是想要將ELMo的向量與具體任務的向量分佈拉平到同一個分佈水平，這個時候便需要這麼一個縮放因子了。另外， 便是針對每一層的輸出向量，利用一個softmax的參數來學習不同層的權值參數，因為不同的任務需要的詞語意義的粒度也不一致，一般認為淺層的表徵比較傾向於句法，而高層輸出的向量比較傾向於語義信息，因此通過一個softmax的結構讓任務自動去學習各層之間的權重，自然也是比較合理的做法。</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RfRw9fX3G99WFK><p>此外，ELMo 還使用了 Exploring the Limits of Language Modeling 中提出的 char-based CNN 結構，應用到輸入層和輸出層上，減少了參數規模，並解決了令人頭痛的 OOV 問題。</p><ul><li><p>ELMo 使用 LSTM 而不是 Transformer 作為特徵抽取器，而很多研究已經證明了Transformer提取特徵的能力是要遠強於LSTM的；另外，ELMO 採取雙向拼接這種融合特徵的能力可能比 Bert 一體化的融合特徵方式弱，但是，這只是一種從道理推斷產生的懷疑，目前並沒有具體實驗說明這一點。</p></li><li><p>ELMo 代表著基於特徵融合的預訓練方法，而 GPT 則是基於Fine-tuning的模式的開創者。</p></li></ul><p><strong>ULMFit</strong></p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw9fkEJ787nR><ul><li><p>在大型通用領域的無監督語料庫上使用 biLM 訓練</p></li><li><p>在目標任務數據上調整 LM</p></li><li><p>對特定任務將分類器進行微調</p></li><li><p>使用合理大小的“1 GPU”語言模型，並不是真的很大</p></li></ul><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRw9g8IpsleWc><ul><li><p>在LM調優中要注意很多</p></li><ul><li><p>ULMFit的預訓練和finetune過程主要可以分為三個階段，分別是在大規模語料集上（比如Wikitext 103，有103million個詞）先預訓練，然後再將預訓練好的模型在具體任務的數據上重新利用語言模型來finetune一下（這是第一次finetune，叫做LM finetune），爾後再根據具體任務設計的一個模型上，將預訓練好的模型當做這個任務模型的多層，再一次finetune（這是第二次finetune，如果是分類問題的話可以叫做Classifier finetune）</p></li></ul></ul><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRwA3d7XSWN86><ul><li><p>AWD-LSTM</p></li></ul><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RfRwA3qGBZGoHA><p>其中 T 是一個閾值，而 K 則是總共的迭代次數，這個式子的意思就是把迭代到第T次之後，對該參數在其後的第 T 輪到最後一輪之間的所有值求平均，從而得到最後模型的該參數值，而相應的，普通的SGD則是直接取</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRwA42w5ZYzM><p>作為最後模型的參數值。並且在每個時間步之間都是用一個全連接層，並且使用DropConnect的方法隨機drop掉一些連接減少一些過擬合的風險</p><p>微調技巧</p><p>有區分的微調</p><p>針對不同的層在訓練更新參數的時候，賦予不同的學習率。這裡的出發點是，一般來說，對於NLP的深度學習模型來說，不同層的表徵有不同的物理含義，比如淺層偏句法信息，高層偏語義信息，因此對於不同層的學習率不同，自然就是比較合理的了。原文也給出了具體的選擇：先指定最後一層的學習率，然後根據下式得到前面層的學習率，基本思想是讓淺層的學習率要更小一些。</p><p>斜三角學習率</p><p>在finetune的第一階段，希望能夠先穩定住原來已經在大規模語料集上已經預訓練好的參數，所以選擇一個比較小的finetune學習率；爾後希望能夠逐步加大學習率，使得學習過程能夠儘量快速；最後，當訓練接近尾聲時，逐步減小學習率，這樣讓模型逐漸平穩收斂（這個思想大概借鑑了2017年穀歌提出Transformer時用到的warm up的學習率調節方法，這個方法也是在訓練的時候先將學習率逐步增大，爾後再逐步減小）。因此，這樣一個三段論式的學習過程，用圖表示如下：</p><p>逐漸解凍</p><p>主要思想是把預訓練的模型在新任務上finetune時，逐層解凍模型，也就是先finetune最後一層，然後再解凍倒數第二層，把倒數第二層和最後一層一起finetune，然後再解凍第三層，以此類推，逐層往淺層推進，最終finetune整個模型或者終止到某個中間層。這樣做的目的也是為了finetune的過程能夠更平穩。</p><p>因為ULMFiT中包含了兩次finetune，即在新任務上用語言模型finetune和在新任務上finetune訓練一個最終的task-specifi-model（比如分類器），而論文中主要把discriminative fine-tuning, slanted triangular learning rates這兩個技巧用在了語言模型的finetune階段，把最後一個gradual unfreezing的技巧應用在最終task-specifi-model的finetune階段。</p><p>使用大型的預訓練語言模型是一種提高性能的非常有效的方法</p><p><strong>GPT</strong></p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRwA4RHsrX1Tw><ul><li><p>GPT也採用兩階段過程，第一個階段是利用語言模型進行預訓練，第二階段通過Fine-tuning的模式解決下游任務。下圖展示了GPT的預訓練過程（按照論文中的說法，GPT中使用的Transformer是隻用了Decoder），其實和ELMO是類似的，主要不同在於兩點：首先，特徵抽取器不是用的RNN，而是用的Transformer，上面提到過它的特徵抽取能力要強於RNN，這個選擇很明顯是很明智的；其次，GPT的預訓練雖然仍然是以語言模型作為目標任務，但是採用的是單向的語言模型，所謂“單向”的含義是指：語言模型訓練的任務目標是根據單詞的上下文去正確預測單詞 ，GPT則只採用這個單詞的上文來進行預測，而拋開了下文。</p></li><li><p>下游任務怎麼使用 GPT 呢？首先，對於不同的下游任務來說，本來你可以任意設計自己的網絡結構，現在不行了，你要向GPT的網絡結構看齊，把任務的網絡結構改造成和GPT的網絡結構是一樣的。然後，在做下游任務的時候，利用第一步預訓練好的參數初始化GPT的網絡結構，這樣通過預訓練學到的語言學知識就被引入到你手頭的任務裡來了，這是個非常好的事情。再次，你可以用手頭的任務去訓練這個網絡，對網絡參數進行Fine-tuning，使得這個網絡更適合解決手頭的問題。</p></li></ul><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRwAMu5nE0t9o><ul><li><p>對於NLP各種花樣的不同任務，怎麼改造才能靠近GPT的網絡結構呢？</p></li><li><p>GPT論文給了一個改造施工圖如上，其實也很簡單：對於分類問題，不用怎麼動，<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-5">加上</i>一個起始和終結符號即可；對於句子關係判斷問題，比如Entailment，兩個句子中間再加個分隔符即可；對文本相似性判斷問題，把兩個句子順序顛倒下做出兩個輸入即可，這是為了告訴模型句子順序不重要；對於多項選擇問題，則多路輸入，每一路把文章和答案選項拼接作為輸入即可。從上圖可看出，這種改造還是很方便的，不同任務只需要在輸入部分施工即可。</p></li></ul><p><strong>BERT</strong></p><p>Bert 採用和GPT完全相同的兩階段模型，首先是語言模型預訓練；其次是使用Fine-Tuning模式解決下游任務。和GPT的最主要不同在於在預訓練階段採用了類似ELMO的雙向語言模型，當然另外一點是語言模型的數據規模要比GPT大。</p><p>BERT最主要的幾個特徵分別是</p><ul><li><p>利用了真雙向的Transformer</p></li><li><p>為了利用雙向信息，改進了普通語言模型成為完形填空式的Mask-LM(Mask-Language Model)</p></li><li><p>利用Next Sentence Prediction任務學習句子級別信息</p></li><li><p>進一步完善和擴展了GPT中設計的通用任務框架，使得BERT能夠支持包括：句子對分類任務、單句子分類任務、閱讀理解任務和序列標註任務</p></li></ul><p>預訓練階段</p><p>因為Encoder中用了Self-attention機制，而這個機制會將每一個詞在整個輸入序列中進行加權求和得到新的表徵，更通俗的說法是每一個詞在經過Self-attention之後，其新的表徵將會是整個輸入序列中所有詞（當然也包括它本身）的加權求和。在ELMo與GPT中，它們並沒有用上這種交融模式，也就是它們本質上還是一個單向的模型，ELMo稍微好一點，將兩個單向模型的信息concat起來，GPT則只用了單向模型，這是因為它沒有用上Transformer Encoder，只用了Decdoer的天生基因決定的，其實，很多人就把這種left-to-right的Transformer框架叫做Decoder，因為事實上Decoder就是如此（具體做的時候需要提前把未來待生成的詞做好mask，細節上通過上三角矩陣來實現），這也是OpenAI把他們的模型叫做"Generative"的原因所在。</p><blockquote><div><p>“GPT則只用了單向模型，這是因為它沒有用上Transformer Encoder，只用了Decdoer的天生基因決定的” ，個人認為主要想表述的是 GPT 使用 Transformer 架構時，受限於傳統的語言模型，產生的假雙向的問題，簡單來說就是GPT並沒有像 Transformer 中的 encoder 一樣對整句話進行 self-attention ，而是像 Decoder 一樣，預測每個單詞時只有其上文進行了 self-attention</p></div></blockquote><p>Masked-LM 雙向Transformer下的語言模型</p><ul><li><p>然而在語言模型中，我們通過某個詞的上下文語境預測當前詞的概率，如果直接把這個套用到Transformer的Encoder中，會發現待預測的輸出和序列輸入已經糅合在一塊了。那麼，如何解決Self-attention中帶來了表徵性能卓越的雙向機制，卻又同時帶來了信息<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-3">洩露</i>的這一問題？Bert 受到完形填空任務的啟發：輸入序列依然和普通Transformer保持一致，只不過把挖掉的一個詞用"[MASK]"替換 ，Transformer的Encoder部分按正常進行，輸出層在被挖掉的詞位置，接一個分類層做詞典大小上的分類問題，得到被mask掉的詞概率大小。</p></li></ul><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRwANBCTpmwoi><ul><li><p>直接把普通語言模型中的生成問題（正如GPT中把它當做一個生成問題一樣，雖然其本質上也是一個序列生成問題），變為一個簡單的分類問題，並且也直接解決了Encoder中多層Self-attention的雙向機制帶來的洩密問題（單層Self-attention是真雙向，但不會帶來洩密問題，只有多層<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">累加</i>的Self-attention才會帶來洩密問題），使得語言模型中的真雙向機制變為現實。</p></li><li><p>不過，BERT針對如何做“[MASK]”，做了一些更深入的研究，它做了如下處理</p></li><ul><li><p>選取語料中所有詞的15%進行隨機mask</p></li><li><p>選中的詞在80%的概率下被真實mask</p></li><li><p>選中的詞在10%的概率下不做mask，而被隨機替換成其他一個詞</p></li><li><p>選中的詞在10%的概率下不做mask，仍然保留原來真實的詞</p></li></ul><li><p>這使得Transformer編碼器不知道它將被要求預測哪些單詞或哪些單詞已被隨機單詞替換，因此它被迫保持每個輸入標記的分佈式上下文表示。</p></li></ul><p>預測下一句</p><ul><li><p>利用和借鑑了Skip-thoughts方法中的句子預測問題，來學習句子級別的語義關係，具體做法則是將兩個句子組合成一個序列，當然組合方式會按照下面將要介紹的方式，然後讓模型預測這兩個句子是否是先後近鄰的兩個句子，也就是會把"Next Sentence Prediction"問題建模成為一個二分類問題。訓練的時候，數據中有50%的情況這兩個句子是先後關係，而另外50%的情況下，這兩個句子是隨機從語料中湊到一起的，也就是不具備先後關係，以此來構造訓練數據。句子級別的預測思路和之前介紹的Skip-thoughts基本一致，當然更本質的思想來源還是來自於word2vec中的skip-gram模型。</p></li><li><p>要求模型除了做上述的Masked語言模型任務外，附帶再做個句子關係預測，判斷第二個句子是不是真的是第一個句子的後續句子。之所以這麼做，是考慮到很多NLP任務是句子關係判斷任務，單詞預測粒度的訓練到不了句子關係這個層級，增加這個任務有助於下游句子關係判斷任務。所以可以看到，它的預訓練是個多任務過程。這也是Bert的一個創新。</p></li><ul><li><p>spanBERT 與 RoBERTa 兩篇文章對此提出了質疑，感興趣的讀者可以自行閱讀~</p></li></ul><li><p>在預訓練階段，因為有兩個任務需要訓練：Mask-LM和Next Sentence Prediction，因此BERT的預訓練過程實質上是一個Multi-task Learning，具體說來，BERT的損失函數由兩部分組成，第一部分是來自於Mask-LM的單詞級別的分類任務，另一部分是句子級別的分類任務，通過這兩個任務的聯合學習，可以使得BERT學習到的表徵既有token級別的信息，同時也包含了句子級別的語義信息。具體的損失函數如下</p></li></ul><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRwANSJE3w0IA><p>Fine-Tuning 階段</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRwANg7i0XmZm><p>輸入層：如果輸入只有一個句子的話，則直接在句子的前後添加句子的起始標記位和句子的結束符號，在BERT中，起始標記都用“[CLS]”來表示，結束標記符用"[SEP]"表示，對於兩個句子的輸入情況，除了起始標記和結束標記之外，兩個句子間通過"[SEP]"來進行區分。除了這些之外，BERT還用了兩個表示當前是句子A還是句子B的向量來進行表示，對於句子A來說，每一詞都會添加一個同樣的表示當前句子為句子A的向量，相應的，如果有句子B的話，句子B中的每一個詞也都會添加一個表示當前句子為句子B的向量。</p><p>NLP 的四大任務：</p><ol><li><p>序列標註，這是最典型的NLP任務，比如中文分詞，詞性標註，命名實體識別，語義角色標註等都可以歸入這一類問題，它的特點是句子中每個單詞要求模型根據上下文都要給出一個分類類別。</p></li><li><p>分類任務，比如我們常見的文本分類，情感計算等都可以歸入這一類。它的特點是不管文章有多長，總體給出一個分類類別即可。</p></li><li><p>句子關係判斷，比如Entailment，QA，語義改寫，自然語言推理等任務都是這個模式，它的特點是給定兩個句子，模型判斷出兩個句子是否具備某種語義關係。</p></li><li><p>生成式任務，比如機器翻譯，文本摘要，寫詩造句，看圖說話等都屬於這一類。它的特點是輸入文本內容後，需要自主生成另外一段文字。</p></li></ol><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRwAO8Hk1MQJb><p>對於種類如此繁多而且各具特點的下游NLP任務，Bert如何改造輸入輸出部分使得大部分NLP任務都可以使用Bert預訓練好的模型參數呢？上圖給出示例，對於句子關係類任務，很簡單，和GPT類似，<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-5">加上</i>一個起始和終結符號，句子之間加個分隔符即可。對於輸出來說，把第一個起始符號對應的Transformer最後一層位置上面串接一個softmax分類層即可。對於分類問題，與GPT一樣，只需要增加起始和終結符號，輸出部分和句子關係判斷任務類似改造；對於序列標註問題，輸入部分和單句分類是一樣的，只需要輸出部分Transformer最後一層每個單詞對應位置都進行分類即可。從這裡可以看出，上面列出的NLP四大任務裡面，除了生成類任務外，Bert其它都覆蓋到了，而且改造起來很簡單直觀。儘管Bert論文沒有提，但是稍微動動腦子就可以想到，其實對於機器翻譯或者文本摘要，聊天機器人這種生成式任務，同樣可以稍作改造即可引入Bert的預訓練成果。只需要附著在S2S結構上，encoder部分是個深度Transformer結構，decoder部分也是個深度Transformer結構。根據任務選擇不同的預訓練數據初始化encoder和decoder即可。這是相當直觀的一種改造方法。當然，也可以更簡單一點，比如直接在單個Transformer結構上加裝隱層產生輸出也是可以的。不論如何，從這裡可以看出，NLP四大類任務都可以比較方便地改造成Bert能夠接受的方式。這其實是Bert的非常大的優點，這意味著它幾乎可以做任何NLP的下游任務，具備普適性，這是很強的。</p><p>最後，我們再次總結下BERT的幾個主要特點：</p><ul><li><p>Transformer Encoder因為有Self-attention機制，因此BERT自帶雙向功能</p></li><li><p>因為雙向功能以及多層Self-attention機制的影響，使得BERT必須使用Cloze版的語言模型Masked-LM來完成token級別的預訓練</p></li><li><p>為了獲取比詞更高級別的句子級別的語義表徵，BERT加入了Next Sentence Prediction來和Masked-LM一起做聯合訓練</p></li><li><p>為了適配多任務下的遷移學習，BERT設計了更通用的輸入層和輸出層 然後，我們再來看看BERT的工作都站在了哪些“巨人肩膀”上：</p></li><ul><li><p>針對第一點，雙向功能是Transformer Encoder自帶的，因此這個“巨人肩膀”是Transformer</p></li><li><p>第二點中Masked-LM的巨人肩膀是語言模型，CBOW以及Cloze問題</p></li><li><p>第三點中Next Sentence Prediction的“巨人肩膀”是Skip-gram，Skip-thoughts和Quick-thoughts等工作</p></li><li><p>第四點中，對輸入層和輸出層的改造，借鑑了T-DMCA以及GPT的做法</p></li></ul></ul><p>為什麼語言模型效果好?</p><ul><li><p>語言建模是一項非常困難的任務，即使對人類來說也是如此。</p></li><li><p>預期語言模型將任何可能的上下文壓縮到一個向量中，該向量概括了可能的完成後的句子。</p></li><ul><li><p>“They walked down the street to ???”</p></li></ul><li><p>為了有機會解決這個任務，模型必須學習語法、語義、對世界事實編碼等等。</p></li><li><p>給定足夠的數據、一個巨大的模型和足夠的計算，就可以完成一項合理的工作！</p></li><li><p>從經驗上看，語言模型比翻譯，自編碼更有效：“Language Modeling Teaches You More Syntax than Translation Does” (Zhang et al. 2018)</p></li></ul><p><strong>預訓練的有趣屬性</strong></p><p><strong>1. 樣本高效</strong></p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRwAoc8RYT4kV><p>ELMo 中的實驗對比</p><p>預訓練的一個主要好處就是它減少了對有標註數據的需求。在實際使用中，與非預訓練模型相比，遷移學習模型通常只需要十分之一甚至更少的樣本數量就達到類似的表現，如下圖(ULMFIT Howard and Ruder, 2018)所示。</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRwAosAoLw2yJ><p>ULMFit中的實驗對比</p><p><strong>2. 擴大預訓練規模</strong></p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRwApHIiKaiPO><ul><li><p>上圖為 GloVe 中增加語料庫規模對模型準確度的影響。</p></li><li><p>數據集越大越好，並且維基百科數據集比新聞文本數據集要好</p></li><ul><li><p>因為維基百科就是在解釋概念以及他們之間的相互關聯，更多的說明性文本顯示了事物之間的所有聯繫</p></li><li><p>而新聞並不去解釋，而只是去闡述一些事件</p></li></ul></ul><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfRwApWA1JZ4TH><p>不同數據量的 Crawl 數據作為預訓練的平均 GLUE 效果</p><p>通常，我們可以通過同時增加模型參數和預訓練數據量的方式來改善預訓練表徵。但隨著預訓練數據量的增長，回報率開始下降。但如上圖所示的當前的性能曲線，並不表示我們已達到了穩定狀態。因此，我們期待可以在更多數據基礎上訓練出更大的模型。</p><p>最近的這種趨勢的例子是ERNIE 2.0,XLNet,GPT-2 8B, 和 RoBERTa。特別是後者發現，簡單地對 BERT 進行更長時間和更多數據的訓練就可以得到更好的結果，而對 GPT-2 8B 進行更長時間和更多數據的訓練則可以減少語言建模數據集上的困惑度（儘管只是相對較小的因素）</p><p>跨語言預訓練</p><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRwApj50Gl2I0><ul><li><p>在訓練跨語言詞嵌入方面做了大量工作(Overview: Ruder et al. (2017))</p></li><li><p>想法：分別訓練每種語言，然後對齊</p></li><li><p>Recent work aligning ELMo: Schuster et al., (NAACL 2019)</p></li><li><p>ACL 2019 Tutorial on Unsupervised Cross-lingual Representation Learning</p></li></ul><p>關鍵思想:通過在多種語言上訓練一個模型，實現跨語言的詞彙表和表示。</p><p>優點:易於實現，可單獨進行跨語言預培訓</p><p>缺點:低資源語言導致其表示學習的不夠好</p><ul><li><p>LASER: Use parallel data for sentence representations(Artetxe & Schwenk, 2018)</p></li><li><p>Multilingual BERT: BERT trained jointly on 100 languages</p></li><li><p>Rosita:Polyglot contextual representations(Mulcaire et al., NAACL 2019)</p></li><li><p>XLM: Cross lingual LM (Lample & Conneau, 2019)</p></li></ul><img alt=自然語言處理中的遷移學習(上) onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/R69F3rGCWOEqko><p>招聘</p><p><strong>1</strong></p><p><strong>論文內容運營</strong></p><p toutiao-origin=p>AI研習社是一個服務AI學生、學者、從業者的UGC內容平臺，目標是從教學課程、技術經驗分享、學術見解討論、比賽和工作機會等角度提供資訊，也是用戶輸出觀點、互相交流、打造個人品牌的土壤。</p><p>工作內容：</p><p>1. AI研習社社區論文板塊運營</p><p>2. 聯繫、維護外部兼職稿源</p><p class=pgc-end-source>3. 參與多樣化內容輸出、活動策劃</p><p class=pgc-end-source>4. 參與社區產品的長期維護和改進</p><p class=pgc-end-source>任職要求：</p><p class=pgc-end-source>1. 有一定IT、計算機科學知識見聞</p><p class=pgc-end-source>2. 熱情友善，善於溝通</p><p class=pgc-end-source>3. 英語能力是加分項</p><p class=pgc-end-source>簡歷投遞地址：yangxiaofan@leiphone<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-4">.com</i></p><p class=pgc-end-source>招聘</p><p class=pgc-end-source>2</p><p class=pgc-end-source>論文內容編輯（可招實習生）</p><p toutiao-origin=p>AI研習社是一個服務AI學生、學者、從業者的UGC內容平臺，目標是從教學課程、技術經驗分享、學術見解討論、比賽和工作機會等角度提供資訊，也是用戶輸出觀點、互相交流、打造個人品牌的土壤。</p><p class=pgc-end-source>工作內容：</p><p toutiao-origin=p>1. <i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">關注</i>、瞭解人工智能相關領域學術研究動向，形成兼具專業度和傳播力的報道內容（發表在雷鋒網、<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-3">公眾號</i>以及AI研習社社區）</p><p class=pgc-end-source>2. 採訪高校學術青年領袖，輸出人工智能領域的深度觀點；</p><p class=pgc-end-source>3. 解讀國內外學術熱點，深入剖析學術動態</p><p class=pgc-end-source>任職要求：</p><p class=pgc-end-source>1. 理工科背景，有一定計算機科學知識</p><p class=pgc-end-source>2. 英語好，能閱讀英文科技網站&博客</p><p class=pgc-end-source>3. 樂於鑽研，認真嚴謹；有文字功底更佳</p><p class=pgc-end-source>簡歷投遞地址：yangxiaofan@leiphone<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-4">.com</i></p><p class=pgc-end-source>招聘</p><p class=pgc-end-source>3</p><p class=pgc-end-source>兼職外翻</p><p toutiao-origin=p>AI研習社是一個服務AI學生、學者、從業者的UGC內容平臺，目標是從教學課程、技術經驗分享、學術見解討論、比賽和工作機會等角度提供資訊，也是用戶輸出觀點、互相交流、打造個人品牌的土壤。</p><p class=pgc-end-source>工作內容：</p><p toutiao-origin=p>1. 翻譯一些人工智能相關領域的學術動態、技術博客等（文章發表在雷鋒網、<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-3">公眾號</i>以及AI研習社社區）</p><p class=pgc-end-source>2.無需坐班，工作時間、地點自由</p><p class=pgc-end-source>任職要求：</p><p class=pgc-end-source>1.英文讀寫能力較強，能閱讀英文科技網站&博客</p><p class=pgc-end-source>2.具備一定的計算機科學知識，能看懂相關的專業詞彙</p><p class=pgc-end-source>3.認真嚴謹，做事不拖拉，能夠按時完成派下來的外翻任務</p><p class=pgc-end-source>簡歷投遞地址：xinglijuan@leiphone<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-4">.com</i></p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>語言</a></li><li><a>處理</a></li><li><a>移學習</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/10c7d8fd.html alt=神州泰嶽：公司在自然語言處理領域的基礎技術研究和應用落地均走在行業前列 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/10c7d8fd.html title=神州泰嶽：公司在自然語言處理領域的基礎技術研究和應用落地均走在行業前列>神州泰嶽：公司在自然語言處理領域的基礎技術研究和應用落地均走在行業前列</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/45b722bf.html alt=第12屆自然語言處理和知識工程國際會議將在西華大學舉行 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/45b722bf.html title=第12屆自然語言處理和知識工程國際會議將在西華大學舉行>第12屆自然語言處理和知識工程國際會議將在西華大學舉行</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/49bb3bbd.html alt=第12屆自然語言處理與知識工程國際學術會議在西華大學舉行 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/4e62000034a58600d55e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/49bb3bbd.html title=第12屆自然語言處理與知識工程國際學術會議在西華大學舉行>第12屆自然語言處理與知識工程國際學術會議在西華大學舉行</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d3668904.html alt=自然語言處理（NLP）常用庫整理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/235e94cda81a4858a3000bb62b4f970d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d3668904.html title=自然語言處理（NLP）常用庫整理>自然語言處理（NLP）常用庫整理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/eabb9fa9.html alt=你對自然語言處理了解多少呢？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/150674bcc0e44efcae3427c70ad2f072 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/eabb9fa9.html title=你對自然語言處理了解多少呢？>你對自然語言處理了解多少呢？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e0b5c472.html alt=自然語言處理中的深度學習：評析與展望 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/3738e409cd4648ef9d28084a94faaade style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e0b5c472.html title=自然語言處理中的深度學習：評析與展望>自然語言處理中的深度學習：評析與展望</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/49d71ab7.html alt=自然語言處理中的語言模型簡介 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/0da5799ae4d94824b62b9e71c6e07aa3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/49d71ab7.html title=自然語言處理中的語言模型簡介>自然語言處理中的語言模型簡介</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/316cbcad.html alt=自然語言處理的十大應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/dea6cbd6fbef4e9c935b6f56cb9b0097 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/316cbcad.html title=自然語言處理的十大應用>自然語言處理的十大應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2070e90b.html alt=一文看懂自然語言處理-NLP（4個典型應用+5個難點+6個實現步驟） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d1504f3b2d614621bd4081a64ef145ca style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2070e90b.html title=一文看懂自然語言處理-NLP（4個典型應用+5個難點+6個實現步驟）>一文看懂自然語言處理-NLP（4個典型應用+5個難點+6個實現步驟）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/49c42cc2.html alt=人工智能之自然語言處理初探 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/S4bjUwAFhO20v style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/49c42cc2.html title=人工智能之自然語言處理初探>人工智能之自然語言處理初探</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ca1cc7d7.html alt=人工智能的研究熱點：自然語言處理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/5fdd13a7-6c6d-45d6-9fcd-2829793b5dd3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ca1cc7d7.html title=人工智能的研究熱點：自然語言處理>人工智能的研究熱點：自然語言處理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cb76338d.html alt=什麼是自然語言處理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/64bcc3b1fb8a4f9ca59d452035ca25cb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cb76338d.html title=什麼是自然語言處理>什麼是自然語言處理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1f73ce58.html alt=復旦大學黃萱菁：自然語言處理中的表示學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/77184e60d8e74b9da944a638e38aedfa style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1f73ce58.html title=復旦大學黃萱菁：自然語言處理中的表示學習>復旦大學黃萱菁：自然語言處理中的表示學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/84eef54a.html alt=有關自然語言處理的深度學習知識有哪些？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/b6e6ac2f1c1948158c7edbe790f52b66 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/84eef54a.html title=有關自然語言處理的深度學習知識有哪些？>有關自然語言處理的深度學習知識有哪些？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b0abef72.html alt="深度學習自然語言處理模型實現大集合（精簡版<100行）" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d1461dcb71974b569c9b1ae64e150139 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b0abef72.html title="深度學習自然語言處理模型實現大集合（精簡版<100行）">深度學習自然語言處理模型實現大集合（精簡版&lt;100行）</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>