<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>編碼器-解碼器網絡：神經翻譯模型詳解 | 极客快訊</title><meta property="og:title" content="編碼器-解碼器網絡：神經翻譯模型詳解 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/9dbe472c424b4267b58f746c9756d31e"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/91b2f7d.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/91b2f7d.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/91b2f7d.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/91b2f7d.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/91b2f7d.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/91b2f7d.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/91b2f7d.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/91b2f7d.html><meta property="article:published_time" content="2020-10-29T21:05:25+08:00"><meta property="article:modified_time" content="2020-10-29T21:05:25+08:00"><meta name=Keywords content><meta name=description content="編碼器-解碼器網絡：神經翻譯模型詳解"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/91b2f7d.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>編碼器-解碼器網絡：神經翻譯模型詳解</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9dbe472c424b4267b58f746c9756d31e><p class=pgc-img-caption></p></div><p>作者：Mac Brennan</p><p>編譯：weakish</p><p>【編者按】Mac Brennan詳細講解了用於機器翻譯的編碼器-解碼器架構。</p><h2 class=pgc-h-arrow-right>概要</h2><p>本文將講解如何創建、訓練一個法翻英的神經翻譯模型。本文的重點是解釋概念，具體的項目代碼請參考配套的Jupyter notebook（鏈接見文末）。</p><p>這篇文章大致分成兩部分：</p><ol start=1><li>理解模型概念</li><li>簡短總結模型表現</li></ol><p>這篇文章參考了PyTorch序列到序列教程，不過試圖更深入一點。感謝 <strong>Sean Robertsan</strong> 和PyTorch提供了這麼棒的教程。</p><h2 class=pgc-h-arrow-right>理解模型</h2><p>編碼器-解碼器網絡是一個很成功的翻譯模型。這個模型接受一個序列作為輸入，並將序列中的信息編碼為中間表示。然後解碼器解碼中間表示為目標語言。在我們的這個項目中，輸入序列是法語句子，輸出是相應的英語翻譯。</p><p>在我們深入編碼器和解碼器如何工作之前，我們需要了解下模型是如何表示我們的數據的。在對模型的工作機制一無所知的情況下，我們可以合理地推測如果我們給模型一個法語句子，模型能給我們對應的英語句子。也就是說，輸入一個單詞序列，模型應該輸出另一個單詞序列。然而，模型只不過是一組參數，在輸入上進行多種運算。模型並不知道什麼是單詞。類似ASCII編碼將字母映射到數字，我們的單詞也需要轉成數值表是。為此，數據集中的每個唯一的單詞需要有一個唯一的索引。模型接受的實際上不是一個單詞序列，而是一個索引序列。</p><p>一次傳入一個句子，這沒什麼問題。不過，怎樣才能一次傳入多個句子以加速訓練過程呢？句子長短不一。這些數字序列又該如何組織呢？答案是輸入序列將表示為維度等於(batch大小 × 最大句子長度)的張量（矩陣）。這樣就可以一次輸入一組句子，短於數據集中最長句的句子可以用事先確定的“補齊索引”補齊。如下圖所示：</p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/088f49ac024347b9aeca22e7b9b41ea6><p class=pgc-img-caption></p></div><h2 class=pgc-h-arrow-right>編碼器</h2><p><strong>詞嵌入</strong></p><p>輸入張量讓我們能夠以索引序列的形式輸入多個句子。這個方向是對的，但這些索引並沒有保留什麼信息。索引54代表的單詞，和索引55代表的單詞可能全無關係。基於這些索引數字進行計算沒什麼意義。這些索引需要以其他格式表示，讓模型可以計算一些有意義的東西。一種更好的表示單詞的方法是詞嵌入。</p><p>詞嵌入用N維向量表示每個單詞。相似單詞具有相似詞嵌入，在N維嵌入空間中距離相近。詞嵌入基於在某種語言任務上訓練的模型得到。幸運的是，其他研究人員已經完成了這項工作，同時發佈了相關成果。我們的項目使用的是FastText的300維詞嵌入。</p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0de78bd01ac940f58aa2fb90340b7fc8><p class=pgc-img-caption></p></div><p>將輸入句子表示為詞嵌入序列後，可以傳入編碼器的循環層。</p><p><strong>編碼器架構</strong></p><p>上述嵌入過程通過一個嵌入層完成。整個編碼器的架構如下圖所示。</p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/4514e7343244493e815df0add6300a53><p class=pgc-img-caption></p></div><p>從上圖我們可以看到，輸入張量通過嵌入層之後，到達雙向RNN層。雙向RNN既從前往後處理序列，又從後往前處理序列。從後往前處理序列時，已經看過整個序列。</p><p>獲取嵌入輸入張量後，RNN逐步處理序列中的每一項（單詞）。在每次迭代中，輸出一個長度等於編碼器隱藏尺寸的編碼向量。RNN並行處理批次中的每個樣本。每一步的輸出可以看成一個大小為(batch大小 × 編碼向量大小)的矩陣，不過實際上整個RNN所有步驟輸出一個最終張量。</p><p>在處理序列的每一步中，RNN的隱藏狀態傳給接受序列下一項作為輸入的RNN的下一次迭代。迭代同時為批次中的每個樣本輸出一個編碼向量。序列處理的每一步都輸出這樣一個“矩陣”，並與相應的反向處理序列的RNN步驟輸出的矩陣相連接。在我們的項目中，RNN單元使用了兩個循環層，中間隔著一個dropout層。另外，我們比較了兩種不同的RNN：LSTM（長短時記憶網絡）和GRU（門控循環單元）。</p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1b6a1596067d45ff88032432abb18644><p class=pgc-img-caption></p></div><p>RNN層的最終輸出是一個張量，其中每步的“矩陣”輸出堆疊在一起，如下圖所示。</p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e946221420974f70b19a2d125be5a9ab><p class=pgc-img-caption></p></div><h2 class=pgc-h-arrow-right>解碼器</h2><p>編碼器的最終隱藏狀態可以傳給另一個RNN（解碼器）。該RNN的每個輸出都是輸出序列中的一個單詞，並作為RNN下一步的輸入。然而，這樣的架構需要編碼器編碼整個輸入序列為最終隱藏狀態。相反，如果使用注意力模型，解碼器不僅接受最終隱藏狀態作為輸入，還接受編碼器處理輸入序列的每一步的輸出作為輸入。編碼器可以賦予編碼器輸出不同的權重，在計算解碼器輸出序列的每次迭代中使用。</p><p>解碼器循環層的最終輸入為注意力加權的編碼器輸出和循環單元前一步的預測單詞索引。下為這一過程的示意圖，其中“Context”（上下文）表示編碼器輸出張量。為了簡化圖形，示意圖中省略了嵌入層。</p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3287ada6e2e54cc6b2a098d9df91b25a><p class=pgc-img-caption></p></div><p>下面讓我們詳細討論下注意力模塊加權編碼器權重的方式。</p><p><strong>注意力</strong></p><p>回顧下編碼器輸出張量，序列維度的每一項保存了RNN輸出的向量。注意力模塊就批次中的每個樣本在序列維度上取這些向量的加權和。這樣，每個樣本得到一個向量，表示當前輸出序列步驟計算所需的相關信息。</p><p>下面我們將舉一個具體的例子。如果輸入句子的第一個單詞包含了給定輸出單詞所需的所有最重要的信息，那麼第一個單詞分配的權重是一，其他各項權重為零。也就是加權向量等於輸入句子的第一個單詞對應的向量。</p><p>模型需要學習如何分配這些權重，所以我們使用了一個全連接層。序列中的每個單詞對應一個權重，所以權重數量等於最長句子長度。權重之和應等於一，所以全連接層將使用softmax激活函數。注意力模塊將接受解碼器先前的隱藏狀態與解碼器前一步輸出的預測單詞的詞嵌入的連接作為輸入，從而決定這些權重的值。下為這一過程的示意圖。</p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1080f61c801148e5ae57dcc410913602><p class=pgc-img-caption></p></div><p>計算出這些權重之後，就批次中的每個樣本，對權重和編碼器輸出應用矩陣乘法，得到整個序列的編碼向量的加權和。表示批次中每個樣本的編碼器輸出的矩陣，可以看成編碼器張量的一個水平切片。下為單個樣本的計算過程示意圖。實際運算時堆疊批次中的每個樣本以構成維度為(batch大小 × 2 × 編碼器隱藏向量)的矩陣，得到加權編碼器輸出。</p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/77410c48899249bbb08f46a572ecb47c><p class=pgc-img-caption></p></div><p><strong>循環計算</strong></p><p>編碼器輸出經注意力模塊加權後，可以傳給解碼器的RNN層了。RNN層同時接受解碼器上一步預測的單詞的詞嵌入作為輸入。RNN不直接接受這兩個矩陣的連接作為輸入，它們在此之前還需通過一個使用ReLU激活的全連接層。這一層的輸出作為RNN的輸入。</p><p>RNN的輸出傳給一個全連接層，該全連接層使用對數softmax激活，節點數等於輸出語言的詞彙量。這一層的輸出表示對輸出序列中的下一個單詞的預測。這個單詞和RNN的隱藏狀態傳至注意力模塊和RNN的下一步，用來計算序列的下一項。下為這一過程的示意圖。不斷重複這一過程，直到整個輸出序列輸出完畢。</p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c09c007292e8469185e84cad1ad1d4dc><p class=pgc-img-caption></p></div><p><br></p><h2 class=pgc-h-arrow-right>訓練模型</h2><p>為訓練模型，我們需要計算一個損失函數，反向傳播誤差以更新模型參數。我們的模型計算的損失函數為輸出預測和目標翻譯之前的負對數似然，在序列上累加，在批次中取均值。在整個數據集上重複這一過程，經過足夠多的epoch後達到要求的結果。</p><p>然而，訓練語言模型要稍微複雜一點。因為解碼器依賴序列前面的項預測後面的項，較早的誤差會帶偏整個序列。這使得模型學習起來很困難。為了解決這一問題，我們使用了一種稱為教師強制（teacher forcing）的技術。這一技術的思路是某些批次（通常是隨機選擇半數）不將解碼器前一步的預測傳給下一步，而是將前一步的目標翻譯傳給下一步。應用教師強制時，解碼器每一步的計算使用的是正確的前序單詞。這一技術大大降低了訓練模型的難度。</p><p>創建和訓練模型的細節參見配套的Jupyter notebook（鏈接見文末）。下面簡短地總結下模型的表現。</p><h2 class=pgc-h-arrow-right>數據集</h2><p>我們使用了兩個數據集。第一個數據集相對簡單，詞彙量較低，句式看起來也不怎麼多樣。不過，它倒是有一個優勢，訓練起來相對較快。第二個數據集更加多樣化，儘管句長較短，但詞彙量較高，句式也更加多樣。</p><p><strong>簡單數據集</strong></p><ul><li>樣本數：137861</li><li>法語詞彙量：356</li><li>英語詞彙量：228</li><li>最長法語句長度：23個單詞</li><li>最長英語句長度：17個單詞</li></ul><p>樣本示例：</p><pre><code>法語句 paris est jamais agréable en décembre , et il est relaxant au mois d' août .  英語句 paris is never nice during december , and it is relaxing in august .法語句 elle déteste les pommes , les citrons verts et les citrons .  英語句 she dislikes apples , limes , and lemons .法語句 la france est généralement calme en février , mais il est généralement chaud en avril .  英語句 france is usually quiet during february , but it is usually hot in april .法語句 la souris était mon animal préféré moins .  英語句 the mouse was my least favorite animal .法語句 paris est parfois clémentes en septembre , et il gèle habituellement en août .  英語句 paris is sometimes mild during september , and it is usually freezing in august .</code></pre><p><strong>多樣化數據集</strong></p><ul><li>樣本數：139692</li><li>法語詞彙量：25809</li><li>英語詞彙量：12603</li><li>最長法語句長度：10個單詞</li><li>最長英語句長度：10個單詞</li></ul><pre><code>法語句 je vais laver les plats  英語句 ill wash dishes法語句 les nouvelles les rendirent heureux  英語句 the news made them happy法語句 globalement la conférence internationale fut un succès  英語句 all in all the international conference was a success法語句 comment marche cet appareil photo  英語句 how do you use this camera法語句 cest ton jour de chance  英語句 this is your lucky day</code></pre><h2 class=pgc-h-arrow-right>損失圖形</h2><p>如前所述，我們分別實現了雙向LSTM和雙向GRU。</p><p>在第一個數據集上訓練一個epoch後的損失圖形如下：</p><p><br></p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e21e80e60ea040d1b36c6e1b343f9be8><p class=pgc-img-caption></p></div><p><br></p><p>在第二個數據集上訓練50個epoch後的損失圖形如下：</p><p><br></p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/17b665d4aec249aeb3d086522104e243><p class=pgc-img-caption></p></div><p><br></p><h2 class=pgc-h-arrow-right>翻譯樣本和注意力可視化</h2><p><strong>翻譯樣本：簡單數據集</strong></p><p>如上一節所示，在簡單數據集上訓練一個epoch後，LSTM和GRU都能非常精準地生成正確翻譯。然而，模型在翻譯句子時看起來並沒有用到注意力機制。下面是3個翻譯示例，以及相應的輸出序列的注意力權重。從這些例子來看，注意力權重說不上隨機，但也沒有落在我們期望的單詞上。</p><p>示例一</p><pre><code>輸入句 californie est sec en janvier , mais il est généralement occupé en mars . &lt;/s&gt;輸出句 california is dry during january , but it is usually busy in march .&lt;/s&gt;LSTM模型輸出 california is dry during january , but it is usually busy in march . &lt;/s&gt;GRU模型輸出 california is dry during january , but it is usually busy in march . &lt;/s&gt;</code></pre><p><br></p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7798a33f539e47eea3fd8f9a0369c5ac><p class=pgc-img-caption></p></div><p><br></p><p>示例二</p><pre><code>輸入句 new jersey est généralement chaud en juin , et il est parfois merveilleux en hiver . &lt;/s&gt;輸出句 new jersey is usually hot during june , and it is sometimes wonderful in winter .&lt;/s&gt;LSTM模型輸出 new jersey is usually warm during june , and it is sometimes wonderful in winter . &lt;/s&gt;GRU模型輸出 new jersey is usually hot during june , and it is sometimes wonderful in winter . &lt;/s&gt;</code></pre><p><br></p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/491554c81c914cd1802cf12029334827><p class=pgc-img-caption></p></div><p><br></p><p>示例三</p><pre><code>輸入句&lt;unk&gt; les fraises , les mangues et le pamplemousse . &lt;/s&gt;輸出句i like strawberries , mangoes , and grapefruit .&lt;/s&gt;LSTM模型輸出i like strawberries , mangoes , and grapefruit . &lt;/s&gt;GRU模型輸出i like strawberries , mangoes , and grapefruit . &lt;/s&gt;</code></pre><p><br></p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/97db31c02ea14799836df8e733054c90><p class=pgc-img-caption></p></div><p><strong>翻譯樣本：多樣化數據集</strong></p><p>我們的模型能夠正確翻譯更加多樣化的句子。不過，這需要長很多的時間訓練，總共花了50個epoch才得到看起來合理的結果。下面顯示了其中幾個示例。GRU模型的注意力權重開始揭示模型使用了注意力機制，但LSTM模型看起來仍然沒有學習利用注意力機制。這可能是因為LSTM可以訪問保存了長期依賴的單元狀態。也許注意力計算沒能為模型提供足夠有用的信息，使模型優先學習更好的注意力計算的參數。</p><p>示例一</p><pre><code>輸入句jai perdu mon intérêt pour le golf &lt;/s&gt;輸出句ive lost interest in golf&lt;/s&gt;LSTM模型輸出i lost my interest golf &lt;/s&gt;GRU模型輸出ive lost interest in golf &lt;/s&gt;</code></pre><p><br></p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2667c26881f64f18a3034a300ecb26ec><p class=pgc-img-caption></p></div><p><br></p><p>示例二</p><pre><code>輸入句le livre était meilleur que le film &lt;/s&gt;輸出句the book was better than the movie&lt;/s&gt;LSTM模型輸出the book was better than the movie &lt;/s&gt;GRU模型輸出the book was better than the movie &lt;/s&gt;</code></pre><p><br></p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ba13009364f74112af8acb885c730d78><p class=pgc-img-caption></p></div><p><br></p><p>示例三</p><pre><code>輸入句 quel genre de trucs &lt;unk&gt; le weekend &lt;/s&gt;輸出句 what sort of things do you do on weekends&lt;/s&gt;LSTM模型輸出 what sort of things do you do on weekends &lt;/s&gt;GRU模型輸出 what sort of stuff do you do on weekends &lt;/s&gt;</code></pre><p><br></p><div class=pgc-img><img alt=編碼器-解碼器網絡：神經翻譯模型詳解 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6c7474cf0d724d5cb2c23bf81fd8ed7c><p class=pgc-img-caption></p></div><p><br></p><h2 class=pgc-h-arrow-right>結語</h2><p>GRU模型演示了注意力計算讓模型重點關注編碼序列的不同部分。然而，我們並不清楚為什麼LSTM看起來要麼沒有利用注意力信息，要麼基於一種不同的方式使用注意力信息。如果有更多時間，我們想調查下這是為什麼。如果使用句長更長的數據集還會這樣嗎？還可以和不帶注意力機制的簡單編碼器-解碼器網絡比較一下，看看錶現是否優於不帶注意力機制的架構，如果優於不帶注意力機制的架構，那麼是在哪些情況下？</p><p>我們選擇的架構和PyTorch教程中的模型略有不同。這個項目使用的模型使用了batching，而原教程中的模型每次處理一個序列。因此，原模型不必處理輸出補齊。我們本來覺得batching可以通過並行化縮短訓練時間，但原模型聲稱只需大約40分鐘就可以在CPU上完成訓練，而這個項目所用的模型在GPU上訓練了將近12小時，才得到良好的翻譯。</p><p>一些改進也許可以彌合這一差異。首先，PyTorch有內置的處理補齊序列的函數，這樣循環單元不會看到補齊項。這可能提高模型的學習能力。其次，第二個數據集沒有處理成token，只是直接移除了標點。這可能導致轉換單詞為索引時，一些單詞無法辨識。這意味著它們會被替換為未知token，使模型更難識別句子的內容。儘管還有提升的空間，總體上而言這個項目是成功的，因為它能夠成功地翻譯法語為英語。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>編碼器</a></li><li><a>解碼器</a></li><li><a>網絡</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/dfc16c0.html alt=用編碼器-解碼器-重構器框架實現英語-日語的神經機器翻譯 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/04e01b83d68b4296803a0991ae351b03 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dfc16c0.html title=用編碼器-解碼器-重構器框架實現英語-日語的神經機器翻譯>用編碼器-解碼器-重構器框架實現英語-日語的神經機器翻譯</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ea1bb612.html alt=光纜——未來網絡主導 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/e75c1afe12354a93bad8495ad1057693 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ea1bb612.html title=光纜——未來網絡主導>光纜——未來網絡主導</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/bdc59733.html alt="網絡詞名場面是什麼意思 名場面是什麼梗" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bdc59733.html title="網絡詞名場面是什麼意思 名場面是什麼梗">網絡詞名場面是什麼意思 名場面是什麼梗</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a002ca18.html alt=王一博那句年度網絡流行語「不愧是我」的：正版英文翻譯 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/26add5cdc08e4214800b25e21b623eb1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a002ca18.html title=王一博那句年度網絡流行語「不愧是我」的：正版英文翻譯>王一博那句年度網絡流行語「不愧是我」的：正版英文翻譯</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html alt=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/65c4000bda98898dcdbb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html title=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼>谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8dce33e7.html alt=理解生成對抗網絡，一步一步推理得到GANs（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/bee194d6fbec4d6f82e82998def3f7a3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8dce33e7.html title=理解生成對抗網絡，一步一步推理得到GANs（一）>理解生成對抗網絡，一步一步推理得到GANs（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1fe1c2dd.html alt=瞭解生成對抗網絡（GAN） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/634604de44ad4d17931ccc0bcf3e46ef style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1fe1c2dd.html title=瞭解生成對抗網絡（GAN）>瞭解生成對抗網絡（GAN）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2a9a3956.html alt="100 個網絡基礎知識普及，看完成半個網絡高手！" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/955ea722-be77-4b2d-b87a-64a8a04c8ea8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2a9a3956.html title="100 個網絡基礎知識普及，看完成半個網絡高手！">100 個網絡基礎知識普及，看完成半個網絡高手！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/78c87167.html alt=奧迪A7車載電路與網絡連接之網絡連接 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d34213e3b03545bd8e87ab074b54ca0f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/78c87167.html title=奧迪A7車載電路與網絡連接之網絡連接>奧迪A7車載電路與網絡連接之網絡連接</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8a15db0b.html alt=網絡上共享跨平臺的點對點文件 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/666bf87a9e16425ea1c9adc4f2c10acd style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8a15db0b.html title=網絡上共享跨平臺的點對點文件>網絡上共享跨平臺的點對點文件</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2b6ecb90.html alt=網絡基礎知識術語你知道哪些？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2b6ecb90.html title=網絡基礎知識術語你知道哪些？>網絡基礎知識術語你知道哪些？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6474ccee.html alt=點對點網絡的基本知識分享 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8678a34470a144ad88fc0487f0d8da91 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6474ccee.html title=點對點網絡的基本知識分享>點對點網絡的基本知識分享</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a61e29b6.html alt="幫助信息網絡犯罪活動罪 | 廈門刑事律師--辦案資料" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a61e29b6.html title="幫助信息網絡犯罪活動罪 | 廈門刑事律師--辦案資料">幫助信息網絡犯罪活動罪 | 廈門刑事律師--辦案資料</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ded83afe.html alt=博士論文摘要｜馬下平：“陸態網絡”並址站歸心基線精密解算及GNSS基準站數據處理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/53350006726e50ef72f9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ded83afe.html title=博士論文摘要｜馬下平：“陸態網絡”並址站歸心基線精密解算及GNSS基準站數據處理>博士論文摘要｜馬下平：“陸態網絡”並址站歸心基線精密解算及GNSS基準站數據處理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/65c66d61.html alt=網絡分析儀校準很頭疼，三大要點得記住！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/41dd3f31e88f48688e75d64978b36acf style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/65c66d61.html title=網絡分析儀校準很頭疼，三大要點得記住！>網絡分析儀校準很頭疼，三大要點得記住！</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>