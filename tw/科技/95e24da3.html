<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>CVPR2019！谷歌最新成果，YouTube視頻數據驅動生成深度圖 | 极客快訊</title><meta property="og:title" content="CVPR2019！谷歌最新成果，YouTube視頻數據驅動生成深度圖 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/4630f7fc786d4e2294b1f21003ae6c0d"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/95e24da3.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/95e24da3.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/95e24da3.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/95e24da3.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/95e24da3.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/95e24da3.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/95e24da3.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/95e24da3.html><meta property="article:published_time" content="2020-11-14T21:04:21+08:00"><meta property="article:modified_time" content="2020-11-14T21:04:21+08:00"><meta name=Keywords content><meta name=description content="CVPR2019！谷歌最新成果，YouTube視頻數據驅動生成深度圖"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/95e24da3.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>CVPR2019！谷歌最新成果，YouTube視頻數據驅動生成深度圖</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p>地址：https://arxiv.org/pdf/1904.11111.pdf</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube視頻數據驅動生成深度圖 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/4630f7fc786d4e2294b1f21003ae6c0d><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><h1 class=ql-align-center><strong>摘要</strong></h1><p class=ql-align-justify>本文提出了一種在自由移動的場景中預測密集深度的方法。現有的從單目視頻中恢復動態非剛性物體深度的方法對物體的運動有很強的假設，只能恢復稀疏深度。<strong>在本文中採用數據驅動的方法，從一個新的數據源中學習人類的深度先驗信息：成千上萬的模仿人體模型的人的互聯網視頻，即凍結在不同的自然姿勢，而手持攝像機巡視現場。</strong>由於人是靜止的，訓練數據可以通過多視圖立體重建生成。<strong>在推理時，使用來自場景靜態區域的運動視差提示來指導深度預測</strong>。我們演示了移動手持相機捕捉到的複雜人體動作的真實序列的方法，展示了對最先進的<strong>單目深度預測方法的改進</strong>，並展示了使用我們的預測深度產生的各種三維效果。</p><h1 class=ql-align-center><strong>簡介</strong></h1><p class=ql-align-justify>手持攝像機觀看動態場景是現代攝影中常見的場景。在這種情況下，恢復密集的幾何圖形是一項具有挑戰性的任務：移動對象違反了三維視覺中使用的極線約束，並且為了可視化的目的，經常被視為現有結構中的噪波或異常值，並將其稱為深度圖:從運動（SFM）和多視圖立體（MVS）方法。然而，人類的深度感知並不容易被物體運動所愚弄，相反，即使物體和觀察者都在移動，<strong>即使只有一隻眼睛觀察到場景，我們仍然可以對物體的幾何結構和深度順序進行可行的解釋</strong>[11]。在這項工作中，我們朝著計算上實現這一能力邁出了一步。</p><p class=ql-align-justify><strong>我們專注於從普通視頻中預測準確、密集的深度</strong>，在普通視頻中，攝像機和場景中的人都是自然移動的。我們關注人類有兩個原因：i）在許多應用中（例如，增強現實），人類構成了場景中的突出物體；ii）人類的運動是有關節的，難以建模。通過採用數據驅動的方法，我們可以避免對人的形狀或變形進行明確的假設，而是從數據中學習這些先驗。</p><p class=ql-align-justify>我們從哪裡獲取數據來訓練這種方法？生成高質量的合成數據，在這些數據中，攝像機和場景中的人都是自然移動的，這是非常具有挑戰性的。深度傳感器（如Kinect）可以提供有用的數據，但此類數據通常僅限於室內環境，需要在捕獲和處理過程中進行大量的人工工作。此外，很難將不同年齡和性別、不同姿勢的人聚集在一起。相反，<strong>我們從一個令人驚訝的來源獲得數據：YouTube視頻中，人們模仿人體模型，即，在精心製作的自然姿勢中凍結，而手持攝像機則在現場巡視（圖2）</strong>。</p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube視頻數據驅動生成深度圖 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/02dd9f59b4a142419228b70bacc7f21e><p class=pgc-img-caption></p></div><p class=ql-align-justify>這些視頻包括我們的新人體模型（MC）數據集，我們計劃為研究社區發佈，因為整個場景，包括人，都是靜止的，我們使用SFM和MVS估計相機的姿勢和深度，並使用這個衍生的三維數據作為訓練的監督。</p><p class=ql-align-justify>特別是，<strong>我們設計並訓練了一個深度神經網絡，它接收輸入的RGB圖像、人類區域的遮罩和環境的初始深度（即非人類區域），並在整個圖像上輸出一個密集的深度圖，包括環境和人（見圖1）</strong>。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube視頻數據驅動生成深度圖 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f7d52548f1ce4be89b0c5ba52decc53a><p class=pgc-img-caption></p></div><p class=ql-align-justify>請注意，<strong>使用視頻兩幀之間的運動視差計算環境的初始深度，為網絡提供單幀中不可用的信息。一旦經過訓練，我們的模型可以處理自然視頻與任意攝像機和人體運動。</strong></p><p class=ql-align-justify>我們展示了我們的方法在各種真實的互聯網視頻上的適用性，這些視頻是用手持攝像機拍攝的，描繪了複雜的人類行為，如行走、跑步和跳舞。<strong>我們的模型預測深度的精度比最先進的單目深度預測和運動立體方法更高</strong>。我們還將進一步展示我們的深度貼圖如何用於產生各種3D效果，例如合成景深、深度感知繪製以及將虛擬對象插入具有正確遮擋的3D場景中。</p><p class=ql-align-justify>貢獻是：<strong>i）一個新的深度預測數據源，包括大量互聯網視頻，其中攝像頭以自然姿勢圍繞“凍結”的人移動，以及生成精確深度圖和攝像頭姿勢的方法；ii）一個基於深度網絡的模型，設計和訓練用於預測密集度深度圖在同時相機運動和複雜人體運動的挑戰情況下。</strong></p><h1 class=ql-align-center><strong>相關工作</strong></h1><p class=ql-align-justify>基於學習的深度預測。許多基於有監督和無監督學習的算法最近被提出用於預測單個RGB圖像的密集深度。<strong>一些最新的基於學習的方法也考慮多個圖像，要麼假設已知的相機姿勢，要麼同時預測相機姿勢和深度。然而，它們都不能用來預測動態物體的深度，這是我們工作的重點。</strong></p><p class=ql-align-justify><strong>動態場景的深度估計。</strong>RGBD數據已廣泛用於動態場景的三維建模，但只有少數幾種方法試圖估計單眼相機的深度。提出了幾種方法來重建動態場景的稀疏幾何體。羅素等]和Ranftl等人建議基於運動/對象分割的算法將動態場景分解為分段剛性部分。然而，這些方法對物體的運動施加了強烈的假設，而這種假設是人類關節運動所違反的。Konstantinos等人利用國際足聯視頻遊戲的綜合訓練數據預測足球運動員的移動深度。然而，他們的方法僅限於足球運動員，不能處理一般人在野外。</p><p class=ql-align-justify><strong>用於學習深度的RGBD數據。</strong>有許多室內場景的RGBD數據集，使用深度傳感器或綜合渲染捕獲。然而，這些數據集都不能為在自然環境中移動的人提供深度監控。有幾種動作識別方法使用深度傳感器來捕捉人類的動作，但大多數都使用靜態攝像機，只提供有限數量的室內場景。Refresh是一個最新的半合成場景流數據集，通過將動畫人物疊加到nyuv2圖像上創建。在這裡，數據也侷限於室內，由人造人組成，他們被放置在與周圍不現實的配置中。</p><p class=ql-align-justify><strong>人體形狀和姿勢預測。</strong>從單個RGB圖像中恢復構成的三維人體網格引起了極大的關注。最近的方法在跨越各種姿勢的自然圖像上取得了令人印象深刻的效果。然而，這種方法只是模擬人體，不理會頭髮、衣服和非人體部位的場景。最後，這些方法中的許多依賴於正確地檢測人體關鍵點，要求身體的大部分都在框架內。</p><h1 class=ql-align-center><strong>MannequinChallenge Dataset</strong></h1><p class=ql-align-justify>人體模型挑戰[42]是一種流行的視頻趨勢，在這種趨勢中，當攝像機操作員在場景周圍移動拍攝人體模型時，人們通常會以有趣的姿勢原地不動（如圖2）。自2016年底以來，已有數千個此類視頻被創建並上傳至YouTube。如果人們在視頻中成功地保持靜止，我們可以假設場景是靜態的，並通過使用SFM和MVS算法對其進行處理來獲得準確的攝像機姿態和深度信息。我們發現大約2000個候選視頻，這個處理是可能的。這些視頻包括我們的新的（MC）數據集，它涵蓋了不同年齡的人的廣泛場景，自然呈現在不同的組配置中。接下來，我們將詳細描述如何處理這些視頻並導出我們的培訓數據。</p><p class=ql-align-justify><strong>估計相機姿態。</strong>採用與周等類似的方法。我們使用ORB-SLAM2[24]來識別每個視頻中的可跟蹤序列，並估計每個幀的初始攝像機姿勢。在這個階段，我們處理低分辨率的視頻以提高效率，並將視場設置為60度（現代手機攝像頭的典型值）。然後，我們使用視覺SFM系統[32]以更高的分辨率重新處理每個序列，該系統優化了初始攝像機姿態和固有參數。該方法提取並匹配跨幀的特徵，然後執行全局束調整優化。最後，利用周等的技術去除了攝像機運動不平穩的序列。</p><p class=ql-align-justify><strong>用MVS計算密集深度。</strong>一旦對每個片段的相機姿勢進行了估計，我們就可以重建每個場景的密集幾何體。特別是，我們使用最先進的MVS系統colmap恢復每幀密集深度圖。</p><p class=ql-align-justify><strong>過濾剪輯。</strong>有幾個因素會使視頻剪輯不適合訓練。例如，人們可能在視頻中的某一點“解凍”（開始移動），或者視頻可能在背景中包含合成圖形元素。動態對象和合成背景不受多視圖幾何約束，因此被視為異常值，並被MVS過濾掉，可能只留下少量有效像素。因此，在經過兩次清洗階段後，我們刪除了小於20%像素具有有效MVS深度的幀。</p><p class=ql-align-justify>此外，我們移除了估算徑向畸變係數k1>0.1（表示魚眼相機）或估算焦距小於等於0.6或大於等於1.2（相機參數可能不準確）的幀。我們保持的序列至少有30幀長，縱橫比為16:9，並且具有超過1600像素的寬度。最後，我們手動檢查剩餘序列的軌跡和點雲，並消除明顯不正確的重建。刪除的圖像示例顯示在補充材料中。</p><p class=ql-align-justify><strong>經過處理，得到4690個序列，有效圖像深度對超過170K。我們用80:3:17的片段將我們的mc數據集分解為培訓、驗證和測試集。</strong></p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube視頻數據驅動生成深度圖 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ed65129ccb2448aabc4e44e6570e0098><p class=pgc-img-caption></p></div><h1 class=ql-align-center><strong>模型</strong></h1><p>我們以有監督的方式對人體模型數據集的深度預測模型進行訓練，即通過迴歸到MVS管道生成的深度。一個關鍵的問題是如何構造網絡的輸入，以允許對凍結的人進行訓練，但對自由移動的人進行推理。一種選擇是從單個RGB圖像迴歸到深度，但是這種方法忽略了有關場景靜態區域的幾何信息，這些靜態區域通過考慮多個視圖而可用。為了從這些信息中獲益，我們向網絡輸入一張靜態非人類區域的深度圖，根據運動視差w.r.t.另一個場景視圖進行估算。</p><h1 class=ql-align-center><strong>結果</strong></h1><p>我們對我們的方法進行了定量和定性的測試，並將其與幾種最先進的單視圖和基於運動的深度預測算法進行了比較。我們對複雜的人體運動和自然的攝像機運動的挑戰性互聯網視頻顯示了額外的定性結果，並演示了我們預測的深度圖如何用於幾種視覺效果。</p><p class=ql-align-center><strong>MCtest set 和TUM RGBD dataset</strong></p><ul><li class=ql-align-center><strong>MCtest set</strong></li></ul><p class=ql-align-justify>為了量化我們設計的模型輸入的重要性，我們比較了幾個模型的性能，每個模型在我們的mc數據集上訓練，具有不同的輸入配置。兩種主要配置是：（i）單視圖模型（輸入為RGB圖像）和（ii）我們的全雙幀模型，其中輸入包括參考圖像、初始遮罩深度圖DPP、置信圖C和人體遮罩M。我們還通過將輸入深度替換為光流F、從輸入中刪除C以及添加人類關鍵點圖。</p><p class=ql-align-justify>定量評價如表1所示。通過比較第（i）、（iii）和（iv）行，可以清楚地看出，添加初始環境深度和置信度圖可以顯著提高人類和非人類區域的性能。在網絡輸入中添加人工關鍵點位置可以進一步提高性能。請注意，如果我們向網絡輸入光流場而不是深度（ii），則性能僅與單視圖方法相當。從二維光流到深度的映射取決於攝像機的相對姿態，而這些姿態並沒有提供給網絡。結果表明，該網絡不能隱式學習相對姿態，不能提取深度信息。</p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube視頻數據驅動生成深度圖 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c3ac57b5620544d0bced40d3e6a6aa1b><p class=pgc-img-caption></p></div><p class=ql-align-justify>圖4顯示了我們的單視圖模型（I）和我們的完整模型（IDPPCMK）之間的定性比較。我們的全模型結果在人類區域（例如，第一列）和非人類區域（例如，第二列）中更準確。此外，在所有例子中，人們與周圍環境之間的深度關係都得到了改善。</p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube視頻數據驅動生成深度圖 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a7e79abf3cc64dde8c065b4638b69cb6><p class=pgc-img-caption></p></div><p class=ql-align-justify>我們使用了tum-rgbd數據集[38]的一個子集，其中包含執行復雜動作的人的室內場景，這些場景是從不同的相機姿勢捕獲的。該數據集的樣本圖像如圖5（a-b）所示。</p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube視頻數據驅動生成深度圖 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d70c39eb8ba2466da9936631714bd111><p class=pgc-img-caption></p></div><p class=ql-align-justify>為了運行我們的模型，我們首先使用ORB-SLAM2 3估計相機姿態。</p><p class=ql-align-justify>定量比較如表2所示，其中我們報告了5種不同的尺度不變性誤差測量，以及標準的RMSE和相對誤差；最後兩種是通過應用一個單一的比例因子來計算的，該比例因子在最小二乘意義上對齊預測和地面真值深度。我們的單視圖模型已經優於其他單視圖模型，證明了用於培訓的MC數據集的好處。注意，由於挑戰性的攝像機和物體運動，Viopopop[ 31 ]未能產生有意義的結果。我們的完整模型，通過使用初始（掩蔽）深度圖，顯著地改善了所有錯誤度量的性能。與我們的MC測試集結果一致，當我們使用光流作為輸入（而不是初始深度圖）時，性能只比單視圖網絡稍好。最後，我們展示了我們提出的“深度清理”方法的重要性，該方法應用於培訓數據（見等式1）。與同一模型相比，僅使用原始MVS深度預測作為監督進行培訓，我們發現性能下降了約15%。</p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube視頻數據驅動生成深度圖 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/94622093a67e44e1b8347ab7d8f9e230><p class=pgc-img-caption></p></div><p class=ql-align-justify>圖5顯示了不同方法之間的定性比較。我們的模型的深度預測（圖5（f-g））與地面情況非常相似，顯示出高度的細節和尖銳的深度不連續。這一結果是對競爭方法的無表改進，這通常會在兩個人類區域（如圖5第二行中的腿）和非人類區域（如最後兩行中的桌子和天花板）中產生顯著錯誤。</p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube視頻數據驅動生成深度圖 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/94622093a67e44e1b8347ab7d8f9e230><p class=pgc-img-caption></p></div><p class=ql-align-justify>如圖6所示，我們的深度預測明顯優於基線方法。我們預測的深度圖描繪了場景中人與其他物體之間（如人與建築物之間、圖6第四行）以及人類區域內（如圖6前三行中人的胳膊和腿）的精確深度排序。</p><div class=pgc-img><img alt=CVPR2019！谷歌最新成果，YouTube視頻數據驅動生成深度圖 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eb78e98d51af486e81da53e176dc4bff><p class=pgc-img-caption></p></div><p class=ql-align-justify>基於深度的視覺效果。我們的深度可以用來應用一系列基於深度的視覺效果。圖7顯示了基於深度的散焦、插入合成的3D圖形，以及去除附近的人。更多示例，包括單聲道到立體聲轉換，請參見補充材料。</p><p class=ql-align-justify>隨著時間的推移，深度估計值足夠穩定，可以從視頻中的其他幀進行繪製。為了使用幀進行修復，我們從深度圖、紋理與視頻幀的高程場構造三角高程場，並使用相對攝像機變換渲染目標幀的高程場。圖7（D，F）顯示了修復兩個街道場景的結果。攝像機附近的人是用人臉M移除的，在視頻的後期，洞裡充滿了200幀的顏色。一些人工製品可以在人體面具漏掉的地方看到，例如地上的陰影。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>CVPR2019</a></li><li><a>YouTube</a></li><li><a>視頻</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/1c46e69.html alt=YouTube基於多任務學習的視頻排序推薦系統 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/b2a04bc0d08f48e8885d8a7976214969 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1c46e69.html title=YouTube基於多任務學習的視頻排序推薦系統>YouTube基於多任務學習的視頻排序推薦系統</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6639baec.html alt=在線播放視頻網站應該如何選擇服務器 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6639baec.html title=在線播放視頻網站應該如何選擇服務器>在線播放視頻網站應該如何選擇服務器</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b91b414b.html alt=講述者│一起活到好事發生的那一天（視頻） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b91b414b.html title=講述者│一起活到好事發生的那一天（視頻）>講述者│一起活到好事發生的那一天（視頻）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/22525cee.html alt=數字音視頻壓縮編碼標準及H.265的編碼優勢 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/558a023201a84604894ebc359d178ed2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/22525cee.html title=數字音視頻壓縮編碼標準及H.265的編碼優勢>數字音視頻壓縮編碼標準及H.265的編碼優勢</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e71fb88c.html alt=和麵、揉麵技巧，視頻詳細講解，10年和麵經驗，全部告訴你 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/8cd49cb368f1488b8c4a65a3fe556d75 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e71fb88c.html title=和麵、揉麵技巧，視頻詳細講解，10年和麵經驗，全部告訴你>和麵、揉麵技巧，視頻詳細講解，10年和麵經驗，全部告訴你</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2565ddd2.html alt=PHP100視頻教程07：PHP函數和自定義函數 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a348885afabf4af3859d03ef12a81979 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2565ddd2.html title=PHP100視頻教程07：PHP函數和自定義函數>PHP100視頻教程07：PHP函數和自定義函數</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a4defce7.html alt=PHP視頻教程01：環境配置與代碼調試 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9189621803214d559b7c006fc2b9b411 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a4defce7.html title=PHP視頻教程01：環境配置與代碼調試>PHP視頻教程01：環境配置與代碼調試</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/513caa91.html alt="php視頻教程 PHP編程從入門到精通2019實戰開發在線課程" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d4bcba1beecd439da2426778b71972ac style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/513caa91.html title="php視頻教程 PHP編程從入門到精通2019實戰開發在線課程">php視頻教程 PHP編程從入門到精通2019實戰開發在線課程</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/af4897c3.html alt="素描排線 素描排線技巧 視頻教程" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/99f60e6707bf4ff5bf6d620263a2ac4b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/af4897c3.html title="素描排線 素描排線技巧 視頻教程">素描排線 素描排線技巧 視頻教程</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/83c61a56.html alt=一個視頻揭祕集成電路到底有多難造 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1529403855728b32a45b881 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/83c61a56.html title=一個視頻揭祕集成電路到底有多難造>一個視頻揭祕集成電路到底有多難造</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d64fdbdf.html alt=視頻丨花樣畢業季！湖師大附屬德山學校迎來首屆小學、初中畢業生 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d64fdbdf.html title=視頻丨花樣畢業季！湖師大附屬德山學校迎來首屆小學、初中畢業生>視頻丨花樣畢業季！湖師大附屬德山學校迎來首屆小學、初中畢業生</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d998cad8.html alt=剪切視頻的幾個小步驟 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/cf49e46b4c174287b9f1ec155ffae737 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d998cad8.html title=剪切視頻的幾個小步驟>剪切視頻的幾個小步驟</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6228e163.html alt=剪輯視頻都有哪些流程？記住這8條！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4c88896a12ca4ddbae06fb9ec934bb5c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6228e163.html title=剪輯視頻都有哪些流程？記住這8條！>剪輯視頻都有哪些流程？記住這8條！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b0313850.html alt=視頻丨小小鋼桶“裝”下大市場“寧波造”危化品容器遠銷全球 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RZ6VPMzBsHyIpw style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b0313850.html title=視頻丨小小鋼桶“裝”下大市場“寧波造”危化品容器遠銷全球>視頻丨小小鋼桶“裝”下大市場“寧波造”危化品容器遠銷全球</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/abccefa3.html alt="視頻丨上班第一天 寧波窗口單位工作狀態如何？來看記者暗訪" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RmUHZxnIRJyjnB style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/abccefa3.html title="視頻丨上班第一天 寧波窗口單位工作狀態如何？來看記者暗訪">視頻丨上班第一天 寧波窗口單位工作狀態如何？來看記者暗訪</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>