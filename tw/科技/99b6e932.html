<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>單目視覺深度估計測距的前生今世 | 极客快訊</title><meta property="og:title" content="單目視覺深度估計測距的前生今世 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/b0360f4d73104a549efe60c54479f1d5"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/99b6e932.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/99b6e932.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/99b6e932.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/99b6e932.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/99b6e932.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/99b6e932.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/99b6e932.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/99b6e932.html><meta property="article:published_time" content="2020-11-14T21:03:55+08:00"><meta property="article:modified_time" content="2020-11-14T21:03:55+08:00"><meta name=Keywords content><meta name=description content="單目視覺深度估計測距的前生今世"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/99b6e932.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>單目視覺深度估計測距的前生今世</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><blockquote><p>最近通過深度學習直接從單目攝像頭的圖像預測/估計深度圖的方法成為一個應用的熱點，惹來不少爭議。</p></blockquote><hr><p>深度學習直接通過大數據的訓練得到/調整一個深度NN模型的參數，在當今計算能力日新月異的平臺（GPU/FPGA/ASIC/Muli-core）上實現了計算機視覺/語音識別/自然語言處理（NLP）等領域一些應用的突破。但是專家們還是對今後深度學習的發展有些期待和展望，比如</p><ul><li class=ql-align-justify>非監督學習方法的引入減輕大數據標註的負擔，比如GAN；</li><li class=ql-align-justify>NN模型的壓縮和精簡以普及深度學習在移動終端甚至物聯網終端的廣泛應用；</li><li class=ql-align-justify>還有深度學習能夠更多的引入人類知識和簡單可靠的推理，以減少“暴力“學習的誤差和錯誤，比如貝葉斯理論，知識圖譜，邏輯推理，符號學習，多任務聯合訓練和遷移學習等等。</li></ul><p>這裡從單目深度估計在計算機視覺中的發展歷程，特別是最近在採用深度學習NN模型的實驗中，總結一下如何通過深度學習求解傳統視覺問題，並從中發現可借鑑的地方。</p><hr><p>深度估計問題在計算機視覺領域屬於3-D重建的一部分，即Shape from X。這個X包括stereo, multiple view stereo, silhouette, motion (SfM, SLAM), focusing, hazing, shading, occlusion, texture, vanishing points, ...前面5個都是多圖像的輸入，從空間幾何，時域變換和焦距變化的關係推導深度距離。剩下的都是單目的輸入。</p><p>如果把圖像模糊度建模，下圖是圖像邊緣模糊的響應模型，那麼單目圖像也能估算深度，即shape from defocusing。</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b0360f4d73104a549efe60c54479f1d5><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/be154b0cf5a1430b926bd935cf067636><p class=pgc-img-caption></p></div><p>另外一個現象是大氣散射 (Atmosphere scattering ) 造成的霾 (haze)提供了深度信息，即depth from haze，一般有天空被拍攝下來的圖像，通過散射模型能夠推斷像素深度。這裡給出的是圖像亮度C和深度z之間計算的公式：C0是沒有散射情況下的圖像亮度值，S是天空的圖像亮度值。</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eeae528114cf47369cf820b13fae5b79><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7a762ca83f8747e6afda527c9377bd3f><p class=pgc-img-caption></p></div><p>以上兩個任務可認為是deconvolution問題，blind或者non-blind。</p><p>物體表面陰影的變化可以提供深度的信息，利用圖像亮度和其中物體表面的形狀之間的關係，即Shape from shading。和SFM一樣，這是一個病態問題，需要附加約束條件，如物體表面的特性。SFS一般假設四種表面模型：純Lambertian，純鏡面，混合和更復雜的模型。大部分情況下都是Lambertian，即均勻照明的表面從任何一個方向觀察其亮度不變。其目標函數是一個積分，求解的算法比較複雜，類似有限元之類。</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/867708d367bf4f009fa2c6ad766b9a71><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/63e45ff5c52b4e61b4db6e22678a405a><p class=pgc-img-caption></p></div><p>紋理的變形提供了該紋理表面形狀的線索。下圖是一個示意流程圖：</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/839cc005aa1148dea9609a87a898e9f7><p class=pgc-img-caption></p></div><p>中間第三圖是表面法向圖，第四個才是深度圖。紋理分割是必備的基礎（估計是很難的一部分），此外求解這個shape from texture的優化問題，必須加上幾個紋理元素（textels）約束條件：homogeneity，isotropy，stationary。</p><p>遮擋（occlusion）也是深度的一個線索，曲率（curvature）是其中的一個體現，即shape from curvature。isophote這個詞指一種封閉的物體外輪廓，一般通過對圖像灰度設門限可以得到，而它的曲率用來推導對應該門限的深度，見下圖所示。門限在【0，255】範圍變化就能得到最終疊加平均的深度圖。分割仍然是一個求解遮擋的基礎，要知道當時分割算法是計算機視覺很頭疼的難題，俗稱“chicken-and-egg"。</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/79de25feaca3417b97a351ef1db93b63><p class=pgc-img-caption></p></div><p>最後再說消失點，即某平面的一組平行線在透視投影下會聚的點。那麼，它相應的平面就能得到深度圖，如下圖所示，在人工（特別室內）環境下可以推導深度圖，沿著平行線的平面，靠近消失點的賦予大的深度值。該方法叫depth from geometrical perspective。</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/f56c5487789e4e32ae15be8a19da0866><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7f55ad7d5f2443baba4ecf8668d23a19><p class=pgc-img-caption></p></div><hr><p>順便提一下，在3維電視熱的時期（2008-2010年左右）大家希望把以前拍攝的單目視頻變成立體視頻，給3-D電視提供更多的內容，包括3-D顯示技術的普及（比如紅綠眼鏡）大家也想在家裡share一些3-D的UGC。這個技術被稱為2D-to-3D，通過深度圖估計和虛擬立體視覺假設可以生成立體視頻，其繪製技術稱為DIBR，如下圖：</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3d9f1955cf2747d3a3b88a844b09864a><p class=pgc-img-caption></p></div><p>以上是典型的傳統計算機視覺，需要加約束求解病態的優化問題。下面談談機器學習如何解決這個視覺問題：</p><p>最早看到用機器學習的方法是基於MRF的，把各種約束關係放在模型的data term和connectivity term求解。這是我看到Andrew Ng在計算機視覺方面的第一篇論文，發表在NIPS 2005年，當時他剛剛在斯坦福大學建立自己的研究組。</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c7dc7acf95af46ed9e65b8af49caed44><p class=pgc-img-caption></p></div><p>如果採用圖像分割得到的super-pixels，可以得到更平滑的結果，該系統叫做Make3D。</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2e876d73bdcc410498215ef6f77e97d1><p class=pgc-img-caption></p></div><p>值得一提的是，當時CMU的博士生Derek Hoiem也在研究如何從單目圖像中提取出景物的3-D結構，只是他採用機器學習方法在圖像分割基礎上做了一個簡單的語義分割，即“ground”, “sky”, 和 “vertical”標註像素，然後採用簡單的平面billboard做紋理映射後變成“pop-up”的3-D景物：</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/34aef776279f4303a36eaf9d005905b5><p class=pgc-img-caption></p></div><p>還有一種方法是把深度圖估計變成一個搜索問題，即假設相似圖像具有相似深度圖：</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/64bad9345f1d404991c13cbea27c0689><p class=pgc-img-caption></p></div><p>針對視頻，可以利用optic flow做motion segmentation，那麼修正上面的方法得到：</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eeae528114cf47369cf820b13fae5b79><p class=pgc-img-caption></p></div><p>另外一種改進是利用dictionary learning優化整個搜索過程：</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8adfb07acf134c6e92234678a78c1a51><p class=pgc-img-caption></p></div><hr><p>下面我們看看深度學習是如何做的。</p><p>首先就是“暴力”方法直接喂數據訓練模型：2篇論文</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6ec01b1ffacc4626bf193c28d9b52887><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eeae528114cf47369cf820b13fae5b79><p class=pgc-img-caption></p></div><p>有些方法開始考慮傳統方法的結合，比如CRF：</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eeae528114cf47369cf820b13fae5b79><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eeae528114cf47369cf820b13fae5b79><p class=pgc-img-caption></p></div><p>隨後，雙目立體視覺的空間約束被用作無監督學習單目的深度估計：三篇論文</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eeae528114cf47369cf820b13fae5b79><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eeae528114cf47369cf820b13fae5b79><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eeae528114cf47369cf820b13fae5b79><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/19de6d07dc78470fab452f1ef2db3abb><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a7498b00738c498386db2fa580477009><p class=pgc-img-caption></p></div><p>自然地採用幀間運動為單目視頻的深度估計提供幫助，實際上是雙任務聯合訓練的例子：</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d77cd3c06380445984a936ad428b7fae><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eeae528114cf47369cf820b13fae5b79><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/11058d6f3a3d4811b66363745f208783><p class=pgc-img-caption></p></div><p>這是結合表面法向圖的聯合訓練例子：GeoNet，Geometric Neural Network</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1297955da2ac4c86b3a61c0a718ac25d><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/60e3a9cd46bb4646b6ef35842ebc25d7><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8406f37171b14053b39b271437c231c7><p class=pgc-img-caption></p></div><p>結合view synthesis也是利用幾何約束和成像特性的工作：之前已經有文章直接通過Deep3D模型做單目到雙目的合成，這裡的工作只是最終結果是深度估計而不是圖像。</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3bebf399ab514b53825c476546e96db4><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e2c172e539eb4066ad4196902429a447><p class=pgc-img-caption></p></div><p>這是結合運動和邊緣信息的聯合訓練例子：</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/84edffa0407c41628c133c3e70e00f55><p class=pgc-img-caption></p></div><p>這個工作延續了以前利用分割提高深度估計的想法，只是假設上attention機制：</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9cfbcac2656742b9bd267b6558c050d5><p class=pgc-img-caption></p></div><p>谷歌最近的工作延續了camera motion的結合，同時加上了其中物體各自運動的信息：可以認為是將optic flow分成了camera ego motion和object motion的工作，和加入語義分割有類似的思路吧，文章發表在AAAI‘19.</p><div class=pgc-img><img alt=單目視覺深度估計測距的前生今世 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/dee139cd580b4a1aa43c8be14bf75dea><p class=pgc-img-caption></p></div><p>總之，深度學習在這個計算機視覺的傳統問題上一開始是暴力的數據學習方法，慢慢地加入了傳統方法的約束和先驗知識，一定程度上緩解了數據標註的壓力（pixel級別的ground truth是個挑戰性的工作，structured light帶來的數據多半是室內的，激光雷達的數據也存在“黑洞”現象），同時設計新的loss function同時多任務聯合訓練都能提升模型算法的性能。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>單目</a></li><li><a>視覺</a></li><li><a>估計測</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/cfb83fa2.html alt=機器視覺｜晏磊：航空遙感平臺通用物理模型及可變基高比系統精度評價 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1530929302432561dc6cda6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cfb83fa2.html title=機器視覺｜晏磊：航空遙感平臺通用物理模型及可變基高比系統精度評價>機器視覺｜晏磊：航空遙感平臺通用物理模型及可變基高比系統精度評價</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/403ada1f.html alt=航攝中如何掌控視覺差異 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6554/5338856107 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/403ada1f.html title=航攝中如何掌控視覺差異>航攝中如何掌控視覺差異</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ee69f5ac.html alt=基於機器視覺技術快速準確地確定收穫後幹大豆種子的品質 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/519b968bf69146fda9bf55f89779d373 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ee69f5ac.html title=基於機器視覺技術快速準確地確定收穫後幹大豆種子的品質>基於機器視覺技術快速準確地確定收穫後幹大豆種子的品質</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1a697cad.html alt=視覺享受的進化!顯示技術的分類和演進 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1a697cad.html title=視覺享受的進化!顯示技術的分類和演進>視覺享受的進化!顯示技術的分類和演進</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2bc1496a.html alt=為了更好的深度神經網絡視覺，只需添加反饋（循環） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/17fccfd7096d44eeb3921bbd0dc29a13 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2bc1496a.html title=為了更好的深度神經網絡視覺，只需添加反饋（循環）>為了更好的深度神經網絡視覺，只需添加反饋（循環）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cc025c3f.html alt=來自封測官視覺—珠海潮門裡巷的第一場廣華米粉品鑑會 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a5106bbff49f4935b42c19b3abe5fcb7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cc025c3f.html title=來自封測官視覺—珠海潮門裡巷的第一場廣華米粉品鑑會>來自封測官視覺—珠海潮門裡巷的第一場廣華米粉品鑑會</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b792ed3a.html alt=機器視覺檢測設備的外觀缺陷檢測的原理淺析 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/35e3f8d87f5d4c96937309010c5a5ec7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b792ed3a.html title=機器視覺檢測設備的外觀缺陷檢測的原理淺析>機器視覺檢測設備的外觀缺陷檢測的原理淺析</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ab89c564.html alt=機器視覺表面缺陷檢測綜述 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/bd400c5fc7d049df8d5ce365faec81f6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ab89c564.html title=機器視覺表面缺陷檢測綜述>機器視覺表面缺陷檢測綜述</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/af8d9e1a.html alt=機器視覺表面缺陷檢測設備綜述 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/b75804e4e383426595973d8b7b2b2c6f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/af8d9e1a.html title=機器視覺表面缺陷檢測設備綜述>機器視覺表面缺陷檢測設備綜述</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d23e1957.html alt=CCD視覺檢測設備外觀缺陷檢測功能介紹 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4a6904c2f55c4e27b12951e9d9580755 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d23e1957.html title=CCD視覺檢測設備外觀缺陷檢測功能介紹>CCD視覺檢測設備外觀缺陷檢測功能介紹</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6938b43a.html alt=【乾貨】機器視覺檢測基礎——鏡頭篇（1） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ad30e323d1c749d98d78304890799ce1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6938b43a.html title=【乾貨】機器視覺檢測基礎——鏡頭篇（1）>【乾貨】機器視覺檢測基礎——鏡頭篇（1）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/bfbcef65.html alt=印刷質量缺陷的視覺檢測原理概述 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/08950fffb8f447888dbe20c0bf9e672f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bfbcef65.html title=印刷質量缺陷的視覺檢測原理概述>印刷質量缺陷的視覺檢測原理概述</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7ff8bb95.html alt=工作場景下的視覺篩查 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/152259909123671d94d80e7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7ff8bb95.html title=工作場景下的視覺篩查>工作場景下的視覺篩查</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/50af5e85.html alt=工業機器人「視覺檢測」的原理剖析 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2a361cbe48874f73892d766805cbb40a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/50af5e85.html title=工業機器人「視覺檢測」的原理剖析>工業機器人「視覺檢測」的原理剖析</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/31c4b4f2.html alt=手機屏幕缺陷視覺檢測 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7f7b55b4ee6741059aff7914cb40a135 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/31c4b4f2.html title=手機屏幕缺陷視覺檢測>手機屏幕缺陷視覺檢測</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>