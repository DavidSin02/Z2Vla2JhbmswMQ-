<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>「ML」一文詳盡系列之CatBoost | 极客快訊</title><meta property="og:title" content="「ML」一文詳盡系列之CatBoost - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/f2a5caed2d82451b8d9e0b0f1135f42f"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/99e43ab.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/99e43ab.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/99e43ab.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/99e43ab.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/99e43ab.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/99e43ab.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/99e43ab.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/99e43ab.html><meta property="article:published_time" content="2020-10-29T21:01:06+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:06+08:00"><meta name=Keywords content><meta name=description content="「ML」一文詳盡系列之CatBoost"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/99e43ab.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>「ML」一文詳盡系列之CatBoost</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>來源：Datawhale</p><p>作者：潘華引</p><p><br></p><p><strong>CatBoost</strong></p><p>CatBoost是俄羅斯的搜索巨頭Y andex在2017年開源的機器學習庫，也是Boosting族算法的一種，同前面介紹過的XGBoost和LightGBM類似，依然是在GBDT算法框架下的一種改進實現，是一種基於對稱決策樹（oblivious trees）算法的參數少、支持類別型變量和高準確性的GBDT框架，主要說解決的痛點是高效合理地處理類別型特徵，這個從它的名字就可以看得出來，CatBoost是由catgorical和boost組成，另外是處理梯度偏差（Gradient bias）以及預測偏移（Prediction shift）問題，提高算法的準確性和泛化能力。</p><div class=pgc-img><img alt=「ML」一文詳盡系列之CatBoost onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f2a5caed2d82451b8d9e0b0f1135f42f><p class=pgc-img-caption></p></div><p>集成學習</p><p>CatBoost主要有以下五個特性：</p><ul class=list-paddingleft-2><li>無需調參即可獲得較高的模型質量，採用默認參數就可以獲得非常好的結果，減少在調參上面花的時間</li><li>支持類別型變量，無需對非數值型特徵進行預處理</li><li>快速、可擴展的GPU版本，可以用基於GPU的梯度提升算法實現來訓練你的模型，支持多卡並行</li><li>提高準確性，提出一種全新的梯度提升機制來構建模型以減少過擬合</li><li>快速預測，即便應對延時非常苛刻的任務也能夠快速高效部署模型</li></ul><p>CatBoost的主要算法原理可以參照以下兩篇論文：</p><ul class=list-paddingleft-2><li><strong>Anna Veronika Dorogush, Andrey Gulin, Gleb Gusev, Nikita Kazeev, Liudmila Ostroumova Prokhorenkova, Aleksandr Vorobev "Fighting biases with dynamic boosting". arXiv:1706.09516, 2017</strong></li><li><strong>Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin "CatBoost: gradient boosting with categorical features support". Workshop on ML Systems at NIPS 2017<br></strong><br></li></ul><p><strong>Categorical features</strong></p><p>所謂類別型變量（Categorical features）是指其值是離散的集合且相互比較並無意義的變量，比如用戶的ID、產品ID、顏色等。因此，這些變量無法在二叉決策樹當中直接使用。常規的做法是將這些類別變量通過預處理的方式轉化成數值型變量再餵給模型，比如用一個或者若干個數值來代表一個類別型特徵。</p><p>目前廣泛用於<strong>低勢</strong>（一個有限集的元素個數是一個自然數）類別特徵的處理方法是One-hot encoding：將原來的特徵刪除，然後對於每一個類別加一個0/1的用來指示是否含有該類別的數值型特徵。One-hot encoding可以在數據預處理時完成，也可以在模型訓練的時候完成，從訓練時間的角度，後一種方法的實現更為高效，CatBoost對於低勢類別特徵也是採用後一種實現。</p><p>顯然，在<strong>高勢</strong>特徵當中，比如 user ID，這種編碼方式會產生大量新的特徵，造成維度災難。一種折中的辦法是可以將類別分組成有限個的群體再進行 One-hot encoding。一種常被使用的方法是根據目標變量統計（Target Statistics，以下簡稱TS）進行分組，目標變量統計用於估算每個類別的目標變量期望值。甚至有人直接用TS作為一個新的數值型變量來代替原來的類別型變量。重要的是，可以通過對TS數值型特徵的閾值設置，基於對數損失、基尼係數或者均方差，得到一個對於訓練集而言將類別一分為二的所有可能劃分當中最優的那個。在LightGBM當中，類別型特徵用每一步梯度提升時的梯度統計（Gradient Statistics，以下簡稱GS）來表示。雖然為建樹提供了重要的信息，但是這種方法有以下兩個缺點：</p><ul class=list-paddingleft-2><li>增加計算時間，因為需要對每一個類別型特徵，在迭代的每一步，都需要對GS進行計算；</li><li>增加存儲需求，對於一個類別型變量，需要存儲每一次分離每個節點的類別。</li></ul><p>為了克服這些缺點，LightGBM以損失部分信息為代價將所有的長尾類別歸位一類，作者聲稱這樣處理高勢特徵時比起 One-hot encoding還是好不少。不過如果採用TS特徵，那麼對於每個類別只需要計算和存儲一個數字。</p><p>如此看到，採用TS作為一個新的數值型特徵是最有效、信息損失最小的處理類別型特徵的方法。TS也被廣泛採用，在點擊預測任務當中，這個場景當中的類別特徵有用戶、地區、廣告、廣告發布者等。接下來我們著重討論TS，暫時將One-hot encoding和GS放一邊。</p><p><strong>Target statistics</strong></p><p>一個有效和高效的處理類別型特徵的方式是用一個與某些TS相等的數值型變量來代替第個訓練樣本的類別。通常用基於類別的目標變量的期望來進行估算：。</p><p><strong>Greedy TS</strong></p><p>估算最直接的方式就是用訓練樣本當中相同類別的目標變量的平均值。</p><p>顯然，這樣的處理方式很容易引起過擬合。舉個例子，假如在整個訓練集當中所有樣本的類別都互不相同，即個樣本有個類別，那麼新產生的數值型特徵的值將與目標變量的值相同。某種程度上，這是一種目標穿越（target leakage），非常容易引起過擬合。比較好的一種做法是採用一個先驗概率進行平滑處理：</p><p>其中是先驗概率的權重，而對於先驗概率，通常的做法是設置為數據集當中目標變量的平均值。</p><p>不過這樣的平滑處理依然無法完全避免目標穿越：特徵是通過自變量的目標計算所得。這將會導致條件偏移：對於訓練集和測試集，的分佈會有所不同。再舉個例子，假設第個特徵為類別型特徵，並且特徵所有取值為無重複的集合，然後對於每一個類別，對於一個分類任務，我們有。然後在訓練集當中，，於是用閾值就可以僅用一次分裂就訓練集完美分開。但是，對於測試集，因為還無法判斷此時目標變量的類別，所以這一項，最後得到的TS值為，並且得到的模型在$p</p><p>其中，是第個訓練樣本。</p><p>在我們的例子當中，， ，顯然無法滿足上述條件。</p><p><strong>Holdout TS</strong></p><p>留出TS，就是將訓練集一分為二：，然後根據下式用來計算TS，並將作為訓練樣本。</p><p>這樣處理能夠滿足同分布的問題，但是卻大大減少了訓練樣本的數量。</p><p><strong>Leave-one-out TS</strong></p><p>初看起來，留一TS（Leave-one-out TS）能夠非常好地工作：</p><ul class=list-paddingleft-2><li>對於訓練樣本：</li><li>對於測試樣本：</li></ul><p>但事實上，這並沒有給預防target leakage帶來多少益處。舉個例子，考慮一個常數類別型特徵：對於所有的樣本，，在二分類的條件下，讓表示的樣本數量，則有：</p><ul class=list-paddingleft-2><li><br></li><li>對於測試樣本：</li></ul><p>此時，同樣可以用閾值將訓練集完美的分類。</p><p><strong>Ordered TS</strong></p><p>從在線學習按照時間序列獲得樣本得到的啟發，CatBoost依靠排序原則，採用了一種更為有效的策略。主要有以下幾個步驟：</p><ul class=list-paddingleft-2><li>產生一個隨機排列順序並對數據集進行編號</li><li>對於訓練樣本：</li><li>對於測試樣本：</li><li>根據帶先驗概率的Greedy TS計算</li></ul><p>這樣計算得到的 Ordered TS能夠滿足P1，同時也能夠使用所有的訓練樣本。且比在線學習的劃窗（sliding window）處理能夠進一步減小的方差。需要注意的是，CatBoost在不同的迭代上會採用不同的排列順序。</p><p>下面是Ordered TS與其它各種TS在不同數據集上面在logloss/zero-one loss上面的效果比較：</p><div class=pgc-img><img alt=「ML」一文詳盡系列之CatBoost onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a9bd59bacdce42ffbb4c84ee6fc6cd7b><p class=pgc-img-caption></p></div><p>TS比較</p><p><strong>特徵組合</strong></p><p>CatBoost的另外一項重要實現是將不同類別型特徵的組合作為新的特徵，以獲得高階依賴（high-order dependencies），比如在廣告點擊預測當中用戶ID與廣告話題之間的聯合信息，又或者在音樂推薦引用當中，用戶ID和音樂流派，如果有些用戶更喜歡搖滾樂，那麼將用戶ID和音樂流派分別轉換為數字特徵時，這種用戶內在的喜好信息就會丟失。然而，組合的數量會隨著數據集中類別型特徵的數量成指數增長，因此在算法中考慮所有組合是不現實的。為當前樹構造新的分割點時，CatBoost會採用貪婪的策略考慮組合。對於樹的第一次分割，不考慮任何組合。對於下一個分割，CatBoost將當前樹的所有組合、類別型特徵與數據集中的所有類別型特徵相結合，並將新的組合類別型特徵動態地轉換為數值型特徵。CatBoost還通過以下方式生成數值型特徵和類別型特徵的組合：樹中選定的所有分割點都被視為具有兩個值的類別型特徵，並像類別型特徵一樣地被進行組合考慮。</p><p><strong>Gradient bias</strong></p><p>CatBoost，和所有標準梯度提升算法一樣，都是通過構建新樹來擬合當前模型的梯度。然而，所有經典的提升算法都存在由有偏的點態梯度估計引起的過擬合問題。在每個步驟中使用的梯度都使用當前模型中的相同的數據點來估計，這導致估計梯度在特徵空間的任何域中的分佈與該域中梯度的真實分佈相比發生了偏移，從而導致過擬合。為了解決這個問題，CatBoost對經典的梯度提升算法進行了一些改進，簡要介紹如下：</p><p>在許多利用GBDT框架的算法（例如，XGBoost、LightGBM）中，構建下一棵樹分為兩個階段：選擇樹結構和在樹結構固定後計算葉子節點的值。為了選擇最佳的樹結構，算法通過枚舉不同的分割，用這些分割構建樹，對得到的葉子節點中計算值，然後對得到的樹計算評分，最後選擇最佳的分割。兩個階段葉子節點的值都是被當做梯度或牛頓步長的近似值來計算。在CatBoost中，第二階段使用傳統的GBDT框架執行，第一階段使用修改後的版本。</p><p>既然原來的梯度估計是有偏的，那麼能不能改成無偏估計呢？</p><p>設為構建棵樹後的模型，為構建棵樹後第個訓練樣本上面的梯度值。為了使得 無偏於模型 ，我們需要在沒有參與的情況下對模型進行訓練。由於我們需要對所有訓練樣本計算無偏的梯度估計，乍看起來對於的訓練不能使用任何樣本，貌似無法實現的樣子。我們運用下面這個技巧來處理這個問題：對於每一個樣本，我們訓練一個單獨的模型，且該模型從不使用基於該樣本的梯度估計進行更新。我們使用來估計上的梯度，並使用這個估計對結果樹進行評分。用偽碼描述如下，其中是需要優化的損失函數，是標籤值， 是公式計算值。</p><div class=pgc-img><img alt=「ML」一文詳盡系列之CatBoost onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6e8fff15f1e04d7f822d8360a9c7d789><p class=pgc-img-caption></p></div><p>Gradient bias</p><p>值得注意的是模型的建立並沒有樣本 的參與，並且CatBoost中所有的樹的共享同樣的結構。</p><p>在CatBoost中，我們生成訓練數據集的個隨機排列。採用多個隨機排列是為了增強算法的魯棒性，這在前面的Odered TS當中對於類別型特徵的處理有介紹到：針對每一個隨機排列，計算得到其梯度，為了與Ordered TS保持一致，這裡的排列與用於計算Ordered TS時的排列相同。我們使用不同的排列來訓練不同的模型，因此不會導致過擬合。對於每個排列，我們訓練個不同的模型，如上所示。這意味著為了構建一棵樹，需要對每個排列存儲並重新計算，其時間複雜度近似於：對於每個模型，我們必須更新。因此，時間複雜度變成。當然，在具體實現當中，CatBoost使用了其它的技巧，可以將構建一個樹的時間複雜度降低到。</p><p><strong>Prediction shift</strong></p><p>預測偏移（Prediction shift）是由上一節所討論的梯度偏差造成的。本節希望用數學語言嚴格地對預測偏差進行描述和分析。</p><p>首先來看下梯度提升的整體迭代過程：</p><ul class=list-paddingleft-2><li>對於梯度提升：</li><li><br></li><li><br></li><li><br></li></ul><p>在這個過程當中，偏移是這樣發生的：</p><ul class=list-paddingleft-2><li>根據進行隨機計算的條件分佈與測試集的分佈發生偏移</li><li>這樣導致基學習器與 產生偏差</li><li>最後影響模型的泛化能力</li></ul><p>下面以一個迴歸任務為例，從理論上分析計算偏差的值。</p><p>假設以下邊界條件：</p><ul class=list-paddingleft-2><li>損失函數：</li><li>兩個相互獨立的特徵，隨機變量，符合伯努利分佈，先驗概率</li><li>目標函數：</li><li>梯度提升迭代次數為2</li><li>樹深度為1</li><li>學習率：</li></ul><p>最後得到的模型為：，其中分別基於和 。</p><p>區分數據集是否獨立，我們有以下兩個推論：</p><ul class=list-paddingleft-2><li>如果使用了規模為的兩個獨立數據集和 來分別估算 和，則對於任意，有：</li><li>如果使用了相同的數據集來估算 和，則有：</li></ul><p>顯然，偏差部分與數據集的規模成反比，與映射關係也有關係，在我們的例子當中，與成正比。</p><p><strong>Ordered boosting</strong></p><p>為了克服上一節所描述的預測偏移問題，我們提出了一種新的叫做Ordered boosting的算法。假設用棵樹來學習一個模型，為了確保無偏，需要確保模型的訓練沒有用到樣本。由於我們需要對所有訓練樣本計算無偏的梯度估計，乍看起來對於的訓練不能使用任何樣本，貌似無法實現的樣子，但是事實上可以通過一些技巧來進行克服，具體的算法在前面已經有所描述，而且是作者較新的論文當中的描述，這裡不再贅述。本節主要講講Ordered boosting的具體實現。</p><p>Ordered boosting算法好是好，但是在大部分的實際任務當中都不具備使用價值，因為需要訓練個不同的模型，大大增加的內存消耗和時間複雜度。在CatBoost當中，我們實現了一個基於GBDT框架的修改版本。</p><p>前面提到過，在傳統的GBDT框架當中，構建下一棵樹分為兩個階段：選擇樹結構和在樹結構固定後計算葉子節點的值。CatBoost主要在第一階段進行優化。</p><p><strong>First phase</strong></p><p>在建樹的階段，CatBoost有兩種提升模式，Ordered和Plain。Plain模式是採用內建的ordered TS對類別型特徵進行轉化後的標準GBDT算法。Ordered則是對Ordered boosting算法的優化。</p><p><strong>Ordered boosting mode</strong></p><p>一開始，CatBoost對訓練集產生個獨立的隨機序列。序列用來評估定義樹結構的分裂，用來計算所得到的樹的葉子節點的值。因為，在一個給定的序列當中，對於較短的序列，無論是TS的計算還是基於Ordered boosting的預測都會有較大的方差，所以僅僅用一個序列可能引起最終模型的方差，這裡我們會有多個序列進行學習。</p><p>CatBoost採用對稱樹作為基學習器，對稱意味著在樹的同一層，其分裂標準都是相同的。對稱樹具有平衡、不易過擬合併能夠大大減少測試時間的特點。建樹的具體算法如下偽碼描述。</p><div class=pgc-img><img alt=「ML」一文詳盡系列之CatBoost onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/75e16d74259c463aa64dae34d45c58b2><p class=pgc-img-caption></p></div><p>Building a tree in CatBoost</p><p>在Ordered boosting模式的學習過程當中，我們維持一個模型，其中表示基於在序列當中的前個樣本學習得到的模型對於第個樣本的預測。在算法的每一次迭代，我們從當中抽樣一個隨機序列，並基於此構建第步的學習樹。然後，基於，計算相應的梯度。接下來，我們會用餘弦相似度來近似梯度，其中對於每一個樣本，我們取梯度。在候選分裂評估過程當中，第個樣本的葉子節點的值由與同屬一個葉子的的所有樣本的前個樣本的梯度值求平均得到。需要注意的是，取決於選定的序列，因為會影響第個樣本的Ordered TS。當樹的結構確定以後，我們用它來提升所有的模型，我們需要強調下，一個相同的樹結構會被用於所有的模型，但是會根據和 的不同設置不同的葉子節點的值以後應用於不同的模型。</p><p><strong>Plain boosting mode</strong></p><p>Plain boosting模式的算法與標準GBDT流程類似，但是如果出現了類別型特徵，它會基於得到的TS維持個支持模型。</p><p><strong>Second phase</strong></p><p>當所有的樹結構確定以後，最終模型的葉子節點值的計算與標準梯度提升過程類似。第個樣本與葉子進行匹配，我們用來計算這裡的TS。當最終模型在測試期間應用於新的樣本，我們採用整個訓練集來計算TS。</p><p><strong>GPU加速</strong></p><p>就GPU內存使用而言，CatBoost至少與LightGBM一樣有效，CatBoost的GPU實現可支持多個GPU，分佈式樹學習可以通過樣本或特徵進行並行化。</p><p><strong>sklearn參數</strong></p><p>sklearn本身的文檔當中並沒有CatBoost的描述，CatBoost python-reference_parameters-list上面看到主要參數如下：</p><ul class=list-paddingleft-3><li>iterations: 迭代次數， 解決機器學習問題能夠構建的最大樹的數目，default=1000</li><li>learning_rate: 學習率，default=0.03</li><li>depth: 樹的深度，default=6</li><li>l2_leaf_reg: 正則化數，default=3.0</li><li>model_size_reg:模型大小正則化係數，數值越到，模型越小，僅在有類別型變量的時候起作用，取值範圍從0到，GPU計算時不可用， default=None</li><li>rsm: =None,</li><li>loss_function: 損失函數，字符串 (分類任務，default=Logloss，迴歸任務，default=RMSE)</li><li>border_count: 數值型變量的分箱個數</li><ul class=list-paddingleft-2><li>CPU：1～65535的整數，default=254</li><li>GPU：1～255的整數，default=128</li></ul><li>feature_border_type: 數值型變量分箱個數的初始量化模式，default=GreedyLogSum</li><ul class=list-paddingleft-2><li>Median</li><li>Uniform</li><li>UniformAndQuantiles</li><li>MaxLogSum</li><li>MinEntropy</li><li>GreedyLogSum</li></ul><li>per_float_feature_quantization: 指定特定特徵的分箱個數，default=None,</li><li>input_borders=None,</li><li>output_borders=None,</li><li>fold_permutation_block: 對數據集進行隨機排列之前分組的block大小，default=1</li><li>od_pval: 過擬合檢測閾值，數值越大，越早檢測到過擬合，default=0</li><li>od_wait: 達成優化目標以後繼續迭代的次數，default=20</li><li>od_type: 過擬合檢測類型，default=IncToDec</li><ul class=list-paddingleft-2><li>IncToDec</li><li>Iter</li></ul><li>nan_mode: 缺失值的預處理方法，字符串類型，default=Min</li><ul class=list-paddingleft-2><li>Forbidden: 不支持缺失值</li><li>Min: 缺失值賦值為最小值</li><li>Max: 缺失值賦值為最大值</li></ul><li>counter_calc_method: 計算Counter CTR類型的方法，default=None</li><li>leaf_estimation_iterations: 計算葉子節點值時候的迭代次數，default=None,</li><li>leaf_estimation_method: 計算葉子節點值的方法，default=Gradient</li><ul class=list-paddingleft-2><li>Newton</li><li>Gradient</li></ul><li>thread_count: 訓練期間的進程數，default=-1，進程數與部件的核心數相同</li><li>random_seed: 隨機數種子，default=0</li><li>use_best_model: 如果有設置eval_set設置了驗證集的話可以設為True，否則為False</li><li>verbose: 是否顯示詳細信息，default=1</li><li>logging_level: 打印的日誌級別，default=None</li><li>metric_period: 計算優化評估值的頻率，default=1</li><li>ctr_leaf_count_limit: 類別型特徵最大葉子數，default=None</li><li>store_all_simple_ctr: 是否忽略類別型特徵，default=False</li><li>max_ctr_complexity: 最大特徵組合數，default=4</li><li>has_time: 是否採用輸入數據的順序，default=False</li><li>allow_const_label: 使用它為所有對象用具有相同標籤值的數據集訓練模型，default=None</li><li>classes_count: 多分類當中類別數目上限，defalut=None</li><li>class_weights: 類別權重，default=None</li><li>one_hot_max_size: one-hot編碼最大規模，默認值根據數據和訓練環境的不同而不同</li><li>random_strength: 樹結構確定以後為分裂點進行打分的時候的隨機強度，default=1</li><li>name: 在可視化工具當中需要顯示的實驗名字</li><li>ignored_features: 在訓練當中需要排除的特徵名稱或者索引，default=None</li><li>train_dir: 訓練過程當中文件保存的目錄</li><li>custom_loss: 用戶自定義的損失函數</li><li>custom_metric: 自定義訓練過程當中輸出的評估指標，default=None</li><li>eval_metric: 過擬合檢測或者最優模型選擇的評估指標</li><ul class=list-paddingleft-2><li><strong>loss-functions</strong></li></ul><li>bagging_temperature: 貝葉斯bootstrap強度設置，default=1</li><li>save_snapshot: 訓練中斷情況下保存快照文件</li><li>snapshot_file: 訓練過程信息保存的文件名字</li><li>snapshot_interval: 快照保存間隔時間，單位秒</li><li>fold_len_multiplier: 改變fold長度的係數，default=2</li><li>used_ram_limit: 類別型特徵使用內存限制，default=None</li><li>gpu_ram_part: GPU內存使用率，default=0.95</li><li>allow_writing_files: 訓練過程當中允許寫入分析和快照文件，default=True</li><li>final_ctr_computation_mode: Final CTR計算模式</li><li>approx_on_full_history: 計算近似值的原則，default=False</li><li>boosting_type: 提升模式</li><ul class=list-paddingleft-2><li>Ordered</li><li>Plain</li></ul><li>simple_ctr: 單一類別型特徵的量化設置</li><ul class=list-paddingleft-2><li>CtrType</li><li>TargetBorderCount</li><li>TargetBorderType</li><li>CtrBorderCount</li><li>CtrBorderType</li><li>Prior</li></ul><li>combinations_ctr: 組合類別型特徵的量化設置</li><ul class=list-paddingleft-2><li>CtrType</li><li>TargetBorderCount</li><li>TargetBorderType</li><li>CtrBorderCount</li><li>CtrBorderType</li><li>Prior</li></ul><li>per_feature_ctr: 以上幾個參數的設置具體可以細看下面的文檔</li><ul class=list-paddingleft-2><li><strong>Categorical features</strong></li></ul><li>task_type: 任務類型，CPU或者GPU，default=CPU</li><li>device_config: =None</li><li>devices: 用來訓練的GPU設備號，default=NULL</li><li>bootstrap_type: 自採樣類型，default=Bayesian</li><ul class=list-paddingleft-2><li>Bayesian</li><li>Bernoulli</li><li>MVS</li><li>Poisson</li><li>No</li></ul><li>subsample: bagging的採樣率，default=0.66</li><li>sampling_unit: 採樣模式，default=Object</li><ul class=list-paddingleft-2><li>Object</li><li>Group</li></ul><li>dev_score_calc_obj_block_size: =None,</li><li>max_depth: 樹的最大深度</li><li>n_estimators: 迭代次數</li><li>num_boost_round: 迭代輪數</li><li>num_trees: 樹的數目</li><li>colsample_bylevel: 按層抽樣比例，default=None</li><li>random_state: 隨機數狀態</li><li>reg_lambda: 損失函數範數，default=3.0</li><li>objective: =同損失函數</li><li>eta: 學習率</li><li>max_bin: =同border_coucnt</li><li>scale_pos_weight: 二分類任務當中1類的權重，default=1.0</li><li>gpu_cat_features_storage: GPU訓練時類別型特徵的存儲方式，default=GpuRam</li><ul class=list-paddingleft-2><li>CpuPinnedMemory</li><li>GpuRam</li></ul><li>data_partition: 分佈式訓練時數據劃分方法</li><ul class=list-paddingleft-2><li>特徵並行</li><li>樣本並行</li></ul><li>metadata: =None</li><li>early_stopping_rounds: 早停輪次，default=False</li><li>cat_features: =指定類別型特徵的名稱或者索引</li><li>grow_policy: 樹的生長策略</li><li>min_data_in_leaf: 葉子節點最小樣本數，default=1</li><li>min_child_samples: 葉子節點最小樣本數，default=1</li><li>max_leaves: 最大葉子數，default=31</li><li>num_leaves: 葉子數</li><li>score_function: 建樹過程當中的打分函數</li><li>leaf_estimation_backtracking: 梯度下降時回溯類型</li><li>ctr_history_unit: =None</li><li>monotone_constraints: =None</li></ul><p>如果有遺漏，具體可以參閱<strong>CatBoost python-reference_parameters-list</strong></p><p>區分具體的機器學習任務有：</p><p><strong>CatBoostClassifier</strong></p><p><strong>CatBoostClassifier</strong></p><ul class="code-snippet__line-index code-snippet__js"><li><br></li></ul><pre>class CatBoostClassifier(iterations=None, learning_rate=None, depth=None, l2_leaf_reg=None, model_size_reg=None, rsm=None, loss_function=None, border_count=None, feature_border_type=None, per_float_feature_quantization=None, input_borders=None, output_borders=None, fold_permutation_block=None, od_pval=None, od_wait=None, od_type=None, nan_mode=None, counter_calc_method=None, leaf_estimation_iterations=None, leaf_estimation_method=None, thread_count=None, random_seed=None, use_best_model=None, verbose=None, logging_level=None, metric_period=None, ctr_leaf_count_limit=None, store_all_simple_ctr=None, max_ctr_complexity=None, has_time=None, allow_const_label=None, classes_count=None, class_weights=None, one_hot_max_size=None, random_strength=None, name=None, ignored_features=None, train_dir=None, custom_loss=None, custom_metric=None, eval_metric=None, bagging_temperature=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, fold_len_multiplier=None, used_ram_limit=None, gpu_ram_part=None, allow_writing_files=None, final_ctr_computation_mode=None, approx_on_full_history=None, boosting_type=None, simple_ctr=None, combinations_ctr=None, per_feature_ctr=None, task_type=None, device_config=None, devices=None, bootstrap_type=None, subsample=None, sampling_unit=None, dev_score_calc_obj_block_size=None, max_depth=None, n_estimators=None, num_boost_round=None, num_trees=None, colsample_bylevel=None, random_state=None, reg_lambda=None, objective=None, eta=None, max_bin=None, scale_pos_weight=None, gpu_cat_features_storage=None, data_partition=None metadata=None, early_stopping_rounds=None, cat_features=None, grow_policy=None, min_data_in_leaf=None, min_child_samples=None, max_leaves=None, num_leaves=None, score_function=None, leaf_estimation_backtracking=None, ctr_history_unit=None, monotone_constraints=None)br</pre><p><strong>CatBoostRegressor</strong></p><p><strong>CatBoostRegressor</strong></p><ul class="code-snippet__line-index code-snippet__js"><li><br></li></ul><pre>class CatBoostRegressor(iterations=None, learning_rate=None, depth=None, l2_leaf_reg=None, model_size_reg=None, rsm=None, loss_function='RMSE', border_count=None, feature_border_type=None, per_float_feature_quantization=None, input_borders=None, output_borders=None, fold_permutation_block=None, od_pval=None, od_wait=None, od_type=None, nan_mode=None, counter_calc_method=None, leaf_estimation_iterations=None, leaf_estimation_method=None, thread_count=None, random_seed=None, use_best_model=None, best_model_min_trees=None, verbose=None, silent=None, logging_level=None, metric_period=None, ctr_leaf_count_limit=None, store_all_simple_ctr=None, max_ctr_complexity=None, has_time=None, allow_const_label=None, one_hot_max_size=None, random_strength=None, name=None, ignored_features=None, train_dir=None, custom_metric=None, eval_metric=None, bagging_temperature=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, fold_len_multiplier=None, used_ram_limit=None, gpu_ram_part=None, pinned_memory_size=None, allow_writing_files=None, final_ctr_computation_mode=None, approx_on_full_history=None, boosting_type=None, simple_ctr=None, combinations_ctr=None, per_feature_ctr=None, ctr_target_border_count=None, task_type=None, device_config=None, devices=None, bootstrap_type=None, subsample=None, sampling_unit=None, dev_score_calc_obj_block_size=None, max_depth=None, n_estimators=None, num_boost_round=None, num_trees=None, colsample_bylevel=None, random_state=None, reg_lambda=None, objective=None, eta=None, max_bin=None, gpu_cat_features_storage=None, data_partition=None, metadata=None, early_stopping_rounds=None, cat_features=None, grow_policy=None, min_data_in_leaf=None, min_child_samples=None, max_leaves=None, num_leaves=None, score_function=None, leaf_estimation_backtracking=None, ctr_history_unit=None, monotone_constraints=None)br</pre><p><br></p><p><strong>應用場景</strong></p><p>作為GBDT框架內的算法，GBDT、XGBoost、LightGBM能夠應用的場景CatBoost也都適用，並且在處理類別型特徵具備獨有的優勢，比如廣告推薦領域。</p><p><strong>優缺點</strong></p><p><strong>優點</strong></p><ul class=list-paddingleft-2><li>能夠處理類別特徵</li><li>能夠有效防止過擬合</li><li>模型訓練精度高</li><li>調參時間相對較多</li></ul><p><strong>缺點</strong></p><ul class=list-paddingleft-2><li>對於類別特徵的處理需要大量的內存和時間</li><li>不同隨機數的設定對於模型預測結果有一定的影響</li></ul><p><strong>參考</strong></p><ul class=list-paddingleft-2><li>https://book.douban.com/subject/26708119/</li><li>https://book.douban.com/subject/33437381/</li><li>https://catboost.ai/</li><li>https://github.com/catboost/catboost</li><li>https://papers.nips.cc/paper/7898-catboost-unbiased-boosting-with-categorical-features.pdf</li><li>http://learningsys.org/nips17/assets/papers/paper_11.pdf</li><li>https://catboost.ai/docs/concepts/python-reference_parameters-list.html</li></ul><p><br></p><p>The End</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>ML</a></li><li><a>詳盡</a></li><li><a>CatBoost</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/b3820d4.html alt=「ML」深入理解CatBoost class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/f7e6e06fe4c243baa728efbdb3621da6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b3820d4.html title=「ML」深入理解CatBoost>「ML」深入理解CatBoost</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b64e226d.html alt=AI和ML在網絡安全中的用例 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2ecb2e5d786743a688e69abfd136a5a0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b64e226d.html title=AI和ML在網絡安全中的用例>AI和ML在網絡安全中的用例</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/cf4d035d.html alt="養眼又詳盡的蘭花美圖和介紹 自然界真的很神奇" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/1527560392235d8eece7518 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/cf4d035d.html title="養眼又詳盡的蘭花美圖和介紹 自然界真的很神奇">養眼又詳盡的蘭花美圖和介紹 自然界真的很神奇</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/1ebabf70.html alt=ML基礎：協方差矩陣 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a8ebdba18ae5461a8e462d5fcce85ee4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/1ebabf70.html title=ML基礎：協方差矩陣>ML基礎：協方差矩陣</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/ab053769.html alt=《零度實驗室》：解析王者榮耀中未詳盡描述的7個恐怖技能 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/46f000053b64c196074b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/ab053769.html title=《零度實驗室》：解析王者榮耀中未詳盡描述的7個恐怖技能>《零度實驗室》：解析王者榮耀中未詳盡描述的7個恐怖技能</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/ebed9a9e.html alt=為ML帶來拓撲學基礎，Nature子刊提出拓撲數據分析方法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/3c8b122558234a6fa193f00f54ae1b1f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/ebed9a9e.html title=為ML帶來拓撲學基礎，Nature子刊提出拓撲數據分析方法>為ML帶來拓撲學基礎，Nature子刊提出拓撲數據分析方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/fc53b4df.html alt=一文詳盡之支持向量機算法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/eb3419f34fb5463eb06573b2573e25cb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/fc53b4df.html title=一文詳盡之支持向量機算法>一文詳盡之支持向量機算法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/87eee7d.html alt=關於光圈，這是我為你整理的最詳盡的乾貨，值得攝影愛好者們收藏 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d1922a8183ae4ddc9441b7f8492e1e03 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/87eee7d.html title=關於光圈，這是我為你整理的最詳盡的乾貨，值得攝影愛好者們收藏>關於光圈，這是我為你整理的最詳盡的乾貨，值得攝影愛好者們收藏</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/059e7a6.html alt="吳恩達 ML Yearning 關於學習曲線的分析&與人類級別的表現對比" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/f6a60d9d13f84286a71f27b8edb150f1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/059e7a6.html title="吳恩達 ML Yearning 關於學習曲線的分析&與人類級別的表現對比">吳恩達 ML Yearning 關於學習曲線的分析&與人類級別的表現對比</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0625680.html alt=詳盡介紹過硫酸氫鉀的各方面知識，並結合塘口實戰技術 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/57000c856520426ebc663c68673ed418 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0625680.html title=詳盡介紹過硫酸氫鉀的各方面知識，並結合塘口實戰技術>詳盡介紹過硫酸氫鉀的各方面知識，並結合塘口實戰技術</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/23e2014.html alt="吳恩達《ML Yearning》｜基礎的誤差分析& 偏差、方差分析" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/6f70b8b7cd0f4dad93aced78cc14ffcf style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/23e2014.html title="吳恩達《ML Yearning》｜基礎的誤差分析& 偏差、方差分析">吳恩達《ML Yearning》｜基礎的誤差分析& 偏差、方差分析</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/22f5e9d.html alt=一文讀懂ML中的解析解與數值解 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/15250924930865b0bf65466 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/22f5e9d.html title=一文讀懂ML中的解析解與數值解>一文讀懂ML中的解析解與數值解</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/779d7c9.html alt=詳盡闡述電流互感器的方方面面 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/27911ada51a2413d979ebba0da7d3b91 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/779d7c9.html title=詳盡闡述電流互感器的方方面面>詳盡闡述電流互感器的方方面面</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/70d88e3.html alt="文章太長不想看？ML 文本自動摘要了解一下" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/34f7fc9ba9814b28baa788e9b67df503 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/70d88e3.html title="文章太長不想看？ML 文本自動摘要了解一下">文章太長不想看？ML 文本自動摘要了解一下</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>