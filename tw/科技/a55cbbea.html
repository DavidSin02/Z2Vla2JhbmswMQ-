<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習總結（基礎）：偏差和方差、iid、分佈 | 极客快訊</title><meta property="og:title" content="機器學習總結（基礎）：偏差和方差、iid、分佈 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a55cbbea.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a55cbbea.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a55cbbea.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a55cbbea.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a55cbbea.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a55cbbea.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a55cbbea.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a55cbbea.html><meta property="article:published_time" content="2020-11-14T21:05:42+08:00"><meta property="article:modified_time" content="2020-11-14T21:05:42+08:00"><meta name=Keywords content><meta name=description content="機器學習總結（基礎）：偏差和方差、iid、分佈"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/a55cbbea.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習總結（基礎）：偏差和方差、iid、分佈</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e><p class=pgc-img-caption></p></div><h1><strong>偏差和方差</strong></h1><p>在監督機器學習中，我們為訓練數據提供標籤以構建f（x）機器學習模型。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/203ea61fe1984b1e93c210752552d84f><p class=pgc-img-caption></p></div><p>均方誤差由方差和偏差組成。由於噪聲與機器學習模型無關，並且在訓練機器學習模型時不可減少，因此我們可以從討論中忽略它。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/7c6f9b0ea20441d190e2d4d69404b70d><p class=pgc-img-caption></p></div><p>方差是對採樣數據變化敏感的誤差。如果機器學習訓練數據集太小，則複雜模型容易過度擬合。一旦發生這種情況，如果對不同的訓練數據進行採樣，模型預測可能會發生顯著變化。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3db226eae51348739207bfb0f8d3ab4f><p class=pgc-img-caption></p></div><p>在N 1-D的訓練數據輸入下，我們可以用一個N階多項式完美地擬合一個機器學習模型。下面的紅色矩形區域過度擬合，如果ground truth更接近線性方程，則會導致錯誤的預測。如果最後2個數據點不包含在訓練數據集中，或者在紅色矩形區域內採樣更多的數據點，預測結果將會不同。簡而言之，如果機器學習模型是複雜的，並且沒有足夠的數據來擬合它，那麼訓練的方差就會很大。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ab61bcd39c134cc6b536d2e2e772dba5><p class=pgc-img-caption></p></div><p>另一方面，當模型不夠複雜，無法做出準確預測時，就會出現偏差。如果x的ground truth在下面的圓心附近，偏差會將答案朝一個特定的方向移動，而方差則會將預測散佈在ground truth周圍。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e9435d21d52a4c448688fb53660287d0><p class=pgc-img-caption></p></div><p>下面是模型複雜性的演示。隨著複雜度的提高，它會更好地擬合訓練數據，直到過擬合。此時，訓練誤差繼續減小。但是當我們使用不用於訓練的數據點來驗證機器學習模型時，驗證誤差很大，並且隨著迭代次數的增加而增加。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9a3f534471fc4af8b01cb4c47f53f258><p class=pgc-img-caption></p></div><p>下圖總結了模型複雜性、偏差和方差之間的關係。減小方差的方法是增加訓練數據集的大小，降低模型的複雜度或增加正則化。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b93132d050494b2cbc5f7c6a883d72f4><p class=pgc-img-caption></p></div><h1><strong>獨立同分布（iid）</strong></h1><p>在許多機器學習（ML）問題中，我們假設數據點來自相同的數據分佈，並且觀測值不影響其他觀測值。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/019c71fdc42d4768a5925ddc18a567bf><p class=pgc-img-caption></p></div><p>這是一個簡單的概念，許多機器學習（ML）和深度學習（DL）算法的成功很大程度上取決於它。當數據點和訓練批次接近i.i.d時，模型訓練更加穩定。</p><p>當我們擲100次骰子時，每個數據點都來自相同的分佈。當擲骰子的時候，結果是獨立於之前的結果的，因此擲骰子是i.i.d。但是，如果我們從一副紙牌中抽取而不進行替換，採樣分佈就會發生變化，因此不是i.i.d。這樣的假設可以在很多情況下簡化我們的數學運算。例如，最大似然估計(MLE)中的聯合分佈可以簡化為獨立事件的乘法。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/48b3e41ee2a943718bf4e7b341c58a35><p class=pgc-img-caption></p></div><p>這很關鍵。通過將模型分解為可管理的獨立子組件，我們可以顯著降低複雜性</p><p><strong>協變量偏移</strong></p><p>然而，如果我們有一個用骰子訓練的模型，如果我們在推理時間內的輸入具有不同的分佈，那麼預測將是錯誤的，比如莊家切換到一個有偏差的骰子。這就是協變量偏移。最近，有一篇關於在醫學領域使用深度學習的報告。雖然深度學習（DL）預測的準確性似乎很高，但在其他醫院卻顯示出不一致的結果。結果發現，一些模型被從少數醫院收集的數據過度擬合。當該模型適用於移動設備使用頻率較高的地區時，準確度會下降。這是一個協變量偏移問題！</p><p>在一些機器學習（ML）或深度學習（DL）問題中，狀態可能是高度相關的。時間序列模型的相關性更高。如果我們在優化這些模型時使用梯度下降法，則i.i.d.假設不成立，相應的訓練可能非常不穩定。</p><h1><strong>分佈</strong></h1><p>在這一節中，我們將討論高斯分佈、伽馬分佈、β分佈、狄裡克萊分佈等分佈，不同的分佈有不同的參數和不同的形狀，用於不同的目的。找到合適的模型可以大大簡化計算</p><p><strong>期望</strong></p><p>對於連續隨機變量，f（x）的期望值為</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/490857c6fc3449d4a088694f0e022527><p class=pgc-img-caption></p></div><p>對於離散隨機變量，它是</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2dda0f1beea94d1eb2f47021cf58d361><p class=pgc-img-caption></p></div><p><strong>方差和協方差</strong></p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/c63ace8b2c634e12b8047537034a7925><p class=pgc-img-caption></p></div><p>對於連續隨機變量，它是</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/70dcbb1758f04afca54e28757dc8cb87><p class=pgc-img-caption></p></div><p>協方差表明兩個變量是正相關還是負相關。如果它為零，則它們是無關的。</p><p>方差可以表示為下面的等式</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/11fedbcb70e64dbbb2f7b6485e26b34f><p class=pgc-img-caption></p></div><p>這種形式化對於許多證明是很方便的，包括找到 MLE估計的偏差和方差。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/477e5505e43d4cf99e7f7fb5086976d7><p class=pgc-img-caption></p></div><p>X＆Y的協方差是</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c4fc753c25d54e6786fa39e676d3f7ba><p class=pgc-img-caption></p></div><p><strong>樣本方差</strong></p><p>樣本方差是通過採樣m個數據點來估計總體方差的方法。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2839701e1d944b77b143f8861ae849e0><p class=pgc-img-caption></p></div><p>但是，採樣數據的平均值與數據本身相關。因此，（xᵢ-μ）²將小於總體的數量。為了彌補這一點，我們需要將其除以m-1而不是m（證明）。但是對於樣本均值，我們只需要將總和除以m，估計是無偏的。</p><p><strong>相關性</strong></p><p>協方差顯示變量之間的關係，但不能很好地量化變量。每個變量可以是不同的比例（或單位）。相關性對協方差進行歸一化，使得每個變量對應於相同的比例。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cc0e7736823e4207a9e2dc72eaf3401b><p class=pgc-img-caption></p></div><p><strong>高斯分佈/正態分佈</strong></p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e6427912ea094442918911191d7b14ac><p class=pgc-img-caption></p></div><p>在高斯分佈中，68%的數據在1σ內，95%的數據在2σ內。標準正態分佈為μ=0和σ=1。</p><p><strong>多元高斯分佈</strong></p><p>多元高斯分佈使用高斯模型模擬多變量分佈。下圖是雙變量高斯分佈，涉及兩個隨機變量。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/887975ae66ec4861988e5b22c370a458><p class=pgc-img-caption></p></div><p>多元高斯分佈定義為</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5f6c2ab2dc29482d91b0797652dd5d73><p class=pgc-img-caption></p></div><p>其中Σ是協方差矩陣。 該矩陣中的每個元素記錄兩個隨機變量的相關性。我們可以通過下面的來進行採樣多元高斯分佈</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e5b0f9375cd84e80a722daa15a96bda4><p class=pgc-img-caption></p></div><p>協方差矩陣可以用相關性表示。這是一個二元高斯分佈的例子。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4525c56e4f0d42448e2211257ed11198><p class=pgc-img-caption></p></div><p><strong>中心極限定理</strong></p><p>考慮擲骰子，如果骰子是公平的，則擲出的值均勻分佈。讓我們多次擲骰子並平均結果。我們多次重複實驗並收集所有彙總結果。當我們繪製這些結果時，我們將認識到結果是高斯分佈的。中心極限定理說：</p><blockquote><p>大量相同分佈的隨機變量的和傾向於高斯分佈。</p></blockquote><p>即我們可以有許多獨立的隨機變量，比如說每個變量代表一個拋出的結果。當我們將它們加在一起並將其歸一化（平均值）時，經過多次試驗後的歸一化結果往往是高斯分佈。該定理推廣到自變量的任何分佈。因此，無論骰子是公平的，有偏的還是任何數據分佈，歸一化結果都是高斯分佈的。這意味著單個變量可能具有不同的數據分佈，但我們可以使用高斯分佈來模擬其聚合結果。</p><p>如果隨機變量已經是高斯分佈，且方差為σ²，則歸一化結果將是</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8445999473f049818421dd3c247890c8><p class=pgc-img-caption></p></div><p>即聚合結果的方差隨著較大樣本量的平均值而下降。兩個高斯分佈變量的和或差也是高斯分佈的。</p><p><strong>伯努利分佈</strong></p><p>伯努利分佈是具有兩種可能結果的事件的離散分佈，一個具有概率φ而另一個具有1-φ。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/662afe36fe7444a1b0360fd53527a48a><p class=pgc-img-caption></p></div><p>分佈也可以寫成：</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/033ef3af3ed6479aa19eb69aa5fa31a4><p class=pgc-img-caption></p></div><p><strong>二項分佈</strong></p><p>二項分佈是獨立伯努利試驗的綜合結果。例如，我們擲硬幣n次，然後模擬有x次反面的機會。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/2c34653aa3154da589bba4afc8bccc6b><p class=pgc-img-caption></p></div><p>其中p是反面的概率。二項分佈的均值和方差是</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3edea97fd0364fb5a33a2735d89d5f82><p class=pgc-img-caption></p></div><p>伯努利分佈中θ的方差是：</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/33a5225e7d174170a4337974e1b63ea4><p class=pgc-img-caption></p></div><p><strong>分類分佈</strong></p><p>伯努利分佈僅有兩種可能的結果。在分類分佈中，我們有K個可能的結果，概率為p 1，p 2 p 3，...和pk，所有這些概率加起來為1。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/8340531d01d3461abfe95af43d580f7c><p class=pgc-img-caption></p></div><p><strong>多項式分佈</strong></p><p>多項式分佈是二項分佈的推廣。它有k個可能的結果。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2787f88eb8df4f2dbad6990ec564572a><p class=pgc-img-caption></p></div><p>假設這些結果分別與概率p 1，p 2，...和pk相關。我們收集大小為n的樣本，xᵢ表示結果i的計數。聯合概率是</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a88c9277d8884aea89ca95556795d848><p class=pgc-img-caption></p></div><p>在貝葉斯推理中，我們經常通過Beta分佈對此概率進行建模（稍後討論）。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b9bccc44ff784473a716700dae8bbafc><p class=pgc-img-caption></p></div><p>其中</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/968a96af8e83453d9d7d3c2786ffde6f><p class=pgc-img-caption></p></div><p><strong>Beta分佈</strong></p><p>Beta分佈的定義與二項分佈非常相似。beta分佈定義為：</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c9b0f58b0f864f0484ee0a4176b020d3><p class=pgc-img-caption></p></div><p>B是歸一化因子</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/24dcdeaf475a4ec7b57dd51f0b25ba84><p class=pgc-img-caption></p></div><p>它對下面的分子進行歸一化，因此計算結果f是概率分佈。如果我們將f（x）除以所有可能的x，它等於1。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6920d411eabc4e4cb9b52f099e8c0d83><p class=pgc-img-caption></p></div><p>當a = b = 1時，f（x）對於所有x都是常數，因此，分佈是均勻分佈的。Beta分佈中的兩個參數a和b極大地改變了分佈的形狀。如下所示，Beta分佈可以模擬許多形狀的分佈，包括U形和均勻分佈。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c4bbf714733c40e48cfa6fc25fec2f2f><p class=pgc-img-caption></p></div><p>二項分佈具有與Beta分佈類似的形式（紅色下劃線）。因此，很容易同時操作二項分佈和Beta分佈。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2bc15dd0e41047b7bb84249a055c4497><p class=pgc-img-caption></p></div><p>此外，可以訓練上述Beta分佈中的θ以在二項分佈中對參數p進行建模。Beta分佈可以被認為是對分佈上的分佈進行建模。簡而言之，我們可以通過調整a和b來使用Beta分佈來學習二項分佈的p。簡而言之，Beta分佈模擬了二項分佈的p的所有可能值及其確定性。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/41615a5fa7014c45b8a77b40eb4e34ce><p class=pgc-img-caption></p></div><p>如前所述，後者通常是難以處理的。但是，如果我們能用特殊的分佈對似然和先驗進行建模，我們可以用解析的方法很容易地解決後驗問題。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/de863f21c8294096993e9112de091d19><p class=pgc-img-caption></p></div><p>我們將在貝葉斯推理中進一步闡述它。但作為預覽，後驗非常簡單：</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/99ed071145644023908698416cc18e27><p class=pgc-img-caption></p></div><p>Beta分佈的均值和方差為：</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/5065183c14ab4c248f9074ea164643d4><p class=pgc-img-caption></p></div><p>其中ψ是Digamma函數：</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a186d8ede79544cc96088d440dc10148><p class=pgc-img-caption></p></div><p>給定觀測數據，我們可以利用上述方程逆向設計模型參數。</p><p><strong>Dirichlet分佈</strong></p><p>如果Beta分佈是二項分佈，則Dirichlet分佈是多項式分佈。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b80cbdf6e1a24396af794e6d378057d1><p class=pgc-img-caption></p></div><p>Dirichlet分佈</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f0fd4b25cca74102a940df6d84ac0742><p class=pgc-img-caption></p></div><p>其中α是Dirichlet分佈的參數。它控制分佈的形狀。由於θ有3個分量，α也由3個值組成，因此我們可以很容易地控制θ的分佈形狀。Dirichlet分佈定義為：</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4e69c9edef4a42819e83adc0d119eee1><p class=pgc-img-caption></p></div><p>我們可以看到Dirichlet分佈與下面的多項式分佈之間有著密切的相似性。這就是為什麼通過調整α的值來使用它來學習多項式分佈是一個很好的選擇。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e4c5dbec78394f14b503cd04e4400e37><p class=pgc-img-caption></p></div><p>在視覺上，θ1，θ2，θ3的可能值位於三角形平面上，以強制執行這些值總和為1的約束。對於K變量，所有采樣值都位於（K -1）-simplex內。即所有K變量總和為1。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/71cbc8108e0f48bea5007ee49e895755><p class=pgc-img-caption></p></div><p>下圖顯示了K = 1和K = 3的一些可能的形狀（不同的αᵢ)。如圖所示，它可以表示均勻分佈，單峰形狀和多個模態形狀。Dirichlet分佈允許我們模擬不同形狀的概率分佈(給定θ1，θ2，θ3，...總是加1)。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d75fa5314b8147a983a5ae0326a40d0c><p class=pgc-img-caption></p></div><p>要理解α與形狀之間的關係，首先讓我們總結一下它的一些屬性。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/93daa35841b949d59753682f499bdb82><p class=pgc-img-caption></p></div><p>當所有αᵢ都相等時,E（θ ₁）=E（θ ₂）=E（θ ₃），即要使θ ₁，θ ₂，θ ₃均勻分佈，我們可以設置所有αᵢ等於1。如果我們想要一個特定的類型，我們使相應的αᵢ遠大於1並且也遠高於其他αⱼ（假設αⱼ≥1）。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5336ec4bc04c48a4896ffb8d52c4adf3><p class=pgc-img-caption></p></div><p>當θᵢ接近0時，p（θᵢ）將接近無窮大，即θᵢ等於0的概率很高。</p><p>讓所有αᵢ相等，並將其命名為α。這是Dirichlet分佈的一種特殊情況，稱為對稱Dirichlet分佈。α不是向量，而是用標量來建模。如下所示，隨著我們增加α，我們將非常稀疏的分佈變為均勻分佈。當α很小時，單個樣本中的大多數值xᵢ為零。隨著它的增加，分佈變得均勻。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d582c53dc7644ad9b07341e789f25f1><p class=pgc-img-caption></p></div><p>dirichlet分佈的參數可以重新表示為α和n（α’ᵢ → α nᵢ）的乘積，其中α是稱為濃度參數（稀疏性度量）的標量，n是稱為其元素總和為1的基度量的向量。</p><p><strong>泊松分佈</strong></p><p>泊松分佈模擬在固定時間間隔內發生的給定數量事件的概率。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5f9e596c78564f279d57ae772b427d4f><p class=pgc-img-caption></p></div><p>當事件數量與持續時間尺度相比相對較小時，事件在特定持續時間內發生x次的概率為：</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3a65350f1eb346918a1f994fe90f2ac4><p class=pgc-img-caption></p></div><p>這是不同λ的概率質量函數。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a673d9a232ea401bac0fbe846919ac2a><p class=pgc-img-caption></p></div><p>泊松過程是事件以恆定平均速率獨立地和連續地發生的方法。</p><p>在泊松過程中，</p><ul><li>您知道平均事件發生率。</li><li>事件是隨機的，彼此獨立。事件的發生不會影響另一個事件的發生。</li><li>等待時間的分佈是無記憶的。無論你等待多久都沒關係，未來的分佈仍然是一樣的，即</li></ul><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/cb8709fe9fa948b28ea147050a9445fd><p class=pgc-img-caption></p></div><p><strong>指數和拉普拉斯分佈</strong></p><p>指數分佈是泊松過程中下一個事件發生之前等待時間的概率分佈。該等待時間被建模為具有指數分佈的隨機變量。如前所述，等待時間的這種分配是無記憶的。即使在過去20分鐘內沒有發生任何事件，PDF也是一樣的。下圖顯示了分佈的形狀和數學定義。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a1a053e65b7d4c14b79a066a9506ea57><p class=pgc-img-caption></p></div><p>對於λ= 0.1，等待超過15的機會是0.22。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e0f01286a4134dfdbeb284658ba7c31b><p class=pgc-img-caption></p></div><p>拉普拉斯分佈也稱為雙指數分佈，可以認為是兩個背靠背粘合在一起的指數分佈。</p><p><strong>Gamma分佈</strong></p><p>指數分佈和卡方分佈是Gamma分佈的特例。Gamma分佈可以被認為是k個獨立的指數分佈隨機變量的總和。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b7bf40cb380f4841961f6a55c31df698><p class=pgc-img-caption></p></div><p>直觀地說，它是第k個事件發生的等待時間的分佈。這是Gamma分佈的數學定義。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/14997b0a094649d9908b571d1f2eb1b6><p class=pgc-img-caption></p></div><p>Gamma分佈通常使用許多不同的符號：</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/98249dd86b834074882715c830f06824><p class=pgc-img-caption></p></div><p>α通常被寫成k，這是第k個事件。如圖所示，α控制形狀。k=1是具有指數衰減形狀的指數分佈。隨著k的增加，曲線的偏度減小。分佈將與累積分佈趨於平穩。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ca7ff69278024c3d9e972bc1b4a955cb><p class=pgc-img-caption></p></div><p>它看起來更接近中心極限定理所建議的正態分佈。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/6af88fcaa06e4240ae68cdee6ef611ea><p class=pgc-img-caption></p></div><p>接下來，我們分別確定k=2和繪製β=1和2。如下圖所示，形狀保持不變。但如果我們更靠近x軸和y軸上的單位，隨著β的增加，值的擴展會減小。直觀地說，當事件率增加時，預期等待時間減少。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6d314195e76940cfb8a11cd34544dcc3><p class=pgc-img-caption></p></div><p>如果兩條曲線具有相同的比例，則第二個圖中的值範圍將更窄。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f3628db51ea247fcaea38e0d3f7b51bb><p class=pgc-img-caption></p></div><p>根據α和β的值，可以看大一部分正在增長，而另一部分正在衰減。因此它可以模擬大範圍的密度形狀。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ad31554d781847f198ae73b5cb302251><p class=pgc-img-caption></p></div><p>根據定義，如果x為負，伽馬分佈的密度為零。在建模實際問題時，這是一個很好的約束條件，X和高度一樣，永遠不能為負。這是伽馬分佈的不同參數圖。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f645dc9095bc4a609a8a9c98500514be><p class=pgc-img-caption></p></div><p>Gamma分佈的期望和方差是：</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/19a91ca05e22483cb6a34c7481e082bd><p class=pgc-img-caption></p></div><p><strong>卡方分佈</strong></p><p>假設Z 1，Z 2，...和ZK是ķ個獨立隨機變量，每個都具有標準正態分佈。我們對每個變量進行平方並將它們相加。我們多次重複這些實驗，平方結果的分佈稱為卡方分佈²，k是自由度數。以下是具有不同k值的數學定義及其分佈圖。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/08b680340b3b468592f80fcc716b56de><p class=pgc-img-caption></p></div><p>讓我們用一個k為3的例子來說明它。在每次測試中，我們抽取3名學生。我們把它們的體重歸一化，平方，然後加在一起。假設它們的體重（磅）是（165，175，145）和歸一化值（0.17,0.5，-0.5）。它的平方和是0.53。我們重複運行1000次並繪製結果。它應該看起來像上面的淡藍色曲線，k=3。</p><p>讓我們繪製100個學生樣本(i從1到100)並計算下面的卡方統計量。如果該值過高，則表明它在表示總體方面是一個糟糕的樣本。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e3cc895296624e668e585b5d21ae0ef2><p class=pgc-img-caption></p></div><p>卡方分佈是偏態的。但毫不奇怪，隨著自由度的增加，分佈看起來更接近正態分佈。簡而言之，如果樣本量增加，我們應該期望分佈接近正態分佈。</p><p>卡方檢驗的另一個重要應用是對獨立性的檢驗。</p><p>卡方分佈是Gamma分佈的一個特例。當α=v/2和β=1/2時，gamma（α，β）隨機變量是具有v自由度的卡方隨機變量。</p><p><strong>Dirac分佈</strong></p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e6f496c6f74b493990f18233ae9a303f><p class=pgc-img-caption></p></div><p><strong>歸一化因子</strong></p><p>許多分佈可以表示為</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1c478a1265a94975a67231ea1a446009><p class=pgc-img-caption></p></div><p>其中分子是非歸一化分佈。在許多概率模型中，我們首先對非歸一化分佈進行建模，然後對所有可能的x求和，以創建歸一化因子。這將我們的結果轉化為概率，對於所有可能的x，p（x）總和為1 。例如，使用圖模型，我們使用Graph對問題進行建模，分子是從中導出的因子。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/db0f54b56b3143c0848fd747fae8dfef><p class=pgc-img-caption></p></div><p>在圖模型中，這個歸一化因子取決於我們如何對因子（θ）建模，因此，我們將其稱為配分函數（z（θ））。一般來說，配分函數或標準化因子很難計算。在所有x上求和或積分是不容易的。</p><p>但對於眾所周知的分佈，我們可以首先關注計算分佈參數，如μ，σ²，可以從這些參數中輕鬆找到歸一化因子。</p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/459f21ffc8c341cd83b3abe3581ac6b8><p class=pgc-img-caption></p></div><p><strong>小結</strong></p><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9b7444a1ee054a9ba9290bed339adc1a><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=機器學習總結（基礎）：偏差和方差、iid、分佈 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b369902b600b4c8ea49470ccdbc00233><p class=pgc-img-caption></p></div></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>總結</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/59b3843e.html alt=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/84c47890a2c44654997e63bd5cdf0c72 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/59b3843e.html title=機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等>機器學習總結（基礎）：指數分佈、矩匹配、矩陣分解等</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/5199ece.html alt=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/76d31ab63a7249a5abaeec98d8891354 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/5199ece.html title=機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等>機器學習總結（算法）：聚類、決策樹、能量模型、LSTM等</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>