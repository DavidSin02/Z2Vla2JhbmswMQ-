<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>在深度學習模型的優化上，梯度下降並非唯一的選擇 | 极客快訊</title><meta property="og:title" content="在深度學習模型的優化上，梯度下降並非唯一的選擇 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RkOc1gl6anBXuH"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a5f0956.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a5f0956.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a5f0956.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a5f0956.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a5f0956.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a5f0956.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/a5f0956.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/a5f0956.html><meta property="article:published_time" content="2020-10-29T21:04:52+08:00"><meta property="article:modified_time" content="2020-10-29T21:04:52+08:00"><meta name=Keywords content><meta name=description content="在深度學習模型的優化上，梯度下降並非唯一的選擇"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/a5f0956.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>在深度學習模型的優化上，梯度下降並非唯一的選擇</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p><strong>雷鋒網AI科技評論按</strong>：如果你是一名機器學習從業者，一定不會對基於梯度下降的優化方法感到陌生。對於很多人來說，有了 SGD，Adam，Admm 等算法的開源實現，似乎自己並不用再過多關注優化求解的細節。然而在模型的優化上，梯度下降並非唯一的選擇，甚至在很多複雜的優化求解場景下，一些非梯度優化方法反而更具有優勢。而在眾多非梯度優化方法中，演化策略可謂最耀眼的那顆星！</p><p>對於深度學習模型的優化問題來說，隨機梯度下降（SGD）是一種被廣為使用方法。然而，實際上 SGD 並非我們唯一的選擇。當我們使用一個「黑盒算法」時，即使不知道目標函數 f(x):Rn→R 的精確解析形式（因此不能計算梯度或 Hessian 矩陣）你也可以對 f(x) 進行評估。經典的黑盒優化方法包括「模擬退火算法」、「爬山法」以及「單純形法」。演化策略（ES）是一類誕生於演化算法（EA）黑盒優化算法。在本文中，我們將深入分析一些經典的演化策略方法，並介紹演化策略在深度強化學習中的一些應用。</p><p></p><h2>目錄</h2><blockquote><p>一、演化策略是什麼？</p><p>二、 簡單的高斯演化策略</p><p>三、協方差矩陣自適應演化策略（CMA-ES）</p><p>1、均值更新</p><p>2、步長控制</p><p>3、協方差自適應</p><p>四、自然演化策略（NES）</p><p>1、自然梯度</p><p>2、使用費舍爾（Fisher）信息矩陣進行估計</p><p>3、NES 算法</p><p>五、應用：深度強化學習中的演化策略</p><p>1、OpenAI 用於強化學習的演化策略</p><p>2、演化策略的探索方式</p><p>3、 CEM-RL：結合演化策略和梯度下降方法的強化學習策略搜索</p><p>六、擴展：深度學習中的演化策略</p><p>1、超參數調優：PBT</p><p>2、網絡拓撲優化：WANN</p><p>七、參考文獻</p></blockquote><p></p><h2>一、演化策略是什麼？</h2><p>演化策略（ES）從屬於演化算法的大家族。ES 的優化目標是實數向量 x∈R<sup>n</sup>。</p><p>演化算法（EA）指的是受自然選擇啟發而產生的一類基於種群的優化算法。自然選擇學說認為，如果某些個體具有利於他們生存的特性，那麼他們就可能會繁衍幾代，並且將這種優良的特性傳給下一代。演化是在選擇的過程中逐漸發生的，整個種群會漸漸更好地適應環境。</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RkOc1gl6anBXuH><p>圖 1：自然選擇工作原理示意圖（圖片來源：可汗學院「達爾文、進化與自然選擇」：https://www.khanacademy.org/science/biology/her/evolution-and-natural-selection/a/darwin-evolution-natural-selection）。</p><p>（左）一群老鼠移動到了一個岩石顏色非常暗的地區。由於自然遺傳的變異，有些老鼠毛色是黑色，有的則是棕褐色。（中）相較於黑色的老鼠來說，棕褐色的老師更容易被肉食性鳥類發現。因此，褐色老鼠比黑色老鼠更頻繁地被鳥類捕食。只有存活下來的老鼠到了生育年齡後會留下後代。（右）由於黑色老鼠比褐色老鼠留下後代的機會更大，在下一代老鼠中黑色的佔比上一代更高。</p><p>我們可以通過以下方式將演化算法概括為一種通用的優化方案：</p><p>假設我們想要優化一個函數 f(x)，而且無法直接計算梯度。但是，我們在給定任意 x 的情況下仍然可以評估 f(x)，而且得到確定性的結果。我們認為隨機變量 x 的概率分佈 p<sub>θ</sub>(x) 是函數 f(x) 優化問題的一個較優的解，θ 是分佈 p<sub>θ</sub>(x) 的參數。目標是找到 θ 的最優設置。</p><blockquote><p>在給定固定分佈形式（例如，高斯分佈）的情況下，參數 θ 包含了最優解的知識，在一代與一代間進行迭代更新。</p></blockquote><p>假設初始值為 θ，我們可以通過循環進行下面的三個步驟持續更新 θ：</p><ul><li><p>1. 生成一個樣本的種群 D={(x<sub>i</sub>,f(x<sub>i</sub>)}，其中 x<sub>i</sub>∼p<sub>θ</sub>(x)。</p></li><li><p>2. 估計 D 中樣本的「適應度」。</p></li><li><p>3. 根據適應度或排序，選擇最優的個體子集，並使用它們來更新 θ。</p></li></ul><p>在遺傳算法（GA，另一種流行的演化算法子類）中，x 是二進制編碼的序列，其中 x∈{0,1}<sup>n</sup>。但是在演化策略中，x 僅僅是一個實數向量，x∈R<sup>n</sup>。</p><p></p><h2>二、簡單的高斯演化策略</h2><p>高斯演化策略是最基礎、最經典的演化策略（相關閱讀可參考：http://blog.otoro.net/2017/10/29/visual-evolution-strategies/）。它將 p<sub>θ</sub>(x) 建模為一個 n 維各向同性的高斯分佈，其中 θ 僅僅刻畫均值 μ 和標準差 σ。</p><p>給定 x∈R<sup>n</sup>，簡單的高斯演化策略的過程如下：</p><p>1. 初始化 θ=θ<sup>(0)</sup>以及演化代數計數器 t=0。</p><p>2. 通過從高斯分佈中採樣生成大小為 Λ 的後代種群：</p><p>其中，</p><p>3. 選擇出使得 f(x<sub>i</sub>) 最優的 λ 個樣本組成的子集，該子集被稱為「精英集」。為了不失一般性，我們可以考慮 D<sup>(t+1)</sup>中適應度排名靠前的 k 個樣本，將它們放入「精英集」。我們可以將其標註為：</p><p>4. 接著，我們使用「精英集」為下一代種群估計新的均值和標準差：</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RkOc1ojDCX921t><p>5. 重複步驟 2—步驟 4，直到結果滿足要求。</p><p></p><h2>三、協方差矩陣自適應演化策略（CMA-ES）</h2><p>標準差 σ 決定了探索的程度：當 σ 越大時，我們就可以在更大的搜索空間中對後代種群進行採樣。在簡單高斯演化策略中，σ<sup>(t+1)</sup>與 σ<sup>(t)</sup>密切相關，因此算法不能在需要時（即置信度改變時）迅速調整探索空間。</p><p>「協方差矩陣自適應演化策略」（CMA-ES）通過使用協方差矩陣 C 跟蹤分佈上得到的樣本兩兩之間的依賴關係，解決了這一問題。新的分佈參數變為了：</p><p>其中，σ 控制分佈的整體尺度，我們通常稱之為「步長」。</p><p>在我們深入研究 CMA-ES 中的參數更新方法前，不妨先回顧一下多元高斯分佈中協方差矩陣的工作原理。作為一個對稱陣，協方差矩陣 C 有下列良好的性質（詳見「Symmetric Matrices and Eigendecomposition」：http://s3.amazonaws.com/mitsloan-php/wp-faculty/sites/30/2016/12/15032137/Symmetric-Matrices-and-Eigendecomposition.pdf；以及證明：http://control.ucsd.edu/mauricio/courses/mae280a/lecture11.pdf）：</p><ul><li><p>C 始終是對角陣</p></li><li><p>C 始終是半正定矩陣</p></li><li><p>所有的特徵值都是非負實數</p></li><li><p>所有特徵值都是正交的</p></li><li><p>C 的特徵向量可以組成 Rn 的一個標準正交基</p></li></ul><p>令矩陣 C 有一個特徵向量 B=[b<sub>1</sub>,...,b<sub>n</sub>] 組成的標準正交基，相應的特徵值分別為 λ<sub>1</sub><sup>2</sup>,…,λ<sub>n</sub><sup>2</sup>。令 D=diag(λ<sub>1</sub>,…,λ<sub>n</sub>)。</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RkOc1pLCLr79LS><p>C 的平方根為：</p><p><strong>相關的符號和意義如下：</strong></p><blockquote><p>x<sub>i</sub><sup>(t)</sup>∈R<sup>n</sup>：第 t 代的第 i 個樣本</p><p>y<sub>i</sub><sup>(t)</sup>∈R<sup>n</sup>：x<sub>i</sub><sup>(t)</sup>=μ<sup>(t−1)</sup>+σ<sup>(t−1)</sup>y<sub>i</sub><sup>(t)</sup></p><p>μ<sup>(t)</sup>：第 t 代的均值</p><p>σ<sup>(t)</sup>：步長</p><p>C<sup>(t)</sup>：協方差矩陣</p><p>B<sup>(t)</sup>：將 C 的特徵向量作為行向量的矩陣</p><p>D<sup>(t)</sup>：對角線上的元素為 C 的特徵值的對角矩陣</p><p>p<sub>σ</sub><sup>(t)</sup>：第 t 代中用於 σ 的演化路徑</p><p>p<sub>c</sub><sup>(t)</sup>：第t 代中用於 C 的演化路徑</p><p>α<sub>μ</sub>：用於更新 μ 的學習率</p><p>α<sub>σ</sub>：p<sub>σ</sub>的學習率</p><p>d<sub>σ</sub>：σ 更新的衰減係數</p><p>Α<sub>cp</sub>：p<sub>c</sub>的學習率</p><p>α<sub>cλ</sub>：矩陣 C 的秩 min(λ, n) 更新的學習率</p><p>α<sub>c1</sub>：矩陣 C 的秩 1 更新的學習率</p></blockquote><p><strong>1、更新均值</strong></p><p>CMA-ES 使用 α<sub>μ</sub>≤1 的學習率控制均值 μ 更新的速度。通常情況下，該學習率被設置為 1，從而使上述等式與簡單高斯演化策略中的均值更新方法相同：</p><p><strong>2、控制步長</strong></p><p>採樣過程可以與均值和標準差的更新解耦：</p><p>參數 σ 控制著分佈的整體尺度。它是從協方差矩陣中分離出來的，所以我們可以比改變完整的協方差更快地改變步長。步長較大會導致參數更新較快。為了評估當前的步長是否合適，CMA-ES 通過將連續的移動步長序列相加</p><p>，構建了一個演化路徑（evolution path）p<sub>σ</sub>。通過比較該路徑與隨機選擇（意味著每一步之間是不相關的）狀態下期望會生成的路徑長度，我們可以相應地調整 σ（詳見圖 2）。</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RkOc1wy3ozKQ8p><p>圖 2：將每一步演化以不同的方式關聯起來的三種情況，以及它們對步長更新的影響。（左）每個步驟之間互相抵消，因此演化路徑很短。（中）理想情況：每個步驟之間並不相關。（右）每個步驟指向同一個方向，因此演化路徑較長。（圖片來源：CMA-ES 教程論文中圖 5 的附加註釋，https://arxiv.org/abs/1604.00772）</p><p>每次演化路徑都會以同代中的平均移動步長 y<sub>i </sub>進行更新。</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RkOc1xEDl58Eo8><blockquote><div><p>通過與 C<sup>-1/2 </sup>相乘，我們將演化路徑轉化為與其方向相獨立的形式。</p><p>的工作原理如下：</p></div><p>1. B<sup>(t)</sup>包含 C 的特徵向量的行向量。它將原始空間投影到了正交的主軸上。</p><div><p>2.</p><p>將各主軸的長度放縮到相等的狀態。</p></div><p>3. B<sup>(t)</sup><sup>⊤</sup>將空間轉換回原始座標系。</p></blockquote><p>為了給最近幾代的種群賦予更高的權重，我們使用了「Polyak平均 」算法（平均優化算法在參數空間訪問軌跡中的幾個點），以學習率 α<sub>σ</sub>更新演化路徑。同時，我們平衡了權重，從而使p<sub>σ</sub>在更新前和更新後都為服從 N(0,I) 的共軛分佈（更新前後的先驗分佈和後驗分佈類型相同）。</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RkOc24z2ex5l3T><p>隨機選擇得到的Pσ 的期望長度為 E‖N(0,I)‖，該值是服從 N(0,I) 的隨機變量的 L2 範數的數學期望。按照圖 2 中的思路，我們將根據 ‖p<sub>σ</sub><sup>(t+1)</sup>‖/E‖N(0,I)‖ 的比值調整步長：</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RkOc2568caCTrf><p>其中，d<sub>σ</sub>≈1 是一個衰減參數，用於放縮 ln<sup>σ</sup>被改變的速度。</p><p><strong>3、自適應協方差矩陣</strong></p><p>我們可以使用精英樣本的 y<sub>i</sub>從頭開始估計協方差矩陣（y<sub>i</sub>∼N(0,C)）</p><p>只有當我們選擇出的種群足夠大，上述估計才可靠。然而，在每一代中，我們確實希望使用較小的樣本種群進行快速的迭代。這就是 CMA-ES 發明了一種更加可靠，但同時也更加複雜的方式去更新 C 的原因。它包含兩種獨立的演化路徑：</p><ol><li><p>秩 min(λ, n) 更新：使用 {C<sub>λ</sub>} 中的歷史，在每一代中都是從頭開始估計的。</p></li><li><p>秩 1 更新：根據歷史估計移動步長 y<sub>i</sub>以及符號信息</p></li></ol><p>第一條路徑根據 {C<sub>λ</sub>} 的全部歷史考慮 C 的估計。例如，如果我們擁有很多代種群的經驗，</p><p>就是一種很好的估計方式。類似於 p<sub>σ</sub>，我們也可以使用「polyak」平均，並且通過學習率引入歷史信息：</p><p>通常我們選擇的學習率為：</p><p>第二條路徑試圖解決 y<sub>i</sub>y<sub>i</sub><sup>⊤</sup>=(−y<sub>i</sub>)(−y<sub>i</sub>)<sup>⊤</sup>丟失符號信息的問題。與我們調整步長 σ 的方法相類似，我們使用了一個演化路徑 p<sub>c</sub>來記錄符號信息，p<sub>c</sub>仍然是種群更新前後都服從於 N(0,C) 的共軛分佈。</p><p>我們可以認為 p<sub>c</sub>是另一種計算 avg<sub>i</sub>(y<sub>i</sub>) 的（請注意它們都服從於 N(0,C)），此時我們使用了完整的歷史信息，並且能夠保留符號信息。請注意，在上一節中，我們已經知道了</p><p>，p<sub>c</sub>的計算方法如下：</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RkOc2EnGat3Urr><p>接下來，我們可以根據通過 p<sub>c </sub>更新協方差矩陣：</p><p>當 k 較小時，秩 1 更新方法相較於秩 min(λ, n) 更新有很大的性能提升。這是因為我們在這裡利用了移動步長的符號信息和連續步驟之間的相關性，而且這些信息可以隨著種群的更新被一代一代傳遞下去。</p><p>最後，我們將兩種方法結合起來：</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RkOc2LOBeTfLKn><p>在上面所有的例子中，我們認為每個優秀的樣本對於權重的貢獻是相等的，都為 1/λ。該過程可以很容易地被擴展至根據具體表現為抽樣得到的樣本賦予不同權重 w<sub>1</sub>,…,w<sub>λ</sub>的情況。詳情請參閱教程：「The CMA Evolution Strategy: A Tutorial」（https://arxiv.org/abs/1604.00772）</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RkOc2Ld6uvRVz5><p>圖 3：CMA-ES 在二維優化問題上的工作原理示意圖（顏色越亮的部分性能越好）。黑點是當前代中的樣本。樣本在初始階段較分散，但當模型在後期較有信心找到較好的解時，樣本在全局最優上變得非常集中。樣本在初始階段較分散，但當模型在後期以更高的置信度找到較好的解時，樣本會集中於全局最優點。</p><p></p><h2>四、自然演化策略</h2><p>自然演化策略（Wierstra 等人於 2008 年發表的 NES，論文地址：https://arxiv.org/abs/1106.4487）在參數的搜索分佈上進行優化，並將分佈朝著自然梯度所指向的高適應度方向移動。</p><p><strong>1、自然梯度</strong></p><p>給定一個參數為 θ 的目標函數 J(θ)，我們的目標是找到最優的 θ，從而最大化目標函數的值。樸素梯度會以當前的 θ 為起點，在很小的一段歐氏距離內找到最「陡峭」的方向，同時我們會對參數空間施加一些距離的限制。換而言之，我們在 θ 的絕對值發生微小變化的情況下計算出樸素梯度。優化步驟如下：</p><p>不同的是，自然梯度用到了參數為 θ, p<sub>θ</sub>(x)（在 NES 的原始論文中被稱為「搜索分佈」，論文鏈接：https://arxiv.org/abs/1106.4487）的概率分佈空間。它在分佈空間中的一小步內尋找最「陡峭」（變化最快）的方向，其中距離由 KL 散度來度量。在這種限制條件下，我們保證了每一步更新都是沿著分佈的流形以恆定的速率移動，不會因為其曲率而減速。</p><p><strong>2、使用 Fisher 信息矩陣進行估計</strong></p><p>但是，如何精確地計算出 KL[p<sub>θ</sub>‖p<sub>θ+Δθ</sub>] 呢？通過推導 log<sub>pθ+d</sub>在 θ 處的泰勒展式，我們可以得到：</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RkOc2VpGVtv8gv><p>其中，</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RkOc2W74d6JSfz><p>請注意，p<sub>θ</sub>(x) 是概率分佈。最終，我們得到了：</p><p>其中，Fθ 被稱為 Fisher 信息矩陣。由於E[∇<sub>θ</sub>logp<sub>θ</sub>]=0，所以 Fθ 也是 ∇<sub>θ</sub>logp<sub>θ</sub>的協方差矩陣：</p><p>針對以下的優化問題：</p><p>我們可以通過拉格朗日乘子法找到上述問題的解：</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RkOc2fMIKxF7VR><p>其中 d<sub>N</sub><sup>∗</sup>僅僅提取了忽略標量 β<sup>−1</sup>的情況下，在 θ 上最優移動步長的方向。</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RkOc2ffGjfdA8d><p>圖 4：右側的自然梯度樣本（黑色實箭頭）是左側的樸素梯度樣本（黑色實箭頭）乘以其協方差的逆的結果。這樣一來，可以用較小的權重懲罰具有高不確定性的梯度方向（由與其它樣本的高協方差表示）。因此，合成的自然梯度（紅色虛箭頭）比原始的自然梯度（綠色虛箭頭）更加可信（圖片來源：NES 原始論文中圖 2 的附加說明，https://arxiv.org/abs/1106.4487）</p><p><strong>3、NES 算法</strong></p><p>我們將與一個樣本相關聯的適應度標記為 f(x)，關於 x 的搜索分佈的參數為 θ。我們希望 NES 能夠優化參數 θ，從而得到最大的期望適應度：</p><p>在這裡，我們使用與蒙特卡洛策略梯度強化中相同的似然計算技巧：</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RkOc2gJ8TZ8c2B><p>除了自然梯度，NES 還採用了一些重要的啟發式方法讓算法的性能更加魯棒。</p><ul><li><p>NES 應用了基於排序的適應度塑造（Rank-Based Fitness Shaping）算法，即使用適應度值單調遞增的排序結果，而不是直接使用 f(x)。它也可以是對「效用函數」進行排序的函數，我們將其視為 NES 的一個自由參數。</p></li><li><p>NES 採用了適應性採樣（Adaptation Sampling）在運行時調整超參數。當進行 θ→θ′ 的變換時，我們使用曼-惠特尼 U 檢驗（ [Mann-Whitney U-test）對比從分佈 p<sub>θ</sub>上採樣得到的樣本以及從分佈 p<sub>θ</sub>′ 上採樣得到的樣本。如果出現正或負符號，則目標超參數將減少或增加一個乘法常數。請注意，樣本 x<sub>i</sub>′∼p<sub>θ</sub>′(x) 的得分使用了重要性採樣權重 w<sub>i</sub>′=p<sub>θ</sub>(x)/p<sub>θ</sub>′(x)。</p></li></ul><p></p><h2>五、應用：深度強化學習中的演化策略</h2><p><strong>1、OpenAI 用於強化學習的演化策略</strong></p><p>將演化算法應用於強化學習的想法可以追溯到很久以前的論文「Evolutionary Algorithms for Reinforcement Learning」（論文地址：https://arxiv.org/abs/1106.0221），但是由於計算上的限制，這種嘗試僅僅止步於「表格式」強化學習（例如，Q-learning）。</p><p>受到 NES 的啟發，OpenAI 的研究人員（詳見 Salimans 等人於 2017 年發表的論文「Evolution Strategies as a Scalable Alternative to Reinforcement Learning」，論文鏈接：https://arxiv.org/abs/1703.03864）提出使用 NES 作為一種非梯度黑盒優化器，從而尋找能夠最大化返回函數 F(θ) 的最優策略參數 θ。</p><p>這裡的關鍵是，為模型參數 θ 加入高斯噪聲 ε，並使用似然技巧將其寫作高斯概率密度函數的梯度。最終，只剩下噪聲項作為衡量性能的加權標量。</p><p>假設當前的參數值為 θ^（區別於隨機變量 θ）。我們將 θ 的搜索分佈設計為一個各向同性的多元高斯分佈，其均值為 θ^，協方差矩陣為 σ<sup>2</sup>I</p><p>θ 更新的梯度為：</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RkOc2qZ7GLqWCz><p>其中，高斯分佈為</p><p>。</p><p>是我們用到的似然計算技巧。</p><p>在每一代中，我們可以採樣得到許多 ε<sub>i</sub>，i=1,…,n，然後並行地估計其適應度。一種優雅的設計方式是，無需共享大型模型參數。只需要在各個工作線程之間傳遞隨機種子，就足以事主線程節點進行參數更新。隨後，這種方法又被拓展成了以自適應的方試學習損失函數。詳情請查閱博文「Evolved Policy Gradient」：</p><ul><li><p>https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#meta-learning-the-loss-function</p></li></ul><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RkOc2rKABdhxIb><p>圖 5：使用演化策略訓練一個強化策略的算法（圖片來源：論文「ES-for-RL」，https://arxiv.org/abs/1703.03864）</p><p>為了使算法的性能更加魯棒，OpenAI ES 採用了虛擬批量歸一化（Virtual BN，用於計算固定統計量的 mini-batch 上的批量歸一化方法），鏡面採樣（Mirror Sampling，採樣一對 (−ϵ,ϵ) 用於估計），以及適應度塑造（Fitness Shaping）技巧。</p><p><strong>2、使用演化策略進行探索</strong></p><p>在強化學習領域，「探索與利用」是一個很重要的課題。上述演化策略中的優化方向僅僅是從累積返回函數 F(θ) 中提取到的。在不進行顯式探索的情況下，智能體可能會陷入局部最優點。</p><p>新穎性搜索（Novelty-Search）演化策略（NS-ES，詳見 Conti 等人於 2018 年發表的論文「Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents」，論文地址：https://arxiv.org/abs/1712.06560）通過朝著最大化「新穎性得分」的方向更新參數來促進探索。</p><p>「新穎性得分」取決於一個針對於特定領域的行為特徵函數 b(π<sub>θ</sub>)。對 b(π<sub>θ</sub>) 的選擇取決於特定的任務，並且似乎具有一定的隨機性。例如，在論文裡提到的人形機器人移動任務中，b(π<sub>θ</sub>) 是智能體最終的位置 (x,y)。</p><p>1. 將每個策略的 b(π<sub>θ</sub>) 加入一個存檔集合 A。</p><p>2. 通過 b(π<sub>θ</sub>) 和 A 中所有其它實體之間的 K 最近鄰得分衡量策略 π<sub>θ</sub>的新穎性。（文檔集合的用例與「情節記憶」很相似）</p><p>在這裡，演化策略優化步驟依賴於新穎性得分而不是適應度：</p><p>NS-ES 維護了一個由 M 個獨立訓練的智能體組成的集合（「元-種群」），M={θ<sub>1</sub>,…,θ<sub>M</sub>}。然後選擇其中的一個智能體，將其按照與新穎性得分成正比的程度演化。最終，我們選擇出最佳策略。這個過程相當於集成，在 SVPG 中也可以看到相同的思想。</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RkOc3Cr4szajlR><p>其中，N 是高斯擾動噪聲向量的數量，α 是學習率。</p><p>NS-ES 完全捨棄了獎勵函數，僅僅針對新穎性進行優化，從而避免陷入極具迷惑性的局部最優點。為了將適應度重新考慮到公式中，研究人員又提出了兩種變體。</p><p><strong>NSR-ES:</strong></p><p><strong>NSRAdapt-ES (NSRA-ES)</strong>：自適應的權重參數初始值為 w=1.0。如果算法的性能經過了很多代之後沒有變化，我們就開始降低 w。然後，當性能開始提升時，我們停止降低 w，反而增大 w。這樣一來，當性能停止提升時，模型更偏向於提升適應度，而不是新穎性。</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RkOc3DYBRsbZGm><p>圖 6：（左圖）環境為人形機器人移動問題，該機器人被困在一個三面環繞的強中，這是一個具有迷惑性的陷阱，創造了一個局部最優點。（右圖）實驗對比了 ES 基線和另一種促進探索的變體。（圖片來源，論文「NS-ES」，https://arxiv.org/abs/1712.06560）</p><p><strong>3、CEM-RL</strong></p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RkOc3Dx5mNjb3D><p>圖 7：CEM-RL 和 ERL 算法（https://papers.nips.cc/paper/7395-evolution-guided-policy-gradient-in-reinforcement-learning.pdf）的架構示意圖（圖片來源：論文「CEM-RL」，https://arxiv.org/abs/1810.01222）</p><p>CEM-RL 方法（詳見 Pourchot 和 Sigaud 等人於 2019 年發表的論文「CEM-RL: Combining evolutionary and gradient-based methods for policy search」，論文地址：https://arxiv.org/abs/1810.01222）結合了交叉熵方法（CEM）和 DDPG 或 TD3。</p><p>在這裡，CEM 的工作原理與上面介紹的簡單高斯演化策略基本相同，因此可以使用 CMA-ES 替換相同的函數。CEM-RL 是基於演化強化學習（ERL，詳見 Khadka 和 Tumer 等人於 2018 年發表的論文「Evolution-Guided Policy Gradient in Reinforcement Learning」，論文地址：https://papers.nips.cc/paper/7395-evolution-guided-policy-gradient-in-reinforcement-learning.pdf）的框架構建的，它使用標準的演化算法選擇並演化「Actor」的種群。隨後，在這個過程中生成的首次展示經驗也會被加入到經驗回放池中，用於訓練強化學習的「Actor」網絡和「Critic」網絡。</p><p>工作流程：</p><p>1. π<sub>μ </sub>為 CEM 種群的「Actor」平均值，使用隨機的「Actor」網絡對其進行初始化。</p><p>2. 「Critic」網絡 Q 也將被初始化，通過 DDPG/TD3 算法對其進行更新。</p><p>3. 重複以下步驟直到滿足要求：</p><ul><li><p>在分佈 N(π<sub>μ</sub>,Σ) 上採樣得到一個「Actor」的種群。</p></li><li><p>評估一半「Actor」的種群。將適應度得分用作累積獎勵 R，並將其加入到經驗回放池中。</p></li><li><p>將另一半「Actor」種群與「Critic」一同更新。</p></li><li><p>使用性能最佳的優秀樣本計算出新的 π<sub>mu</sub>和 Σ。也可以使用 CMA-ES 進行參數更新。</p></li></ul><p></p><h2>六、拓展：深度學習中的演化算法</h2><p>（本章並沒有直接討論演化策略，但仍然是非常有趣的相關閱讀材料。）</p><p>演化算法已經被應用於各種各樣的深度學習問題中。「POET」（詳見 Wang 等人於 2019 年發表的論文「Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions」，論文地址：https://arxiv.org/abs/1901.01753）就是一種基於演化算法的框架，它試圖在解決問題的同時生成各種各樣不同的任務。關於 POET 的詳細介紹請參閱下面這篇關於元強化學習的博文：https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html#task-generation-by-domain-randomization。</p><p>另一個例子則是演化強化學習（ERL），詳見圖 7（b）。</p><p>下面，我將更詳細地介紹兩個應用實例：基於種群的訓練（PBT），以及權重未知的神經網絡（WANN）</p><p><strong>1、超參數調優：PBT</strong></p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RkOc3eVDEKxuWw><p>圖 8：對比不同的超參數調優方式的範例（圖片來源：論文「Population Based Training of Neural Networks」，https://arxiv.org/abs/1711.09846）</p><p>基於種群的訓練（PBT，詳見 Jaderberg 等人於 2017 年發表的論文「Population Based Training of Neural Networks」，論文地址：https://arxiv.org/abs/1711.09846）將演化算法應用到了超參數調優問題中。它同時訓練了一個模型的種群以及相應的超參數，從而得到最優的性能。</p><p>PBT 過程起初擁有一組隨機的候選解，它們包含一對模型權重的初始值和超參數 {(θ<sub>i</sub>,h<sub>i</sub>)∣i=1,…,N}。我們會並行訓練每個樣本，然後週期性地異步評估其自身的性能。當一個成員準備好後（即該成員進行了足夠的梯度更新步驟，或當性能已經足夠好），就有機會通過與整個種群進行對比進行更新：</p><ul><li><p>「exploit」：當模型性能欠佳時，可以用性能更好的模型的權重來替代當前模型的權重。</p></li><li><p>「explore」：如果模型權重被重寫，「explore」步驟會使用隨機噪聲擾動超參數。</p></li></ul><p>在這個過程中，只有性能良好的模型和超參數對會存活下來，並繼續演化，從而更好地利用計算資源。</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RkOc3f32nWLXFa><p>圖 9：基於種群的訓練算法示意圖。（圖片來源，論文「Population Based Training of Neural Networks」，https://arxiv.org/abs/1711.09846）</p><p><strong>2、網絡拓撲優化：WANN</strong></p><p>權重未知的神經網絡（WANN，詳見 Gaier 和 Ha 等人於 2019 年發表的論文「Weight Agnostic Neural Networks」，論文地址：https://arxiv.org/abs/1906.04358）在不訓練網絡權重的情況下，通過搜索最小的網絡拓撲來獲得最優性能。</p><p>由於不需要考慮網絡權值的最佳配置，WANN 更加強調網絡架構本身，這使得它的重點與神經網絡架構搜索（NAS）不同。WANN 在演化網絡拓撲的時候深深地受到了一種經典的遺傳算法「NEAT」（增廣拓撲的神經演化，詳見 Stanley 和 Miikkulainen 等人於 2002 年發表的論文「Efficient Reinforcement Learning through Evolving Neural Network Topologies」，論文地址：http://nn.cs.utexas.edu/downloads/papers/stanley.gecco02_1.pdf）的啟發。</p><p>WANN 的工作流程看上去與標準的遺傳算法基本一致：</p><p>1. 初始化：創建一個最小網絡的種群。</p><p>2. 評估：使用共享的權重值進行測試。</p><p>3. 排序和選擇：根據性能和複雜度排序。</p><p>4. 變異：通過改變最佳的網路來創建新的種群。</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RkOc3fN3Y7RbDI><p>圖 10:：WANN 中用於搜索新網絡拓撲的變異操作。（從左到右分別為）最小網絡，嵌入節點，增加連接，改變激活值，節點的激活。</p><p>在「評估」階段，我們將所有網絡權重設置成相同的值。這樣一來，WANN 實際上是在尋找可以用最小描述長度來描述的網絡。在「選擇」階段，我們同時考慮網絡連接和模型性能。</p><img alt=在深度學習模型的優化上，梯度下降並非唯一的選擇 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RkOc3fWHVDypFF><p>圖 11：將 WANN 發現的網絡拓撲在不同強化學習任務上的性能與常用的基線 FF 網絡進行了比較。「對共享權重調優」只需要調整一個權重值。</p><p>如圖 11 所示，WANN 的結果是同時使用隨機權重和共享權重（單一權重）評估得到的。有趣的是，即使在對所有權重執行權重共享並對於這單個參數進行調優的時候，WANN 也可以發現實現非常出色的性能的拓撲。</p><p></p><h2>參考文獻</h2><p>[1] Nikolaus Hansen. “The CMA Evolution Strategy: A Tutorial” arXiv preprint arXiv:1604.00772 (2016).</p><p>[2] Marc Toussaint. Slides: “Introduction to Optimization”</p><p>[3] David Ha. “A Visual Guide to Evolution Strategies” blog.otoro.net. Oct 2017.</p><p>[4] Daan Wierstra, et al. “Natural evolution strategies.” IEEE World Congress on Computational Intelligence, 2008.</p><p>[5] Agustinus Kristiadi. “Natural Gradient Descent” Mar 2018.</p><p>[6] Razvan Pascanu & Yoshua Bengio. “Revisiting Natural Gradient for Deep Networks.” arXiv preprint arXiv:1301.3584 (2013).</p><p>[7] Tim Salimans, et al. “Evolution strategies as a scalable alternative to reinforcement learning.” arXiv preprint arXiv:1703.03864 (2017).</p><p>[8] Edoardo Conti, et al. “Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents.” NIPS. 2018.</p><p>[9] Aloïs Pourchot & Olivier Sigaud. “CEM-RL: Combining evolutionary and gradient-based methods for policy search.” ICLR 2019.</p><p>[10] Shauharda Khadka & Kagan Tumer. “Evolution-guided policy gradient in reinforcement learning.” NIPS 2018.</p><p>[11] Max Jaderberg, et al. “Population based training of neural networks.” arXiv preprint arXiv:1711.09846 (2017).</p><p>[12] Adam Gaier & David Ha. “Weight Agnostic Neural Networks.” arXiv preprint arXiv:1906.04358 (2019).</p><p><strong>via</strong>https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html雷鋒網 AI 科技評論編譯 雷鋒網</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>學習</a></li><li><a>優化</a></li><li><a>並非</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3d54fe0b.html alt="深度研究自然梯度優化，從入門到放棄 | Deep Reading" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2cc1b8ef47a5458190c22d26d8bd164c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3d54fe0b.html title="深度研究自然梯度優化，從入門到放棄 | Deep Reading">深度研究自然梯度優化，從入門到放棄 | Deep Reading</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html alt=直流鍋爐給水控制學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/eba10edcc8d14d9f8cde6fd5b212d90e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html title=直流鍋爐給水控制學習>直流鍋爐給水控制學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html alt=HTMLCSS學習筆記（六）——元素類型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb5988349894ce9bf568c6418f85b7d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html title=HTMLCSS學習筆記（六）——元素類型>HTMLCSS學習筆記（六）——元素類型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html alt="web前端（從零開始），每天更新學習筆記 HTML5元素分類" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/46d70004fcd55e1ddad3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html title="web前端（從零開始），每天更新學習筆記 HTML5元素分類">web前端（從零開始），每天更新學習筆記 HTML5元素分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html alt="MySQL 學習筆記" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html title="MySQL 學習筆記">MySQL 學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html alt=深入學習MySQL事務：ACID特性的實現原理「轉」 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/cdc702d66d6943499997d11e931425eb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html title=深入學習MySQL事務：ACID特性的實現原理「轉」>深入學習MySQL事務：ACID特性的實現原理「轉」</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html alt=如何學習模擬IC設計？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html title=如何學習模擬IC設計？>如何學習模擬IC設計？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html alt=小猿圈python學習-三大特性之多態 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ad0e8e3777854337abeb7c779ad79a04 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html title=小猿圈python學習-三大特性之多態>小猿圈python學習-三大特性之多態</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/03a295fc.html alt=地理學習5——地球的運動（地球的公轉及其地理意義） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7b2b74c871eb40beb8ee143627d29611 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/03a295fc.html title=地理學習5——地球的運動（地球的公轉及其地理意義）>地理學習5——地球的運動（地球的公轉及其地理意義）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ebad378f.html alt=繼續學習打卡，還真心學不會了，努力，堅持 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/f36d6d47a06840aaaf78138853b9d9d1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ebad378f.html title=繼續學習打卡，還真心學不會了，努力，堅持>繼續學習打卡，還真心學不會了，努力，堅持</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>