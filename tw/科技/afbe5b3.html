<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>深入淺出理解神經網絡 | 极客快訊</title><meta property="og:title" content="深入淺出理解神經網絡 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/5e70000151f2abbd0da2"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/afbe5b3.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/afbe5b3.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/afbe5b3.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/afbe5b3.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/afbe5b3.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/afbe5b3.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/afbe5b3.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/afbe5b3.html><meta property="article:published_time" content="2020-10-29T20:53:45+08:00"><meta property="article:modified_time" content="2020-10-29T20:53:45+08:00"><meta name=Keywords content><meta name=description content="深入淺出理解神經網絡"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/afbe5b3.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>深入淺出理解神經網絡</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p>更多騰訊海量技術文章，請關注雲+社區：https://cloud.tencent.com/developer</p><h1>介紹</h1><p>如今，科學家正在努力探索人腦的奧祕，他們試圖通過模仿人腦，來找到大數據的解決方案。</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/5e70000151f2abbd0da2></p><p>我感覺現在沒有深入淺出的、實用的介紹神經網絡(NN)的文章。我一直想弄清楚人腦是怎麼工作的，但我還有很多問題沒有答案，而且總是很難把握神經網絡工作的細節。這篇文章中，我想回答的最關鍵的問題是：</p><p>1. 人腦是如何工作的？</p><p>2. “感知器”如何充當人工神經元？——前向神經網絡</p><p>3. 什麼是神經網絡的權重？</p><p>4. 生物體內的神經元權重是多少？</p><p>5. 神經網絡中激勵函數起什麼作用？</p><p>6. 生物體內什麼東西起到了神經元激勵函數的功能？</p><p>7. 反向傳播如何工作？</p><p>8. 反向傳播神經網絡的確切的數學邏輯是什麼？</p><p>9. 如何實現反向傳播神經網絡？</p><h1>1.人腦是如何工作的？</h1><p>要理解神經網絡如何工作，最好先研究一下人腦的運作機理。人腦有約一千億個神經元，彼此之間緊密相連。當你看到一個動物——比如貓，其大小、顏色、形狀等特徵會從眼睛傳入大腦。然後，這些輸入的信息將被一些小細胞處理，這些細胞被稱作神經元，它們負責處理輸入大腦的數據。</p><p>首先，神經元會搜索你以前見過的貓的圖像，然後比較你記憶中的貓和新看到的貓的圖像。這種比對是“監督學習”的基礎，它讓你的大腦就像一個比較器一樣，這就是為什麼人們看見什麼東西都想要比較一番。經過比較後會得到結果：你看到的是一隻貓——因為你之前看到過貓。</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/5b5b00034ca9302dabf3></p><p>藉助眼睛，你的大腦可以每秒處理20 Mb的信息。這很神奇，因為這與你的學習能力、分辨某人聲音或者聽覺的能力如何沒有關係。所以，你的大腦是一個難以置信的巨大的CPU。</p><p>如果把這個龐大系統的規模簡化到很小，就可以解決今天不同領域的問題。比如說，用很小一部分的人腦的特徵就可以解決諸如語言、面部或圖像的識別問題，情感分析、觀點或情緒的理解問題，自動駕駛汽車，甚至疾病診斷等問題。</p><p>下圖就是神經元細胞，包括樹突、軸突和細胞核。從左到右看，樹突負責接收信息，細胞核負責處理數據，處理結果將從軸突傳遞到神經元的尾部。這一整個神經元的結構，在人工智能中被稱作“感知器”。</p><p>右側的另一個神經元細胞將受到某些化學物質的激勵，從而接收左側神經元的反應。這種激勵造成了數據在細胞間的傳遞、轉移。所以，左側神經元的輸出就是右側神經元的輸入。其他的神經單元也存在同樣的過程。就這樣，大腦一千億個神經元相互協作，以達成目標。</p><p>你每天都要做這麼多的信號處理來過好你的生活，而大腦做這些都毫不費力。一天結束時，你只需要7個小時來恢復這些小細胞。神經科學家發現，越是學習，你的樹突就會越強，因為激發神經元細胞之間的聯繫也是一種鍛鍊，就像鍛鍊肌肉一樣。因此，大腦用得越多，越不可能患上阿爾茨海默病。</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/5e70000151f1e3bf50a7></p><h1>2.“感知器”如何充當人工神經元？——前向神經網絡</h1><p>下圖是一個簡單的感知器模型，以及他獲得輸出結果的計算操作。首先有兩個輸入X1和X2，然後每個連接都有相應的權重。在神經網絡中，所有的處理單元都是節點。儘管計算機系統有複雜的處理單元，但在神經網絡中存在著簡單的處理單元。</p><p>我們先讓輸入(X)和權重(W)相乘，然後對所有結果求和，並把結果代入一個激勵函數，最終的結果就是這個感知器的輸出。</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/5e6f0001522c4eae9e30></p><p>上述過程稱作神經網絡的前向傳播。但實際上，在神經網絡學習中，我們用更多的節點和層數。我之前說過，在我們的大腦中有數十億層，這個龐大的系統造就了我們今天的人類。所以，為了學習得更好，我們把層數增多，這樣一般會提高學習、訓練的結果。下圖中有兩個隱含層，用淺藍色表示，每個節點都像上述的簡單感知器一樣工作。</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/5b5b00034ca722d68a96></p><h1>3.什麼是神經網絡的權重？</h1><p>權重是指節點之間的連接強度。無符號（沒有正負）的權重大小取決於節點之間的連接有多強。</p><p>而權重可以是正值或負值，正值代表傳遞數據的可能性更大、神經元之間有強大的聯繫，反之亦然。一開始，我們隨機選擇權重，但為了讓結果更合理，最好將輸入數據進行如下的歸一化，其中X是輸入數據：</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/5b5a000371ef13d8e2c7></p><p>因為本文中的激勵函數是累加型的，所以有一個簡便方法來隨機地選擇權重值：把權重限制在一個特定的區間內——如下面的公式。這個公式取決於平均分佈，其內在邏輯在此暫且不表，下一篇文章中我們會展開來說。</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/5e6f0001565c69bb9485></p><p>以上公式請參考：Deep Learning Tutorials (深度學習教程); Glorot and Bengio (2010)</p><p>我提到說，神經網絡是高度互連的，而權重是實現這種互連的最重要的因素。我們在第一階段中隨機選擇權重值。首先，從左到右地完成前向傳播。</p><p>然後比較一下結果和真實值的誤差有多大，真實值是訓練數據集中的“Y”值。然後進行反向傳播計算——即在相反的路徑上進行計算。</p><p>前向傳播是從左到右，而反向傳播是從右到左，以此優化並獲得新的權重，使下一次的輸出值被優化。如果下一次的輸出值與“Y”的誤差小於之前輸出值的誤差，則表明我們的優化方式是正確的。</p><p>可見，權重是將節點彼此連接起來的一種工具，也是訓練神經網絡減少錯誤的一個因素。通過多次測量反向傳播、前向傳播並進行權重校準，可以獲得新的權重和準確的輸出值，從而降低誤差。</p><p>為了更好地理解神經網絡中的權重扮演的角色，我請您閱讀我關於“機器學習和梯度下降”的文章。神經網絡中的權重基本接近Y值的預測線中的斜率“a”，Y=aX+b。準確的“a”值可以幫助我們找到更好的預測線來對數據進行分類。這裡神經網絡的權重也是一個類似“a”的因素，我們努力找出它的精確值，以求更準確的分類。</p><h1>4. 生物體內的神經元權重是多少？</h1><p>所有的神經網絡結構都是從人腦中得到靈感啟發的。因此，加權的無符號值表示：神經元之間的樹突連接數+樹突之間的突觸數+突觸的前、後末端+神經元之間間隙的形狀+連接強度；還有髓鞘的狀態也很重要。</p><p>髓鞘是在神經元細胞軸突周圍的白色的脂肪物質，它像保護膜一樣保護著它們。下圖中，信號在有髓鞘的右側神經元的比在沒有髓鞘的左側神經元中傳播得快得多。這種現象是由跳躍傳導產生的。</p><p>頻繁、快速的傳播會形成更多、更強的突觸，這對大腦學習能力起著重要的作用。對大腦更強大的人進行的功能性磁共振成像（FMRI）的結果表明，他們的突觸和腦活化區域更多。</p><p>因此，神經網絡中的權重與生物中的上述因素的組合是一樣的。</p><p>https://en.wikipedia.org/wiki/Myelin</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/5e7000015200697db45c></p><h1>5.神經網絡中激勵函數起什麼作用？</h1><p>激勵函數（雖然只在一定程度上）起到“極化”和“穩定化”的效果。我想舉例說明一下“極化”和數學上的“穩定化”。為方便計算，我們需要對數據做一點極化處理，尤其是對十進制數。例如，我們有1.298456，我們只需要一個數字作為小數。</p><p>為了四捨五入、極化、計算簡捷，我們將1.298456化為1.3——因為小數點後2的下一個數字是9，9大於5，所以我們將2轉換為3。這種情況下，四捨五入可以有更簡潔的值和結果。</p><p>在神經網絡中，我們希望更精確地分類和預測，而非線性函數圖像有更多的弧度、彎曲。如下圖，相比線性和非線性函數，顯然非線性函數預測的精度更高，並且有能更好的區分兩類不同事物的分界線。</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/5b5a00036e4eb6783101></p><p>我在梯度下降中使用了“誤差平方和（SSE）”。神經網絡的激勵函數應該是像指數或正切一類的非線性函數。而因為在反向傳播中，我們要找到全局最小點，所以函數也必須是可微的。實際上，正是反向傳播造成了梯度下降。詳情請閱讀關於梯度下降的文章：https://www.codeproject.com/Articles/1196024/Machine-Learning-Gradient-Descent。</p><p>誤差平方和(SSE)= ½Σ(Y真實值-Y預測值)²</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/5b5c0002ee54d16ab82f></p><p>SSE代表Y的預測值和真實值之間的誤差。因此，為了獲得最佳預測線而不是直接用上圖的藍線，我們要對SSE求微分，然後計算出該線的新的斜率。</p><p>您可以從下列函數中選一個作為激勵函數，詳情請參考https://en.wikipedia.org/wiki/Activation_function。</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/5b5a0003737db6bef0b4></p><h1>6.生物體內什麼東西起到了神經元激勵函數的功能？<br></h1><p>神經網絡中的激勵函數也稱作傳遞函數，其根據自己的輸入給出該節點的輸出。激勵函數在生物學中被稱作動作電位，其與信號在軸突中傳播方式有關。</p><p>化學物質引起電信號激勵，並刺激神經元，然後刺激其軸突，以便在神經元上單向傳輸信號。它輔助神經元產生輸出結果。詳情請參考https://en.wikipedia.org/wiki/Action_potential。</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/5e70000151f870d7f223></p><h1>7.反向傳播如何工作？</h1><p>為了串聯起以上所有的概念，我想把反向傳播工作流程分成以下幾個步驟來說明：</p><ul class=list-paddingleft-2><li><p>一開始有一組訓練數據，數據中既有X也有Y，X有一列或多列，而Y作為一個要考慮的參量。這樣，輸入和輸出層單元的數量就確定了。</p></li><li><p>接著我們需要選擇隱藏層的層數。隱藏層的層數代表了學習的深度，層數越多，模仿人腦就越像，結果也就越準確。但主要問題是隱藏層越多，計算量越大，內存消耗也就越大，反向傳播時尤其如此。</p></li><li><p>隱藏層確定後，我們需要正態隨機選擇的權重值，例如在本文第三部分我解釋過一個公式。</p></li><li><p>正向傳播得到輸出結果：</p></li></ul><p>每一層都有節點，我假設它們輸出有兩種值：一種是不套用Sigmoid函數的“Inputsigma”或“hiddensigma”，另一種是套用了Sigmoid函數的“hiddennode”和“outputnode”。然後我們從左到右計算，進行前向傳播。</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/5e7000015929a97a7523></p><ul class=list-paddingleft-2><li><p>反向傳播優化權重：</p></li></ul><p>在反向傳播中，因為我們要找到最優權重值，所以我們對Sigmoid函數求微分，然後從右向左反向計算，以找到新的權重值。</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/5b5a000375ccacc110cd></p><p>(10) 如果當前誤差小於之前的誤差，則說明我們優化的方向正確，就用新得到的權重值重複執行1-5步。</p><p>(11) 重複步驟1到10，直到結果接近“Y”。</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/5e7100021d0a21b7594c></p><h1>8.反向傳播神經網絡的確切的數學邏輯是什麼？</h1><p>反向傳播和梯度下降作用相同，我們需要對激勵函數求微分。它的計算過程如下：</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/5b5c0002ee58650cf7a1></p><p>XOR（異或運算）是測試我們神經網絡的最簡單的樣例。XOR的真值表如下，它有兩個操作數和一個結果：</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/5e70000151fa4450a108></p><p>例如我想要實現第四行操作：(1,1)=0</p><p>Y是0，輸出是0.68，所以邊際誤差是-0.68。下一個輸出是0.57，邊際誤差小於0.68。</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/5b5900037e574ea3629b></p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/5e7100021d0f90d4c112></p><h1>9.如何實現反向傳播神經網絡？</h1><p><strong>執行文件：</strong></p><p>基於上述知識，我要在Matlab中實現神經網絡。首先，我創建了“execution.m”文件用來調用我的預測函數。</p><pre>%% 機器學習 - 神經網絡 - 簡要例程%% 初始化clear ; close all; clcinput_node = [1 1]; %1*2% 通過正態分佈獲得權重Weight_1 = [ -0.5 1.01 0.23 ; -0.32 -0.24 -0.12 ]; %2*3Weight_2 = [ 0.15 1.32 -0.37 ]; %1*3pred = mypredict(Weight_1, Weight_2, input_node);fprintf('反向傳播最終輸出: %f\n', perd);</pre><p><strong>預測文件：</strong></p><p>然後，我寫了“myprerdict.m”，這是最主要的代碼：</p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/5e7100022745aeaffb63></p><p><strong>Sigmoid 函數：</strong></p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/5b5c0002f9a4cb2a6ea3></p><p><strong>Sigmoid 派生函數：</strong></p><p><img alt=深入淺出理解神經網絡 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/5b5900038a106fc2e3b3></p><h1>亮點</h1><p>我發現神經網絡非常令人激動，我認為我們可以把它稱作人工智能之母。</p><p>神經網絡的最大優缺點是：</p><ol class=list-paddingleft-2><li><p>歸一化的數據集和選擇最優解可以使我們在海量的訓練數據中得到更精確的輸出；</p></li><li><p>系統性能和準確度取決於權重，如果你的權重在合適的區間中，那就可以得到性能和準確性的提高；</p></li><li><p>相比其他的方式，反向傳播會消耗更多的內存。</p></li></ol><p>最後，我強烈建議您在coursera.org上加入機器學習：https://www.coursera.org/learn/machine-learning</p><p>另外，我的github可以用作學習指導：</p><p>https://github.com/Hassankashi?tab=repositories</p><h1>反饋</h1><p>歡迎對本文留言，期待看到您對此代碼的意見和支持。如果您有任何問題，請直接在這裡問我。</p><blockquote><p>翻譯人：元卅，該成員來自雲+社區翻譯社</p><p>原文譯題：神經網絡</p><p>原文鏈接：https://www.codeproject.com/Articles/1200392/Neural-Network</p><p>原文作者：Mahsa Hassankashi</p></blockquote><h1>神經網絡例程下載</h1><p>https://ask.qcloudimg.com/draft/1050107/d8zgt7ubrt.zip</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>淺出</a></li><li><a>神經</a></li><li><a>網絡</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html alt=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/65c4000bda98898dcdbb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html title=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼>谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2bc1496a.html alt=為了更好的深度神經網絡視覺，只需添加反饋（循環） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/17fccfd7096d44eeb3921bbd0dc29a13 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2bc1496a.html title=為了更好的深度神經網絡視覺，只需添加反饋（循環）>為了更好的深度神經網絡視覺，只需添加反饋（循環）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fd4c22a3.html alt=你還不知道神經網絡是啥？十分鐘教你跟上人工智能熱潮 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/470f0001d893b2ad09e2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fd4c22a3.html title=你還不知道神經網絡是啥？十分鐘教你跟上人工智能熱潮>你還不知道神經網絡是啥？十分鐘教你跟上人工智能熱潮</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5d2a6211.html alt=神經網絡與圖靈機的複雜度博弈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/4af200040ff1f5233c1c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5d2a6211.html title=神經網絡與圖靈機的複雜度博弈>神經網絡與圖靈機的複雜度博弈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cc9d1be9.html alt=基於二維材料、用於人工神經網絡的高密度憶阻陣列的晶圓級集成 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/3c0b503678da4b15be05f6f56c0d213f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cc9d1be9.html title=基於二維材料、用於人工神經網絡的高密度憶阻陣列的晶圓級集成>基於二維材料、用於人工神經網絡的高密度憶阻陣列的晶圓級集成</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/6062a4c0.html alt=BP神經網絡的線性本質的理解和剖析-卷積小白的隨機世界 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/6d474536ff3d4b1fba0cbfc85968ff6f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/6062a4c0.html title=BP神經網絡的線性本質的理解和剖析-卷積小白的隨機世界>BP神經網絡的線性本質的理解和剖析-卷積小白的隨機世界</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/f149efd9.html alt=用於調整深度神經網絡的簡單參考指南 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/15379529924702cde52ac04 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/f149efd9.html title=用於調整深度神經網絡的簡單參考指南>用於調整深度神經網絡的簡單參考指南</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0396dba3.html alt=貝葉斯神經網絡(系列)：第二篇 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RKYlnth9DPo8ac style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0396dba3.html title=貝葉斯神經網絡(系列)：第二篇>貝葉斯神經網絡(系列)：第二篇</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a4bbdd29.html alt=針對深度神經網絡的簡單黑盒對抗攻擊 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/b9ec712cd33442338496141ebfcecb45 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a4bbdd29.html title=針對深度神經網絡的簡單黑盒對抗攻擊>針對深度神經網絡的簡單黑盒對抗攻擊</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cafcc06.html alt=模式識別與神經網絡的發展 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1523254283784d3d276a90f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cafcc06.html title=模式識別與神經網絡的發展>模式識別與神經網絡的發展</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fcf9e89.html alt=BP神經網絡學習筆記 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/fc5cec456c184c48b1ee22a233b9ee0b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fcf9e89.html title=BP神經網絡學習筆記>BP神經網絡學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d7196c1.html alt=手工打造神經網絡：透視分析 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6ee200033390f3f6b2ca style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d7196c1.html title=手工打造神經網絡：透視分析>手工打造神經網絡：透視分析</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9f3924a.html alt=機器學習：神經網絡學習之多層前饋神經網絡（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a0a4cd0f7d9244a6a12da3c0af6893a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9f3924a.html title=機器學習：神經網絡學習之多層前饋神經網絡（一）>機器學習：神經網絡學習之多層前饋神經網絡（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/556321d.html alt=機器學習：神經網絡學習之多層前饋神經網絡（二） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/2d53a815-ab09-4da3-94a2-5b6843366e3a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/556321d.html title=機器學習：神經網絡學習之多層前饋神經網絡（二）>機器學習：神經網絡學習之多層前饋神經網絡（二）</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/f3732f4.html alt=一文幫你梳理清楚深度神經網絡的基礎知識！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2f16bcb220e14085a04994454ea4998a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/f3732f4.html title=一文幫你梳理清楚深度神經網絡的基礎知識！>一文幫你梳理清楚深度神經網絡的基礎知識！</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>