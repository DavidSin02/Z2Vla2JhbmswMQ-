<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>「ML」深入理解CatBoost | 极客快訊</title><meta property="og:title" content="「ML」深入理解CatBoost - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/f7e6e06fe4c243baa728efbdb3621da6"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b3820d4.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b3820d4.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/b3820d4.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b3820d4.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b3820d4.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/b3820d4.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/b3820d4.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b3820d4.html><meta property="article:published_time" content="2020-10-29T21:01:06+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:06+08:00"><meta name=Keywords content><meta name=description content="「ML」深入理解CatBoost"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/b3820d4.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>「ML」深入理解CatBoost</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><blockquote class=pgc-blockquote-abstract><p>文章來源 | Microstrong</p><p>作者 | Microstrong</p></blockquote><p><strong>本文主要內容概覽：</strong></p><div class=pgc-img><img alt=「ML」深入理解CatBoost onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f7e6e06fe4c243baa728efbdb3621da6><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>1. CatBoost簡介</strong></h1><p>CatBoost是俄羅斯的搜索巨頭Yandex在2017年開源的機器學習庫，是Boosting族算法的一種。CatBoost和XGBoost、LightGBM並稱為GBDT的三大主流神器，都是在GBDT算法框架下的一種改進實現。XGBoost被廣泛的應用於工業界，LightGBM有效的提升了GBDT的計算效率，而Yandex的CatBoost號稱是比XGBoost和LightGBM在算法準確率等方面表現更為優秀的算法。</p><p>CatBoost是一種基於對稱決策樹（oblivious trees）為基學習器實現的參數較少、支持類別型變量和高準確性的GBDT框架，主要解決的痛點是高效合理地處理類別型特徵，這一點從它的名字中可以看出來，CatBoost是由Categorical和Boosting組成。此外，CatBoost還解決了梯度偏差（Gradient Bias）以及預測偏移（Prediction shift）的問題，從而減少過擬合的發生，進而提高算法的準確性和泛化能力。</p><p>與XGBoost、LightGBM相比，CatBoost的創新點有：</p><ul class=list-paddingleft-2><li>嵌入了自動將類別型特徵處理為數值型特徵的創新算法。首先對categorical features做一些統計，計算某個類別特徵（category）出現的頻率，之後加上超參數，生成新的數值型特徵（numerical features）。</li><li>Catboost還使用了組合類別特徵，可以利用到特徵之間的聯繫，這極大的豐富了特徵維度。</li><li>採用排序提升的方法對抗訓練集中的噪聲點，從而避免梯度估計的偏差，進而解決預測偏移的問題。</li><li>採用了完全對稱樹作為基模型。</li></ul><h1 class=pgc-h-arrow-right><strong>2. 類別型特徵</strong></h1><h2 class=pgc-h-arrow-right><strong>2.1 類別型特徵的相關工作</strong></h2><p>所謂類別型特徵，即這類特徵不是數值型特徵，而是離散的集合，比如省份名（山東、山西、河北等），城市名（北京、上海、深圳等），學歷（本科、碩士、博士等）。在梯度提升算法中，最常用的是將這些類別型特徵轉為數值型來處理，一般類別型特徵會轉化為一個或多個數值型特徵。</p><p>如果某個<strong>類別型特徵基數比較低（low-cardinality features）</strong>，即該特徵的所有值去重後構成的集合元素個數比較少，一般利用One-hot編碼方法將特徵轉為數值型。One-hot編碼可以在數據預處理時完成，也可以在模型訓練的時候完成，從訓練時間的角度，後一種方法的實現更為高效，CatBoost對於基數較低的類別型特徵也是採用後一種實現。</p><p>顯然，在<strong>高基數類別型特徵（high cardinality features）</strong> 當中，比如 user ID，這種編碼方式會產生大量新的特徵，造成維度災難。一種折中的辦法是可以將類別分組成有限個的群體再進行One-hot編碼。一種常被使用的方法是根據目標變量統計（Target Statistics，以下簡稱TS）進行分組，目標變量統計用於估算每個類別的目標變量期望值。甚至有人直接用TS作為一個新的數值型變量來代替原來的類別型變量。重要的是，可以通過對TS數值型特徵的閾值設置，基於對數損失、基尼係數或者均方差，得到一個對於訓練集而言將類別一分為二的所有可能劃分當中最優的那個。在LightGBM當中，類別型特徵用每一步梯度提升時的梯度統計（Gradient Statistics，以下簡稱GS）來表示。雖然為建樹提供了重要的信息，但是這種方法有以下兩個缺點：</p><ul class=list-paddingleft-2><li>增加計算時間，因為需要對每一個類別型特徵，在迭代的每一步，都需要對GS進行計算；</li><li>增加存儲需求，對於一個類別型變量，需要存儲每一次分離每個節點的類別；</li></ul><p>為了克服這些缺點，LightGBM以損失部分信息為代價將所有的長尾類別歸為一類，作者聲稱這樣處理高基數類別型特徵時比One-hot編碼還是好不少。不過如果採用TS特徵，那麼對於每個類別只需要計算和存儲一個數字。</p><p>因此，採用TS作為一個新的數值型特徵是最有效、信息損失最小的處理類別型特徵的方法。TS也被廣泛應用在點擊預測任務當中，這個場景當中的類別型特徵有用戶、地區、廣告、廣告發布者等。接下來我們著重討論TS，暫時將One-hot編碼和GS放一邊。</p><h2 class=pgc-h-arrow-right><strong>2.2 目標變量統計（Target Statistics）</strong></h2><p>CatBoost算法的設計初衷是為了更好的處理GBDT特徵中的categorical features。在處理 GBDT特徵中的categorical features的時候，最簡單的方法是用 categorical feature 對應的標籤的平均值來替換。在決策樹中，標籤平均值將作為節點分裂的標準。這種方法被稱為 Greedy Target-based Statistics , 簡稱 Greedy TS，用公式來表達就是：</p><p>這種方法有一個顯而易見的缺陷，就是通常特徵比標籤包含更多的信息，如果強行用標籤的平均值來表示特徵的話，當訓練數據集和測試數據集數據結構和分佈不一樣的時候會出條件偏移問題。</p><p>一個標準的改進 Greedy TS的方式是添加先驗分佈項，這樣可以減少噪聲和低頻率類別型數據對於數據分佈的影響：</p><p>其中 是添加的先驗項， 通常是大於 的權重係數。添加先驗項是一個普遍做法，針對類別數較少的特徵，它可以減少噪聲數據。對於迴歸問題，一般情況下，先驗項可取數據集label的均值。對於二分類，先驗項是正例的先驗概率。利用多個數據集排列也是有效的，但是，如果直接計算可能導致過擬合。CatBoost利用了一個比較新穎的計算葉子節點值的方法，這種方式（oblivious trees，對稱樹）可以避免多個數據集排列中直接計算會出現過擬合的問題。</p><p>當然，在論文《CatBoost: unbiased boosting with categorical features》中，還提到了其它幾種改進Greedy TS的方法，分別有：Holdout TS、Leave-one-out TS、Ordered TS。我這裡就不再翻譯論文中的這些方法了，感興趣的同學可以自己翻看一下原論文。</p><h2 class=pgc-h-arrow-right><strong>2.3 特徵組合</strong></h2><p>值得注意的是幾個類別型特徵的任意組合都可視為新的特徵。例如，在音樂推薦應用中，我們有兩個類別型特徵：用戶ID和音樂流派。如果有些用戶更喜歡搖滾樂，將用戶ID和音樂流派轉換為數字特徵時，根據上述這些信息就會丟失。結合這兩個特徵就可以解決這個問題，並且可以得到一個新的強大的特徵。然而，組合的數量會隨著數據集中類別型特徵的數量成指數增長，因此不可能在算法中考慮所有組合。為當前樹構造新的分割點時，CatBoost會採用貪婪的策略考慮組合。對於樹的第一次分割，不考慮任何組合。對於下一個分割，CatBoost將當前樹的所有組合、類別型特徵與數據集中的所有類別型特徵相結合，並將新的組合類別型特徵動態地轉換為數值型特徵。CatBoost還通過以下方式生成數值型特徵和類別型特徵的組合：樹中選定的所有分割點都被視為具有兩個值的類別型特徵，並像類別型特徵一樣被進行組合考慮。</p><h2 class=pgc-h-arrow-right><strong>2.4 CatBoost處理Categorical features總結</strong></h2><ul class=list-paddingleft-2><li>首先會計算一些數據的statistics。計算某個category出現的頻率，加上超參數，生成新的numerical features。這一策略要求同一標籤數據不能排列在一起（即先全是之後全是這種方式），訓練之前需要打亂數據集。</li><li>第二，使用數據的不同排列（實際上是個）。在每一輪建立樹之前，先扔一輪骰子，決定使用哪個排列來生成樹。</li><li>第三，考慮使用categorical features的不同組合。例如顏色和種類組合起來，可以構成類似於blue dog這樣的特徵。當需要組合的categorical features變多時，CatBoost只考慮一部分combinations。在選擇第一個節點時，只考慮選擇一個特徵，例如A。在生成第二個節點時，考慮A和任意一個categorical feature的組合，選擇其中最好的。就這樣使用貪心算法生成combinations。</li><li>第四，除非向gender這種維數很小的情況，不建議自己生成One-hot編碼向量，最好交給算法來處理。</li></ul><div class=pgc-img><img alt=「ML」深入理解CatBoost onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/01c638fb8d394ad19898857cabb4e769><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>3. 克服梯度偏差</strong></h1><p>對於學習CatBoost克服梯度偏差的內容，我提出了三個問題：</p><ul class=list-paddingleft-2><li>為什麼會有梯度偏差？</li><li>梯度偏差造成了什麼問題？</li><li>如何解決梯度偏差？</li></ul><p>CatBoost和所有標準梯度提升算法一樣，都是通過構建新樹來擬合當前模型的梯度。然而，所有經典的提升算法都存在由有偏的點態梯度估計引起的過擬合問題。在每個步驟中使用的梯度都使用當前模型中的相同的數據點來估計，這導致估計梯度在特徵空間的任何域中的分佈與該域中梯度的真實分佈相比發生了偏移，從而導致過擬合。為了解決這個問題，CatBoost對經典的梯度提升算法進行了一些改進，簡要介紹如下。</p><p>許多利用GBDT技術的算法（例如，XGBoost、LightGBM），構建下一棵樹分為兩個階段：選擇樹結構和在樹結構固定後計算葉子節點的值。為了選擇最佳的樹結構，算法通過枚舉不同的分割，用這些分割構建樹，對得到的葉子節點計算值，然後對得到的樹計算評分，最後選擇最佳的分割。兩個階段葉子節點的值都是被當做梯度或牛頓步長的近似值來計算。在CatBoost中，第一階段採用梯度步長的無偏估計，第二階段使用傳統的GBDT方案執行。既然原來的梯度估計是有偏的，那麼怎麼能改成無偏估計呢？</p><p>設 為構建 棵樹後的模型， 為構建 棵樹後第 個訓練樣本上面的梯度值。為了使得 無偏於模型 ，我們需要在沒有 參與的情況下對模型 進行訓練。由於我們需要對所有訓練樣本計算無偏的梯度估計，乍看起來對於 的訓練不能使用任何樣本，貌似無法實現的樣子。我們運用下面這個技巧來處理這個問題：對於每一個樣本 ，我們訓練一個單獨的模型 ，且該模型從不使用基於該樣本的梯度估計進行更新。我們使用 來估計 上的梯度，並使用這個估計對結果樹進行評分。用偽碼描述如下，其中 是需要優化的損失函數， 是標籤值， 是公式計算值。</p><div class=pgc-img><img alt=「ML」深入理解CatBoost onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0203084c5bc947a391676127244afc5b><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right><strong>4. 預測偏移和排序提升</strong></h1><h2 class=pgc-h-arrow-right><strong>4.1 預測偏移</strong></h2><p>對於學習預測偏移的內容，我提出了兩個問題：</p><ul class=list-paddingleft-2><li>什麼是預測偏移？</li><li>用什麼辦法解決預測偏移問題？</li></ul><p>預測偏移（Prediction shift）是由梯度偏差造成的。在GDBT的每一步迭代中, 損失函數使用相同的數據集求得當前模型的梯度, 然後訓練得到基學習器, 但這會導致梯度估計偏差, 進而導致模型產生過擬合的問題。CatBoost通過採用排序提升 （Ordered boosting） 的方式替換傳統算法中梯度估計方法，進而減輕梯度估計的偏差，提高模型的泛化能力。下面我們對預測偏移進行詳細的描述和分析。</p><p><strong>首先來看下GBDT的整體迭代過程：</strong></p><p>GBDT算法是通過一組分類器的串行迭代，最終得到一個強學習器，以此來進行更高精度的分類。它使用了前向分佈算法，弱學習器使用分類迴歸樹（CART）。</p><p>假設前一輪迭代得到的強學習器是 , 損失函數是 ，則本輪迭代的目的是找到一個CART迴歸樹模型的弱學習器 ，讓本輪的損失函數最小。式（1）表示的是本輪迭代的目標函數 。</p><p>GBDT使用損失函數的負梯度來擬合每一輪的損失的近似值，式（2）中 表示的是上述梯度。</p><p>通常用式（3）近似擬合 。</p><p>最終得到本輪的強學習器，如式（4）所示：</p><p><strong>在這個過程當中，偏移是這樣發生的：</strong></p><p>根據 進行隨機計算的條件分佈 與測試集的分佈 發生偏移，這樣由公式（3）定義的基學習器 與公式（1）定義的產生偏差，最後影響模型 的泛化能力。</p><h2 class=pgc-h-arrow-right><strong>4.2 排序提升</strong></h2><p>為了克服預測偏移問題，CatBoost提出了一種新的叫做Ordered boosting的算法。</p><div class=pgc-img><img alt=「ML」深入理解CatBoost onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4188f92c6590467192ed81038c109573><p class=pgc-img-caption></p></div><p>由上圖的Ordered boosting算法可知，為了得到無偏梯度估計, CatBoost對每一個樣本 都會訓練一個單獨的模型 ，模型 由使用不包含樣本的訓練集訓練得到。我們使用 來得到關於樣本的梯度估計，並使用該梯度來訓練基學習器並得到最終的模型。</p><p>Ordered boosting算法好是好，但是在大部分的實際任務當中都不具備使用價值，因為需要訓練 個不同的模型，大大增加的內存消耗和時間複雜度。在CatBoost當中，我們以決策樹為基學習器的梯度提升算法的基礎上，對該算法進行了改進。</p><p>前面提到過，在傳統的GBDT框架當中，構建下一棵樹分為兩個階段：選擇樹結構和在樹結構固定後計算葉子節點的值。CatBoost主要在第一階段進行優化。在建樹的階段，CatBoost有兩種提升模式，Ordered和Plain。Plain模式是採用內建的ordered TS對類別型特徵進行轉化後的標準GBDT算法。Ordered則是對Ordered boosting算法的優化。兩種提升模式的具體介紹可以翻看論文《CatBoost: unbiased boosting with categorical features》。</p><h1 class=pgc-h-arrow-right><strong>5. 快速評分</strong></h1><p>CatBoost使用對稱樹（oblivious trees）作為基預測器。在這類樹中，相同的分割準則在樹的整個一層上使用。這種樹是平衡的，不太容易過擬合。梯度提升對稱樹被成功地用於各種學習任務中。在對稱樹中，每個葉子節點的索引可以被編碼為長度等於樹深度的二進制向量。這在CatBoost模型評估器中得到了廣泛的應用：我們首先將所有浮點特徵、統計信息和獨熱編碼特徵進行二值化，然後使用二進制特徵來計算模型預測值。</p><h1 class=pgc-h-arrow-right><strong>6. 基於GPU實現快速訓練</strong></h1><ul class=list-paddingleft-2><li><strong>密集的數值特徵。</strong> 對於任何GBDT算法而言，最大的難點之一就是搜索最佳分割。尤其是對於密集的數值特徵數據集來說，該步驟是建立決策樹時的主要計算負擔。CatBoost使用oblivious 決策樹作為基模型，並將特徵離散化到固定數量的箱子中以減少內存使用。就GPU內存使用而言，CatBoost至少與LightGBM一樣有效。主要改進之處就是利用了一種不依賴於原子操作的直方圖計算方法。</li><li><strong>類別型特徵。</strong> CatBoost實現了多種處理類別型特徵的方法，並使用完美哈希來存儲類別型特徵的值，以減少內存使用。由於GPU內存的限制，在CPU RAM中存儲按位壓縮的完美哈希，以及要求的數據流、重疊計算和內存等操作。通過哈希來分組觀察。在每個組中，我們需要計算一些統計量的前綴和。該統計量的計算使用分段掃描GPU圖元實現。</li><li><strong>多GPU支持。</strong> CatBoost中的GPU實現可支持多個GPU。分佈式樹學習可以通過數據或特徵進行並行化。CatBoost採用多個學習數據集排列的計算方案，在訓練期間計算類別型特徵的統計數據。</li></ul><h1 class=pgc-h-arrow-right><strong>7. CatBoost的優缺點</strong></h1><h2 class=pgc-h-arrow-right><strong>7.1 優點</strong></h2><ul class=list-paddingleft-2><li><strong>性能卓越：</strong> 在性能方面可以匹敵任何先進的機器學習算法；</li><li><strong>魯棒性/強健性：</strong> 它減少了對很多超參數調優的需求，並降低了過度擬合的機會，這也使得模型變得更加具有通用性；</li><li><strong>易於使用：</strong> 提供與scikit集成的Python接口，以及R和命令行界面；</li><li><strong>實用：</strong> 可以處理類別型、數值型特徵；</li><li><strong>可擴展：</strong> 支持自定義損失函數；</li></ul><h2 class=pgc-h-arrow-right><strong>7.2 缺點</strong></h2><ul class=list-paddingleft-2><li>對於類別型特徵的處理需要大量的內存和時間；</li><li>不同隨機數的設定對於模型預測結果有一定的影響；</li></ul><h1 class=pgc-h-arrow-right><strong>8. CatBoost實例</strong></h1><p>本篇文章所有數據集和代碼均在我的GitHub中，地址：https://github.com/Microstrong0305/WeChat-zhihu-csdnblog-code/tree/master/Ensemble%20Learning/CatBoost</p><h2 class=pgc-h-arrow-right><strong>8.1 安裝CatBoost依賴包</strong></h2><ul class="code-snippet__line-index code-snippet__js"><li><br></li></ul><pre><code>pip install catboost</code></pre><h2 class=pgc-h-arrow-right><strong>8.2 CatBoost分類</strong></h2><h3 class=pgc-h-arrow-right><strong>（1）數據集</strong></h3><p>這裡我使用了 2015 年航班延誤的 Kaggle 數據集，其中同時包含類別型變量和數值變量。這個數據集中一共有約 500 萬條記錄，我使用了 1% 的數據：5 萬行記錄。數據集官方地址：https://www.kaggle.com/usdot/flight-delays#flights.csv 。以下是建模使用的特徵：</p><ul class=list-paddingleft-2><li><strong>月、日、星期：</strong> 整型數據</li><li><strong>航線或航班號：</strong> 整型數據</li><li><strong>出發、到達機場：</strong> 數值數據</li><li><strong>出發時間：</strong> 浮點數據</li><li><strong>距離和飛行時間：</strong> 浮點數據</li><li><strong>到達延誤情況：</strong> 這個特徵作為預測目標，並轉為二值變量：航班是否延誤超過 10 分鐘</li></ul><p><strong>實驗說明：</strong> 在對 CatBoost 調參時，很難對類別型特徵賦予指標。因此，同時給出了不傳遞類別型特徵時的調參結果，並評估了兩個模型：一個包含類別型特徵，另一個不包含。如果未在cat_features參數中傳遞任何內容，CatBoost會將所有列視為數值變量。注意，如果某一列數據中包含字符串值，CatBoost 算法就會拋出錯誤。另外，帶有默認值的 int 型變量也會默認被當成數值數據處理。在 CatBoost 中，必須對變量進行聲明，才可以讓算法將其作為類別型變量處理。</p><h3 class=pgc-h-arrow-right><strong>（2）不加Categorical features選項的代碼</strong></h3><pre><code>import pandas as pd, numpy as npfrom sklearn.model_selection import train_test_split, GridSearchCVfrom sklearn import metricsimport catboost as cb# 一共有約 500 萬條記錄，我使用了 1% 的數據：5 萬行記錄# data = pd.read_csv("flight-delays/flights.csv")# data = data.sample(frac=0.1, random_state=10)  # 500-&gt;50# data = data.sample(frac=0.1, random_state=10)  # 50-&gt;5# data.to_csv("flight-delays/min_flights.csv")# 讀取 5 萬行記錄data = pd.read_csv("flight-delays/min_flights.csv")print(data.shape)  # (58191, 31)data = data[["MONTH", "DAY", "DAY_OF_WEEK", "AIRLINE", "FLIGHT_NUMBER", "DESTINATION_AIRPORT",             "ORIGIN_AIRPORT", "AIR_TIME", "DEPARTURE_TIME", "DISTANCE", "ARRIVAL_DELAY"]]data.dropna(inplace=True)data["ARRIVAL_DELAY"] = (data["ARRIVAL_DELAY"] &gt; 10) * 1cols = ["AIRLINE", "FLIGHT_NUMBER", "DESTINATION_AIRPORT", "ORIGIN_AIRPORT"]for item in cols:    data[item] = data[item].astype("category").cat.codes + 1train, test, y_train, y_test = train_test_split(data.drop(["ARRIVAL_DELAY"], axis=1), data["ARRIVAL_DELAY"],                                                random_state=10, test_size=0.25)cat_features_index = [0, 1, 2, 3, 4, 5, 6]def auc(m, train, test):    return (metrics.roc_auc_score(y_train, m.predict_proba(train)[:, 1]),            metrics.roc_auc_score(y_test, m.predict_proba(test)[:, 1]))# 調參，用網格搜索調出最優參數params = {'depth': [4, 7, 10],          'learning_rate': [0.03, 0.1, 0.15],          'l2_leaf_reg': [1, 4, 9],          'iterations': [300, 500]}cb = cb.CatBoostClassifier()cb_model = GridSearchCV(cb, params, scoring="roc_auc", cv=3)cb_model.fit(train, y_train)# 查看最佳分數print(cb_model.best_score_)  # 0.7088001891107445# 查看最佳參數print(cb_model.best_params_)  # {'depth': 4, 'iterations': 500, 'l2_leaf_reg': 9, 'learning_rate': 0.15}# With Categorical features，用最優參數擬合數據clf = cb.CatBoostClassifier(eval_metric="AUC", depth=4, iterations=500, l2_leaf_reg=9,                            learning_rate=0.15)clf.fit(train, y_train)print(auc(clf, train, test))  # (0.7809684655761157, 0.7104617034553192)</code></pre><h3 class=pgc-h-arrow-right><strong>（3）有Categorical features選項的代碼</strong></h3><pre><code>import pandas as pd, numpy as npfrom sklearn.model_selection import train_test_split, GridSearchCVfrom sklearn import metricsimport catboost as cb# 讀取 5 萬行記錄data = pd.read_csv("flight-delays/min_flights.csv")print(data.shape)  # (58191, 31)data = data[["MONTH", "DAY", "DAY_OF_WEEK", "AIRLINE", "FLIGHT_NUMBER", "DESTINATION_AIRPORT",             "ORIGIN_AIRPORT", "AIR_TIME", "DEPARTURE_TIME", "DISTANCE", "ARRIVAL_DELAY"]]data.dropna(inplace=True)data["ARRIVAL_DELAY"] = (data["ARRIVAL_DELAY"] &gt; 10) * 1cols = ["AIRLINE", "FLIGHT_NUMBER", "DESTINATION_AIRPORT", "ORIGIN_AIRPORT"]for item in cols:    data[item] = data[item].astype("category").cat.codes + 1train, test, y_train, y_test = train_test_split(data.drop(["ARRIVAL_DELAY"], axis=1), data["ARRIVAL_DELAY"],                                                random_state=10, test_size=0.25)cat_features_index = [0, 1, 2, 3, 4, 5, 6]def auc(m, train, test):    return (metrics.roc_auc_score(y_train, m.predict_proba(train)[:, 1]),            metrics.roc_auc_score(y_test, m.predict_proba(test)[:, 1]))# With Categorical featuresclf = cb.CatBoostClassifier(eval_metric="AUC", one_hot_max_size=31, depth=4, iterations=500, l2_leaf_reg=9,                            learning_rate=0.15)clf.fit(train, y_train, cat_features=cat_features_index)print(auc(clf, train, test))  # (0.7817912095285117, 0.7152541135019913)</code></pre><h2 class=pgc-h-arrow-right><strong>8.3 CatBoost迴歸</strong></h2><pre><code>from catboost import CatBoostRegressor# Initialize datatrain_data = [[1, 4, 5, 6],              [4, 5, 6, 7],              [30, 40, 50, 60]]eval_data = [[2, 4, 6, 8],             [1, 4, 50, 60]]train_labels = [10, 20, 30]# Initialize CatBoostRegressormodel = CatBoostRegressor(iterations=2,                          learning_rate=1,                          depth=2)# Fit modelmodel.fit(train_data, train_labels)# Get predictionspreds = model.predict(eval_data)print(preds)</code></pre><h1 class=pgc-h-arrow-right><strong>9. 關於CatBoost若干問題思考</strong></h1><h2 class=pgc-h-arrow-right><strong>9.1 CatBoost與XGBoost、LightGBM的聯繫與區別？</strong></h2><p>（1）2014年3月XGBoost算法首次被陳天奇提出，但是直到2016年才逐漸著名。2017年1月微軟發佈LightGBM第一個穩定版本。2017年4月Yandex開源CatBoost。自從XGBoost被提出之後，很多文章都在對其進行各種改進，CatBoost和LightGBM就是其中的兩種。</p><p>（2）CatBoost處理類別型特徵十分靈活，可直接傳入類別型特徵的列標識，模型會自動將其使用One-hot編碼，還可通過設置 one_hot_max_size參數來限制One-hot特徵向量的長度。如果不傳入類別型特徵的列標識，那麼CatBoost會把所有列視為數值特徵。對於One-hot編碼超過設定的one_hot_max_size值的特徵來說，CatBoost將會使用一種高效的encoding方法，與mean encoding類似，但是會降低過擬合。處理過程如下：</p><ul class=list-paddingleft-2><li>將輸入樣本集隨機排序，並生成多組隨機排列的情況；</li><li>將浮點型或屬性值標記轉化為整數；</li><li>將所有的類別型特徵值結果都根據以下公式，轉化為數值結果；</li></ul><p>其中 countInClass 表示在當前類別型特徵值中有多少樣本的標記值是；prior 是分子的初始值，根據初始參數確定。totalCount 是在所有樣本中（包含當前樣本）和當前樣本具有相同的類別型特徵值的樣本數量。</p><p>LighGBM 和 CatBoost 類似，也可以通過使用特徵名稱的輸入來處理類別型特徵數據，它沒有對數據進行獨熱編碼，因此速度比獨熱編碼快得多。LighGBM 使用了一個特殊的算法來確定屬性特徵的分割值。</p><ul class="code-snippet__line-index code-snippet__js"><li><br></li></ul><pre><code>train_data = lgb.Dataset(data, label=label, feature_name=['c1', 'c2', 'c3'], categorical_feature=['c3'])</code></pre><p>注意，在建立適用於 LighGBM 的數據集之前，需要將類別型特徵變量轉化為整型變量，此算法不允許將字符串數據傳給類別型變量參數。</p><p>XGBoost 和 CatBoost、 LighGBM 算法不同，XGBoost 本身無法處理類別型特徵，而是像隨機森林一樣，只接受數值數據。因此在將類別型特徵數據傳入 XGBoost 之前，必須通過各種編碼方式：例如，序號編碼、獨熱編碼和二進制編碼等對數據進行處理。</p><h1 class=pgc-h-arrow-right><strong>10. Reference</strong></h1><p><strong>CatBoost論文解讀：</strong></p><p>【1】Prokhorenkova L, Gusev G, Vorobev A, et al. CatBoost: unbiased boosting with categorical features[C]//Advances in Neural Information Processing Systems. 2018: 6638-6648.<br>【2】Dorogush A V, Ershov V, Gulin A. CatBoost: gradient boosting with categorical features support[J]. arXiv preprint arXiv:1810.11363, 2018.<br>【3】機器學習算法之Catboost，地址：https://www.biaodianfu.com/catboost.html<br>【4】oblivious tree在機器學習中有什麼用？- 李大貓的回答 - 知乎 https://www.zhihu.com/question/311641149/answer/593286799<br>【5】CatBoost算法梳理，地址：http://datacruiser.io/2019/08/19/DataWhale-Workout-No-8-CatBoost-Summary/</p><p><strong>CatBoost算法講解：</strong></p><p>【6】catboost完全指南，地址：https://zhuanlan.zhihu.com/p/102570430<br>【7】CatBoost原理及實踐 - Dukey的文章 - 知乎 https://zhuanlan.zhihu.com/p/37916954<br>【8】 22(7).模型融合---CatBoost，地址：https://www.cnblogs.com/nxf-rabbit75/p/10923549.html#auto_id_0<br>【9】catboost對類別特徵處理的簡單總結，地址：https://blog.csdn.net/weixin_42944192/article/details/102463796<br>【10】Python3機器學習實踐：集成學習之CatBoost - AnFany的文章 - 知乎 https://zhuanlan.zhihu.com/p/53591724</p><p><strong>CatBoost實戰：</strong></p><p>【11】Catboost 一個超級簡單實用的boost算法，地址：https://www.jianshu.com/p/49ab87122562<br>【12】苗豐順, 李巖, 高岑, 王美吉, 李冬梅. 基於CatBoost算法的糖尿病預測方法. 計算機系統應用, 2019, 28(9): 215-218.http://www.c-s-a.org.cn/1003-3254/7054.html<br>【13】Battle of the Boosting Algos: LGB, XGB, Catboost，地址：https://www.kaggle.com/lavanyashukla01/battle-of-the-boosting-algos-lgb-xgb-catboost<br>【14】Simple CatBoost，地址：https://www.kaggle.com/nicapotato/simple-catboost<br>【15】CatBoost Usage examples，地址：https://catboost.ai/docs/concepts/python-usages-examples.html</p><p><strong>CatBoost的若干思考：</strong></p><p>【16】入門 | 從結構到性能，一文概述XGBoost、Light GBM和CatBoost的同與不同，地址：https://mp.weixin.qq.com/s/TD3RbdDidCrcL45oWpxNmw</p><p>The End</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>ML</a></li><li><a>CatBoost</a></li><li><a>深入</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/99e43ab.html alt=「ML」一文詳盡系列之CatBoost class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/f2a5caed2d82451b8d9e0b0f1135f42f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/99e43ab.html title=「ML」一文詳盡系列之CatBoost>「ML」一文詳盡系列之CatBoost</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/33348acb.html alt=《深入精通Mysql（三）》深入底層Mysql各種鎖機制（面試必問） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/470f515e8fe44c6184b07227fdb51333 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/33348acb.html title=《深入精通Mysql（三）》深入底層Mysql各種鎖機制（面試必問）>《深入精通Mysql（三）》深入底層Mysql各種鎖機制（面試必問）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0068a92b.html alt=各地稅務機關深入開展“不忘初心、牢記使命”主題教育：以永遠在路上的執著把全面從嚴治黨引向深入 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/88ec2ac9c2404737b532a570fdf887b5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0068a92b.html title=各地稅務機關深入開展“不忘初心、牢記使命”主題教育：以永遠在路上的執著把全面從嚴治黨引向深入>各地稅務機關深入開展“不忘初心、牢記使命”主題教育：以永遠在路上的執著把全面從嚴治黨引向深入</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f7a9ec23.html alt=音響人，深入瞭解一下話筒吧 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/a77fff56-22cb-4b6e-a8da-4ef1005fcfc7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f7a9ec23.html title=音響人，深入瞭解一下話筒吧>音響人，深入瞭解一下話筒吧</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/753585cd.html alt=超讚的母差保護深入解讀，附口訣！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/f8fe8384104a45d293f6622287f34684 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/753585cd.html title=超讚的母差保護深入解讀，附口訣！>超讚的母差保護深入解讀，附口訣！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b64e226d.html alt=AI和ML在網絡安全中的用例 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2ecb2e5d786743a688e69abfd136a5a0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b64e226d.html title=AI和ML在網絡安全中的用例>AI和ML在網絡安全中的用例</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4ea6119e.html alt=深入理解C語言的指針 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4e2601d6d7b04dc2b8107b4555ef6910 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4ea6119e.html title=深入理解C語言的指針>深入理解C語言的指針</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/1ebabf70.html alt=ML基礎：協方差矩陣 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a8ebdba18ae5461a8e462d5fcce85ee4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/1ebabf70.html title=ML基礎：協方差矩陣>ML基礎：協方差矩陣</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/ebed9a9e.html alt=為ML帶來拓撲學基礎，Nature子刊提出拓撲數據分析方法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/3c8b122558234a6fa193f00f54ae1b1f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/ebed9a9e.html title=為ML帶來拓撲學基礎，Nature子刊提出拓撲數據分析方法>為ML帶來拓撲學基礎，Nature子刊提出拓撲數據分析方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/dcbf547f.html alt=深入理解支持向量機 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a4485e2df65444c4a76999521943821c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/dcbf547f.html title=深入理解支持向量機>深入理解支持向量機</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/903fad48.html alt=深入瞭解差動放大器 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/20db318d62ac4664992bfb076d548a5e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/903fad48.html title=深入瞭解差動放大器>深入瞭解差動放大器</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/05f1045.html alt=怎樣深入理解堆和棧 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/eff2f829a30a4e9c9cf29a1cee71b6a2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/05f1045.html title=怎樣深入理解堆和棧>怎樣深入理解堆和棧</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/059e7a6.html alt="吳恩達 ML Yearning 關於學習曲線的分析&與人類級別的表現對比" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/f6a60d9d13f84286a71f27b8edb150f1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/059e7a6.html title="吳恩達 ML Yearning 關於學習曲線的分析&與人類級別的表現對比">吳恩達 ML Yearning 關於學習曲線的分析&與人類級別的表現對比</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/23e2014.html alt="吳恩達《ML Yearning》｜基礎的誤差分析& 偏差、方差分析" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/6f70b8b7cd0f4dad93aced78cc14ffcf style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/23e2014.html title="吳恩達《ML Yearning》｜基礎的誤差分析& 偏差、方差分析">吳恩達《ML Yearning》｜基礎的誤差分析& 偏差、方差分析</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/22f5e9d.html alt=一文讀懂ML中的解析解與數值解 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/15250924930865b0bf65466 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/22f5e9d.html title=一文讀懂ML中的解析解與數值解>一文讀懂ML中的解析解與數值解</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>