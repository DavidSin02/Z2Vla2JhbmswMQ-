<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>賽爾筆記 | 自然語言處理中的遷移學習(下) | 极客快訊</title><meta property="og:title" content="賽爾筆記 | 自然語言處理中的遷移學習(下) - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RfXcGOlHDeeb4s"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b4547cff.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b4547cff.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/b4547cff.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b4547cff.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b4547cff.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/b4547cff.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/b4547cff.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/b4547cff.html><meta property="article:published_time" content="2020-11-14T21:01:58+08:00"><meta property="article:modified_time" content="2020-11-14T21:01:58+08:00"><meta name=Keywords content><meta name=description content="賽爾筆記 | 自然語言處理中的遷移學習(下)"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/b4547cff.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>賽爾筆記 | 自然語言處理中的遷移學習(下)</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>作者：哈工大SCIR 徐嘯</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcGOlHDeeb4s><p><strong>本文小結：</strong>本文為教程的第二篇，包含教程的 3-6 部分。</p><p><strong>相關鏈接：</strong>賽爾筆記 | 自然語言處理中的遷移學習(上)</p><p></p><h1 toutiao-origin=h2>提綱</h1><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfRw76fAlAwY3S><ol><li><p>介紹：本節將介紹本教程的主題：遷移學習當前在自然語言處理中的應用。在不同的遷移學習領域中，我們主要定位於順序遷移學習 sequential transfer learning 。</p></li><li><p>預訓練：我們將討論無監督、監督和遠程監督的預訓練方法。</p></li><li><p>表示捕獲了什麼：在討論如何在下游任務中使用預訓練的表示之前，我們將討論分析表示的方法，以及觀察到它們捕獲了哪些內容。</p></li><li><p>調整：在這個部分，我們將介紹幾種調整這些表示的方法，包括特徵提取和微調。我們將討論諸如學習率安排、架構修改等的實際考慮。</p></li><li><p>下游應用程序：本節，我們將重點介紹預訓練的表示是如何被用在不同的下游任務中的，例如文本分類、自然語言生成、結構化預測等等。</p></li><li><p>開放問題和方向：在最後一節中，我們將提出對未來的展望。我們將突出待解決的問題以及未來的研究方向。</p></li></ol><p></p><h1 toutiao-origin=h2>3. 表示捕獲了什麼</h1><p><strong>為什麼要關心表示捕獲了什麼？</strong></p><p>Swayamdipta, 2019</p><ul><li><p>在下游任務進行的外部評估</p></li><ul><li><p>複雜多樣，隨特定任務而不同</p></li></ul><li><p>Language-aware representations 語言感知表示</p></li><ul><li><p>泛化到其他任務的新的輸入</p></li><li><p>作為可能改進預訓練工作的中間步驟</p></li></ul><li><p>可解釋！</p></li><ul><li><p>我們得到結果的原因是否正確?</p></li><li><p>發現偏見……</p></li></ul></ul><p><strong>分析什麼？</strong></p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcGP08Fo1zqO><ul><li><p>嵌入</p></li><ul><li><p>單詞</p></li><li><p>上下文的</p></li></ul><li><p>網絡激活</p></li><li><p>變化</p></li><ul><li><p>結構 (RNN / Transformer)</p></li><li><p>層</p></li><li><p>預訓練目標</p></li></ul></ul><p><strong>分析方法 1：</strong><strong>可視化</strong></p><p>保持嵌入/網絡激活靜態或凍結</p><p><strong>可視化嵌入</strong></p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcGPK7mQkpd4><ul><li><p>在低維(2D/3D)空間內繪製嵌入</p></li><ul><li><p>t-SNE (van der Maaten & Hinton, 2008)</p></li><li><p>PCA projections</p></li></ul></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcGPZ77VJa0n><p>可視化單詞類比 (Mikolov et al. 2013)</p><p>空間關聯</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcGrRDX8ZB69><p>詞彙語義的高級視圖</p><p>只有有限的例子</p><p>與其他任務的連接尚不清楚 (Goldberg, 2017)</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcGrg1nRoaqK><p>Radford et al., 2017</p><ul><li><p>神經元激活值與特徵/標籤相關</p></li></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcGrr3RHC9e8><p>Karpathy et al., 2016</p><ul><li><p>標識學習可識別的功能</p></li><ul><li><p>如何選擇某個神經元？難以擴展！</p></li><li><p>可解釋 != 重要(Morcos et al., 2018)</p></li></ul></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcGs3JAmLgR2><ul><li><p>流行於機器翻譯，或其他seq2seq架構:</p></li><ul><li><p>源字與目標字之間的對齊。</p></li><li><p>長距離詞與詞之間的依賴(句內注意)</p></li></ul><li><p>結構上的亮點</p></li><ul><li><p>擁有複雜的注意力機制可能是一件好事!</p></li><li><p>分層的</p></li></ul><li><p>解釋可能很棘手</p></li><ul><li><p>只有幾個例子？</p></li><li><p>Robust corpus-wide trends? Next !</p></li></ul></ul><p>Attention is not explanation | Attention is not not explanation</p><p><strong>分析方法 2: 行為探測器</strong></p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcGsLGL8Nj0J><ul><li><p>RNN-based 語言模型</p></li><ul><li><p>主謂關係中的數字一致性 number agreement in subject-verb dependencies</p></li><li><p>自然的、不自然的或不合語法的句子</p></li><li><p>對輸出困惑度進行評估</p></li></ul><li><p>RNNs優於其他非神經方法的 Baseline</p></li></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcHOV3YvkBvg><ul><li><p>當顯式地使用語法訓練時，性能會提高(Kuncoro et al. 2018)</p></li></ul><p>Linzen et al., 2016; Gulordava et al. 2018; Marvin et al., 2018</p><ul><li><p>這種 probe 可能易受共現偏差的影響</p></li><ul><li><p>“dogs in the neighborhood bark(s)”</p></li><li><p>以前的句子可能和原來的太不一樣了…</p></li></ul></ul><p><strong>分析方法 3: Classifier Probes</strong></p><p>保持嵌入/網絡激活並在頂部訓練一個簡單的監督模型</p><p><strong>探測表層特徵</strong></p><ul><li><p>給定一個句子，預測屬性如</p></li><ul><li><p>長度</p></li><li><p>這個句子裡有一個單詞嗎？</p></li></ul><li><p>給出句子中的單詞的預測屬性，例如：</p></li><ul><li><p>以前見過的詞，與語言模型形成對比</p></li><li><p>詞在句子中的位置</p></li></ul><li><p>檢查記憶的能力</p></li><ul><li><p>訓練有素的、更豐富的體系結構往往運行得更好</p></li><li><p>在語言數據上訓練能記憶的更好</p></li></ul></ul><p>Zhang et al. 2018; Liu et al., 2018; Conneau et al., 2018</p><p><strong>探測詞法，句法，語義</strong></p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcHOuBdg8irN><ul><li><p>詞法學</p></li><li><p>詞級別的語法</p></li><ul><li><p>POS tags, CCG supertags</p></li><li><p>Constituent parent, grandparent</p></li></ul><li><p>部分語法</p></li><ul><li><p>依賴關係</p></li></ul><li><p>部分語義</p></li><ul><li><p>實體關係</p></li><li><p>共指</p></li><li><p>角色</p></li></ul></ul><p>Adi et al., 2017; Conneau et al., 2018; Belinkov et al., 2017; Zhang et al., 2018; Blevins et al., 2018; Tenney et al. 2019; Liu et al., 2019</p><p><strong>探測分類結果</strong></p><ul><li><p>Contextualized > non-contextualized</p></li><ul><li><p>尤其是在句法任務上</p></li><li><p>更緊密的語義任務表現</p></li><li><p>雙向上下文很重要</p></li></ul><li><p>BERT (large) 幾乎總是獲得最佳效果</p></li><ul><li><p>Grain of salt: 不同的上下文表示在不同的數據上訓練，使用不同的架構……</p></li></ul></ul><p><strong>探測網絡各層</strong></p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXcHP7IBigaU6><p>Fig. from Liu et al. (NAACL 2019)</p><p>李如 對該篇文章做了簡潔的總結</p><ul><li><p>CWRs（上下文詞表徵）編碼了語言的哪些特徵？</p></li><ul><li><p>在各類任務中，BERT>ELMo>GPT，發現“bidirectional”是這類上下文編碼器的必備要素</p></li><li><p>相比於其他任務，編碼器們在NER和糾錯任務表現較差 => 沒有捕獲到這方面信息</p></li><li><p>在獲得CWRs編碼後，再針對任務增加MLP(relu)或者LSTM會提升效果</p></li><li><p>引出了問題：什麼時候直接fine-tune編碼器？什麼時候freeze編碼器，增加task-specific layer？</p></li></ul><li><p>編碼器中不同層的遷移性是怎樣變化的？</p></li><ul><li><p>對於ELMo(LSTM)來說，靠前的層更 transferable，靠後的層更 task-specific</p></li><li><p>對於 Transformer 來說，靠中間的層更 transferable ，但是把各個層加權起來的效果會更好</p></li><li><p>模型需要進行 trade off ，在任務上表現越好，遷移性越差</p></li></ul><li><p>預訓練任務會對任務和遷移性有怎樣的影響？</p></li><ul><li><p>雙向語言模型預訓練出來平均效果越好</p></li><li><p>預訓練任務越接近特定任務，在特定任務的表現越好</p></li><li><p>預訓練數據越多，表現越好</p></li></ul></ul><p>以上引用其總結的三點並稍作修改</p><ul><li><p>RNN 的各層：通用語言屬性</p></li><ul><li><p>最低層：形態學</p></li><li><p>中間層：語法</p></li><li><p>最高層次：特定於任務的語義</p></li></ul><li><p>Transformer 的各層</p></li><ul><li><p>不同任務的不同趨勢；mi<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-2">dd</i>le-heavy</p></li><li><p>參見Tenney et. al., 2019</p></li></ul></ul><p><strong>探測預訓練目標</strong></p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcHPiCjSapJm><p>Zhang et al., 2018; Blevins et al., 2018; Liu et al., 2019;</p><ul><li><p>語言建模優於其他非監督和監督目標。</p></li><ul><li><p>機器翻譯</p></li><li><p>依存分析</p></li><li><p>Skip-thought 預測上下文的句子</p></li></ul><li><p>低資源時(訓練數據的大小)可能導致相反的趨勢。</p></li></ul><p><strong>迄今為止我們學到了什麼？</strong></p><ul><li><p>表徵是對某些語言現象的預測:</p></li><ul><li><p>翻譯中的對齊，句法層次結構</p></li></ul><li><p>有語法和沒有語法的預訓練:</p></li><ul><li><p>有語法的預訓練具有更好的性能</p></li><li><p>但是如果沒有語法，至少還是會學到些語法概念 (Williams et al. 2018)</p></li></ul><li><p>網絡架構決定了表示中的內容</p></li><ul><li><p>句法與Bert Transformer (Tenney et al., 2019; Goldberg, 2019)</p></li><li><p>跨架構的不同的逐層趨勢</p></li></ul></ul><p><strong>關於探測器的開放問題</strong></p><ul><li><p>一個好的探測器應該尋找什麼信息?</p></li><ul><li><p>Probing a probe！</p></li></ul><li><p>探測性能告訴我們什麼？</p></li><ul><li><p>很難綜合各種基線的結果…</p></li></ul><li><p>它本身會帶來一些複雜性嗎</p></li><ul><li><p>線性或非線性分類</p></li><li><p>行為：輸入句子的設計</p></li></ul><li><p>我們應該使用 probe 作為評估指標嗎?</p></li><ul><li><p>可能會破壞目的…</p></li></ul></ul><p><strong>分析方法 4：</strong><strong>改變模型</strong></p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcHQ045Mdqk8><p>Li et al., 2016</p><ul><li><p>逐步刪除或屏蔽網絡組件</p></li><ul><li><p>詞嵌入維度</p></li><li><p>隱藏單位</p></li><li><p>輸入——單詞/短語</p></li></ul></ul><p><strong>表示捕捉到了什麼</strong><strong>？</strong></p><ul><li><p>這要看你怎麼看了!</p></li><ul><li><p>可視化：</p></li><ul><li><p>鳥瞰</p></li><li><p>很少的樣本——可能會讓人想起 cherry-picking (最佳選擇)</p></li></ul><li><p>調查：</p></li><ul><li><p>發現語料層面的特定屬性</p></li><li><p>可能會引入自己的偏見…</p></li></ul><li><p>網絡修改：</p></li><ul><li><p>對改進建模很有幫助</p></li><li><p>可以是特定於任務的</p></li></ul></ul><li><p>分析方法作為輔助模型開發的工具！</p></li></ul><p><strong>可解釋性和可遷移性對下游任務而言是重要的。</strong></p><p></p><h1 toutiao-origin=h2>4. 調整</h1><p><strong>如何調整預訓練模型</strong></p><p>我們可以在幾個方向上做決定:</p><ul><li><p>結構的修改？</p></li><ul><li><p>為了適應，需要對預訓練的模型體系結構進行多大的更改</p></li></ul><li><p>優化方案？</p></li><ul><li><p>在適應過程中需要訓練哪些權重以及遵循什麼時間表</p></li></ul><li><p>更多信號：弱監督、多任務和集成</p></li><ul><li><p>如何為目標任務獲取更多的監督信號</p></li></ul></ul><p></p><h2 toutiao-origin=h3>4.1 結構</h2><p>兩個通用選項：</p><ul><li><p>保持預訓練模型內部不變</p></li><ul><li><p>在頂部添加分類器，在底部添加嵌入，將輸出作為特徵</p></li></ul><li><p>修改預訓練模型的內部架構</p></li><ul><li><p>初始化編碼器-解碼器、特定於任務的修改、適配器</p></li></ul></ul><p></p><h2 toutiao-origin=h3>4.1.1 – 結構：保持模型不變</h2><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcHrB3CG5ZWJ><p>常規工作流：</p><ul><li><p>如果對目標任務無效，則刪除預訓練的任務頭</p></li><ul><li><p>示例：從預訓練語言模型中刪除softmax分類器</p></li><li><p>不總是需要：一些調整方案重用了預訓練的目標/任務，例如用於多任務學習</p></li></ul><li><p>在預訓練模型的頂部/底部添加特定於任務的目標層</p><br></li></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXcHrbCX9TV5s><ul><li><p>簡單：在預訓練的模型上添加線性層</p></li></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXcHrt2GZLxcL><ul><li><p>更復雜的：將模型輸出作為單獨模型的輸入</p></li><li><p>當目標任務需要預訓練嵌入中所沒有的交互時，通常是有益的</p></li></ul><p></p><h2 toutiao-origin=h3>4.1.2 – 結構：修改模型內部</h2><p>各種各樣的原因:</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcHs7IDChibj><ul><li><p>適應結構上不同的目標任務</p></li><ul><li><p>例如：使用單個輸入序列(例如:語言建模)進行預訓練，但是適應多個輸入序列的任務(例如:翻譯、條件生成……)</p></li><li><p>使用預訓練的模型權重儘可能初始化結構不同的目標任務模型</p></li><li><p>例如：使用單語語言模型初始化機器翻譯的編碼器和解碼器參數 (Ramachandran et al., EMNLP 2017; Lample & Conneau, 2019)</p></li></ul></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXcHsZ7E76kUn><ul><li><p>特定於任務的修改</p></li><ul><li><p>提供對目標任務有用的預訓練模型</p></li><li><p>例如：添加跳過/殘差連接，注意力(Ramachandran et al., EMNLP 2017)</p></li></ul></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcIJnIl1CYbC><ul><li><p>使用較少的參數進行調整:</p></li><ul><li><p>更少的參數進行微調</p></li><li><p>在模型參數不斷增大的情況下，非常有用</p></li><li><p>例如:在預訓練模型的層之間添加瓶頸模塊(“適配器”) (Rebuffi et al., NIPS 2017;CVPR 2018)</p></li></ul></ul><p>Adapters</p><ul><li><p>通常使用剩餘連接與現有層並行的層相連</p></li><li><p>每層之間都放置時效果最佳(底層效果較小)</p></li><li><p>不同的操作(卷積，自我注意)是可能的</p></li><li><p>特別適合 Transformer 等模塊化架構 (Houlsby et al., ICML 2019; Stickland and Murray, ICML 2019</p></li></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcIKA8m8D2ul><p>Adapters (Stickland & Murray, ICML 2019)</p><ul><li><p>多頭的<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">關注</i>(MH；跨層共享)與BERT的 self-attention (SA)層並行使用</p></li><li><p>兩者都被加在一起，並輸入到 Layer-norm (LN)中</p></li></ul><p></p><h2 toutiao-origin=h3>4.2 優化</h2><p>涉及到優化本身的幾個方向:</p><ul><li><p>選擇我們應該更新的權重</p></li><ul><li><p>Feature extraction, fine-tuning, adapters</p></li></ul><li><p>選擇如何以及何時更新權重</p></li><ul><li><p>From top to bottom, gradual unfreezing, discriminative fine-tuning</p></li></ul><li><p>考慮實事求是的權衡</p></li><ul><li><p>Space and time comple<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">xi</i>ty, performance</p></li></ul></ul><p></p><h2 toutiao-origin=h3>4.2.1 – 優化: 什麼權重？</h2><p>主要問題:調整還是不調整(預先訓練好的重量)?</p><ul><li><p>不改變預先訓練的重量</p></li><ul><li><p>Feature extraction</p></li><ul><li><p>(預訓練的)權重被凍結</p></li></ul></ul></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXcIKf8mQiyuE><ul><ul><li><p>線性分類器是在預訓練的表示上進行訓練的</p></li></ul></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcIKvGVwf8um><ul><ul><li><p>不要只使用頂層的特性!</p></li><li><p>學習層的線性組合 (Peters et al., NAACL 2018, Ruder et al., AAAI 2019)</p></li></ul></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXcIL932JYblQ><ul><ul><ul><li><p>或者，在下游模型中使用預先訓練的表示作為特性</p></li></ul><li><p>Adapters</p></li></ul></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcIqm93Z8euG><ul><ul><li><p>在現有層之間添加的特定於任務的模塊</p></li><li><p>只有 adapters 被訓練</p></li></ul><li><p>改變預訓練權重</p></li><ul><li><p>fine-tuning</p></li><ul><li><p>採用預訓練的權重作為下游模型參數的初始化</p></li><li><p>整個預訓練的體系結構在適應階段進行訓練</p></li></ul></ul></ul><p><strong>4.2.2 – 優化：</strong><strong>什麼方式？</strong></p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXcIr1191T4wR><p>我們已經決定要更新哪些權重，但是以什麼順序以及如何更新它們？</p><p>動機：我們希望避免覆蓋有用的預訓練信息，並最大化積極的知識遷移</p><p>相關概念：災難遺忘 (McCloskey＆Cohen, <i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-3">198</i>9; French, <i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-3">199</i>9) :一個模型忘記了它最初受過訓練的任務</p><p><strong>指導原則：</strong><strong>從上到下更新</strong></p><ul><li><p>時間上逐步更新：凍結</p></li><li><p>強度上逐步更新：改變學習速度</p></li><li><p>Progressively vs. the pretrained model 逐步更新 vs 預訓練模型：正則化</p></li></ul><p><strong>優化：</strong><strong>凍結</strong></p><ul><li><p>主要直覺：在不同分佈和任務的數據上同時訓練所有層可能導致不穩定的不良解決方案</p></li><li><p>解決方案：單獨訓練每一層，使他們有時間適應新的任務和數據。</p></li><li><p>回到早期深度神經網絡的分層訓練（Hinton et al., 2006; Bengio et al., 2007）</p></li></ul><p>相關實例</p><ul><li><p>凍結頂層以外的所有層 (Long et al., ICML 2015)</p></li><li><p>Chain-thaw (Felbo et al., EMNLP 2017):每次訓練一層</p></li><ul><li><p>先訓練新增的層</p></li><li><p>再自底向上，每次訓練一層（不再訓練新增的那一層，其餘層以會在不訓練時被同時凍結）</p></li><li><p>訓練所有層（包括新增層）</p></li></ul><li><p>Gradually unfreezing (Howard & Ruder, ACL 2018): 逐層解凍（自頂向下）</p></li><li><p>Sequential unfreezing (Chronopoulou et al., NAACL 2019): 超參數控制微調輪數</p></li><ul><li><p>微調 n 輪次新增參數（凍結除了新增層以外的層）</p></li><li><p>微調 k 輪次嵌入層以外的預訓練層</p></li><li><p>訓練所有層直到收斂</p></li></ul></ul><p></p><h2 toutiao-origin=h3>4.2.2 – 優化：學習率</h2><p><strong>主要想法：使用更低的學習率來避免覆蓋掉有用的信息</strong></p><p>在哪裡以及在什麼時候？</p><ul><li><p>低層(捕獲一般信息)</p></li><li><p>訓練初期(模型仍需適應目標分佈)</p></li><li><p>訓練後期(模型接近收斂)</p></li></ul><p>相關實例 (Howard & Ruder, ACL 2018)</p><p>較低層捕獲一般信息 → 對較低層使用較低的學習速率</p><p>Discriminative fine-tuning</p><p>較低的層捕獲一般信息對較低的層次使用較低的學習率</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcIrH3QxQFxi><p>Triangular learning rates</p><p>快速移動到一個合適的區域，然後隨著時間慢慢收斂</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXcIrUFm13ezA><ul><ul><li><p>也被稱為 “learning rate warm-up”</p></li><li><p>用於 Transformer (Vaswani et al., NIPS 2017) 和 Transformer-based methods (BERT, GPT) 等</p></li><li><p>有利於優化；更容易擺脫次優局部極小值</p></li></ul></ul><p></p><h2 toutiao-origin=h3>4.2.2 – 優化：正則化</h2><p>主要思想：通過使用正則化項 ，鼓勵目標模型參數接近預先訓練的模型參數，將災難性遺忘最小化。</p><p>簡單的方法:將新參數正則化，不要與預訓練的參數偏離太多 (Wiese et al., CoNLL 2017)</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcIrg8PG7k2o><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcJB3BvwAHpj><p>更高級(elastic weight consolidation; EWC)(Kirkpatrick et al., PNAS 2017)</p><p>基於 Fisher 信息矩陣 F ，<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-1">關注</i>對預訓練任務重要的參數</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcJBJ2W0TLVy><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcJBX1wT4tGw><p>EWC在持續學習方面有缺點:</p><p>可能過度約束參數</p><p>計算成本與任務數量成線性關係(Schwarz et al., ICML 2018)</p><p>如果任務相似，我們也可以鼓勵基於交叉熵的源和目標預測接近，類似於蒸餾:</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXcJBuAZH9AZw><p></p><h2 toutiao-origin=h3>4.2.3 – 優化：權衡</h2><p>在選擇更新哪些權重時，需要權衡以下幾個方面：</p><p>A. 空間複雜度</p><ul><li><p>特定於任務的修改、附加參數、參數重用</p></li></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcJC8Demze43><p>B. 時間複雜度</p><ul><li><p>訓練時間</p></li></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXcJX8EvPThrN><p>C. 性能</p><ul><li><p>經驗法則：如果任務源和目標任務不相似*，使用特徵提取 (Peters et al., 2019)</p></li><li><p>否則，特徵提取和微調常常效果類似（此時用微調更好）</p></li><li><p>在文本相似性任務上對 BERT 進行微調，效果明顯更好</p></li><li><p>適配器實現了與微調相比具有競爭力的性能</p></li><li><p>有趣的是，Transformer 比 LSTMs 更容易微調(對超參數不那麼敏感)</p></li></ul><p>*不相似：某些能力(例如句子間關係建模)對目標任務是有益的，但預訓練的模型缺乏這些能力能(參見後面的更多內容)</p><p></p><h2 toutiao-origin=h3>4.3 – 獲得更多信號</h2><p>目標任務通常是低資源任務。我們經常可以通過組合不同的信號，提高遷移學習的效果：</p><ul><li><p>在單個適應任務上微調單個模型</p></li><ul><li><p>基本原理：用一個簡單的分類目標對模型進行微調</p></li></ul><li><p>其他數據集和相關任務中收集信號</p></li><ul><li><p>微調與弱監督，多任務和順序調整</p></li></ul><li><p>集成模型</p></li><ul><li><p>結合幾個微調模型的預測</p></li></ul></ul><p></p><h2 toutiao-origin=h3>4.3.1 – 獲得更多信號：基本的 fine-tuning</h2><p>微調文本分類任務的簡單例子：</p><ul><li><p>從模型中提取單個定長向量</p></li><ul><li><p>第一個/最後一個令牌的隱藏狀態，或者是隱藏狀態的平均值/最大值</p></li></ul><li><p>使用附加的分類器投影到分類空間</p></li><li><p>用分類目標函數訓練</p></li></ul><p></p><h2 toutiao-origin=h3>4.3.2 – 獲得更多信號：相關數據集/任務</h2><ul><li><p>順序調整 Sequential adaptation</p></li><ul><li><p>對相關數據集和任務進行中間微調</p></li></ul><li><p>與相關任務進行多任務微調</p></li><ul><li><p>如 GLUE 中的 NLI 任務</p></li></ul><li><p>數據集分割</p></li><ul><li><p>當模型在特定的數據片上始終表現不佳時</p></li></ul><li><p>半監督學習</p></li><ul><li><p>使用未標記的數據來提高模型的一致性</p></li></ul></ul><p></p><h2 toutiao-origin=h3>4.3.2 – 獲得更多信號：順序調整</h2><p>在相關高資源數據集進行微調</p><ol><li><p>在擁有更多的數據的相關任務對模型進行微調</p></li><li><p>在目標任務上微調數據集</p></li></ol><ul><li><p>對於數據有限並且有類似任務的任務尤其有用(Phang et al., 2018)</p></li><li><p>提高目標任務的樣本複雜度(Yogatama et al., 2019)</p></li></ul><p></p><h2 toutiao-origin=h3>4.3.2 – 獲得更多信號：多任務 fine-tuning</h2><p>在相關任務上共同微調模型</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcJXPCXHYiAk><ul><li><p>對於每個優化步驟，取樣一個任務和一批數據進行訓練</p></li><li><p>通過多任務學習訓練多輪</p></li><li><p>只在最後幾個階段對目標任務進行微調</p></li></ul><p>使用無監督的輔助任務微調模型</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXcJXd6GmZby2><p>語言建模是一個相關的任務！</p><p>微調語言模型有助於將預訓練的參數調整到目標數據集</p><p>即使沒有預訓練，也會起到幫助 (Rei et al., ACL 2017)</p><p>可選退火比</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXcJXvCpYMCCK><p>(Chronopoulou et al., NAACL 2019)</p><p>作為 ULMFiT 中的一個單獨步驟使用</p><p></p><h2 toutiao-origin=h3>4.3.2 – 獲得更多信號：數據集切分</h2><p>使用僅在數據的特定子集上訓練的輔助頭</p><ul><li><p>分析模型誤差</p></li><li><p>使用啟發式方法自動識別訓練數據的挑戰性子集</p></li><li><p>與主頭一起聯合訓練輔助頭</p></li></ul><p>See also Massive Multi-task Learning with Snorkel MeTaL</p><p></p><h2 toutiao-origin=h3>4.3.2 – 獲得更多信號：半監督學習</h2><p>使用未標記的數據可以使模型預測更加一致</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcJY9F7Uvq3D><ul><li><p>主要思想:使對原始輸入 x 和擾動輸入 x' 的預測之間的距離最小化</p></li><li><p>擾動可以是噪聲、掩蔽(Clark et al., EMNLP 2018)、數據增強，例如 back-translation (Xie et al., 2019)</p></li></ul><p></p><h2 toutiao-origin=h3>4.3.3 – 獲得更多信號：集成</h2><p>通過集成獨立的微調模型達到最先進水平</p><ul><li><p>集成模型：使用各種超參數微調模型預測的組合</p></li></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcJw411ql39t><ul><ul><li><p>在不同的任務</p></li><li><p>在不同的數據集分塊</p></li><li><p>使用不同的參數(dropout, initializations…)</p></li><li><p>來自預訓練模型的變體(例如 cased/uncased )</p></li></ul><li><p>知識蒸餾：在一個更小的模型中提取一組調優模型</p></li></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcJwYGqVYAtZ><ul><ul><li><p>知識蒸餾：在教師（集成模型）製作的軟目標上訓練學生模型</p></li><li><p>教師標籤的相對概率包含教師如何概括的信息</p></li></ul></ul><p></p><h1 toutiao-origin=h2>5. 下游應用程序</h1><p>在本節中，我們將沿兩個方向探索下游的應用和實際考慮:</p><ul><li><p>遷移學習在自然語言處理中的各種應用是什麼</p></li><ul><li><p>文檔/句子分類、令牌級分類、結構化預測和語言生成</p></li></ul><li><p>如何利用多個框架和庫來實現實際應用</p></li><ul><li><p>Tensorflow、PyTorch、Keras和第三方庫，例如 fast.ai, HuggingFace……</p></li></ul></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcJwq3XuwzOq><ul><li><p>句子和文檔級分類</p></li><ul><li><p>動手實踐：文檔級分類(fast.ai)</p></li></ul><li><p>令牌分類</p></li><ul><li><p>實踐：問答(谷歌BERT & Tensorflow/TF Hub)</p></li></ul><li><p>語言生成</p></li><ul><li><p>實踐：對話生成(OpenAI GPT & HuggingFace/PyTorch Hub)</p></li></ul></ul><p>本部分內容偏向編程實踐，將在本教程的第三篇中進行補充</p><p></p><h2 toutiao-origin=h3>5.1 – 句子和文檔級別分類</h2><p>使用 fast.ai 庫完成文檔分類的遷移學習</p><ul><li><p>目標任務</p></li><ul><li><p>IMDB：一個二元情緒分類數據集，包含用於訓練的25k個高度極性的電影評論，用於測試的25k個，以及其他未標記的數據。https://ai.stanford.edu/~amaas/data/sentiment/</p></li><li><p>Fast.ai 特別提供了:</p></li><ul><li><p>一個預先訓練的英文模型可供下載</p></li><li><p>一個標準化的數據塊API</p></li><li><p>易於訪問標準數據集，如IMDB</p></li></ul><li><p>fast.ai 基於 PyTorch</p></li></ul></ul><p>fast.ai 為視覺、文本、表格數據和協同過濾提供了許多開箱即用的高級API</p><p>庫的設計是為了加快實驗的速度，例如在互動計算環境中一次導入所有必需的模塊，例如:</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcJxP3nZliRd><p>Fast.ai 包含快速設置遷移學習實驗所需的所有高級模塊。</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcJxcINZAcho><ul><li><p>加載數據集</p></li><li><p>使用語言模型和分類器的 DataBunch</p></li><li><p>使用語言模型損失函數，在 WikiText-103 上預訓練的 AWD-LSTM 並在 IMDB 數據集上微調</p></li></ul><p>一旦我們有了微調的語言模型(AWD-LSTM)，我們可以創建一個文本分類器，添加一個分類頭:</p><ul><li><p>將RNN的最終輸出的最大值與所有中間輸出(沿著序列長度)的平均值連接起來的層</p></li><li><p>Two blocks of nn.BatchNorm1d ⇨ nn.Dropout ⇨ nn.Linear ⇨ nn.ReLU 的隱藏維度為50</p></li><li><p>分兩步微調</p></li><ul><li><p>只訓練分類頭，同時保持語言模型不變</p></li><li><p>微調整個結構</p></li></ul><li><p>Colab: tiny.cc/NAACLTransferFastAiColab</p></li></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcKSs3uFSnpX><p></p><h2 toutiao-origin=h3>5.2 – Token 級別分類: BERT & Tensorflow</h2><p>用於令牌級分類的遷移學習：谷歌的 BERT in TensorFlow</p><ul><li><p>目標任務:</p></li><ul><li><p>SQuAD: 回答問題的數據集 https://rajpurkar.github.io/SQuAD-explorer/</p></li></ul><li><p>在本例中，我們將直接使用 Tensorflow checkpoint</p></li><ul><li><p>例如：https://github<i class="chrome-extension-mutihighlight chrome-extension-mutihighlight-style-6">.com</i>/google-research/bert/</p></li><li><p>我們使用通常的Tensorflow工作流：創建包含核心模型和添加/修改元素的模型圖</p></li><li><p>加載檢查點時要注意變量分配</p></li></ul></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RfXcKTDFGbvS3n><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcKTcuEoRPv><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcKTp4fJVj7Y><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcKUBApFqyOx><p>語言生成遷移學習：OpenAI GPT 和 HuggingFace 庫</p><ul><li><p>目標任務</p></li><ul><li><p>ConvAI2 -第二屆會話智能挑戰，用於訓練和評估非目標導向對話系統的模型，例如閒聊</p></li><ul><li><p>http://convai.io</p></li></ul></ul><li><p>預訓練模型的 HuggingFace 倉庫</p></li><ul><li><p>大型預先訓練模型 BERT, GPT, GPT-2, Transformer-XL 的倉庫</p></li><li><p>提供一個簡單的方法來下載、實例化和訓練PyTorch中預先訓練好的模型</p></li></ul><li><p>HuggingFace的模型現在也可以通過PyTorch Hub訪問</p></li></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXcL3bDwVuMC6><p>語言生成任務接近語言建模訓練前的目標，但是:</p><ul><li><p>語言建模前的訓練只需要一個輸入：一系列單詞</p></li><li><p>在對話框設置中：提供了幾種類型的上下文來生成輸出序列</p></li><ul><li><p>知識庫：角色句</p></li><li><p>對話的歷史：至少是用戶的最後一句話</p></li><li><p>已生成的輸出序列的標記</p></li></ul></ul><p>我們應該如何適應這種模式？</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcL3yE9cvTvm><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcL4HIkhkfTr><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RfXcL4W6VZSbFh><p></p><h1 toutiao-origin=h2>6. 開放問題和方向</h1><p>預訓練的語言模型的缺點</p><ul><li><p>概述：語言模型可視為一般的預訓練任務；有了足夠的數據、計算和容量，LM可以學到很多東西</p></li><li><p>在實踐中，許多在文本中表示較少的東西更難學習</p></li><li><p>預先訓練好的語言模型並不擅長</p></li><ul><li><p>細粒度語言任務 (Liu et al., NAACL 2019)</p></li><li><p>常識(當你真的讓它變得困難 Zellers et al., ACL 2019)；自然語言生成(維護長期依賴、關係、一致性等)</p></li><li><p>當微調時，傾向於過度適應表面形成的信息；‘rapid surface learners’</p></li></ul></ul><p>大型的、預訓練的語言模型很難優化。</p><ul><li><p>微調通常是不穩定的，並且有很高的方差，特別是在目標數據集非常小的情況下</p></li><li><p>Devlin et al. (NAACL 2019) 指出，BERT的大版本(24層)特別容易導致性能退化；多次隨機重啟有時是必要的，這在(Phang et al., 2018)中也有詳細的研究</p></li></ul><p>當前的預訓練語言模型非常大</p><ul><li><p>我們真的需要所有這些參數嗎？</p></li><li><p>最近的研究表明，BERT中只需要幾個注意力頭(Voita et al., ACL 2019)</p></li><li><p>需要做更多的工作來理解模型參數</p></li><li><p>修剪和蒸餾是兩種處理方法</p></li><li><p>參見：彩票假說(Frankle et al., ICLR 2019)</p></li></ul><p><strong>預訓練任務</strong></p><p>語言建模目標的不足</p><ul><li><p>並不適用於所有模型</p></li><ul><li><p>如果我們需要更多的輸入，就需要對這些部件進行預培訓</p></li><li><p>例如序列到序列學習中的解碼器(Song et al., ICML 2019)</p></li></ul><li><p>從左到右的偏見並不總是最好的</p></li><ul><li><p>考慮更多上下文(如屏蔽)的目標似乎有用(採樣效率較低)</p></li><li><p>可能組合不同LM變種(Dong et al., 2019)</p></li></ul><li><p>語義和長期上下文的弱信號與語法和短期單詞共存的強信號</p></li><ul><li><p>需要激勵機制來促進我們所關心的編碼，例如語義</p></li></ul></ul><p>更加多樣化的自我監督目標</p><ul><li><p>從計算機視覺中獲得靈感</p></li><li><p>語言中的自我監督主要基於詞的共現(Ando and Zhang, 2005)</p></li><li><p>不同層次意義上的監督</p></li><ul><li><p>論述、文件、句子等</p></li><li><p>使用其他信號，例如元數據</p></li></ul><li><p>強調語言的不同性質</p></li></ul><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RfXcL4vGZnKyHw><p>抽樣一個補丁和一個鄰居，並預測它們的空間配置(Doersch et al., ICCV 2015)</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RfXcLW5FApR9xt><p>圖片著色 (Zhang et al., ECCV 2016)</p><p>專門的預訓練任務來教我們的模型缺少的東西</p><ul><li><p>制定專門的預訓練任務，明確學習這些關係</p></li><ul><li><p>獲取背景知識的單詞對關係 (Joshi et al., NAACL 2019)</p></li><li><p>範圍級表示(Swayamdipta et al., EMNLP 2018)</p></li><li><p>不同的預訓練詞嵌入是有用的(Kiela et al., EMNLP 2018)</p></li></ul><li><p>其他預訓練的任務可以明確地學習推理或理解</p></li><ul><li><p>算術、時間、因果等；話語、敘述、談話等。</p></li></ul><li><p>預訓練的表示可以以稀疏和模塊化的方式連接</p></li><ul><li><p>基於語言子結構(Andreas et al., NAACL 2016) 或專家 (Shazeer et al., ICLR 2017)</p></li></ul></ul><p>需要更加合理的表示</p><ul><li><p>分佈式假設的侷限性——很難從原始文本中學習特定類型的信息</p></li><ul><li><p>人類報告偏見：不陳述顯而易見的(Gordon and Van Durme, AKBC 2013)</p></li><li><p>常識不是寫下來的</p></li><li><p>關於命名實體的事實</p></li><li><p>沒有其他模式的基礎</p></li></ul><li><p>可能的解決方式:</p></li><ul><li><p>吸收其他結構化知識(e.g. knowledge bases like ERNIE, Zhang et al 2019)</p></li><li><p>多模態學習(e.g. with visual representations like VideoBERT, Sun et al. 2019)</p></li><li><p>交互式/human-in-the-loop 的方法(e.g. dialog, Hancock et al. 2018)</p></li></ul></ul><p><strong>任務和任務的相似性</strong></p><p>許多任務可以表示為語言建模的變體</p><ul><li><p>語言本身可以直接用於指定任務、輸入和輸出，例如，通過構建QA (McCann et al., 2018)</p></li><li><p>基於對話的學習，不受正向預測的監督 (Weston, NIPS 2016)</p></li><li><p>將NLP任務制定為完形填空預測目標 (Children Book Test, LAMBADA, Winograd, ...)</p></li><li><p>通過提示觸發任務行為，例如翻譯提示 (Radford, Wu et al. 2019); 使zero-shot適應</p></li><li><p>質疑NLP中的“任務”概念</p></li><li><p>預訓練和目標任務(NLI，分類)的直覺相似性與較好的下游性能相關</p></li><li><p>不清楚兩個任務在什麼時候以及如何相似和相關</p></li><li><p>獲得更多理解的方法之一：大規模的遷移實證研究，如 Taskonomy (Zamir et al., CVPR 2018)</p></li><li><p>是否有助於設計更好和更專業的預訓練任務</p></li></ul><p><strong>持續和元學習</strong></p><ul><li><p>當前遷移學習只進行一次適應。</p></li><li><p>最終，我們希望擁有能夠在許多任務中持續保留和積累知識的模型(Yogatama et al., 2019)</p></li><li><p>預訓練和適應之間沒有區別；只有一個任務流</p></li><li><p>主要的挑戰是:災難性的遺忘</p></li><li><p>不同的研究方法：</p></li><ul><li><p>記憶、正則化、任務特定權重等</p></li></ul><li><p>遷移學習的目的：學習一種對許多任務都通用且有用的表示方法</p></li><li><p>客觀因素不會刺激適應的易用性(通常不穩定)；沒有學會如何適應它</p></li><li><p>元學習與遷移學習相結合可以使這一方法更加可行</p></li><li><p>然而，大多數現有的方法都侷限於few-shot場景，並且只學習了幾個適應步驟</p></li></ul><p><strong>偏見</strong></p><ul><li><p>偏見已經被證明普遍存在於單詞嵌入和一般的神經模型中</p></li><li><p>大型預訓練的模型必然有自己的一套偏見</p></li><li><p>常識和偏見之間的界限很模糊</p></li><li><p>我們需要在適應過程中消除這種偏見</p></li><li><p>一個小的微調模型應該更不易被誤用</p></li></ul><p></p><h1 toutiao-origin=h2>結論</h1><ul><li><p>主題：語境中的詞彙，語言模型預培訓，深度模型</p></li><li><p>預訓練具有較好的 sample-efficiency ，可按比例放大</p></li><li><p>對某些特性的預測——取決於您如何看待它</p></li><li><p>性能權衡，自頂向下</p></li><li><p>遷移學習易於實現，實用性強</p></li><li><p>仍然存在許多不足和尚未解決的問題</p></li></ul><p>本期責任編輯：崔一鳴</p><p>本期編輯：顧宇軒</p><img alt="賽爾筆記 | 自然語言處理中的遷移學習(下)" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/R69F3rGCWOEqko></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>賽爾</a></li><li><a>筆記</a></li><li><a>語言</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/a0e9a5a9.html alt=「C語言筆記」assert怎麼用？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/0816bedea8784129bb9a4b65f01dc337 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a0e9a5a9.html title=「C語言筆記」assert怎麼用？>「C語言筆記」assert怎麼用？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e2a654d5.html alt=請收藏C語言最全入門筆記 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/000a3011a959479e9bd471fd45a266ca style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e2a654d5.html title=請收藏C語言最全入門筆記>請收藏C語言最全入門筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/daee4c3b.html alt=C語言編程筆記丨利用C語言寫一個小程序——胖胖的愛心桃 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/df26c625ad3d44d4b515522c28f5462c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/daee4c3b.html title=C語言編程筆記丨利用C語言寫一個小程序——胖胖的愛心桃>C語言編程筆記丨利用C語言寫一個小程序——胖胖的愛心桃</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7b754ba6.html alt="C語言編程筆記丨編寫第一個C語言程序'hello world'，我教你哇" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/ad42f4fc-8976-4d2d-81e8-c62c3808e213 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7b754ba6.html title="C語言編程筆記丨編寫第一個C語言程序'hello world'，我教你哇">C語言編程筆記丨編寫第一個C語言程序'hello world'，我教你哇</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/65d2605f.html alt=什麼是梯度：用形象的語言解讀梯度的本質原理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/557c7b377c5449bb885df35b2d354e03 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/65d2605f.html title=什麼是梯度：用形象的語言解讀梯度的本質原理>什麼是梯度：用形象的語言解讀梯度的本質原理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4b2ac18d.html alt=我國首部較少民族語言工具書《鄂倫春語常用語發音詞典》出版發行 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/S5YRqTH6awaqmG style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4b2ac18d.html title=我國首部較少民族語言工具書《鄂倫春語常用語發音詞典》出版發行>我國首部較少民族語言工具書《鄂倫春語常用語發音詞典》出版發行</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/10c7d8fd.html alt=神州泰嶽：公司在自然語言處理領域的基礎技術研究和應用落地均走在行業前列 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/10c7d8fd.html title=神州泰嶽：公司在自然語言處理領域的基礎技術研究和應用落地均走在行業前列>神州泰嶽：公司在自然語言處理領域的基礎技術研究和應用落地均走在行業前列</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f4c5c93c.html alt=谷歌推出自然語言框架語義解析器SLING，但沒說有沒有用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/46ea0001172cab9535dc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f4c5c93c.html title=谷歌推出自然語言框架語義解析器SLING，但沒說有沒有用>谷歌推出自然語言框架語義解析器SLING，但沒說有沒有用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html alt=HTMLCSS學習筆記（六）——元素類型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb5988349894ce9bf568c6418f85b7d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html title=HTMLCSS學習筆記（六）——元素類型>HTMLCSS學習筆記（六）——元素類型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html alt="MySQL 學習筆記" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html title="MySQL 學習筆記">MySQL 學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c96865e4.html alt=C語言經典100例004-統計各個年齡階段的人數 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/98c4bb54-8d7e-4234-bf07-0d8190ba1a0c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c96865e4.html title=C語言經典100例004-統計各個年齡階段的人數>C語言經典100例004-統計各個年齡階段的人數</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/00674040.html alt=CC++語言16｜通過繼承、虛函數、指針來實現動態多態 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/00674040.html title=CC++語言16｜通過繼承、虛函數、指針來實現動態多態>CC++語言16｜通過繼承、虛函數、指針來實現動態多態</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0f977c9c.html alt="讀書筆記｜《Think in Java》Ⅷ 多態" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6ee50002bf049682647d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0f977c9c.html title="讀書筆記｜《Think in Java》Ⅷ 多態">讀書筆記｜《Think in Java》Ⅷ 多態</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c489c1e2.html alt="為了買一個馬桶，我做了十幾頁的筆記... 馬桶選購原來水這麼深！" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/6ebfc0f61b9d44818571c56f832e3c84 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c489c1e2.html title="為了買一個馬桶，我做了十幾頁的筆記... 馬桶選購原來水這麼深！">為了買一個馬桶，我做了十幾頁的筆記... 馬桶選購原來水這麼深！</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>