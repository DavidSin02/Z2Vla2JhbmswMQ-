<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法 | 极客快訊</title><meta property="og:title" content="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/R6Ieh75DBRtmcY"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c29afbb.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c29afbb.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/c29afbb.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c29afbb.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c29afbb.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/c29afbb.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/c29afbb.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c29afbb.html><meta property="article:published_time" content="2020-10-29T20:59:02+08:00"><meta property="article:modified_time" content="2020-10-29T20:59:02+08:00"><meta name=Keywords content><meta name=description content="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/c29afbb.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>《測繪學報》</p><p><strong class=highlight-text toutiao-origin=span>構建與學術的橋樑 拉近與權威的距離</strong></p><p><strong class=highlight-text toutiao-origin=span>複製鏈接，關注《測繪學報》抖音！</strong></p><p><strong class=highlight-text toutiao-origin=p>【測繪學報的個人主頁】長按複製此條消息，長按複製打開抖音查看TA的更多作品##7NsBSynuc88##[抖音口令]</strong></p><p><strong class=highlight-text toutiao-origin=span>本文內容來源於《測繪學報》2020年第8期，審圖號GS（2020）4062號。</strong></p><p><strong toutiao-origin=span>遙感影像地物分類多注意力融和U型網絡法</strong></p><p>李道紀<img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/R6Ieh75DBRtmcY>,郭海濤,盧俊<img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/R6W0QpMHySg0Qb>,趙傳, 林雨準,餘東行</p><p>信息工程大學地理空間信息學院, 河南 鄭州 450001</p><p><strong>基金項目：</strong>國家自然科學基金（41601507）</p><p><strong>摘要</strong>：經典的卷積神經網絡在對遙感影像進行地物分類的過程中，由於影像中的地物尺寸和光譜特徵差異較大、待分類目標背景環境複雜等問題，經典影像分類方法很難得到理想的分類結果。針對這些問題，本文借鑑U型卷積神經網絡多層次特徵融和的思想，提出了多注意力融和U型網絡（MAFU-Net）。該網絡利用注意力模塊提取和處理不同層次的語義信息，強化不同位置像素和不同特徵圖之間的相關性，進而提高網絡在複雜背景條件下的分類性能。為了驗證本文提出的網絡在遙感影像地物分類中的效果，分別在ISPRS上的Vaihingen數據集以及北京、河南兩地區高分二號數據集上進行了試驗，並與目前主流的語義分割網絡進行了對比。試驗結果表明，相比其他網絡，本文提出的MAFU-Net在不同特點的數據集上均可以得到最佳的地物分類結果。同時，該網絡結構簡單、計算複雜度低、參數量少，具有很強的實用性。另外，本文充分利用特徵可視化手段進行MAFU-Net和其他網絡的分類性能對比分析，試驗結果表明，目前多數深度學習網絡模型的深層次原理和作用機制較為複雜，無法準確解釋特定網絡為何在某種數據集中會失效。這需要研究人員進一步通過更加高級的可視化表達方法和量化準則來對特定深度學習模型及網絡性能進行分析評價，進而對更加高級的模型結構進行設計。</p><p><strong>關鍵詞：</strong>地物分類 遙感影像 注意力機制 U型卷積神經網絡 語義分割</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RrAUsFz6Oa0Su9><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RrAUsGE2zceNSA><p>引文格式：李道紀, 郭海濤, 盧俊, 等. 遙感影像地物分類多注意力融和U型網絡法. 測繪學報，2020，49(8)：1051-1064. DOI: 10.11947/j.AGCS.2020.20190407.</p><p><strong>閱讀全文：</strong>http://xb.sinomaps.com/article/2020/1001-1595/2020-8-1051.htm</p><p><strong toutiao-origin=span>全文概述</strong></p><p>遙感影像地物分類技術作為遙感影像理解的基石，在土地數據更新、地物觀測、變化檢測等領域都有著十分重要的作用<sup>[1</sup><sup>]</sup>。不同於遙感影像場景分類與目標檢測，遙感影像地物分類的目的是對影像中的每個像素點進行類別歸屬，即把影像中所包含的每個類別都準確地從原圖中標記出來。</p><p>早期遙感影像中的地物分類結果主要依靠人工判讀得到。然而，針對目前海量的遙感影像數據，完全依賴人工解譯不切實際，一方面效率低、成本高，分類質量難以保證；另一方面還容易造成判讀人員視力疲勞和損傷等問題。因此，為減少人力物力資源，同時提高遙感系統智能化水平，適應大數據時代的應用需求，遙感影像地物分類自動化逐漸成為遙感領域的重要課題。然而，遙感影像成像機理和背景環境複雜，在進行地物類別區分的過程中很容易發生混淆<sup>[2</sup><sup>]</sup>。目前，如何對遙感影像地物實現高精度的類別劃分，始終是一個亟待解決的難題。</p><p>遙感影像地物分類的方法按照是否使用先驗知識，可以分為兩大類：非監督分類法與監督分類法。常用的非監督分類法有ISODATA分類法和k-means分類法等<sup>[3</sup><sup>]</sup>。該類方法不需要任何先驗知識，主要依靠地物光譜或空間的統計特性進行分類，節省了人工標註的時間和精力。然而，該類方法所產生的集群組需要大量的分析及後處理，加上“同物異譜”與“異物同譜”的現象，很難與地物類別之間做到一一對應的關係。比較而言，監督分類法可以充分利用先驗知識，預先確定類別，避免了非監督分類對光譜集群組的重新歸類，因此常用於地物分類任務當中。監督分類方法有很多種，當前的研究主要集中於基於機器學習方法的地物分類，常用的有支持向量機<sup>[4</sup><sup>]</sup>、決策樹<sup>[5</sup><sup>]</sup>和集成模型等分類方法。這些機器學習方法都需要人為預先創建特徵，進而對特徵之間的關係進行分析，尋找最佳分類閾值和參數。然而，創建特徵往往需要大量的專業知識，不恰當的特徵反而會降低分類精度。因此，機器學習中的深度學習技術，被逐漸應用到了各個領域。深度學習不需要人工設計特徵，而是通過對訓練樣本的學習自動進行特徵提取。近年來，深度學習技術在計算機視覺領域快速發展，加之卷積神經網絡參數共享的特點，使得圖像解譯、分類、識別的精度及效率均得到了明顯的提升<sup>[6</sup><sup>]</sup>。</p><p>在計算機視覺領域，類似地物分類這種逐像素分類任務又被稱為語義分割<sup>[7</sup><sup>]</sup>。全卷積網絡(fully convolutional network, FCN)在語義分割領域具有開創性的意義，FCN利用反捲積層(上採樣)替換全連接層，大幅提高了分類效率<sup>[8</sup><sup>]</sup>，並在建築物檢測等方面得到了較多的應用<sup>[9</sup><sup>-10</sup><sup>]</sup>。在FCN之後，大量性能優異的語義分割網絡被相繼提出，典型的網絡包括U-Net<sup>[11</sup><sup>]</sup>、SegNet<sup>[12</sup><sup>]</sup>和DeconvNet<sup>[13</sup><sup>]</sup>。其中，U-Net對高低級語義信息進行了融和，改善了物體邊界語義細節的分類效果，提升了網絡的分類性能。此後，很多語義分割網絡均借鑑了U-Net的語義信息融和思想，如Refinenet<sup>[14</sup><sup>]</sup>、Encnet<sup>[15</sup><sup>]</sup>、Denseaspp<sup>[16</sup><sup>]</sup>和Deeplabv2<sup>[17</sup><sup>]</sup>等，這些網絡通過對整體結構的精心設計，並採用多尺度特徵融和策略，在公開的自然影像數據集<sup>[18</sup><sup>-20</sup><sup>]</sup>上取得了很好的分類效果。不同於自然影像，遙感影像涉及的場景範圍更廣，其中包含的地物往往不具有固定尺寸和特徵，因此直接將一些高精度語義分割網絡應用於遙感數據集，通常難以達到預期的精度和效果。</p><p>隨著深度學習技術的研究和發展，相應的注意力模塊被逐漸應用到語義分割網絡當中。注意力模塊借鑑了人類的注意力機制，即利用有限的注意力資源從大量信息中快速篩選出高價值信息。目前已有大量研究將注意力機制應用於U型網絡當中，以最大程度發揮注意力特徵處理性能和U-Net網絡結構的優勢。例如，Attention U-Net(Att-UNet)結構(arXiv preprint arXiv: 1804.03999, 2018)在U-Net網絡高低級語義信息融和的過程中，加入了注意力控制模塊(Attention Gates)，強化了有效信息的傳遞，並對無效信息的傳輸進行抑制。文獻[21]在Att-UNet的基礎上加入了多級監督策略，並重新對損失函數進行設計，顯著提高了醫學影像二分類精度。RAU-Net (arXiv preprint arXiv: 1811.01328, 2018)將殘差注意力模塊和U-Net結構進行結合，同時設計了二維和三維卷積網絡，提高了醫學影像腫瘤分割準確率。此外，很多學者設計了類似於Non-Local的注意力結構<sup>[22</sup><sup>]</sup>，該結構利用矩陣轉置相乘的方法，達到大範圍信息相關性計算的目的。例如，文獻[23]提出的CCNet通過在像素周圍十字交叉的區域內進行相關性計算，進而提取出強依賴性特徵，減少了冗餘信息。文獻[24]為降低模型複雜度，提出一種期望最大化注意力模型，該模型通過期望最大化算法，極大程度地減少了計算量，同時提高了特徵的穩健性。文獻[25]提出新型雙注意力分割網絡DANet，該網絡嵌入了兩種特殊的自注意力特徵融和結構，分別在空間維度和通道維度上捕獲視覺特徵的依賴關係，將所有位置的特徵值進行加權，各個通道中的特徵相互依賴。文獻[26]提出的RAFCN結構同樣是在通道維度和空間維度對特徵進行處理，不同於DANet，該網絡分別對3種不同層次的抽象信息進行注意力強化和反捲積，並最終對處理後的特徵進行融和，輸出類別。</p><p>基於上述分析，本文提出了一種用於遙感影像地物分類的多注意力融和U型網絡(multilevel attention fusion U-Net, MAFU-Net)，並以不同分辨率的遙感數據集作為研究對象，將該網絡與目前主流的語義分割網絡進行了對比試驗。本文的貢獻主要有3點：①本文將注意力機制應用於不同層次語義信息當中，充分發揮注意力機制的優勢，在不顯著增加參數量的情況下，提高地物分類精度；②將注意力機制內部的原理進行了剖析，揭示了注意力機制內部相關性計算的本質和作用；③提出了U型網絡語義融和的模式，利用上採樣中多層次語義信息進行綜合評價，以加強網絡在地物細節方面的分類精度，進一步提升性能。</p><p><strong toutiao-origin=span>1 理論背景與方法</strong></p><p><strong class=highlight-text toutiao-origin=span>1.1 雙注意力模塊</strong></p><p>注意力模塊本質上是通過矩陣轉置相乘進行相關性計算，增大依賴性較強的特徵權重，降低噪聲干擾，提高對有效信息的利用率。為增強網絡的特徵處理和場景理解能力，需要在局部特徵表示上建立豐富的上下文關係模型，並加入全局相關性特徵。為此本文引入位置注意模塊(position attention module, PAM)<sup>[25</sup><sup>]</sup>。PAM的結構如圖 1所示。</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/SBUy93HFKMmABG><p>圖 1 PAM結構 Fig. 1 The illustration of the basis PAM structure</p><p>圖選項</p><p>圖 1中，<em>A</em>為輸入特徵圖，<em>A</em>∈<em>R</em><em>C</em>×<em>H</em>×<em>W</em>。通過對特徵圖<em>A</em>進行卷積，得到特徵圖<em>B</em>、<em>C</em>和<em>D</em>，{<em>B</em>,<em>C</em>,<em>D</em>}∈<em>R</em><em>C</em>×<em>H</em>×<em>W</em>。將特徵圖<em>B</em>和<em>C</em>進行維度重排，並在像素維度上進行轉置相乘，得到空間注意力特徵圖像<em>S</em>，尺寸為(<em>H</em>×<em>W</em>)×(<em>H</em>×<em>W</em>)。具體操作如式(1)</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUy9Pp2UXkX2E><p>(1)</p><p>式中，s<sub>ji</sub>∈<em>S</em>，為第<em>i</em>個像素和第<em>j</em>個像素之間的相關性，二者特徵越相似，則轉置相乘得到的相關性越大，在注意力特徵圖像上具有更高的特徵值。</p><p>之後，將注意力特徵圖<em>S</em>與特徵圖<em>D</em>進行矩陣相乘，並與特徵圖<em>A</em>進行加和操作，得到空間注意力特徵圖E<sub>p</sub>。具體操作如式(2)</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUy9QBGAxdo2T><p>(2)</p><p>式中，E<sub>j</sub>∈E<sub>p</sub>；<em>α</em>為可訓練的權重係數。</p><p>由式(2)可知，空間注意力特徵圖E<sub>p</sub>中每個特徵值均為所有位置特徵值的加權與原始特徵值之和，因此，E<sub>p</sub>中既包含局部語義特徵信息，也包含全局語義特徵信息。這種加權策略在經過權重係數調整後，可以減少多餘特徵的干擾，降低噪聲，加快對重要信息的提取和訓練。</p><p>此外，特徵圖中每個通道特徵都可以看作是網絡模型對特定語義信息的響應，不同的語義信息之間相互關聯。通過建立通道映射之間的相互依賴關係，可以增強特定語義信息的表達。據此，本文在所提出的網絡中引入通道注意力模塊(channel attention module, CAM)，其結構如圖 2所示。CAM的注意力特徵圖X通過在通道維度上轉置相乘得到，其餘結構與PAM相同。</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUy9QQJJ0iYj1><p>圖 2 CAM結構 Fig. 2 The illustration of the basis CAM structure</p><p>圖選項</p><p>在對特徵圖進行處理的過程中，PAM和CAM並行操作，並將所得到的空間注意力特徵圖E<sub>p</sub>和通道注意力特徵圖E<sub>c</sub>進行加和運算，得到最終的注意力特徵圖。具體操作如圖 3所示。為了降低特徵維度減少計算量，首先對輸入特徵圖進行降維卷積操作，並將降維後的特徵圖並行輸入到兩個注意力模塊和卷積層中進行注意力機制處理和信息整合，最終將得到的特徵圖進行加和融和並輸出。</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUy9QnEx7dOI0><p>圖 3 PAM及CAM並行特徵處理 Fig. 3 The parallel feature maps convolution processing in PAM and CAM model</p><p>圖選項</p><p><strong class=highlight-text toutiao-origin=span>1.2 MAFU-Net原理及網絡結構</strong></p><p>語義分割網絡一般是通過多組卷積層對影像進行特徵提取，進而在深層次特徵圖上對類別進行劃分，而不同層次的語義特徵往往會有很大差別。圖 4(a)為原始光譜影像及地物類別標籤，圖 4(b)、(c)分別為淺層語義特徵及深層語義特徵的可視化。可以發現，圖 4(b)中的每張特徵圖均保留了原始影像的細節信息，並且所呈現的特徵皆不相同；圖 4(c)中的每張特徵圖所呈現的信息更加抽象，幾乎無法從中理解原圖像的含義。對比分析可知，淺層語義特徵保留了地物在細節方面的信息，而深層語義特徵則更多地捕獲了地物位置及類別語義信息。因此，將不同層次的特徵圖進行融和，可以使得特徵互補，進而改善分類精度。</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUy9RCFvukSB6><p>圖 4 U型網絡的深層特徵與淺層語義特徵可視化對比 Fig. 4 The visualization of the deep features and shallow features in U-shape network</p><p>圖選項</p><p>基於上述討論，本文提出的MAFU-Net整體結構如圖 5所示，主要分為兩部分：U型卷積網絡和注意力特徵提取網絡。圖 5左側的U型卷積網絡主要用於深層次特徵的提取和上採樣，是MAFU-Net的主要網絡骨架。為了最大限度地保留編碼網絡在降採樣過程中的重要特徵和地物細節信息，不同層次的特徵圖之間通過跳躍結構<sup>[11</sup><sup>]</sup>(skip connection)進行連接，並在通道維度上進行疊合操作。在編碼路徑中，每兩個3×3卷積層後接一個2×2的最大池化層，並且每個卷積層之後都進行批量歸一化和線性整流函數(RELU)激活操作。除此之外，每一次降採樣，特徵圖的通道數會增加一倍。在解碼路徑中，選用雙線性函數對特徵圖進行上採樣，每兩個3×3卷積層後接一個2×2的雙線性上採樣層，每一步的上採樣都會加入來自相對應編碼路徑的特徵圖。</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUyAGWBObRQKo><p>圖 5 嵌入雙注意力模塊的U型網絡結構 Fig. 5 The structure of the U-Net with attention module</p><p>圖選項</p><p>圖 5右側為注意力特徵提取網絡，目的是對高低層次疊合得到的特徵圖進行注意力機制處理和融合。由於卷積層的隨機初始化，所有特徵圖包含的信息是等價的，需要在學習的過程中不斷對卷積層參數和權重進行調整。然而，不同層次的語義信息差別較大，融合得到的特徵圖並不利於卷積層的學習。如果選擇注意力模塊來對特徵圖進行處理，將會使得網絡對特徵的學習有所側重，提高網絡的學習能力。卷積層與注意力模塊的作用機制如圖 6所示，卷積層是對所有特徵進行提取，包括冗餘特徵，如圖 6(a)所示；而注意力模塊則是有選擇地對某些相關性較大的特徵進行提取，如圖 6(b)所示。</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyAH26LmIc2W><p>圖 6 卷積層與注意力模塊的作用機制 Fig. 6 The mechanism diagram of convolutional layer and attention module</p><p>圖選項</p><p>MAFU-Net注意力特徵提取網絡的具體操作如式(3)</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyAHPDBBoV31><p>(3)</p><p>式中，<em>U</em><sub>1</sub>、<em>U</em><sub>2</sub>和<em>U</em><sub>3</sub>為U型網絡上採樣疊合後的3組深層次特徵圖，其中包含了不同層次的語義信息，每組特徵圖的尺寸和通道維度各不相同；<em>C</em><em>x</em>,<em>y</em>(·)代表卷積操作；<em>x</em>代表卷積核尺寸；<em>y</em>代表卷積操作的輸出通道數；<em>A</em>(·)代表雙注意力模塊操作；<em>S</em>(·)代表上採樣操作；<em>F</em>為3個層次語義信息融合後得到的特徵圖，<em>F</em>∈<em>R</em><em>N</em>×<em>H</em>×<em>W</em>，<em>N</em>為地物類別數。</p><p>在得到特徵圖<em>F</em>之後，對其進行softmax函數激活，輸出最終類別概率圖<em>P</em>，具體操作如式(4)</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyAHfDiyShC8><p>(4)</p><p>式中，f<sub>ij</sub>∈<em>F</em>；p<sub>ij</sub>∈<em>P</em>。</p><p>考慮到第1次上採樣得到的特徵圖尺寸小、語義特徵抽象、通道維度高，不適於進行特徵相關性檢測，故MAFU-Net沒有對其進行處理。此外，適當減少特徵圖的融合可以降低網絡複雜度和計算內存的佔用。</p><p><strong toutiao-origin=span>2 數據集及訓練細節</strong></p><p><strong class=highlight-text toutiao-origin=span>2.1 數據集</strong></p><p>為了對MAFU-Net進行測試，試驗選取了3組遙感影像數據集，分別是ISPRS的Vaihingen航空影像數據、北京和河南地區高分二號數據集<sup>[27</sup><sup>]</sup>，如圖 7所示。</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUyAJ5A7h9jXT><p>圖 7 Vaihingen，北京及河南地區高分二號數據集 Fig. 7 The Vaihingen datasets, Beijing and Henan datasets of GF 2</p><p>圖選項</p><p>ISPRS航空影像數據集包含16張已標記的假彩色航空影像(即包含近紅外、紅、綠3個通道)，影像尺寸介於1388×1281像素到2995×3007像素之間，地面分辨率均為0.09 m。每張影像均分為6類地物，即地面、高大植被、建築物、車輛、低矮植被和雜類，雜類中主要包含水域和集裝箱等。經過統計，該數據集中各地物像素佔比差別較大，例如水域等雜類的像素數量僅為地面像素數量的1/38。同時，各地物的尺寸也不盡相同，車輛類似於點狀地物，植被、建築物和地面等則接近面狀地物。</p><p>北京和河南地區遙感影像為紅、綠、藍3通道的真彩色影像，地面分辨率為4 m，影像尺寸均為6800×7200像素。河南地區數據集共包含4類地物，分別為水域、居民地、植被和背景區域；北京地區數據集在這4類地物基礎上額外標註了土路和公路共計6類地物，由於道路在影像中寬度僅為2~3個像素，近似為線狀，因此在地物分類當中檢測難度較高。</p><p>各數據集訓練、驗證和測試的比例分配及具體的地物類別見表 1。</p><p>表 1 3組數據集詳情Tab. 1 The details of the three datasets used in performance testing</p><table><thead><tr><td>數據集</td><td>ISPRS</td><td>北京</td><td>河南</td></tr></thead><tbody><tr><td>空間分辨率/m</td><td>0.09</td><td>4</td><td>4</td></tr><tr><td>地物類別</td><td>地面、雜類(水域)、高大植被、建築物、汽車、低矮植被</td><td>背景、植被、水域、居民地、公路、土路</td><td>背景、植被、水域、居民地</td></tr><tr><td>類別佔比</td><td>38:1:32:33:1.6:26</td><td>54:35:10:10:1:1</td><td>22:27:1:8.7</td></tr><tr><td>訓練、驗證和<br>測試集分配</td><td>1.5:1:1.3</td><td>7:1:2</td><td>7:1:2</td></tr></tbody></table><p>表選項</p><p><strong class=highlight-text toutiao-origin=span>2.2 訓練方法及環境配置</strong></p><p>為擴充訓練樣本，本文對Vaihingen數據集、北京和河南遙感數據集進行隨機旋轉，加噪和<em>γ</em>變換<sup>[28</sup><sup>]</sup>等圖像增強操作，分別生成了13 000、7000和6000張訓練樣本。此外，對驗證集進行隨機裁切分別生成5000、1300和2000張驗證樣本。鑑於設備條件限制，輸入樣本尺寸被設定為128×128像素。在將樣本輸入到網絡之前，對其進行了歸一化處理，以加快收斂和優化的進程。</p><p>在神經網絡訓練的過程中，損失函數直接決定著最終的訓練效果。多分類交叉熵損失函數在圖像語義分割任務當中經常被採用，然而該損失函數在樣本類別比例偏差較大的情況下，無法對小比例樣本進行兼顧。實際上，在圖像多分類任務當中，採用交併比(intersection-over-union，IoU)和平均交併比(mean intersection-over-union，mIoU)進行效果評價是一種更為合適的選擇。交併比反映的是預測值與真值二者的契合程度。對於<em>n</em>類地物，可以得到IoU<sub>1</sub>，IoU<sub>2</sub>，…，IoU<sub>n</sub>。之後，對其進行加和平均，求出平均交併比mIoU。</p><p>本文選擇Lovasz Softmax損失函數<sup>[29</sup><sup>]</sup>間接對IoU進行優化，IoU評價指標計算如式(5)</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyB8CIXq7vel><p>(5)</p><p>式中，<img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/SBUyB8eHd8pdTg><em>y</em><sup>*</sup>為標籤值；<em>c</em>為對應類別的編碼集合；∩為取交集操作；∪為取並集操作。定義Jaccard損失為</p><p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyB8t4JG2IfV>(6)</p><p>本文采用的Lovasz Softmax損失函數計算如式(7)</p><p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyB97AoKr0JH>(7)</p><p>式中，Jaccard損失函數為子模函數，<em>△</em>J<sub>c</sub>相當於把原子模函數的輸出值作為基進行插值；<em>m</em>(<em>c</em>)函數性質如下</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyB9H5yctEKD><p>(8)</p><p>試驗中，初始學習率<em>lr</em><sub>base</sub>設為0.000 5，設定學習率變化如下</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUyBtGBO3AmoW><p>(9)</p><p>式中，epoch為當前迭代次數；EPOCH為總迭代次數；試驗設置EPOCH=150，power=0.9。</p><p>神經網絡的訓練不僅需要對損失函數進行設計，還需要對超參數和優化算法進行選擇。經過試驗探索，本文采用Adam優化器(arXiv preprint arXiv: 1412.6980, 2014)對網絡進行訓練，對應的優化超參數為<em>β</em><sub>1</sub>=0.9，<em>β</em><sub>1</sub>=0.999，<em>ε</em>=10<sup>-8</sup>，批處理大小(batch size)為8。試驗中採用Windows下Pytorch機器學習框架，硬件環境為CPUInter(R)Xeon(R)E2176G，GPU RTX2080Ti，11 GB顯存。</p><p><strong toutiao-origin=span>3 試驗及結果分析</strong></p><p>為了驗證MAFU-Net的有效性，將其與Refinenet<sup>[14</sup><sup>]</sup>、Encnet<sup>[15</sup><sup>]</sup>、Denseaspp<sup>[16</sup><sup>]</sup>、Atlention-Unet、RAV-Net和DANet<sup>[27</sup><sup>]</sup>進行對比，其中文獻[27]中的DANet選用了兩種結構進行對照，即單一損失結構和輔助損失結構，分別以DANet和DANet<sup>*</sup>表示。此外，本文加入了消融試驗用來探究PAM模塊和CAM模塊對MAFU-Net的分類結果影響。</p><p><strong class=highlight-text toutiao-origin=span>3.1 數據集測試及試驗結果定性分析</strong></p><p>本文提出的MAFU-Net與上述文獻中的網絡在3組數據集上的預測結果及細節如圖 8所示。為了更加全面地比較各種方法的分類結果，從Vaihingen數據集中分別選出兩個典型場景進行分析，分別記為場景1~2。其中，場景1為包含水域的影像，用來探究各網絡在地物類別不均衡條件下的分類性能；場景2包含大型建築物和車輛，用來檢測網絡對大尺寸地物的區分能力以及對小尺寸地物的細節分割能力，每種場景影像、標籤及各方法預測結果如圖 8(a)和(b)列所示。</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyBtlO3B7Ks><p>圖 8 8種網絡在Vaihingen及北京、河南地區高分二號數據集上的結果對比 Fig. 8 The predicted results of the eight networks on Vaihingen, Beijing and Henan datasets</p><p>圖選項</p><p>對於場景1，DANet、Denseaspp、Encnet和Att-UNet 4種方法均未檢測出水域部分，而DANet<sup>*</sup>、Refinenet、RAU-Net和MAFU-Net則成功檢測出水域。這說明MAFU-Net使用的注意力模塊和Refinenet、RAU-Net所使用的殘差單元緩解了過擬合，提高了地物類別檢測的穩健性。對於場景2，Encnet和Denseaspp在大型建築物上的檢測效果明顯優於MAFU-Net，這主要是因為Encnet和Denseaspp中使用了擴張卷積和多尺度策略，這種卷積操作可以明顯增大感受野，同時多尺度策略可以提高不同尺寸地物的檢測精度。MAFU-Net僅僅利用網絡輸出的卷積特徵圖進行操作，沒有對感受野範圍進行擴充，因此對於大型地物的檢測在完整性上有所欠缺。但是對於小尺寸或難區分地物的檢測，MAFU-Net具有明顯的優勢。例如場景2中的圓圈區域，有兩輛汽車，由於光線問題，一輛汽車被陰影遮擋，很難進行分辨，其餘方法均未成功檢測。但MAFU-Net則較為完整地將該車輛預測並標記，說明注意力模塊可以很好地剔除冗餘信息、抵抗干擾，通過強相關性找到有價值的地物特徵線索。圖 9(a)為該幅影像的局部放大示意圖，通過亮度調整和對比度拉伸，可以明顯看到該處有車輛，若是單純通過人眼進行辨別，則很容易將該車輛漏檢。值得一提的是，DANet<sup>*</sup>在兩個場景中的檢測效果均明顯優於DANet，這主要是由於DANet<sup>*</sup>中的輔助監督策略強化了細節方面的特徵，在一定程度上維持了DANet下采樣所丟失的信息。</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUyBvN6AR8Og3><p>圖 9 場景2和場景5中的局部放大細節示意 Fig. 9 The local magnification details in scene 2 and scene 5</p><p>圖選項</p><p>圖 8(c)—(d)為北京數據集中選取的兩幅場景及對應檢測結果，分別記為場景3、場景4。從場景3可以發現，RAU-Net、Att-UNet和MAFU-Net在道路的檢測上效果明顯優於其他方法，這說明以U-Net結構作為基礎骨架，可以有效地檢測線狀地物，這也證實了高低級語義信息融和在細節特徵提取中的重要性。此外，該場景中方形框選區域中的居民地，大部分方法均有誤檢，而MAFU-Net檢測結果則相對準確。場景4中的背景環境和紋理特徵相對複雜，給水域和公路的檢測帶來很大難度。例如，Encnet對於該場景中水域區域有很大程度的漏檢，DANet<sup>*</sup>對於橫跨水域的公路也檢測失效。Refinenet網絡在該數據集上的檢測效果最差。相比之下，MAFU-Net對於公路和水域的檢測效果較為穩健，提取的分類圖較為完整清晰。</p><p>對於河南數據集，由於影像中地物細節少、紋理特徵相對單一、地物類別劃分簡單，各方法在該數據集上的分類精度要明顯高於前兩組數據集。對於水域部分，幾種方法得到的結果差別並不明顯；對於植被部分，由於其分佈範圍廣，很難在影像中進行客觀的評價。因此，本文從中選擇了一幅包含居民區的場景進行分析，記為場景5，相應的影像、標籤及分類結果如圖 8(e)所示。</p><p>對於場景5，參考文獻中的方法均將圓圈中的背景區域誤檢為居民區，而MAFU-Net則可以準確地將該區域預測為背景，其放大細節示意圖如圖 9(b)所示。從放大細節中會發現，該部分地物與其他居民區差別較大，經過實地勘察，該區域為一家奶牛場，屬於工業用地，並不屬於居民區範疇。</p><p><strong class=highlight-text toutiao-origin=span>3.2 試驗結果定量分析</strong></p><p>為了對各分割結果進行定量評價，本文選用交併比作為類別評價指標，選擇平均交併比作為總體評價指標。3組數據集上的評價結果分別見表 2、表 3和表 4。</p><p>表 2 不同方法在Vaihingen數據集上的交併比和平均交併比Tab. 2 The experimental results of IoU and mIoU on Vaihingen dataset in different method</p><table><thead><tr><td>方法</td><td>地面</td><td>高大植被</td><td>建築物</td><td>車輛</td><td>低矮植被</td><td>水域等雜類</td><td>mIoU</td></tr></thead><tbody><tr><td>DANet</td><td>71.61</td><td>71.67</td><td>80.34</td><td>—</td><td>58.15</td><td>—</td><td>46.96</td></tr><tr><td>DANet<sup>*</sup></td><td>74.20</td><td>71.80</td><td>80.39</td><td>53.92</td><td>59.62</td><td>35.89</td><td>62.62</td></tr><tr><td>Denseaspp</td><td>76.07</td><td>71.97</td><td><strong>83.66</strong></td><td>58.37</td><td>59.64</td><td>—</td><td>58.28</td></tr><tr><td>Encnet</td><td><strong>76.48</strong></td><td><strong>72.60</strong></td><td><strong toutiao-origin=span>83.07</strong></td><td><strong toutiao-origin=span>64.96</strong></td><td>59.88</td><td>—</td><td>59.50</td></tr><tr><td>Refinenet</td><td>74.96</td><td>71.31</td><td>80.70</td><td>51.21</td><td>59.52</td><td><strong>43.41</strong></td><td><strong toutiao-origin=span>63.52</strong></td></tr><tr><td>RAU-Net</td><td>75.10</td><td>71.87</td><td>82.01</td><td>50.20</td><td>58.53</td><td><strong toutiao-origin=span>39.45</strong></td><td>62.86</td></tr><tr><td>Att-UNet</td><td>73.64</td><td>71.67</td><td>80.66</td><td>49.46</td><td><strong toutiao-origin=span>60.54</strong></td><td>—</td><td>55.99</td></tr><tr><td>MAFU-Net</td><td><strong toutiao-origin=span>76.14</strong></td><td><strong toutiao-origin=span>72.17</strong></td><td>82.20</td><td><strong>65.46</strong></td><td><strong>60.58</strong></td><td>30.19</td><td><strong>64.46</strong></td></tr></tbody><tfoot><tr><td colspan=8>注：加粗字體為每列最優值，加下劃線字體為每列次優值，—代表預測失效。</td></tr></tfoot></table><p>表選項</p><p>表 3 不同方法在北京數據集上的交併比和平均交併比Tab. 3 The experimental results of IoU and mIoU on Vaihingen dataset in different method</p><table><thead><tr><td>方法</td><td>植被</td><td>水域</td><td>居民地</td><td>公路</td><td>小路</td><td>背景</td><td>mIoU</td></tr></thead><tbody><tr><td>DANet</td><td>67.46</td><td>88.33</td><td>71.11</td><td>22.75</td><td>35.89</td><td>58.90</td><td>57.41</td></tr><tr><td>DANet<sup>*</sup></td><td><strong toutiao-origin=span>71.58</strong></td><td>89.02</td><td><strong toutiao-origin=span>73.29</strong></td><td>23.31</td><td>35.87</td><td>59.77</td><td>58.80</td></tr><tr><td>Denseaspp</td><td>63.58</td><td><strong toutiao-origin=span>90.45</strong></td><td>70.18</td><td>28.23</td><td>45.13</td><td>56.48</td><td>59.01</td></tr><tr><td>Encnet</td><td><strong>73.14</strong></td><td>89.15</td><td>73.56</td><td>27.38</td><td>44.00</td><td><strong toutiao-origin=span>62.21</strong></td><td><strong toutiao-origin=span>61.57</strong></td></tr><tr><td>Refinenet</td><td>66.53</td><td>86.28</td><td>65.39</td><td>9.58</td><td>8.64</td><td>48.98</td><td>47.58</td></tr><tr><td>RAU-Net</td><td>62.22</td><td>88.30</td><td>52.51</td><td>32.75</td><td>42.03</td><td>51.29</td><td>54.85</td></tr><tr><td>Att-UNet</td><td>55.65</td><td>79.08</td><td>73.02</td><td><strong toutiao-origin=span>47.70</strong></td><td><strong toutiao-origin=span>49.54</strong></td><td>54.14</td><td>59.01</td></tr><tr><td>MAFU-Net</td><td>71.35</td><td><strong>91.20</strong></td><td><strong>76.00</strong></td><td><strong>47.91</strong></td><td><strong>50.36</strong></td><td><strong>62.39</strong></td><td><strong>66.53</strong></td></tr></tbody><tfoot><tr><td colspan=8>注：加粗字體為每列最優值，加下劃線字體為每列次優值。</td></tr></tfoot></table><p>表選項</p><p>表 4 不同方法在河南數據集上的交併比和平均交併比Tab. 4 The experimental results of IoU and mIoU on Henan dataset of GF2 in different method</p><table><thead><tr><td>方法</td><td>植被</td><td>水域</td><td>居民區</td><td>背景</td><td>mIoU</td></tr></thead><tbody><tr><td>DANet</td><td>82.91</td><td>89.79</td><td>68.45</td><td>52.86</td><td>73.50</td></tr><tr><td>DANet<sup>*</sup></td><td>82.55</td><td>89.38</td><td>68.73</td><td>51.32</td><td>72.99</td></tr><tr><td>Denseaspp</td><td>84.92</td><td><strong>91.90</strong></td><td>69.02</td><td>56.83</td><td>75.67</td></tr><tr><td>Encnet</td><td>84.54</td><td>91.69</td><td><strong>71.00</strong></td><td>58.16</td><td>76.35</td></tr><tr><td>Refinenet</td><td>82.39</td><td>90.26</td><td>68.27</td><td>53.34</td><td>73.57</td></tr><tr><td>RAU-Net</td><td>83.51</td><td>90.48</td><td>68.93</td><td>52.28</td><td>73.80</td></tr><tr><td>Att-UNet</td><td><strong>86.88</strong></td><td>90.37</td><td>70.21</td><td><strong>62.54</strong></td><td><strong toutiao-origin=span>77.45</strong></td></tr><tr><td>MAFU-Net</td><td><strong toutiao-origin=span>86.67</strong></td><td><strong toutiao-origin=span>91.74</strong></td><td><strong toutiao-origin=span>70.72</strong></td><td><strong toutiao-origin=span>60.98</strong></td><td>77.53</td></tr></tbody><tfoot><tr><td colspan=6>注：加粗字體為每列最優值，加下劃線字體為每列次優值。</td></tr></tfoot></table><p>表選項</p><p>表 2為不同方法在Vaihingen數據集上的評價指標，分析可知，DANet<sup>*</sup>在引入了輔助損失對深度特徵進行監督後，相比於DANet，各類地物的檢測精度均有所提升；Denseaspp對建築物的分類交併比達到了83.66%，高於其他方法，但對於小比例樣本預測仍然會失效；Encnet在地面和高大植被的檢測上達到了不錯的精度，在建築物和車輛上的檢測也達到了次優值，但在小比例樣本的預測上卻全部漏檢；Refinenet在總體的分類性能上達到了次優值，mIoU達到了63.52%，並且成功將雜類進行了區分；Att-Unet在低矮植被的檢測上達到了次優值，但水域等雜類仍被漏檢。本文提出的MAFU-Net在所有地物類別的分類上較為均衡。其中，對於地面和高大植被的分類交併比都達到了次優值，在車輛和低矮植被的檢測上則達到了最優值，其交併比分別為65.46%和60.58%，說明MAFU-Net對於小尺寸地物檢測具有較好的效果，同時對於難檢測和難區分的地物仍然可以達到不錯的分類精度。</p><p>表 3為各方法在北京地區高分二號數據集上的評價指標。總體而言，本文提出的MAFU-Net在該數據集上取得了較為理想的分類結果，尤其在水域、居民地、公路和小路等典型地物和難檢測地物的分類上，均取得了最優值，總體的精度高於次優值近5%。除此之外，可以發現基於U-Net改進的網絡在線狀地物的檢測上，精度普遍高於其他方法。</p><p>表 4為各方法在河南地區高分二號數據集上的評價指標。在該數據集中，DANet和DANet*的總體精度較低，不及其他方法。Denseaspp在水域的檢測中達到了比較好的效果，而Encnet在居民區的檢測中達到了最優值。Refinenet網絡在該數據集上的表現相對較差，每種地物的檢測精度都偏低。相比之下，MAFU-Net對各類地物的分類結果均達到了次優值，並且總體精度mIoU達到了77.53%，高於其他方法。</p><p>經過以上分析，可以得出如下結論：①高低級語義信息的融和對遙感影像地物分類是極其重要的，尤其對細節特徵較多的地物，多級語義信息融和更為關鍵；②注意力模塊可以提高小尺寸地物和難區分地物的檢測能力；③基於U-Net改進的注意力機制網絡，在線狀地物的檢測上具有明顯的優勢；④MAFU-Net可以適應不同分辨率的遙感影像，並且對於不同類型地物的分類都具有很好的穩健性。</p><p><strong class=highlight-text toutiao-origin=span>3.3 模型複雜度及輕量化分析</strong></p><p>理論上，適當增加網絡模型的深度，可以提取到更深層次的語義特徵，對於網絡的分類往往更加有利。然而，當網絡模型加深的同時，參數量會不斷增大，模型的計算複雜度(計算複雜度是指在單個樣本的輸入下，模型進行一次完整的前向傳播所發生的浮點運算個數，本試驗中選擇尺寸為128×128像素的影像作為單個樣本進行複雜度計算)也會大幅增加，即在預測的過程中會消耗更多的資源。因此，在實際使用中，需要在模型的分類精度與計算複雜度上進行權衡。實際上，計算複雜度的大小並不代表預測時間的長短，網絡前向傳播一次的時間還取決於計算平臺的帶寬上限和平臺算力，本文試驗中選用的計算平臺為RTX2080Ti。本文中模型的參數量及計算複雜度是通過Github開源的PyTorch-OpCounter模塊進行統計得到，預測時間是在計算硬件和平臺完全相同的情況下，以Vaihingen測試集預測為任務進行統計。</p><p>表 5列出了各模型的參數指標，包括參數量、計算複雜度和在Vaihingen測試集上的預測時間。可以看出，Denseaspp的輕量化水平最高，參數量和計算複雜度都明顯低於其他方法，但預測時間較長；RAU-Net的輕量化水平低於Denseaspp，但預測時間短；本文提出的MAFU-Net在輕量化水平上與RAU-Net相當，預測效率高於RAU-Net近10%。由於PyTorch-OpCounter模塊在統計計算複雜度的過程中，僅統計了加乘運算操作，並未統計模型中通道重排和通道疊加操作，而通道重排和通道疊加雖然不需要進行浮點運算，但卻需要計算機中的內存數據交換，導致計算複雜度與預測時間不成正比。因此，在實際使用中，不能僅僅依靠計算複雜度來對模型進行輕量化判定，還需要在實際任務中，對模型預測效率進行評價。</p><p>表 5 不同網絡的模型參數量、計算複雜度及Vaihingen測試集的預測時間Tab. 5 GFLOPS-parameters-inference time of different models on Vaihingen test dataset</p><table><thead><tr><td>模型</td><td>參數量<br>/MB</td><td>計算複雜度<br>(GFLOPS)</td><td>預測時間<br>/s</td></tr></thead><tbody><tr><td>DANet</td><td>49.49</td><td>12.48</td><td>671.03</td></tr><tr><td>DANet<sup>*</sup></td><td>49.49</td><td>12.48</td><td>650.71</td></tr><tr><td>Denseaspp</td><td><strong>10.20</strong></td><td><strong>3.11</strong></td><td>1 403.78</td></tr><tr><td>Encnet</td><td>38.03</td><td>9.81</td><td>1 115.38</td></tr><tr><td>Refinenet</td><td>113.88</td><td>16.05</td><td>1 570.16</td></tr><tr><td>RAU-Net</td><td><strong toutiao-origin=span>13.40</strong></td><td><strong toutiao-origin=span>8.64</strong></td><td>507.75</td></tr><tr><td>Att-UNet</td><td>34.88</td><td>18.76</td><td><strong>326.92</strong></td></tr><tr><td>MAFU-Net</td><td>13.89</td><td>8.84</td><td><strong toutiao-origin=span>466</strong></td></tr></tbody><tfoot><tr><td colspan=4>注：加粗字體為每列最優值，加下劃線字體為每列次優值。</td></tr></tfoot></table><p>表選項</p><p>各個網絡的參數指標可視化如圖 10所示，其中圓圈的半徑大小代表預測時間的長短，圓圈顏色的深淺程度代表參數量大小。DANet由於缺乏對多級語義信息的處理，丟失了細節，導致雜類及車輛類別誤判，整體精度遠不及其他方法，但當加入了輔助損失後，分類精度大幅提升。Refinenet、MAFU-Net、RAU-Net和DANet<sup>*</sup>雖然在精度上大體相同，但是Refinenet在計算複雜度、參數量和預測時間上則與後者有著較大的差距。總體而言，本文提出的MAFU-Net具有較低的參數量和計算複雜度，同等情況下可以利用少量的資源達到較高的精度，更具實用性。</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/SBUyBvr4kT7OxC><p>圖 10 模型精度mIoU-GFLOPS-參數量-預測時間關係圖(Vaihingen數據集) Fig. 10 GFLOPS-parameters-inference time-mIoU performance of different networks on Vaihingen dataset</p><p>圖選項</p><p><strong class=highlight-text toutiao-origin=span>3.4 消融試驗及可視化分析</strong></p><p>為了探究PAM和CAM模塊對MAFU-Net的影響以及作用機制，本文額外進行了消融試驗和特徵可視化分析。</p><p>從Vaihingen分類結果中選取兩幅場景進行分析，其光譜圖像和標籤如圖 11(a)、(b)所示，CAM模塊和PAM模塊單獨作用得到的分類結果如圖 11(c)、(d)所示，圖 11(e)為MAFU-Net得到的分類結果，即CAM和PAM模塊共同作用得到的分類圖。通過對分類圖進行對比，可以發現PAM作用得到的分類結果輪廓較為清晰，例如圖 11第1幅場景中的建築物區域，PAM作用得到的分類結果較為完整，無過多誤檢，而CAM作用得到的結果則較為雜亂，誤檢率較高；第2幅場景中車輛較多，分類難度相對較大，CAM單獨作用的結果明顯優於PAM，說明CAM模塊在小尺寸地物檢測上的作用大於PAM。MAFU-Net結合了CAM和PAM模塊的優勢，因此達到了最佳的分類效果。</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUyBwLD4iesC><p>圖 11 消融試驗分類結果對比 Fig. 11 The classification results of ablation study</p><p>圖選項</p><p>以上是通過最終的分類結果對CAM和PAM模塊的作用進行的初步分析，至於其在特徵處理上的具體功能還無法得知。為了進一步分析兩種注意力模塊的作用機制，需要分別對CAM和PAM模塊處理後的特徵圖進行可視化。為了更直觀地體現特徵圖的特點，應選取尺寸較大的特徵圖組進行分析。由MAFU-Net網絡結構可知，特徵圖組U<sub>2</sub>經過注意力特徵操作後得到的特徵圖尺寸較大，更利於理解和分析。</p><p>試驗選用圖 4中的光譜影像進行前向傳播生成特徵圖，PAM和CAM模塊分別處理後可得到兩組特徵圖，每組16張，尺寸為64×64。可視化結果如圖 12所示。圖 12(a)為CAM模塊處理後得到的特徵圖，可以發現CAM模塊主要是對地物類別信息進行了強化，各個通道間的信息相互關聯，語義更加明確；圖 12(b)為PAM模塊處理得到的特徵圖，該特徵圖對地物的邊緣信息進行了強化，這說明PAM模塊通過像素注意力相關性處理，成功提取了地物的邊緣輪廓，使得地物分類結果更加清晰。這也從另一方面說明了同種地物邊緣有著相似特徵，在進行像素相關性計算的過程中，邊緣像素在理論上會產生相近的響應強度。</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/SBUyCuJDFHcqt3><p>圖 12 CAM和PAM模塊特徵處理可視化 Fig. 12 The visualization of the processed feature by CAM and PAM module</p><p>圖選項</p><p>經過以上分析可知，CAM模塊通過通道相關性處理，強化了地物類別語義信息；PAM模塊通過長距離像素相關性處理，更多地提取了類別邊緣特徵，使得地物分類結果更加精細。</p><p><strong toutiao-origin=span>4 結論</strong></p><p>本文結合U型網絡的不同層次語義信息融和思想和注意力機制，提出了一種高效輕量化語義分割網絡MAFU-Net，提高了遙感影像高低級語義特徵融和的性能和語義信息相關性處理能力。筆者分別在Vaihingen數據集以及北京、河南地區高分二號數據集上測試，結果表明，MAFU-Net總體分類性能要優於目前主流的語義分割網絡，並且在參數量、計算複雜度以及預測時間上都有較大的優勢。此外，通過消融試驗和可視化分析，發現雙注意力機制的CAM和PAM模塊是通過對語義信息及邊緣信息分別進行了強化，從而達到改善分類精度的目的。</p><p>在語義分割網絡中加入注意力機制可以大幅改善影像的分類性能，但MAFU-Net使用的雙注意力模塊在一定程度上會增加計算量，這主要是模塊內部特徵相關性計算所導致。因此，未來可以對雙注意力模塊進行改進，即將通道維度和像素維度的相關性計算放到同一個模塊中進行，進一步降低網絡的整體參數量和計算複雜度，減少計算過程中內存的佔用。此外，可以將空洞卷積和多尺度結構加入網絡中，增大感受野範圍，以適應不同尺寸的遙感地物，提升遙感影像地物分類的能力。</p><p>最後，本文提出的MAFU-Net在影像分類方面的大量試驗結果及分析表明，深度學習網絡雖然有其優勢，但也有不足之處。首先，深度學習方法對樣本依賴性較強；其次，深度學習網絡的搭建需要不斷地嘗試才能找到適合某任務的網絡結構，例如經典的ResNet系列，VGG系列網絡都是通過提出者不斷嘗試和分析後才找到的最佳網絡結構；最後，深度學習內部的特徵提取較為繁雜，很難用準確的數學原理進行推導，換句話說，神經網絡對於使用者，甚至於設計者來說，相當於一個“黑匣子”，很難說清網絡內部的學習機制，也就無法解釋某網絡為何在某種數據集中會失效。因此，神經網絡所表現出的分類效果，在一定程度上，反映了網絡的某些特性，但如果深究其內部原因，可能還需要一定時間進行探索，或者通過更加高級的可視化表達或者量化準則來進行分析評價。</p><p><strong toutiao-origin=span>作者簡介</strong></p><p>第一作者簡介：李道紀(1994-), 男, 碩士生, 研究方向為遙感影像地物分類。E-mail:wang111@alumni.sjtu.edu.cn</p><p>通信作者：盧俊, E-mail：ljhb45@126.com</p><p><strong toutiao-origin=span>團隊簡介</strong></p><p>郭海濤副教授團隊長期從事數字攝影測量、機載激光雷達點雲處理、遙感影像目標檢測、變化檢測等方面的研究，有100餘篇論文發表在《IEEE Geoscience and Remote Sensing Letters》、《International Journal of Remote Sensing》、《Journal of Applied Remote Sensing》、《測繪學報》、《遙感學報》、《中國圖象圖形學報》、《武漢大學學報•信息科學版》、《光學精密工程》、《測繪科學技術學報》等中英文刊物上。</p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RN4Licj7bGr0PK><p></p><h1 toutiao-origin=h1>《測繪學報（英文版）》（JGGS）專刊徵稿：LiDAR數據處理</h1><p><strong>論文推薦 | 鄭鑫,潘斌,張健：可變形網絡與遷移學習相結合的電力塔遙感影像目標檢測法</strong></p><p><strong>資訊 | 2020年度海洋科學技術獎擬提交終評項目名單/海洋優秀科技圖書擬提交終評項名單</strong></p><p><strong>院士論壇 | 郭毅可院士：人工智能的熱望與冷思考</strong></p><img alt="論文推薦 | 李道紀，郭海濤，盧俊，趙傳，林雨準，餘東行：遙感影像地物分類多注意力融和U型網絡法" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RwfZ0qO5B9LQb6><p>權威 | 專業 | 學術 | 前沿</p><p>微信、抖音小視頻投稿郵箱 | song_qi_fan@163.com</p><p>歡迎加入《測繪學報》作者QQ群：<strong> 751717395</strong></p><p>進群請備註：姓名+單位+稿件編號</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>論文</a></li><li><a>推薦</a></li><li><a>李道紀</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/e55aaf9b.html alt="論文推薦 | 袁修孝：航攝影像密集匹配的研究進展與展望" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/R6Ieh75DBRtmcY style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e55aaf9b.html title="論文推薦 | 袁修孝：航攝影像密集匹配的研究進展與展望">論文推薦 | 袁修孝：航攝影像密集匹配的研究進展與展望</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4653cf8e.html alt=論文推薦｜王濤：國產機載大視場三線陣CCD相機GNSS偏心矢量和IMU視軸偏心角標定技術 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/R6Ieh75DBRtmcY style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4653cf8e.html title=論文推薦｜王濤：國產機載大視場三線陣CCD相機GNSS偏心矢量和IMU視軸偏心角標定技術>論文推薦｜王濤：國產機載大視場三線陣CCD相機GNSS偏心矢量和IMU視軸偏心角標定技術</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5718fd72.html alt="論文推薦 | 閆廣峰：L1範數探測粗差失效的觀測量識別方法" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/R6W0QpMHySg0Qb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5718fd72.html title="論文推薦 | 閆廣峰：L1範數探測粗差失效的觀測量識別方法">論文推薦 | 閆廣峰：L1範數探測粗差失效的觀測量識別方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d2c7df1e.html alt=「論文推薦」左建平教授談岩層移動研究進展及重點 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RmTmvrAHAwNOUb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d2c7df1e.html title=「論文推薦」左建平教授談岩層移動研究進展及重點>「論文推薦」左建平教授談岩層移動研究進展及重點</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6d77a0e0.html alt=「論文推薦」郭廣禮等：無井式煤炭地下氣化岩層及地表移動與控制 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ReafrDd7EHXniF style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6d77a0e0.html title=「論文推薦」郭廣禮等：無井式煤炭地下氣化岩層及地表移動與控制>「論文推薦」郭廣禮等：無井式煤炭地下氣化岩層及地表移動與控制</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c63c80c1.html alt="論文推薦 | 劉照欣：高光譜亞像元定位的線特徵探測法" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/R6W0QpMHySg0Qb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c63c80c1.html title="論文推薦 | 劉照欣：高光譜亞像元定位的線特徵探測法">論文推薦 | 劉照欣：高光譜亞像元定位的線特徵探測法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/be8eee7a.html alt=「論文推薦」狄軍貞​等：粒徑對煤矸石汙染物溶解釋放規律影響研究 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/S0q6oVj53DcDnj style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/be8eee7a.html title=「論文推薦」狄軍貞​等：粒徑對煤矸石汙染物溶解釋放規律影響研究>「論文推薦」狄軍貞​等：粒徑對煤矸石汙染物溶解釋放規律影響研究</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7f16a422.html alt="論文推薦| 李宗春：一種顧及現勢指向的上行天線陣相位中心精確標校方法" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/R6Ieh75DBRtmcY style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7f16a422.html title="論文推薦| 李宗春：一種顧及現勢指向的上行天線陣相位中心精確標校方法">論文推薦| 李宗春：一種顧及現勢指向的上行天線陣相位中心精確標校方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6f4959cd.html alt="論文推薦| 林秀秀:極區慣導編排中地球近似模型的適用性分析" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/R6W0QpMHySg0Qb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6f4959cd.html title="論文推薦| 林秀秀:極區慣導編排中地球近似模型的適用性分析">論文推薦| 林秀秀:極區慣導編排中地球近似模型的適用性分析</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a6b0d6e2.html alt="論文推薦| 皮英冬:利用稀少控制點的線陣推掃式光學衛星在軌幾何定標方法" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/R6W0QpMHySg0Qb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a6b0d6e2.html title="論文推薦| 皮英冬:利用稀少控制點的線陣推掃式光學衛星在軌幾何定標方法">論文推薦| 皮英冬:利用稀少控制點的線陣推掃式光學衛星在軌幾何定標方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b5a90f38.html alt="論文推薦| 閆利：SLAM激光點雲整體精配準位姿圖技術" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/R6W0QpMHySg0Qb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b5a90f38.html title="論文推薦| 閆利：SLAM激光點雲整體精配準位姿圖技術">論文推薦| 閆利：SLAM激光點雲整體精配準位姿圖技術</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c7555ef5.html alt=論文推薦｜姚宜斌：顧及設計矩陣誤差的AR模型新解法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/53410004acf9b032d928 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c7555ef5.html title=論文推薦｜姚宜斌：顧及設計矩陣誤差的AR模型新解法>論文推薦｜姚宜斌：顧及設計矩陣誤差的AR模型新解法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f22ea791.html alt=論文推薦｜楊幸彬：高分辨率遙感影像DSM的改進半全局匹配生成方法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/R6W0QpMHySg0Qb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f22ea791.html title=論文推薦｜楊幸彬：高分辨率遙感影像DSM的改進半全局匹配生成方法>論文推薦｜楊幸彬：高分辨率遙感影像DSM的改進半全局匹配生成方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7c51035d.html alt=論文推薦｜邢志斌：我國陸海統一似大地水準面構建的三維重力矢量法 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/15294875207731e288b7418 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7c51035d.html title=論文推薦｜邢志斌：我國陸海統一似大地水準面構建的三維重力矢量法>論文推薦｜邢志斌：我國陸海統一似大地水準面構建的三維重力矢量法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d07bf9bc.html alt=論文推薦｜馬下平：ITRF中GNSS/SLR並址站歸心基線的“一步解” class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/53350006726e50ef72f9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d07bf9bc.html title=論文推薦｜馬下平：ITRF中GNSS/SLR並址站歸心基線的“一步解”>論文推薦｜馬下平：ITRF中GNSS/SLR並址站歸心基線的“一步解”</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>