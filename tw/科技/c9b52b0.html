<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習入門第1章：監督學習和樸素貝葉斯分類-第2部分（編碼） | 极客快訊</title><meta property="og:title" content="機器學習入門第1章：監督學習和樸素貝葉斯分類-第2部分（編碼） - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/a6894d2d1ea64a8eb3bad2b892648639"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c9b52b0.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c9b52b0.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/c9b52b0.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c9b52b0.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c9b52b0.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/c9b52b0.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/c9b52b0.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/c9b52b0.html><meta property="article:published_time" content="2020-10-29T21:07:51+08:00"><meta property="article:modified_time" content="2020-10-29T21:07:51+08:00"><meta name=Keywords content><meta name=description content="機器學習入門第1章：監督學習和樸素貝葉斯分類-第2部分（編碼）"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/c9b52b0.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習入門第1章：監督學習和樸素貝葉斯分類-第2部分（編碼）</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p><br></p><div class=pgc-img><img alt=機器學習入門第1章：監督學習和樸素貝葉斯分類-第2部分（編碼） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a6894d2d1ea64a8eb3bad2b892648639><p class=pgc-img-caption>The question of whether a computer can think is no more interesting than the question of whether a s</p></div><p><br></p><p>注意：如果您沒有閱讀過第一部分，即樸素貝葉斯理論，我建議您通讀它。</p><p>在這一部分中，我們將探索sklearn庫。 python中的sklearn提供了準備使用流行的機器學習算法，例如Naive Bayes。 （它涉及其他人，例如SVM，這將在以後的文章中介紹）。 有了這個，您就不必手動編碼自己的Naive Bayes實現。</p><div class=pgc-img><img alt=機器學習入門第1章：監督學習和樸素貝葉斯分類-第2部分（編碼） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/21531acc949d4084a774a2de9a017973><p class=pgc-img-caption>Give a man a program, frustrate him for a day.Teach a man to program, frustrate him for a lifetime.</p></div><h1 class=pgc-h-arrow-right><strong>編碼練習</strong></h1><p>在本練習中，我們將使用標記為"垃圾郵件"或"非垃圾郵件"的電子郵件集訓練模型。 有702封電子郵件，均分為垃圾郵件和非垃圾郵件類別。 接下來，我們將在260封電子郵件中測試該模型。 我們將要求模型預測此電子郵件的類別，並將準確性與我們已經知道的正確分類進行比較。</p><p>這是文本數據挖掘的經典示例。</p><h1 class=pgc-h-arrow-right><strong>先決條件</strong></h1><p>本教程假定編碼工作是在基於Debian的Linux上完成的。 安裝說明可能會因所使用的操作系統而異，但是python代碼保持不變。</p><p>· 安裝Python。</p><p>· 安裝Pip。</p><p>· 為python安裝sklearn：pip install scikit-learn</p><p>· 安裝numpy：pip install numpy</p><p>· 安裝SciPy：pip install scipy</p><h1 class=pgc-h-arrow-right><strong>0.下載</strong></h1><p>我已經為數據集和示例代碼創建了一個git存儲庫。 。 本章討論了相同的數據集。 我建議您繼續討論並自己編寫代碼。 萬一它失敗了，您可以使用/參考我的版本來了解工作。</p><h1 class=pgc-h-arrow-right><strong>1.清潔和準備數據</strong></h1><p>我們有兩個文件夾test-mail和train-mail。 我們將使用培訓郵件來訓練模型。 樣本電子郵件數據集如下所示：</p><blockquote class=pgc-blockquote-abstract><p>Subject: re : 2 . 882 s - > np np> deat : sun , 15 dec 91 2 : 25 : 2 est > : michael &lt; mmorse @ vm1 . yorku . ca > > subject : re : 2 . 864 query > > wlodek zadrozny ask " anything interest " > construction " s > np np " . . . second , > much relate : consider construction form > discuss list late reduplication ? > logical sense " john mcnamara name " tautologous thus , > level , indistinguishable " , , here ? " . ' john mcnamara name ' tautologous support those logic-base semantics irrelevant natural language . sense tautologous ? supplies value attribute follow attribute value . fact value name-attribute relevant entity ' chaim shmendrik ' , ' john mcnamara name ' false . tautology , . ( reduplication , either . )</p></blockquote><p><br></p><p>第一行是主題，內容從第三行開始。</p><p>如果您導航到任何培訓郵件或測試郵件，您將看到兩種格式的文件名</p><pre><code>number-numbermsg[number].txt : example 3-1msg1.txt (this are non spam emails)ORspmsg[Number].txt : example spmsga162.txt (these files are of spam emails).</code></pre><p><br></p><p>文本數據挖掘任務的第一步是清理並準備模型數據。 在清潔過程中，我們從文本中刪除了不需要的單詞，表達式和符號。</p><p>考慮一個文本：</p><blockquote class=pgc-blockquote-abstract><p>“Hi, this is Alice. Hope you are doing well and enjoying your vacation.”</p></blockquote><p>這裡的"是"，"這個"，"是"等字樣並沒有真正有助於分析。 這樣的單詞也稱為停用詞。因此，在本練習中，我們僅考慮電子郵件中最常見的3000個單詞。 以下是代碼段。</p><p>清理完我們需要的每個電子郵件文檔之後，我們應該以單詞頻率的某種矩陣表示形式。</p><p>例如，如果文檔包含文本："嗨，這是愛麗絲。 愛麗絲生日快樂"清洗後，我們想要一些東西</p><pre><code>word      :   Hi this is Alice Happy Birthdayfrequency :   1   1    1  2      1      </code></pre><p><br></p><p>我們需要每個文檔。 下面的extract_features（第2節）函數執行此操作，然後為每個文檔刪除不太常見的單詞。</p><pre><code>def make_Dictionary(root_dir):   all_words = []   emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]   for mail in emails:        with open(mail) as m:            for line in m:                words = line.split()                all_words += words   dictionary = Counter(all_words)   # if you have python version 3.x use commented version.   # list_to_remove = list(dictionary)   list_to_remove = dictionary.keys()   for item in list_to_remove:       # remove if numerical.        if item.isalpha() == False:            del dictionary[item]        elif len(item) == 1:            del dictionary[item]    # consider only most 3000 common words in dictionary.   dictionary = dictionary.most_common(3000)   return dictionary</code></pre><p>make_Dictionary從文件夾中讀取電子郵件文件，為所有單詞構建字典。 接下來，我們刪除長度不是1且不是純粹字母的單詞。</p><p>最後，我們只提取出3000個最常用的單詞。</p><h1 class=pgc-h-arrow-right><strong>2.提取特徵和相應的標籤矩陣。</strong></h1><p>接下來藉助字典，我們生成標籤和詞頻矩陣</p><pre><code>word      :   Hi this is Alice Happy Birthdayfrequency :   1   1    1  2      1      1word      :   Hi this is Alice Happy Birthdayfrequency :   1   1    1  2      1      1</code></pre><pre><code>def extract_features(mail_dir):  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]  features_matrix = np.zeros((len(files),3000))  train_labels = np.zeros(len(files))  count = 0;  docID = 0;  for fil in files:    with open(fil) as fi:      for i,line in enumerate(fi):        if i == 2:          words = line.split()          for word in words:            wordID = 0            for i,d in enumerate(dictionary):              if d[0] == word:                wordID = i                features_matrix[docID,wordID] = words.count(word)      train_labels[docID] = 0;      filepathTokens = fil.split('/')      lastToken = filepathTokens[len(filepathTokens) - 1]      if lastToken.startswith("spmsg"):          train_labels[docID] = 1;          count = count + 1      docID = docID + 1  return features_matrix, train_labels</code></pre><p><br></p><h1 class=pgc-h-arrow-right><strong>3，使用sklearn天真貝葉斯進行訓練和預測</strong></h1><p>sklearn Naive Bayes的文檔（在此）清楚地說明了用法和參數。</p><p>基本上，sklearn Naive Bayes為模型訓練提供了三種選擇：</p><p>· 高斯：用於分類，並假設要素服從正態分佈。</p><p>· 多項式：用於離散計數。 例如，假設我們有一個文本分類問題。 在這裡，我們可以考慮進一步進行bernoulli實驗，而不是"在文檔中出現單詞"，而是"計算在文檔中出現單詞的頻率"，您可以將其視為"觀察到結果數x_i的次數" 在n次審判中"。</p><p>· 伯努利（Bernoulli）：如果您的特徵向量是二進制（即零和一），則二項式模型很有用。 一種應用是使用"單詞袋"模型進行文本分類，其中1和0分別是"單詞出現在文檔中"和"單詞沒有出現在文檔中"。</p><p>在本練習中，我們將使用高斯。 示例代碼片段如下所示</p><pre><code>TRAIN_DIR = "../train-mails"TEST_DIR = "../test-mails"dictionary = make_Dictionary(TRAIN_DIR)# using functions mentioned above.features_matrix, labels = extract_features(TRAIN_DIR)test_feature_matrix, test_labels = extract_features(TEST_DIR)from sklearn.naive_bayes import GaussianNBmodel = GaussianNB()#train modelmodel.fit(features_matrix, labels)#predictpredicted_labels = model.predict(test_feature_matrix)</code></pre><p><br></p><h1 class=pgc-h-arrow-right><strong>4.準確性得分</strong></h1><p>接下來，我們比較預測標籤的準確性得分。 準確性分數只是正確預測的百分比。 再次，sklearn為準確度分數計算提供了簡潔的實現。</p><pre><code>from sklearn.metrics import accuracy_scoreaccuracy = accuracy_score(test_labels, predicted_labels)</code></pre><h1 class=pgc-h-arrow-right><strong>5.拼湊在一起</strong></h1><pre><code>import osimport numpy as npfrom collections import Counterfrom sklearn.naive_bayes import GaussianNBfrom sklearn.metrics import accuracy_scoredef make_Dictionary(root_dir):   all_words = []   emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]    for mail in emails:        with open(mail) as m:            for line in m:                words = line.split()                all_words += words    dictionary = Counter(all_words)    list_to_remove = dictionary.keys()    for item in list_to_remove:        if item.isalpha() == False:            del dictionary[item]        elif len(item) == 1:            del dictionary[item]    dictionary = dictionary.most_common(3000)    return dictionarydef extract_features(mail_dir):  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]  features_matrix = np.zeros((len(files),3000))  train_labels = np.zeros(len(files))  count = 0;  docID = 0;  for fil in files:    with open(fil) as fi:      for i,line in enumerate(fi):        if i == 2:          words = line.split()          for word in words:            wordID = 0            for i,d in enumerate(dictionary):              if d[0] == word:                wordID = i                features_matrix[docID,wordID] = words.count(word)      train_labels[docID] = 0;      filepathTokens = fil.split('/')      lastToken = filepathTokens[len(filepathTokens) - 1]      if lastToken.startswith("spmsg"):          train_labels[docID] = 1;          count = count + 1      docID = docID + 1  return features_matrix, train_labelsTRAIN_DIR = "../train-mails"TEST_DIR = "../test-mails"dictionary = make_Dictionary(TRAIN_DIR)print "reading and processing emails from file."features_matrix, labels = extract_features(TRAIN_DIR)test_feature_matrix, test_labels = extract_features(TEST_DIR)model = GaussianNB()print "Training model."#train modelmodel.fit(features_matrix, labels)predicted_labels = model.predict(test_feature_matrix)print "FINISHED classifying. accuracy score : "print accuracy_score(test_labels, predicted_labels)</code></pre><p><br></p><p>在下面寫下您獲得的準確度得分！</p><h1 class=pgc-h-arrow-right><strong>任務給你</strong></h1><p>· 試用其他模型; 多項式和伯努利，並比較您獲得的準確性得分。</p><p>· 嘗試將最常用詞的數量從3000更改為大和小，然後繪製獲得的精度圖表。</p><p>請在您的想法下方留言。</p><h1 class=pgc-h-arrow-right><strong>結論</strong></h1><p>樸素貝葉斯考慮功能的獨立性。 例如，它假設一個單詞/特徵的出現與另一單詞/特徵無關。 但是在現實生活中可能並非如此（Good之後，moring的發生率很高）。 我希望第一章（理論和這一章）對樸素貝葉斯有豐富的見解。</p><p>在這裡，我很樂意為您提供建議和反饋。 隨意評論您的想法，反饋或想法。</p><p><br></p><div class=pgc-img><img alt=機器學習入門第1章：監督學習和樸素貝葉斯分類-第2部分（編碼） onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b0736470d40441459f3a66f3f969d6f9><p class=pgc-img-caption>The bugs!</p></div><p><br></p><p>(本文翻譯自Savan Patel的文章《Chapter 1 : Supervised Learning and Naive Bayes Classification — Part 2 (Coding)》，參考：https://medium.com/machine-learning-101/chapter-1-supervised-learning-and-naive-bayes-classification-part-2-coding-5966f25f1475)</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>入門</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/c75c54fc.html alt=機器學習入門第2章：SVM（支持向量機）—理論 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/db2b59aa64f64e189449ae9773356bed style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/c75c54fc.html title=機器學習入門第2章：SVM（支持向量機）—理論>機器學習入門第2章：SVM（支持向量機）—理論</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4b5cbda.html alt=機器學習入門：偏差和方差 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/e4d7bca8189b4528b0f564ee473d2a68 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4b5cbda.html title=機器學習入門：偏差和方差>機器學習入門：偏差和方差</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2d4007c7.html alt=“黑客”入門學習之“Windows組策略” class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ea21244d5f5c420ebef29650f3fafd1c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2d4007c7.html title=“黑客”入門學習之“Windows組策略”>“黑客”入門學習之“Windows組策略”</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c914526c.html alt=新手入門PLC，掌握學習方法是關鍵 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/15355275026304e8d787f10 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c914526c.html title=新手入門PLC，掌握學習方法是關鍵>新手入門PLC，掌握學習方法是關鍵</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>