<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>基於Seq2Seq與Bi-LSTM的中文文本自動校對模型 | 极客快訊</title><meta property="og:title" content="基於Seq2Seq與Bi-LSTM的中文文本自動校對模型 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/Ru2sBcIBpDQUfV"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/ca8a1fd5.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/ca8a1fd5.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/ca8a1fd5.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/ca8a1fd5.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/ca8a1fd5.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/ca8a1fd5.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/ca8a1fd5.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/ca8a1fd5.html><meta property="article:published_time" content="2020-11-14T21:05:26+08:00"><meta property="article:modified_time" content="2020-11-14T21:05:26+08:00"><meta name=Keywords content><meta name=description content="基於Seq2Seq與Bi-LSTM的中文文本自動校對模型"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/ca8a1fd5.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>基於Seq2Seq與Bi-LSTM的中文文本自動校對模型</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>隨著出版行業電子化的不斷髮展，其中中文文本校對環節的任務越來越重，使用傳統的人工校對顯然無法滿足需求。因此，中文文本自動校對技術的發展就顯得尤其重要。</p><p>本文采用深度學習中的(Recurrent Neural Networks)進行文本自動校對。其特點是能處理任意長度的輸入和輸出序列，因此被廣泛應用在(Natural Language Processing)任務中。在機器翻譯任務上，CHO K等在2014年發表的論文<sup>[1]</sup>中首次提出基於循環神經網絡設計的模型，並且在多個自然語言處理問題上取得突破。因此，Seq2Seq模型的提出為文本校對領域的研究提供了一種新的思路與方法。</p><p>目前，基於深度學習的中文文本自動校對技術的研究仍處於起步階段，本文著重研究了基於Seq2Seq模型與BiRNN網絡結構改進的網絡模型，使其適用於中文文本校對問題，為中文文本校對領域提供了一種新的方法。</p><p><strong>1 背景</strong></p><p><strong>1.1 中文文本校對的研究現狀</strong></p><p>目前，國內在中文文本校對方面的研究主要採用以下3種方法：(1)基於拼音的中文文本校對<sup>[2]</sup>；(2)基於字的中文文本校對<sup>[3]</sup>；(3)基於上下文的中文文本校對<sup>[4]</sup>。這三種方法採用的校對規則又分為3類：(1)利用文本的特徵，如字形特徵、詞性特徵或上下文特徵；(2)利用概率統計特性進行上下文接續關係的分析<sup>[5]</sup>；(3)利用語言學知識，如語法規則、詞搭配規則等<sup>[6]</sup>。</p><p><strong>1.2 Seq2Seq模型</strong></p><p>基礎的Seq2Seq模型包含三部分，即Encoder端、Decoder端以及連接兩者的中間狀態向量<sup>[7]</sup>。Encoder編碼器將輸入序列X=(x<sub>1</sub>，…，x<sub>T</sub>)編碼成一個固定大小的狀態向量S傳給Decoder解碼器，解碼器通過對S的學習生成輸出序列Y=(y<sub>1</sub>，…，y<sub>K</sub>)<sup>[8]</sup>。解碼器主要基於中間狀態向量S以及前一時刻的輸出y(t-1)解碼得到該時刻t的輸出y(t)<sup>[9]</sup>。其結構如圖1所示。</p><p><strong>1.3 Bidirectional-LSTM</strong></p><p>LSTM(Long Short-Term Memory)是門控制循環神經網絡的一種。標準的RNN網絡能夠存儲的信息很有限，並且輸入對於輸出的影響隨著網絡環路的不斷遞增而衰退<sup>[10]</sup>；而LSTM在面對較長的序列時，依然能夠記住序列的全部信息。LSTM是一種擁有輸入門、遺忘門、輸出門3個門結構的特殊網絡結構<sup>[11]</sup>。LSTM通過這些門的結構讓信息有選擇性地影響網絡中每個時刻的狀態<sup>[12]</sup>。LSTM的結構如圖2所示。</p><img alt=基於Seq2Seq與Bi-LSTM的中文文本自動校對模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Ru2sBcIBpDQUfV><img alt=基於Seq2Seq與Bi-LSTM的中文文本自動校對模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Ru2sBcr5aTbBeH><p>Bi-RNN克服了單向RNN當前時刻的輸出與之後時刻的輸出無關的問題<sup>[14]</sup>。在Bi-RNN中，將一個RNN網絡拆成了兩個方向，不僅有從左向右的前向連接層，還存在一個從右向左的反向連接層，這兩個連接層連接同一個輸出層，從而在保證網絡可以處理較長序列不發生遺忘的同時，又保證了能夠提供給輸出層輸入序列的完整上下文信息<sup>[15]</sup>。其結構如圖3所示。</p><img alt=基於Seq2Seq與Bi-LSTM的中文文本自動校對模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Ru2sBdKAXxehPQ><p><strong>2 模型的實現</strong></p><p><strong>2.1 數據預處理</strong></p><p>模型的基本架構是Seq2Seq模型。在構造模型之前，需要先對語料進行預處理，包括以下5個部分：加載數據；清洗數據；切詞編碼；分析統計；語料轉換。加載數據的時候需要對語料文本進行切分，以句子為單位，即每一行代表一個完整的句子，以此讀入訓練數據。切詞部分可以藉助一些成熟的中文分詞工具，如 jieba分詞。完成分詞再加載的過程中，要注意同時清洗數據，去掉數字、特殊字符等<sup>[16]</sup>，再以詞為單位對其進行編碼錄入詞庫。</p><p>完成詞庫到數字的映射之後，需再根據這種映射關係完成數字到詞庫的反映射。分析數據是針對訓練語料進行統計性的描述，瞭解訓練數據的一些信息，例如：訓練語句的個數、最長與最短句含有的單詞數、訓練語料構成的詞庫中非重複詞的個數等。最後可按照句子從少到多進行排列，優化訓練過程<sup>[17]</sup>。</p><p><strong>2.2Bi-LSTM的Seq2Seq網絡模型</strong></p><p><strong>2.2.1 基本Seq2Seq結構的侷限性</strong></p><p>在機器翻譯、文摘生成等問題上，基礎的Seq2Seq模型一直都有不錯的表現，但是針對文本校對這類問題，其結構並不能直接被使用。Encoder將輸入編碼為固定大小狀態向量的過程首先是一個“信息有損壓縮”的過程，如果信息量越大，那麼這個轉化向量的過程對信息的損失就越大，同時，隨著sequence length的增加，意味著時間維度上的序列很長，RNN模型也會出現梯度彌散<sup>[18]</sup>。其次，基礎的模型連接Encoder和Decoder模塊的組件僅僅是一個固定大小的狀態向量，這使得Decoder無法直接去關注到輸入信息的更多細節<sup>[10]</sup>。最後，由於RNN網絡的特性，當前時刻的輸出只與當前時刻的輸入和之前的輸入有關<sup>[19]</sup>，因此模型對於信息的捕獲不夠完整。</p><p><strong>2.2.2 模型的構建</strong></p><p>校對模型由編碼端、解碼端組成，編碼端是由LSTM單元組成的Bi-RNN網絡。在中文文本自動校對中，輸入序列是標記為的完成分詞的中文語句文本，其中上角標j代表句子在語料庫中的位置，下角標代表該詞在第i句中的位置。文本中的每一個詞在輸入到LSTM網絡前要轉化成機器可識別的數字編碼。由於LSTM只能處理定長的數據，因此需要保證輸入語料的長度Tx保持固定。通過預處理部分可以得到最長句子的詞數，假設詞數Tx=20，則應對不足20個詞長的句子進行&lt;PAD>補全。編碼端接收每一箇中文詞語的數字形式和其上一個時間點的隱層狀態，由於採用Bi-RNN網絡，輸出的是當前時間點的隱層狀態，結構如圖4所示，其展示了兩個雙向的LSTM單元。</p><img alt=基於Seq2Seq與Bi-LSTM的中文文本自動校對模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Ru2sBviIex5g9S><img alt=基於Seq2Seq與Bi-LSTM的中文文本自動校對模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Ru2sBw5G5qdTr6><p>解碼端是一個帶注意力機制的RNN網絡，其在t時刻生成一個詞時可以利用到此前的全部上文信息。解碼端接收目標句子中上一個時間點的中文詞語和上一個時間點的隱層狀態與注意力機制產生的語義向量，如圖5所示。</p><img alt=基於Seq2Seq與Bi-LSTM的中文文本自動校對模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Ru2sBwPCopyRQT><p>由於注意力機制的存在，每個時刻生成詞時對輸入序列各個詞的關注程度是不一樣的，因此編碼端在每個時刻給出的C<sub>i</sub>是不一樣的。其計算公式如式(15)所示。其中，h<sub>j</sub>表示編碼端的第j個詞的隱層狀態，α<sub>ij</sub>表示編碼端的第j個詞與解碼端的第i個詞之間的權值，其計算公式如式(16)所示。在式(16)中，e<sub>ij</sub>是一個softmax模型輸出，概率值的和為1。e<sub>ij</sub>表示一個對齊模型，用於衡量編碼端的第j個詞相對於解碼端的第i個詞的對齊程度(影響程度)。對齊模型e<sub>ij</sub>的計算方式如式(17)所示。</p><img alt=基於Seq2Seq與Bi-LSTM的中文文本自動校對模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Ru2sBxBDWWNvKF><p>e<sub>ij</sub>的計算方法有很多種，不同的計算方式，代表不同的Attention模型，本文使用的是Soft Attention模型，它可以被嵌入到模型中去，直接訓練。Soft Attention模型在求注意力分配概率的時候，對於輸入句子X中任意一個詞都給出概率。結構如圖6所示。</p><img alt=基於Seq2Seq與Bi-LSTM的中文文本自動校對模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Ru2sCJGB3o7qqM><p>圖6展示了在預測第t個時間段的輸出y<sub>t</sub>時的結構。通過對Encoder層狀態的加權，從而掌握輸入語句中的所有細節信息，最後將語義向量和解碼端的隱層狀態合拼起來，計算最後的輸出概率。</p><p>以“我愛機器學習”為例，假設當前時刻正準備校對“機器”這個詞，此時需要計算語義向量，如圖7所示。</p><img alt=基於Seq2Seq與Bi-LSTM的中文文本自動校對模型 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Ru2sCJkIjY0kmf><p>圖7中，S<sub>t-1</sub>代表解碼端前一輪的隱層狀態，即代表了校對“機器上一個詞”階段的輸出隱層狀態；a<sub>1</sub>～a<sub>4</sub>分別代表了編碼端每個詞輸入到Bi-RNN後的隱層狀態。Attention根據每個Encoder輸出和Decoder的上一次隱層給出每個邊的得分，然後和上一次訓練的預測值拼合到一起，和Decoder端上一時刻的隱層作為輸入進入當前時刻的RNN。</p><p><strong>2.2.3 模型的訓練與優化</strong></p><p>在完成了模型的構建後，還需要構造解碼端的訓練與預測函數，並將訓練與預測分開。因為解碼器會將前一時刻的輸出作為當前時刻的輸入，如果前一時刻的輸入不夠準確，那麼就會影響後續的預測。所以在訓練過程中，需要知道每一個輸入到網絡中的句子的正確形式，進而採用強制正確輸入來進行訓練，這種方式叫做Teacher Forcing，如圖8所示。</p><img alt=基於Seq2Seq與Bi-LSTM的中文文本自動校對模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Ru2sCKACAr1Wcx><p>前面已經介紹過解碼端某一時刻的概率分佈，所以對於全部的訓練樣本，需要做的就是在整個訓練樣本下，所有樣本的P(y<sub>1</sub>，…，y<sub>T</sub>|x<sub>1</sub>，…，x<sub>T</sub>)概率之和最大，最大化條件似然函數，得到最佳的校對結果。模型使用grid search設計，以便找到最佳架構和超參數值。</p><p><strong>3 結果與分析</strong></p><p>實驗使用阿里雲服務器GN2作為訓練服務器，使用TensorFlow框架，共進行了4組實驗，分別用來測試Seq2Seq、BiRNNSeq2Seq、帶注意力機制的Seq2Seq以及與這兩者結合在一起的4種模型在中文文本校對中的性能。實驗所使用的數據集來源於2018 NLPCC共享的訓練數據Task 2以及一部分搜狗實驗室提供的開源中文語料庫，全部的數據集包含了1 327 608個句子對，分別用Src和Trg表示，Src代表原句，既可能為正確的句子也可能為包含用詞錯誤的句子；Trg表示目標輸出，其均為對應Src的正確句子，其中不包含驗證集。將全部數據集按比例(99.5：0.5)隨機分成兩部分：一個驗證集，其中包含5 310個句子對，源句與目標句子之間存在不一致；另一個訓練集包含所有剩餘的1 322 298個句子對。測試數據包含2 000個句子對。數據集的統計數據如表1所示。</p><img alt=基於Seq2Seq與Bi-LSTM的中文文本自動校對模型 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Ru2sCKY4L5QFMf><p>表1展示了訓練驗證和測試數據的統計。.Src是指源錯誤的句子，.Trg是指目標正確的句子。</p><p>為了合理客觀地評價模型，實驗採用廣泛使用的MaxMatch Scorer工具包進行評估。中文文本校對實驗結果如表2所示，其中F0.5、F1、BLEU為評價分數。</p><img alt=基於Seq2Seq與Bi-LSTM的中文文本自動校對模型 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Ru2sCKw8e97xCq><p>實驗結果表明，Bi-RNN以及注意力機制均有助於提升中文文本校對模型的性能，並且二者結合起來可以進一步改善系統的性能。由於模型的實驗結果受數據量以及迭代次數的影響，因此在此基礎上引入更多的數據做訓練，並且通過改善訓練方法，如引入流暢度學習、推斷機制等進一步提升模型校對的準確率。</p><p><strong>4 結論</strong></p><p>本文給出了基於Seq2Seq和Bi-LSTM設計的中文文本校對模型並且通過公開的數據集全面驗證了模型的性能。模型的核心內容是在引入Bi-LSTM單元和注意力機制的同時，用Seq2Seq結構網絡對中文語料庫進行學習，挖掘詞與詞之間的關係，並以此作為中文文本校對的依據。雖然由於訓練量的原因，在結果上並未達到最好的效果，但是可以看出該模型在中文文本校對領域裡具備了很大的潛力，並可以擴展應用在多箇中文自然語言處理領域。</p><p><strong>參考文獻</strong></p><p class=pgc-end-literature>[1] CHO K，MERRIENBOER B，GULCEHRE C.Learning phrase representations using RNN encoder–decoder for statistical machine translation[J].Computer Science，2014(v1)：52-55.</p><p>[2] 張仰森，俞士汶.文本自動校對技術研究綜述[J].計算機應用研究，2006(6)：8-12.</p><p>[3] 洛衛華，羅振聲.中文文本自動校對技術的研究[J].計算機研究展，2004，33(1)：60-64.</p><p>[4] 劉亮亮，曹存根.中文“非多字錯誤”自動校對方法研究[J].計算機科學，2016(10)：34-39.</p><p>[5] 謝剛.知識圖譜精化研究綜述[J].電子技術應用，2018，44(9)：29-38.</p><p>[6] DE FELICE R，PULMAN S G.A classifier-based approach to preposition and determiner error correction in L2 English[C].Proceeding of the 22nd International Conference on Computational Linguistics.COLING 2008 22nd International Conference，2008：167-176.</p><p>[7] 吳巖，李秀坤，劉挺，等.中文自動校對系統的研究與實現[J].哈爾濱工業大學學報，2001(2)：60-64.</p><p>[8] Chen Yongzhi，WU S H，Yang Pingche，et al.Improve the detection of improperly used Chinese characters in students essays with error model[J].International Journal of Continuing Engineering Education and Lifelong Learning，2012(v1)：93-97.</p><p>[9] 吳林，張仰森.基於知識庫的多層級中文文本查錯推理模型[J].計算機工程，2012，38(20)：21-25.</p><p>[10] 劉亮亮，王石，王東昇，等.領域問答系統中的文本錯誤自動發現方法[J].中文信息學報，2013，27(3)：77-83.</p><p>[11] 張仰森，唐安傑.面向政治新聞領域的中文文本校對方法研究[J].中文信息學報，2014，28(6)：44-49.</p><p>[12] 字雲飛，李業麗，孫華豔.基於深度神經網絡的個性化推薦系統研究[J].電子技術應用，2019，45(1)：14-18.</p><p>[13] TAN Y，YAO T，CHEA Q，et al.Applying conditional random fields to Chinese shallow parsing[C].Proceedings of Clcling-2005，Mexico City，2005：167-176.</p><p>[14] KUDO T，YAMAMOTO K，MATSUMOTO Y.Applying conditional random fields to japanese morphological analysis[C].Natural Language Processing(Emnlp-2004)，Barcelona，2004：230-237.</p><p>[15] 王潔，喬藝璇，彭巖，等.基於深度學習的美國媒體“一帶一路”輿情的情感分析[J].電子技術應用，2018，44(11)：102-106.</p><p>[16] 潘吳，顏車.基於中文分詞的文本自動校對算法[J].武漢理工大學學報，2009，31(3)：18-20，28.</p><p>[17] PINTO D，MCCALLUM A，WEI X.Table extraction using conditional random fields[C].26th ACM SIGIR，Canada，2003：235-242.</p><p>[18] 張仰森，鄭佳.中文文本語義錯誤偵測方法研究[J].計算機學報，2017(3)：63-68.</p><p>[19] ZHOU G D，SU J.Named entity recognition using an HMM-based chunk tagger[C].Proceedings of the 40th Annual Meeting of the ACL′2002，Philadelphia，2002：473-480.</p><p><strong>作者信息:</strong></p><p>龔永罡，吳 萌，廉小親，裴晨晨</p><p>(北京工商大學 計算機與信息工程學院 食品安全大數據技術北京市重點實驗室，北京100048)</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>Seq2Seq</a></li><li><a>Bi</a></li><li><a>LSTM</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/c36f4ae6.html alt=手推公式：LSTM單元梯度的詳細的數學推導 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/68dcd30ac0c0469a9bb85ea2bc9f3e8c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c36f4ae6.html title=手推公式：LSTM單元梯度的詳細的數學推導>手推公式：LSTM單元梯度的詳細的數學推導</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f98b2274.html alt=Bi-Amp雙擴大機，高低音單獨驅動，更精準的高級功放系統 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/152871731978350f3aaf87e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f98b2274.html title=Bi-Amp雙擴大機，高低音單獨驅動，更精準的高級功放系統>Bi-Amp雙擴大機，高低音單獨驅動，更精準的高級功放系統</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ed698ca.html alt=基於CNN-LSTM的太陽能光伏組件故障診斷研究 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RwyETS5HTioYSv style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ed698ca.html title=基於CNN-LSTM的太陽能光伏組件故障診斷研究>基於CNN-LSTM的太陽能光伏組件故障診斷研究</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>