<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 | 极客快訊</title><meta property="og:title" content="萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/ae01d5412e234dcbbc1a7ae250d89754"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/cc892ed6.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/cc892ed6.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/cc892ed6.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/cc892ed6.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/cc892ed6.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/cc892ed6.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/cc892ed6.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/cc892ed6.html><meta property="article:published_time" content="2020-10-29T21:08:52+08:00"><meta property="article:modified_time" content="2020-10-29T21:08:52+08:00"><meta name=Keywords content><meta name=description content="萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/cc892ed6.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ae01d5412e234dcbbc1a7ae250d89754><p class=pgc-img-caption></p></div><p class=ql-align-justify><strong>[ 導讀 ]</strong>達觀數據的聯合創始人高翔詳細講解了自然語言處理中信息抽取算法技術。</p><p class=ql-align-justify><strong>作者介紹：</strong></p><p class=ql-align-justify>高翔是達觀數據聯合創始人，達觀數據前端產品組、文本挖掘組總負責人；自然語言處理技術專家，負責文本閱讀類產品、搜索引擎、文本挖掘及大數據調度系統的開發工作，在自然語言處理和機器學習等技術方向有著豐富的理論與工程經驗。</p><p class=ql-align-justify><strong>目錄：</strong></p><p class=ql-align-justify>第一部分：文本信息抽取詳解</p><p class=ql-align-justify>第二部分：“達觀杯”baseline代碼分享</p><p class=ql-align-justify>第三部分：問題答疑</p><p class=ql-align-center><strong>第一部分：文本信息抽取詳解</strong></p><ul><li class=ql-align-justify><strong>文本挖掘簡介</strong></li></ul><p class=ql-align-justify><br></p><p class=ql-align-justify>下面我們開始介紹一下文本挖掘。下圖中，我們可以把人工智能分為三類——圖像、文本和語音，達觀是一家專注於做文本智能處理的科技公司。文本相對於圖像和語言來說更難處理，因為文本數據需要做一些邏輯分析。圖像和語音屬於感知智能，而文本屬於認知智能，所以號稱是“人工智能的明珠”，難度很大。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ccce592864b648b1bee145b168909d14><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>自然語言處理的任務是什麼？簡單來說就是讓機器知道怎麼看、要麼寫。我們一般把“看”叫自然語言理解（NLU），包括自動化審核、自動文本比對、信息糾錯，搜索推薦等等，它可以大幅度減輕人工的負擔。自動寫作叫自然語言生成（NLG），包括自動填表、生成摘要，文本潤色，還有大家看到的“自動生成股市”、“自動生成對聯”等等。<strong>目前我們主要還是在解決自然語言理解的問題。語言生成因為一些限制，實際落地的效果仍然有待提高的。</strong>所以我們今天主要討論自然語言理解這部分。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9fe8325f37094c4db9d12cf1964491b5><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>其實自然語言處理的歷史非常悠久，甚至出現在“AI”這個概念之前，最早叫“符號主義”。<strong>剛開始的時候人們選擇了一個很不好的場景：機器翻譯。</strong>機器翻譯是一個難度很大的任務，因為涉及了語義的理解和不同語種語法的規則。所以早期自然語言處理不是很成功。過了20-30年，到上世紀80年代開始，我們使用了語法規則，基於自然語言處理的一些基本原理，再通過人工在這些語法的規則上進行修訂，做了一些問答、翻譯和搜索方面的嘗試。</p><p class=ql-align-justify>自然語言處理真正的黃金時期是從上世紀90年代開始，那時候我們搞了統計學，做了很多基於統計機器學習的算法。從下圖中我們可以發現，統計模型的效果讓自然語言處理的應用領域更加廣泛，產生了很大進步。其實在上世紀90年代的時候，自然語言處理已經可以在很多場景表現得很不錯了，比之前的技術要先進很多。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/90b365efe2764bfbb1c35d01f2a9a1cd><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>從2006年到現在，深度學習已經開始起步。之前“神經網絡”這個概念已經有了，只是當時受限於各種各樣的算法和硬件，沒法做得很好。<strong>但現在各方面都成熟之後，大家發現深度學習是一個神器。</strong>其實深度學習最早的時候在圖像領域的應用較多，但目前自然語言處理也逐漸開始過渡到深度學習的階段。尤其是去年像BERT這樣的模型出來之後，我們發現自然語言處理的評測經常被屠榜，這說明神經網絡非常有效，但也說明數據也很重要，後文中我們會解釋數據的重要性。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a977b9d5cd184e72b65d666f01f434e2><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>我們對比一下人類和計算機之間的差異。其實我們人類短時間內閱讀理解文字的能力還不錯，但是時間久了很容易遺忘。<strong>但計算機基本不會忘，只要硬盤不壞。</strong>人腦難以長期記憶，但我們對內容的推理能力比計算機強。因此，我們可以請計算機來做一些比較細節的工作。例如文字比對，我們檢查錯誤要逐字逐句地看，非常累。計算機能做到秒看，卻很難做複雜的邏輯和推理。</p><p class=ql-align-justify>此外，雖然人類閱讀速度很快，但寫作速度很慢。大家高考的時候都要留幾十分鐘來寫作。這是因為寫的時候，我們手速有限。而且在寫的過程中還要進行很多思考。<strong>寫作本質是把腦中的很多語義信息壓縮到一個點，也就是文章的主題。</strong>有了主題後我們還要再把作文展開，所以要花很多時間構思大綱、設計章節結構和文章主線，非常耗時。</p><p class=ql-align-justify>我們在接受信息時能很快地理解整體，但是難以記住細節。我們看完一個東西立刻能知道它的中心思想。例如，我們瀏覽了一個企業的信息之後，就能做出“這個企業比較靠譜，願意投資”的判斷。但是企業收入、競爭利潤、負債這些具體數字很難全部記清楚。所以人去尋找局部信息的能力和計算機比非常慢。計算機的優點就是找這種局部信息，越細的東西它找得越快。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/157e6aeb888647f7998ccf78f64dfc57><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><strong>什麼場景比較適合讓計算機去做？</strong>基於現階段的技術，現在大部分場景計算機還是無法取代人。我們可以看到，很多行業，包括法律，包括企業合同、客戶意見、產品手冊、新聞、問答資料的數據是需要我們親自來看。雖然這些行業領域不同，但做的事情都類似。審一個企業合同的時候，需要看一些關鍵的信息，如甲方、乙方，以及這些東西是否合規，總金額是否正確。在法律行業，法官判案時也要看整個案由，包括被告和原告的相關信息，案件的時間、地點等等。這些都是信息抽取，在很多應用場景下都需要信息抽取。無論我們做了什麼決策，判斷是否投資，是否通過合同，如何進行法律判決，都需要先從文字中提取信息。</p><p class=ql-align-justify>其實在一些比較固定的，相對簡單，不需要特別複雜的邏輯推理的場景中，機器學習算法已經可以完成信息抽取任務。我們正努力讓計算機在這些場景落地，這不僅僅是算法的問題，也是應用的問題。這也是我們一直在思考的問題。</p><ul><li class=ql-align-justify><strong>抽取算法概述</strong></li></ul><p class=ql-align-justify><br></p><p class=ql-align-justify>現在我們具體講講信息抽取的幾種最主流的算法。</p><p class=ql-align-justify><strong>什麼是信息抽取？</strong>其實就是從文本中找到指定類型的實體。大家應該聽過命名實體識別（NER），其實命名實體識別只是抽取中的一種。廣義上的信息抽取，除了命名實體識別之外，還包括關係抽取、事件抽取等。其實在我看來，關係抽取和事件抽取比命名實體識別的應用層次更高級一點。因為這兩個抽取同需要做NER，只是在做NER的基礎之上，還要做一些其他的工作，來滿足場景需求。</p><p class=ql-align-justify>我們先從最簡單的NER開始。命名實體一般是指人物、地點、機構、時間等內容。現在我們以公司抽取為例詳細說明一下。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/884fbb5cf1734759b146d694ea2f5254><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>如果從歷史的角度來說，識別公司的任務就是所謂的“符號主義”任務，簡單來說就是窮舉所有公司的名稱做詞典匹配。這樣就是一個命名實體。但是，這麼做場景其實有限。<strong>為什麼？因為上市公司的集合是有限的，所以直接拿公司字典可能比訓練模型更快。</strong></p><p class=ql-align-justify>但是你會發現這種場景並不常見。比如，如果抽取所有公司（不僅限於上市公司）就不能用這種辦法，因為公司實在太多了。十年前如果你看到“餓了麼”，如果沒有上下文，你不會覺得這是一個公司，但因為現在大家經常點“餓了麼”，都知道這是一個公司的名字。而且，每天都有大量新公司產生，所以整體的公司是一個沒法窮盡的集合。在這種情況下，我們沒辦法用字典很好地完成絕大多數任務。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ec9535a6a38a4f7c81239e3e895cd94d><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>之前我們提到了上下文。<strong>那我們現在加入上下文信息，是不是可以知道某個實體是一個公司呢？</strong>最直接的方法是通過語法規則來做，例如“A是一家公司”、“B作為一家公司”等等。你會看到這樣的一些模板，然後再去分析。如果說得學術/技術一點，相當於把這個任務提煉成一個比較複雜的句法依賴和語法規則。但從代碼角度可能會比較簡單，比如把模板中間的東西摳掉，然後去做匹配，做完匹配再去做填空，填空的內容就是你要的這些公司。</p><p class=ql-align-justify>但這樣做也有很大的問題，因為我們語言表述的方法太多了。例如，“我是A公司的”，“我來自B公司”以及很多種其他不同的表述都是一個意思，我們無法窮盡所有的表述方法。甚至周星弛的電影也能增加這種做法的難度。我們以前說“我先走了”，現在會說“我走了先”、“我吃了先”，這其實跟我們傳統的語法都不太一樣，但現實生活中就有這麼多表述。不過，和上面的字典類似，在特定的場合，比如一些特定領域的公文等文書文章，還是有套路或者標準寫法，也許可以用這種方法。總的來說這種方法比較簡單。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3005a0545c1444d2b26b59ec44684869><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>更高級的是基於統計機器學習的方法，從算法上來說是用序列標註的方式來做。這種方法要求我們標註數據，例如上圖中我們標註了一句話：“達觀數據是人工智能公司”。現在它會預測“上海的虛擬數據”中的“虛擬數據”也是一家公司。它是怎麼做到的？後文會詳細介紹。這種做法就跟模板匹配完全不一樣了。在圖中，可能第一個預測“虛擬數據是人工智能公司”還有模板的性質，但後面兩個表述和前面完全不同，所以這種基於統計機器學習的方式有了一定的預測能力。</p><p class=ql-align-justify>但問題是什麼？它需要兩個條件。<strong>首先是數據。</strong>大部分的機器學習都是監督學習，要做數據標註。而且我們傳統機器學習經常要做特徵工程。甚至在很多任務中，一個特徵工程可能要佔到我們項目時間和精力的90%。我們之前參加CIKM評測並拿到冠軍的任務中，就耗費了大量時間構建特徵。舉個例子，我們實際工作中完成文本分類任務的時候，僅僅把文字的長度這個特徵加進去，效果一下子提升了很多。這種特徵我們很難想到。<strong>特徵的選擇可能有時候還有一定的邏輯推理，但有的時候就是拍腦袋。</strong>所以特徵工程做好是很難的，需要很多的經驗，還需要有擴散性的思維。</p><p class=ql-align-justify><strong>此外訓練和預測需要很多計算資源。</strong>某些機器學習（尤其是傳統的機器學習）的訓練過程中，特徵有時候會特別耗費內存，可能不一定訓練得完，所以對機器有一定的限制。當然，現在做深度學習，限制可能是GPU。深度學習相對於傳統機器學習，對數據量地要求更高。因為傳統的機器學習模型的各種參數沒有深度學習這麼多。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f911ac0c2fd64af286d722fd744be003><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>雖然深度學習的可解釋性經常被人詬病，但也有些模型實際上可以給我們一些解釋。尤其是一些基於Attention機制的模型。這裡就是一個Attention分類器。圖中可以看到它能從句子級別和詞級別告訴你，對一個分類模型來說，哪句話最重要，哪個詞最重要。這些詞和句子都是有權重的。因為有Attention這樣的權重，我們就能把它拿出來做可視化。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/74c38e4f30014cfcb8afaed7f9fb10da><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>所以整體來說還是要通過序列標註來做。上圖有一個序列標註的例子：分詞。要分詞的句子是“它來自達觀數據”。我們有一個叫Label Set，也就是標籤集。圖中我們用的是BMES這個很經典的標籤集，這個標籤集其實對應的英文Begin、Middle、End、Single，大家一看就知道是什麼意思。對於分詞來說，每個字可能組成一個詞（單字成詞），也可能是一個詞的開始、的中間或結尾。</p><p class=ql-align-justify>上圖還可以看到，在分詞之外，命名實體我們用另外一個標籤集。我們做詞性分析可能用不同的標籤集。可以看到，不同的標籤集可以用來做不同的事情。所以無論是傳統的機器學習，還是深度學習，我們都是在解決一個叫做“序列標註”的問題。所以標籤集和標註方式都是基礎的、幾乎是一樣的。有什麼樣不同？後文會具體討論。</p><ul><li class=ql-align-justify><strong>傳統抽取算法介紹</strong></li></ul><p class=ql-align-justify><br></p><p class=ql-align-justify>其實傳統抽取算法有很多，這裡會介紹一些大家比較常用，也比較好理解的模型。第一個模型叫生成式模型。生成式模型的一個代表就是隱馬爾科夫模型（HMM）。另外一個是判別式模型，代表是條件隨機場（CRF）。<strong>這兩個模型都結合了概率論還有圖論的一些內容，也都基於統計機器學習的算法。</strong>它們都能根據訓練集訓練出不同的結果。下面我們詳細介紹一下這兩個模型。</p><p class=ql-align-justify>我人生第一次做序列標註任務的時候，用的就是HMM模型。馬爾可夫這個名字一聽就像是個數學很厲害的俄國人，但其實HMM模型並不難。大家只要記住兩部分內容：兩個序列、三個矩陣。如下圖所示。我們要做的就就是把這五個部分定義好，整個模型和要解決的問題就定義清楚了。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/19dd6845b3264de7af7c9586c491b529><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><strong>首先是觀察序列。</strong>上圖中“他來自達觀數據”，就是我們人看得到的觀察序列，但它背後隱藏了分詞。“他”是一個詞，“來自”是一個詞，“達觀數據”是一個詞，這個是我們說“隱藏序列”，沒有寫到明面上，但需要我們模型預測。怎麼預測？下圖畫了預測模型的示意圖。圖中，X_1、X_2、X_3就是我們說的隱藏內容，人能看到的是y_1、y_2、y_3、y_4，也就是觀察序列。但其實不同狀態是可以不停地轉換的。比如X_1到X_2之間有一條連線說明X_1和X_2之間可以通過概率a_12做轉換；X_2到X_3之間通過概率a_23做轉換。所以這個模型其實比鏈式的HMM還要更復雜一點，因為它有X_2到X_1這樣的轉換。所有的X都可以轉換到y_1、y_2、y_3、y_4這樣的觀察序列，每對轉換關係都有對應的概率。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fb7a64e750b146018dcc17f45454c92e><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><strong>這樣我們就把模型定義好了。我們只需要求模型的哪幾個部分呢？</strong>主要是這三個矩陣：初始狀態矩陣，發射狀態矩陣，以及狀態轉移矩陣。</p><p class=ql-align-justify><strong>第一個是初始狀態矩陣。</strong>我們現在舉的例子都是有序列標註，例如多輪分詞。下圖是一個真實的多輪分詞模型裡面的圖，這是我們自己訓練的一個模型。可以看到，初始狀態只可能是S(ingle)或B（egin），因為不可能從代表詞結尾的標記開始一個句子。所以我們要從所有的語料中統計，單字詞S和多字詞B開始的概率是多少。僅僅統計這兩個矩陣就可以，因為其他兩個標記M（iddle）和E（en）是不可能出現在句首的。圖中的概率有負數，是因為經過log和相關處理，從而可以方便後續的計算，但本質的含義還是概率。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a6f7c50646f846698d88f8c283fcb66e><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><strong>第二個矩陣是發射狀態矩陣。</strong>什麼是發射狀態矩陣？簡單來說就是我們在分詞裡每個字變成任何一個標籤的概率（如下圖所示）。例如“他”這個字如果來自“他來自達觀數據”這句話，就是一個單字詞S（ingle）；但如果在“他”出現在“他們”等多字詞裡，標籤就是B(egin)；在“關心你我他”裡，“他”的標籤可能就是E（end）。所以你會在訓練語料看到“他”有不同的標籤。發射狀態矩陣就是把“他”到每一個標籤的概率集合起來。發射狀態矩陣非常重要，它說明了每一個字到不同標籤的概率。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dae2bd69f4ec4013bc84fc4cbc931395><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><strong>第三個是狀態轉移矩陣。</strong>什麼是狀態轉移矩陣？其實狀態轉移矩陣也是統計出來的，也就是剛才說的X_1和X_2之間的概率。我們訓練語料裡面已經有了SB、BMME這樣的標籤。其實我們可以觀察到一些現象，例如S（ingle）後面不可能跟E（nd）和M（iddle）。這些就是狀態轉移矩陣描述的內容，如下圖所示。它說明E後面跟著S的概率是多少，E後面跟著B的概率又是多少等等。這些值其實都是從語料庫中訓練出來的。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d817fc326775479a9bc8fc00341a888a><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>下面討論兩類學習算法：一種是<strong>“監督學習”</strong>，通過極大似然估計就可以得到這些值，非常好算，簡單地說就是統計次數：統計這個標籤一共有多少，相關概率又是多少，就可以得出結果了。還有是一個<strong>非監督學習Baum-Welch</strong>，這個算法我們用得比較少，因為根據我們自己的經驗，它的整體效果會比做統計差很多。而且監督學習有個好處是因為有了訓練集和相關的數據，所以很容易去查錯。</p><p class=ql-align-justify>解碼算法基本是用Viterbi來做。當然你也可以把當前最好的狀態輸出來，找到在當前序列下能夠輸出的最大標籤，通過自己的一些解碼邏輯（比如B後面一定是M或者E，不可能是S）優化一些內容。但我們經常還是用Viterbi去做整體的解碼，取得最優路徑的概率。Viterbi解碼算法大家一定要掌握，因為後面有有不少算法與它類似。只要把Viterbi學會了，後面的很多東西就很好理解了。</p><p class=ql-align-justify>HMM是我個人學的第一個模型，但是我現在基本上不用這個模型。為什麼不用？因為它的效果還是相對差一點。但它也有優點。因為做極大似然估計就是簡單的統計，速度非常快。所以這個模型的更新可以做到秒級。你做一個數據的修改，跑一遍立刻把數據統計出來，修改矩陣以後很快就對這個模型做一個更新。所以在項目的初始階段，我們可以快速地用這個方法來做baseline或者動態的修改。尤其在實際業務中，可能客戶做了一些修改後他需要實時知道反饋，這時候可以用HMM，雖然可能不能保證有好的效果。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6b9c6bab69364281a6dcb4ba47e71eaa><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>在實際應用中我們用的最多還是條件隨機場（CRF）。因為CRF往往效果更好。下圖說明了HMM和CRF的關係是什麼，我們可以看到一個HMM是鏈式傳遞，但加上一個條件就是我們最常見的鏈式條件隨機場。通用CRF就是下圖中右下角的圖，但是我們做序列標註的話可能是最下面一行中間的這個圖，也就是鏈式的CRF。它跟上面一行的圖的區別是什麼？大家可以看到下面一行圖中有好多小的黑色正方形，這就是我們說的條件。我們是如何得出條件的？下面我們就來介紹一下如何通過真實訓練得到條件。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a8da45323baa449c8f85d6889f4fd96e><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>我們先看下面這張圖。圖中nz在詞性裡表示是一個“其他”類型的實體。這種類型很難歸入時間、地點、人物等常見的實體類型，比如“蘋果手機”可能就可以算是一個nz。我們把所有不太好分類的實體都歸入到nz裡。在這裡，標籤集還是BMES，但是加了一個“O”。標籤後面的後綴其實就是類型。剛才提到的“其他”是nz，還可以有其他類型（如地名、時間、機構等）可以用其他字符串表示，比如nr、ns、nt。定義好這套標籤集後，我們就開始定義特徵函數。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/088bdbb8c42b4361a7e860474a2bc829><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>下圖是我們是用CRF++、CRFPP做的特徵模板。大家可以看到，圖裡有U00到U08，最後還有一個字母“B”，B說明它會學習標籤間的轉移。U00到U08都是特徵，U00表示第一個特徵，U01是第二個特徵。此外還有一個x%，它代表了前面特徵的內容。</p><p class=ql-align-justify><strong>首先看第一個特徵：U00: %X[-3,0]。</strong>U00表示把我們要研究的字左邊的第三個字作為特徵，向量後一個數0表示我們沒有添加人工特徵。我們把這些拼接起來就是一個最終的特徵。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c0a97ceeb65a4f139ace1951cc66845c><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>下圖中包括了特徵函數的權重（weight）。我們可以看到“U06：徑”，這表示當前的字右邊第三個字是一個“徑”字。我們會給出每個標籤的得分。可選的標籤就是BEMOS。這裡的數字代表得分（不是概率），有正有負。我們最終就是要把訓練集所有的數據先通過這個特徵模板變成一個特徵。對於每個字，都有8個特徵，第一個特徵就是當前字左邊的第三個字，第二個特徵是左邊第二個字，U03就是當前字本身。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7202c8c9b7124637926a5b3b47730746><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>所以大家可以看到CRF和HMM最大的不同。我們定義了這樣一個特徵函數（或者特徵模板）。我們還可以人工設置一些特徵影響特徵模板。比如在研究當前字時，如果用了這樣的模板，我就知道前三個字和後三個字會對當前這個字的標籤的輸出產生影響。除此之外，還可以用前一個字和當前字，或者當前字和後一個字的組合作為特徵。有了這些特徵，我們就要計算特徵的結果。這時可以迭代訓練模型，CRF使用了L-BFGS來訓練。最終訓練出來的模型可以告訴我們每個特徵值對於不同的標籤的值是多少，相當於是一個全局最優的值。</p><p class=ql-align-justify>下面這張圖代表了標籤之間的轉移，這跟HMM非常像，也可以算出來。所以CRF最終在一個全局最優的情況下達到了一個最優點。我們可以存儲這個最優點情況下每一個特徵的值，用來解碼。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8a54247e99604bed80e0f459bffe474d><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>CRF的解碼較為簡單，我們根據當前序列的位置，根據特徵的模板生成很多特徵函數，直接去查我們的模型，找到其對應的特徵函數權重，之後每一個特徵函數權重加起來。查到這個特徵函數就把相應的權重取出來，加起來，沒有查到就是0，就不用去做了，最終有一個得分，這樣每一個標籤都會有相關的得分。這個字生成的Score會有BEMOS相對應的，最終得到一個圖，我們就用Viterbi解碼，跟前面一樣就能解出來了。</p><p class=ql-align-justify><strong>為什麼CRF效果好？</strong>因為我們可以定義特徵模板，包括了很多上下文比較遠的特徵。CRF的特徵是人工選擇的，可以選擇前兩個、前三個，甚至更多，所以可以讓模型學到更多上下文，而且是遠距離的上下文，輔助我們判斷，提升整體效果。但條件隨機場需要迭代優化，根據梯度下降的方向去找最優點，所以整體速度相對較慢，算出來的模型也不會小。很多時候必須要篩選或裁剪標籤。</p><p class=ql-align-justify>以上內容就是HMM和CRF這兩個傳統的算法。</p><ul><li class=ql-align-justify><strong>基於深度學習的抽取算法</strong></li></ul><p class=ql-align-justify><br></p><p class=ql-align-justify>經典機器學習的很多算法需要比較強的數學功底，通過數學公式做出優美完整的論證。但現在經典機器學習算法的收益已經沒有以前大了。原因如下圖所示，圖中列出了文本挖掘領域中，經典的機器學習和深度學習的對比。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/904c95ed48cb4cceba472530370e5d81><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><strong>最大的區別就是紫色的框：特徵工程。</strong>其實算法並不多，但特徵工程五花八門，包括我們做文本處理時經常遇到的TF-IDF、互信息、信息增益、期望交叉熵等等。其實這些提取特徵的方式都有一些科學依據，但很多場景下我們需要靠直覺。特徵工程往往佔到項目時間的90%。</p><p class=ql-align-justify>而深度學習不在乎特徵。模型定好之後只管輸入，有了輸入就能輸出一個最好的結果。基本不用改代碼的，只需要調參。如果數據小，還需要修改一下過擬合方面的東西就可以了。但是用經典機器學習做特徵工程可能要改很多代碼才能做出一個非常好的特徵，這就是傳統機器學習和深度學習最大的區別。</p><p class=ql-align-justify>用深度學習做文本處理基本繞不開LSTM。雖然現在有很多模型，但也採用LSTM做baseline。下面是一篇著名的介紹LSTM的文章的截圖，建議大家看一下原文。<strong>文章中最精華的就是下面四張圖，展示了LSTM的工作原理。</strong></p><p class=ql-align-justify><strong>第一個步驟是單元狀態丟棄（如下圖）。</strong>圖中有兩個量x_t和h_t-1。x_t就是當前的輸入，h_t-1是上一時刻的隱層的輸出。這個公式求出來一個0-1之間的值，決定要留下多少東西。（任何東西乘以0-1其實就是計算要留多少東西，乘以0什麼都留不了，乘以1就都留下，乘0.8就留80%。）</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/637fd4d5bb1c4b4bb0398f9f2150450e><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>第一步：單元狀態丟棄</p><p class=ql-align-justify><strong>第二步新信息的選擇。</strong>當前輸入包括上一時刻隱層的輸出和當前的輸入。這一步驟判斷應該留下來多少內容。它還是計算兩個係數，一個i_t，這也是一個0-1之間的值。第二個是C_t，表示當前cell的狀態。計算完畢後需要把這兩個係數的值保存下來。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/120c9e40e9834d81805d82d093086ab9><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>第二步：新信息選擇</p><p class=ql-align-justify><strong>第三步是更新狀態。</strong>上面一步已經決定可以留下的新內容和老內容。這一步要決定如何組合新老內容。老內容可以乘以第一步計算出的f_t，新內容可以乘以第二步算出來的i_t，然後把新老內容相加，就是最新的狀態了。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d8a6a7f0c0804a45a9916ed40f490e48><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>第三步：單元狀態更新</p><p class=ql-align-justify><strong>第四步是得出最後的輸出值。</strong>Cell不會一股腦輸出，而是計算出了係數o_t和狀態相關的函數結果相乘後得出輸出。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0abc99ea76bb47fdb958ef2f32116711><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>第四步：確定輸出</p><p class=ql-align-justify>以上四步定義了LSTM基本的原理。LSTM其實提出來已經很多年了，在很多場景下都經受了考驗。所以希望大家一定要把上面介紹的基礎原理了解好。</p><p class=ql-align-justify>下圖顯示了基於深度學習的信息抽取技術Bi-LSTM+CRF的原理。這個方法代表了深度學習和傳統的機器學習一個很好的結合。<strong>傳統CRF最大的問題是特徵很稀疏，想做一個很好的特徵要花費很多時間。</strong>我們可能會有幾套比較經典的特徵，但不一定保證效果最好，特別是訓練數據發生變化以後。而詞向量和Bi-LSTM可以做很多的特徵提取工作。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/83571150e5b749f2a0ca74ae4fd07181><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><strong>為什麼要用Bi-LSTM而不是簡單的LSTM？</strong>舉個例子，“華為發佈了新一代的麒麟處理X”這句話中，“X”一看就是處理器的“器”。因為我們都知道前文“麒麟處理”後面肯定跟著“器”。類似地，根據“X鮮和美國簽訂了新一輪的諒解備忘錄”很容易猜出X是“朝鮮”的“鮮”，這是根據後文做出的判斷。天然的語言中存在前後文的信號，都會影響當前字的選擇。Bi-LSTM可以兼顧前後文的影響，所以是從理論上來說是個很符合人類直覺的工具。</p><p class=ql-align-justify>如果不用CRF，可能整體效果還不錯，但會出現很多badcase。比如B後面出現S，S後面出現O。因為算法只考慮當前的最優輸出，沒有考慮整個序列的最優結果。而CRF是一個考慮全局的算法，也考慮到標籤間的轉移概率。所以用CRF會得到一個比較可控的結果。</p><p class=ql-align-justify>總得來說，上圖介紹的Bi-LSTM+CRF方法，結合了CRF和Bi-LSTM，把“小明去達觀數據開會”這幾個字變成向量，通過中間的Bi-LSTM隱層，提取出來高維的特徵，輸入CRF層，CRF最後就會給出標籤和結果。</p><p class=ql-align-justify>下面我們會介紹這篇文章最重要的部分：<strong>預訓練模型</strong>。深度學習除了不用做大量的特徵工程，還可以對文本做非常好的表示。這裡的例子是用Word2Vec做出詞向量，然後用TensorBoard可視化，如下圖所示。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/43b35de21bd3435b86312e3c19edef8d><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>在圖中“威海”、“濰坊”、“棗莊”這三個山東的城市的詞彙，被轉化成了三個低維向量，向量中的數都是浮點數，有正數也有負數。如果從空間的角度來看這三個向量，可以發現它們距離很近，說明從語義角度來看它們的含義很接近。而且我們還可以直接對這些詞向量進行計算，例如山東-威海=廣東-佛山，皇帝-皇后+女人=男人，所以詞向量是很優秀的自然語言的表徵方式。</p><p class=ql-align-justify>上圖用的是Word2Vec模型。下圖還有一些其他的模型，比如Glove。這兩個模型都是靜態表示。靜態表示有天然的缺陷，例如它們很難區分“蘋果好吃”和“蘋果手機”中的兩個“蘋果”。就好像我們學技術的時候什麼都想學，但因為時間是有限，所以每種技術學得都不夠深入。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8d7c9fa159a246158b3489fb564a59ba><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>所以從2018年開始，出現了很多新的預訓練模型，不少模型都用《芝麻街》裡怪物的名字命名，比如ELMO、BERT和ERNIE。除此之外還有微軟的MASS，Google最新的XLNet等等。這些模型本質上都用深度學習的神經網絡做表示，雖然有的用Attention，有的用Transform，但本質差別不大。</p><p class=ql-align-justify>這些模型和Word2Vec/Glove最大的區別在於它們是動態模型。下圖是一個真實的例子。輸入“蘋果好吃”和“蘋果手機”後，用BERT對每個字建模，發現前兩個字的向量很不一樣。這說明BERT可以根據不同的上下文語境編碼每個字，或者說可以根據上下文語境對同一個字做出不同的表示。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/44d8a23f1b544d89841a8ba00a9eba24><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>BERT可以根據上下文，對同一個字做出不同的表示</p><p class=ql-align-justify><strong>如何選擇預訓練模型呢？</strong>我建議大家可以都嘗試一下。大部分同學都可以訓練ELMO，它的結構和LSTM很像，我們可以自己訓練一個語言模型。BERT訓練的成本就要高很多，但現在已經有一些其他的框架或語言做處理。我們自己用中文維基百科訓練BERT只用了幾天，也沒有用很多顯卡，當然我們也做了不少優化工作。可以先試著用Word2Vec看看效果，有可能效果已經很不錯。關鍵在於要找到在能力範圍內按時訓練完的模型。</p><ul><li class=ql-align-justify><strong>抽取算法在達觀的具體實踐</strong></li></ul><p class=ql-align-justify><br></p><p class=ql-align-justify>下面我們分享一下在達觀的實踐中完成抽取任務的一些經驗和教訓。</p><p class=ql-align-justify><strong>首先我們要注重場景。</strong>應用場景一般就是客戶提供的文檔，包括財務報表、基金合同等等。文檔處理的核心是自然語言處理，特別是抽取技術。我們也需要考慮實際應用，結合一些其他的工程技術，比如外部系統、分佈式技術、數據庫技術等等。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/faaff868b13d4f48a81e4f2ff3ca7e7a><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><strong>第二是要解決數據不足的問題。</strong>尤其是序列標註比文本分類需要更多的標註成本，所以很可能數據量不夠。雖然目前有一些通用的數據（比如《人民日報》的數據），但針對具體的業務場景可能沒有足夠多的語料和標註數據。這時候我們就要做數據增強。數據增強是一種通用的方法，可以應用於傳統的機器學習和深度學習中。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/faaff868b13d4f48a81e4f2ff3ca7e7a><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>在上圖中，我們可以看到標註數據只有三句話，黃色表示要做機構識別。<strong>怎麼增加標註數據的量？</strong>我們可以直接暴力地把它們兩兩隨機組合。初聽起來可能會覺得有點不可理喻，但確實有效果。上圖中右邊的三段話中，前兩段是兩兩隨機組合，最後一段是把三句話全部混合到一起。把這些新生成的數據加入原數據起去做模型，就會發現效果的確好了很多。數據增強為什麼有效？從模型的角度簡單地說，這樣可以看到更多上下文，特別是可以跨句子看到上下文，所以會有幫助。基本上寫5-10行代碼就能產生一些收益。</p><p class=ql-align-justify><strong>還有一種方法是非監督的Embeddin的學習。</strong>下圖是我們的一個真實的例子。當時登貝萊剛轉會到巴塞羅那俱樂部。我們用標準語料去訓練，發現“登貝萊”這個名字一定會被切開，無論怎麼訓練分詞都不行。潛在的解決方法之一是增加很多登貝萊相關的標註數據，但是這麼做收益不足。所以我們就找了很多外部的語料做嵌入。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ecf2454daeac42acbda6b44e2537d0da><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>如上圖所示，我們在網上找了一些登貝萊的新聞補充到《人民日報》等語料裡一起訓練。在完全沒有修改，只是重新訓練了預訓練模型的情況下，“登貝萊”就成了一個詞。這說明深度學習的預訓練模型，可以非常好地捕捉到上下文，而且我們知道大部分的神經網絡的語言模型訓練是非監督學習，所以不需要很多標註數據。可以有很大數據量。總體來說數據越多，模型會學得越準，效果越好。BERT訓練了一兩千萬的中文後，可以達到非常好的效果，我覺得這是個大力出奇跡的模型。</p><p class=ql-align-justify>除了NER，還可以抽取別的內容。例如知識圖譜就要做關係抽取。輸入一句話，“美國總統特朗普將考察蘋果公司，該公司由喬布斯創立”，怎麼抽取關係？有兩種方法。一種方式是把實體抽出來，然後兩兩實體做一些分類，分到一些關係裡面。另一種依靠序列標註，也就是基於聯合標註的方法。這麼做的好處是不用修改標註框架。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/24ba96ca5c224f84808bcc7f3139827b><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>我們總結一下本文內容。在實際工作中，到底怎麼來用深度學習挖掘文本？最重要的一點是要用預訓練模型，通過非監督數據訓練向量，提升泛化能力。雖然中間步驟難以分解，但因為深度學習有端到端的能力，所以對中間步驟要求較低。而且，深度學習能克服一些傳統模型的缺點，例如LSTM的上下文依賴就比CRF強。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=萬字長文詳解文本抽取：自然語言處理中信息抽取算法技術 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/fe9f47f51b5b4752956cb08ab7403188><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify>但是深度學習也有一些缺點，它在小數據集上的效果難以保證，很可能會過擬合或者難以收斂。例如大家看到TensorBoard經常在抖，就是有這樣的問題。而且大家現在把深度學習調參的工作叫煉丹室，你也不知道好壞就在反覆調。有時候調參的工作量不亞於特徵工程，特徵工程至少知道在做什麼，而想分析調參結果更加困難。另外深度學習對計算資源的要求更高。</p><p class=ql-align-justify>所以我們最終的思考是：<strong>第一要儘可能地收集數據、理解數據</strong>，這是所有做機器學習的同學第一步就應該做的事情。我們應該去分析數據、看數據，而不是一開始就上模型。如果不做數據清洗，好數據、亂數據、髒數據都在裡面，模型是做不好的。就像教孩子一樣，如果好的壞的都教，他就不知道什麼是好壞了。<strong>而且我們要分析問題的本質，選擇合適的模型。</strong>例如，對於已有數據的數據量，選先進模型有用嗎？如果沒有用，就要趕緊去收集數據。</p><p class=ql-align-justify>而且在任務一開始的階段，我比較推薦大家做傳統的機器學習，因為這些模型比較現成，也比較通用。在做了一個非常好的baseline之後，你就知道底線在哪，然後再引用深度學習。去年的達觀杯我們就發現很多參賽者一上來就在用深度學習，結果做了各種調參，效果還不如我們自己20行代碼的傳統的機器學習。所以剛開始的時候一定要讓傳統機器學習幫助你，這樣你更有信心做後面的事情。另外，這句話一定要送給大家：“數據決定效果上限，模型逼近此上限”，所以大家一定要重視數據清理，數據的分析真的比調參調模型收益更大。</p><p class=ql-align-justify>如果遇到疑難雜症，端到端技術經常會有驚喜，但不能保證每次都有驚喜。大家在學習的過程中一定要關心最前沿的技術。</p><p class=ql-align-justify>做機器學習肯定會遇到失敗和挫折，重要的是從挫折中總結規律才是最重要的，不要被同一個坑絆。這樣的經驗很難依靠別人教會，因為所處的環境、場景、場合、數據不可能完全一致，所以需要有自己的思考。</p><p class=ql-align-center><strong>— 完 —</strong></p><p class=ql-align-justify>關注清華-青島數據科學研究院官方微信公眾平臺“<strong>THU數據派</strong>”及姊妹號“<strong>數據派THU</strong>”獲取更多講座福利及優質內容。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>抽取</a></li><li><a>萬字長</a></li><li><a>詳解</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/f8b2a960.html alt="思路方法詳解 高三數學複習難點突破 圓錐曲線中的定點、定值問題" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/afd28158b4384f5eaa05c44c3104e24f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f8b2a960.html title="思路方法詳解 高三數學複習難點突破 圓錐曲線中的定點、定值問題">思路方法詳解 高三數學複習難點突破 圓錐曲線中的定點、定值問題</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/17054f8d.html alt=詳解CD編碼格式（16bit/44.1kHz）合理性之量化篇 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/6f23d12bac3843e2b09628221846ce0f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/17054f8d.html title=詳解CD編碼格式（16bit/44.1kHz）合理性之量化篇>詳解CD編碼格式（16bit/44.1kHz）合理性之量化篇</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c84544ec.html alt=詳解CD編碼格式（16bit/44.1kHz）合理性之採樣篇 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5dc880024dac477f8aed432054712f82 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c84544ec.html title=詳解CD編碼格式（16bit/44.1kHz）合理性之採樣篇>詳解CD編碼格式（16bit/44.1kHz）合理性之採樣篇</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/feaa3a52.html alt=詳解音頻編解碼的原理、演進和應用選型等 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/c0abba2ccef04296b442c6457cc98f49 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/feaa3a52.html title=詳解音頻編解碼的原理、演進和應用選型等>詳解音頻編解碼的原理、演進和應用選型等</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/17eee2b1.html alt="小蜜團隊萬字長文 | 講透對話管理模型最新研究進展" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/249d781057744bdcb94e007250ac41bc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/17eee2b1.html title="小蜜團隊萬字長文 | 講透對話管理模型最新研究進展">小蜜團隊萬字長文 | 講透對話管理模型最新研究進展</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8293e598.html alt=MySQl事務最全詳解 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/c61163c863114226b14bb3760da19e4d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8293e598.html title=MySQl事務最全詳解>MySQl事務最全詳解</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cb944fe7.html alt=Mysql事務詳解(一文讀懂數據庫事務) class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/cb7ba6cbda8c44438d9d3d7c57bd25b9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cb944fe7.html title=Mysql事務詳解(一文讀懂數據庫事務)>Mysql事務詳解(一文讀懂數據庫事務)</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc1a27dd.html alt="詳解Oracle 數據庫啟動過程" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/1538236609349861be8e044 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc1a27dd.html title="詳解Oracle 數據庫啟動過程">詳解Oracle 數據庫啟動過程</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b8bda645.html alt=「Java」隨機數詳解 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/b471807e699b44c2b1dedc580ea3f3af style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b8bda645.html title=「Java」隨機數詳解>「Java」隨機數詳解</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f44ab8f6.html alt=詳解13項鋼筋安裝質量標準及通病防治 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/fd3ff534941a487884c7a67361337cfb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f44ab8f6.html title=詳解13項鋼筋安裝質量標準及通病防治>詳解13項鋼筋安裝質量標準及通病防治</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/464b2d9e.html alt=史上最全電路圖詳解！老電工都收藏了！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1533138404073909636bf43 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/464b2d9e.html title=史上最全電路圖詳解！老電工都收藏了！>史上最全電路圖詳解！老電工都收藏了！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/79cb5594.html alt=每一滴油力爭做到不浪費，詳解各種燃油噴射方式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/3ed00001d75ac552f05c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/79cb5594.html title=每一滴油力爭做到不浪費，詳解各種燃油噴射方式>每一滴油力爭做到不浪費，詳解各種燃油噴射方式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4ea3618a.html alt="Excel萬金油套路詳解 單條件查找返回多行數據" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2633664673fd439a862a2527763e82c7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4ea3618a.html title="Excel萬金油套路詳解 單條件查找返回多行數據">Excel萬金油套路詳解 單條件查找返回多行數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4d1e6d96.html alt=詳解如何使用sum+offset+match函數進行動態區域求和，大神祕籍 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/7360fe7df5e241e7af4b15f75f6474fd style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4d1e6d96.html title=詳解如何使用sum+offset+match函數進行動態區域求和，大神祕籍>詳解如何使用sum+offset+match函數進行動態區域求和，大神祕籍</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e2946ed8.html alt=詳解變配電所的佈置 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9fe764b6e4db4a01b0f251f70eb4e912 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e2946ed8.html title=詳解變配電所的佈置>詳解變配電所的佈置</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>