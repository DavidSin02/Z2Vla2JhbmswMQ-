<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>「語義搜索」愛奇藝深度語義表示學習的探索與實踐 | 极客快訊</title><meta property="og:title" content="「語義搜索」愛奇藝深度語義表示學習的探索與實踐 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/585f4091ce744a8081779ea42cdd1c58"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d212db72.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d212db72.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d212db72.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d212db72.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d212db72.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d212db72.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d212db72.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d212db72.html><meta property="article:published_time" content="2020-11-14T21:01:41+08:00"><meta property="article:modified_time" content="2020-11-14T21:01:41+08:00"><meta name=Keywords content><meta name=description content="「語義搜索」愛奇藝深度語義表示學習的探索與實踐"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/d212db72.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>「語義搜索」愛奇藝深度語義表示學習的探索與實踐</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p><strong>導讀：</strong>基於學術界和工業界經驗，愛奇藝設計和探索出了一套適用於愛奇藝多種業務場景的深度語義表示學習框架。在推薦、搜索、直播等多個業務中的召回、排序、去重、多樣性、語義匹配、聚類等場景上線，提高視頻推薦的豐富性和多樣性，改善用戶觀看和搜索體驗。本文將介紹愛奇藝深度語義表示框架的核心設計思路和實踐心得。</p><p><strong>01</strong></p><p><strong>背景</strong></p><p>英國語言學家 J.R.Firth 在1957年曾說過：" You shall know a word by the company its keeps. " Hinton 於1986年基於該思想首次提出 Distributed representation ( 分佈式表示 ) 的概念，認為具有相似上下文的詞往往具有相似的語義，其中 distributed 是指將詞語的語義分佈到詞向量的各個分量上。該方法可以把詞映射到連續實數向量空間，且相似詞在該空間中位置相近，典型的代表作是基於神經網絡的語言模型 ( Neural Network Language Model，NNLM ) [1]。2003年 Google 提出 word2vec [2] 算法學習 word embedding ( 詞嵌入或詞向量 )，使 Distributed representation 真正受到學術界、工業界的認可， 從而開啟了 NLP embedding 發展的新元代。</p><p>在萬物皆 embedding 的信息流時代，embedding 能夠將文本、圖像、視頻、音頻、用戶等多種實體從一種高維稀疏的離散向量表示 ( one-hot representation ) 映射為一種低維稠密的連續語義表示 ( distributed representation )，並使得相似實體的距離更加接近。其可用於衡量不同實體之間的語義相關性，作為深度模型的語義特徵或離散特徵的預訓練 embedding， 廣泛應用於推薦和搜索等各個業務場景，比如推薦中的召回、排序、去重、多樣性控制等， 搜索中的語義召回、語義相關性匹配、相關搜索、以圖搜劇等。</p><p>相比傳統的 embedding 模型, 深度語義表示學習將實體豐富的 side information ( e.g. 多模態信息, 知識圖譜，meta 信息等 ) 和深度模型 ( e.g. Transformer [3]，圖卷積網絡 [4] 等 ) 進行深度融合，學習同時具有較好泛化性和語義表達性的實體 embedding，為下游各業務模型提供豐富的語義特徵，並在一定程度上解決冷啟動問題, 進而成為提升搜索和推薦系統性能的利器。</p><p>愛奇藝設計和探索出了這套適用於愛奇藝多種業務場景的深度語義表示學習框架，並在推薦的多個業務線以及搜索中成功上線。在短&小視頻、圖文信息流推薦以及搜索、直播等15個業務中的召回、排序、去重、多樣性、語義匹配、聚類等7種場景，完成多個 AB 實驗和全流量上線，短&小視頻以及圖文推薦場景上，用戶的人均消費時長共提升5分鐘以上，搜索語義相關性準確率相比 baseline 單特徵提升6%以上。</p><p><strong>02</strong></p><p><strong>面臨的挑戰</strong></p><p>傳統的 embedding 學習模型主要基於節點序列或基於圖結構隨機遊走生成序列構建訓練集，將序列中的每個節點編碼為一個獨立的 ID，然後採用淺層網絡 ( e.g. item2vec [6]，node2vec [7] ) 學習節點的 embedding。該類模型只能獲取訓練語料中節點的淺層語義表徵，而不能推理新節點的 embedding，無法解決冷啟動問題，泛化性差。將傳統的 embedding 學習模型應用於愛奇藝業務場景中主要面臨以下問題：</p><p><strong>1. Embedding 實體種類及關係多樣性</strong></p><p>傳統的 embedding 模型往往將序列中的 item 視為類型相同的節點，節點之間的關係類型較單一。愛奇藝各業務線中的用戶行為數據往往包含多種類型的數據，比如，文本 ( 長短文本，句子&段落&篇章級別 )、圖像、圖文、視頻 ( 比如，長、短、小視頻 )、用戶 ( 比如 up 主、演員、導演、角色 )、圈子 ( 泡泡、文學等社區 )、query 等；不同類型節點之間具有不同的關係，比如用戶行為序列中節點之間的關係包括點擊、收藏、預約、搜索、關注等，在視頻圖譜中節點之間的關係包括執導、編寫、搭檔、參演等。</p><p><strong>2. Side information 豐富</strong></p><p>傳統的 embedding 模型往往採用淺層網絡 ( 比如3層 DNN，LSTM 等 )，特徵抽取能力較弱；此外將 item 用一個獨立 ID 來表示，並未考慮 item 豐富的 side information 和多模態信息，往往僅能學到 item 的淺層語義表徵。而愛奇藝各業務中的 item 具有豐富的多模態信息 ( 比如，文本、圖像、視頻、音頻 ) 和各種 meta 信息 ( 比如視頻類型、題材、演員屬性等 )，如何有效和充分的利用這些豐富的 side information 以及多模態特徵的融合，對於更好的理解 item 的深層語義至關重要。</p><p><strong>3. 業務場景多樣</strong></p><p>Embedding 可用於推薦中的召回、排序、去重、多樣性以及用戶畫像建模等，搜索中的語義召回、排序、視頻聚類、相關搜索等，以及作為各種下游任務的語義特徵等多種業務場景。不同的業務場景往往需要不同類型的 embedding。</p><p><strong>推薦召回場景：</strong></p><ul><li>基於行為的 embedding 模型召回偏熱門，效果較好；</li><li>基於內容的 embedding 模型召回偏相關性，對相關推薦場景和新內容冷啟動更有幫助；</li><li>基於行為和內容的 embedding 模型介於前兩者之間，能同時保證相關性和效果。</li></ul><p><strong>排序場景：</strong></p><ul><li>往往使用後兩種 embedding 模型，可基於訓練好的模型和內容實時獲取未知節點的 embedding 特徵。</li></ul><p><strong>多樣性控制：</strong></p><ul><li>基於內容原始表示的 embedding 模型用於去重和多樣性打散效果往往較好。</li></ul><p><strong>03</strong></p><p><strong>深度語義表示學習</strong></p><p>深度語義表示學習在傳統的 embedding 學習模型基礎上，引入節點豐富的 side information ( 多模態信息和自身 meta 信息 ) 以及類型的異構性，並對多模態特徵進行有效融合，將淺層模型替換為特徵抽取能力更強的深度模型，從而能夠學習節點的深度語義表徵。</p><p>針對愛奇藝的業務場景和數據特點，我們設計出了一種滿足現有業務場景的深度語義表示學習框架 ( 如圖 1所示 )，該框架主要包含四層：數據層、特徵層、策略層和應用層。</p><ul><li>數據層：主要蒐集用戶的各種行為數據構建節點序列和圖，構建 embedding 模型訓練數據；</li><li>特徵層：主要用於各種模態 ( 文本、圖像、音頻、視頻等 ) 特徵的抽取和融合，作為深度語義表示模型中輸入的初始語義表徵；</li><li>策略層：提供豐富的深度語義表示模型及評估方法，以滿足不同的業務場景；</li><li>應用層：主要為下游各業務線的各種場景提供 embedding 特徵、近鄰以及相關度計算服務。</li></ul><p>下面主要從特徵層和策略層中的各種深度語義表示模型兩方面進行詳細介紹。</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/585f4091ce744a8081779ea42cdd1c58><p class=pgc-img-caption></p></div><p>圖 1 深度語義表示學習框架</p><p><strong>04</strong></p><p><strong>特徵抽取及融合</strong></p><p><strong>1. 多模態特徵抽取</strong></p><p>在自然語言處理 ( NLP ) 領域，預訓練語言模型 ( 比如 BERT [8] ) 能夠充分利用海量無標註語料學習文本潛在的語義信息，刷新了 NLP 領域各個任務的效果。愛奇藝作為中國領先的影音視頻平臺，涵蓋視頻、圖文的搜索、推薦、廣告、智能創作等多種業務場景，除了文本 ( 標題，描述等 ) 外，還需進一步對圖像、視頻和音頻等多種模態信息進行深入理解。借鑑預訓練語言模型的思想，我們嘗試藉助大規模無標註的視頻和圖文語料，學習不同粒度文本 ( query、句子、段落、篇章 )、圖像、音頻和視頻的通用預訓練語義表徵，為後續深度語義表示模型提供初始語義表徵。</p><p><strong>文本語義特徵：</strong></p><p>根據文本長度，可將文本語義特徵抽取分為四個等級：</p><ul><li>詞級別 ( Token-level )，比如用戶搜索串，通常為2~6個字；</li><li>句子級別 ( Sentence-level )，比如視頻&漫畫標題和描述、人物小傳、藝人簡介等；</li><li>段落級別 ( Paragraph-level )，比如影視劇描述，劇本片段等；</li><li>篇章級別 ( Document-level )，比如劇本、小說等長文本。</li></ul><p>受限於現有預訓練語言模型處理長文本的侷限性，對於不同級別的文本需要採用不同的方案。一方面，結合主題模型 [10] 和 ALBert [9] 學習 Topic 粒度的語義特徵；另一方面，基於 ALBert，利用 WME [11]，CPTW [12] 等方法將 token-level 語義組合為段落和篇章級別的細粒度語義特徵。</p><p><strong>圖像語義特徵：</strong></p><p>對於視頻封面圖、視頻幀、影視劇照、藝人圖片、漫畫等圖像，基於 State-of-Art 的 ImageNet 預訓練分類模型 ( e.g. EfficientNet [13] ) 抽取基礎語義表示，並採用自監督表示學習思想 ( e.g. Selfish [14] ) 學習更好的圖像表示。</p><p><strong>音視頻語義特徵：</strong></p><p>對於視頻中的音頻信息，利用基於 YouTube-AudioSet 數據上預訓練的 Vggish [15] 模型從音頻波形中提取具有語義的128維特徵向量作為音頻表示。對於視頻內容的語義建模，我們選擇一種簡單而高效的業界常用方法，僅利用視頻的關鍵幀序表示視頻內容，並通過融合每個關鍵幀的圖像級別語義特徵得到視頻級別的語義特徵。</p><p><strong>2. 多模態特徵融合</strong></p><p><strong>融合時機：</strong></p><p>主要包含 late fusion，early fusion 和 hybrid fusion。顧名思義，early fusion 是指將多個特徵先進行融合 ( e.g. 拼接 )，再通過特徵學習模塊進行訓練；late fusion 是指每個特徵先通過各自的特徵學習模塊變換後再進行融合；hybrid fusion 組合兩種 fusion 時機，可學習豐富的特徵交叉，效果通常最好。</p><p><strong>融合方式：</strong></p><p>高效合理的融合各種多種模態信息，能夠較大程度上提升視頻的語義理解。目前多模態融合方法主要包括三大類方法：</p><ul><li>最為直接的方法：通過 element-wise product/sum 或拼接，融合多模態特徵，但不能有效的捕捉多模態特徵之間的複雜關聯。</li><li>基於 pooling 的方法：主要思想是通過 bilinear pooling 的思想進行多種模態特徵融合，典型代表作包括 MFB [16] 和 MFH [17] 等。</li><li>基於注意力機制的方法: 借鑑 Visual Question Answering ( VQA ) 的思想，注意力機制能夠根據文本表示，讓模型重點關注圖像或視頻中相關的特徵部分，捕捉多種模態之間的關聯性，典型代表作有 BAN ( Bilinear Attention Network ) [18] 等。</li></ul><p><strong>05</strong></p><p><strong>深度語義表示模型</strong></p><p>預訓練模型的應用通常分為兩步：</p><ul><li>先使用大量無監督語料進行進行預訓練 ( pretraining )，學習通用的語義表示；</li><li>再基於該通用語義表示，使用少量標註語料在特定任務上進行微調 ( finetuning )。</li></ul><p>類似地，在文本、圖片、音頻、視頻的通用預訓練語義表徵基礎上，我們嘗試在特定的任務中 ( 比如召回、語義匹配等 ) 引入視頻豐富的 side information ，以及節點和邊類型異構等特點, 並藉助抽取能力更強的深度模型進行微調，以學習滿足不同業務場景的語義特徵。根據建模方式可將深度語義表示模型大致分為以下幾類：</p><p><strong>1. 基於內容的深度語義模型</strong></p><p>基於內容的深度語義模型，顧名思義，模型以單個節點的內容 ( 元數據和多模態信息等 ) 作為輸入，並基於人工標註數據作為監督信號進行訓練，不依賴任何用戶行為數據。該類模型可直接基於節點內容進行推理獲取節點語義表示，無冷啟動問題；但往往需要大量的人工標註數據進行模型訓練。</p><p><strong>① 基於 ImageNet 分類的圖像 embedding 模型</strong></p><p>該類模型主要是基於 State-of-Art的ImageNet 圖像預訓練分類模型的中間層或最後一層，抽取圖像或視頻的純內容表示，並基於自監督表示學習思想 finetuning，作為圖像或視頻的語義表徵，應用於去重 ( 圖 2 ) 和推薦 post-rank 階段多樣性控制兩種場景的效果較好。</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/fdeee04c6e7e478991a355ed062d72ff><p class=pgc-img-caption></p></div><p>圖 2 基於 ImageNet 分類模型和自監督學習方法的去重示例</p><p><strong>② 基於特定任務的 embedding 模型</strong></p><p>該類模型通常基於海量標註數據進行特定任務有監督訓練，並抽取模型中間層或最後一層作為文本或視頻的表徵，比如基於標籤分類任務的 embedding 模型 ( 如圖 3所示 )，該模型基於視頻元數據、文本、圖像、音頻和視頻特徵，在大規模標註數據上訓練，識別視頻的類型標籤和內容標籤。往往抽取模型 fusion 層的表示作為視頻的 topic 粒度語義表徵，可有效解決冷啟動問題，廣泛應用於推薦的召回、排序、多樣性控制場景中。</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c4c0b37c82684987a0b8528745bda0fc><p class=pgc-img-caption></p></div><p>圖 3 基於類型標籤任務的 embedding 模型</p><p><strong>2. 基於匹配的深度語義模型</strong></p><p>該類模型是一種結合內容和行為的深度語義模型，主要通過融合文本、圖像、視頻和音頻等多模態信息，並基於用戶的點擊、觀看或搜索等共現行為作為監督信號，構建正負樣本對</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cc2c077a5bc04144a6ddf843161558f7><p class=pgc-img-caption></p></div><p>，訓練模型使得：</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/08d549107f34431493d665a1c7f7e208><p class=pgc-img-caption></p></div><p>其中 e 表示樣本的語義表徵，x 表示視頻或用戶等。該類模型缺乏對節點的長距離依賴關係和結構相似性建模；但建模相對簡單，模型訓練後可以直接用於推理，可有效解決冷啟動問題，用於召回和排序場景效果較好。</p><p>基於匹配的深度語義模型主要基於 Siamese network ( 孿生網絡或雙塔結構 ) 或多塔結構實現，目前業界較流行的方法包括 DSSM ( Deep Structured Semantic Model ) [5] 和 CDML [20]。DSSM 最初用於搜索建模文本的語義相關性，而 CDML 基於音頻和視頻幀特徵，用於建模視頻的語義相關性，並認為 late fusion 方式的多模態特徵融合效果較好。對於視頻的語義建模，在 DSSM 文本輸入的基礎上，我們額外引入封面圖和視頻兩個模態的預訓練語義表示，改善視頻語義表徵效果。類似地，CDML 還引入文本、封面圖兩種模態的預訓練語義表示，以豐富節點信息；同時針對 CDML 僅採用 late fusion 的特徵融合時機，特徵交互有限且缺乏多樣性的問題，我們採用 hybrid fusion 融合多種模態特徵，學習更為豐富的多模態特徵交叉 ( 如圖 4所示 )。</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7e1c6e362bf549a6a1d04618b7d55227><p class=pgc-img-caption></p></div><p>圖 4 基於 hybrid fusion 的 CDML 模型結構</p><p><strong>3. 基於序列的深度語義模型</strong></p><p>該類模型是一種基於行為的深度語義模型，通過將傳統的淺層網絡 ( e.g. skip-gram，LSTM ) 替換為特徵抽取能力更強的深度網絡 ( e.g. Transformer ) 學習節點的深度語義表徵。給定用戶的行為序列</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3448946f87db4af285de7e324ef0e3eb><p class=pgc-img-caption></p></div><p>，利用 sequential neural network 建模用戶的行為偏好，基於模型的最後一個隱層的表示預測用戶下一個可能點擊的 item。該類模型可用於建模節點的長距離依賴關係，用於推薦場景中的召回效果往往較好，但存在冷啟動問題。</p><p>序列建模的方法主要包含三類：</p><ul><li>基於 MDPs ( Markov decision Processes )：通過狀態轉移概率計算點擊下一個 item 的概率，當前狀態僅依賴前一個狀態，模型較為簡單，適用於短序列和稀疏數據場景建模；</li><li>基於 CNN：利用 CNN 捕獲序列中 item 的短距離依賴關係，比如 Caser [21]，易並行化；</li><li>基於 RNN：可以捕獲長距離依賴關係，適用於長序列和數據豐富的場景，不過模型更復雜，不易並行化，比如 GRU4Rec [22]。</li></ul><p>目前較為流行的序列建模方法主要基於 RNN，為解決 RNN 不易並行和效率較低等問題，我們借鑑業界經驗，採用特徵抽取能力更強, 且易並行的 Transformer ( 如圖 5所示 ) 替換 RNN 進行序列建模，典型的工作包括 SASRec [23]，Bert4Rec [24]。SASRec 使用單向 Transformer decoder ( 右半部分，N=2 )，基於上文建模下一個 item 的點擊概率；而 Bert4Rec 採用雙向 transformer encoder ( 左半部分，N=2 )，借鑑 BERT 的掩碼思想，基於上下文預測 masked items 的點擊概率。此外，由於 BERT 假設 masked items 之間相互獨立，忽略了 masked items 之間的相關性，我們借鑑 XLNet [25] 的自迴歸 ( Auto-regressive ) 思想和排列組合語言模型 ( permutation language model ) 思想，同時建模雙向 context 和 masked item 之間的相關性，提高序列建模效果。</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0ab9101862314e688c1eeffe75b6e191><p class=pgc-img-caption></p></div><p>圖 5 Transformer 網絡結構</p><p><strong>4. 基於 Graph 的深度語義模型</strong></p><p>Graph embedding 模型 ( 又稱為圖嵌入或網絡嵌入 )，可將圖中的節點投影到一個低維連續空間，同時保留網絡結構和固有屬性。深度圖嵌入模型在節點同構圖或異構圖 ( 節點類型或邊類型不同 ) 的基礎上，引入節點豐富的 side information 和多模態特徵，並採用特徵抽取能力更強的網絡，學習節點的深度語義表徵。該類方法建模相比前幾種深度語義模型更加複雜，但可以充分利用豐富的圖結構信息建模節點的高階依賴關係。</p><p><strong>① 引入豐富的 side information 和多模態信息</strong></p><p>傳統 graph embedding 方法主要基於圖結構和某種節點序列採樣策略生成序列數據，並基於 skip-gram 方式學習節點 embedding，如圖 6所示。典型工作包括 DeepWalk，LINE，Node2vec，三者主要區別在於序列生成的採樣策略不同。傳統 graph embedding 模型將所有節點視為 ID，僅能覆蓋訓練集中的高頻節點，無法獲取新節點的 embedding。</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/56bbe4599a274e77a710fbcbea6d3a53><p class=pgc-img-caption></p></div><p>圖 6 傳統 graph embedding 方法基本原理</p><p>為解決新節點的冷啟動問題，一方面，可以在傳統圖嵌入模型中引入節點的多種模態信息，另一方面，還可以充分利用節點豐富的 meta 信息 ( 比如類別，上傳者等 )。屬性網絡 ( Attributed Network Embedding ) 在圖結構的基礎上，額外引入節點的屬性信息，豐富節點的語義表徵，使得具有相似拓撲結構和屬性的節點語義更為接近。對於冷啟動問題，可直接通過節點的屬性 embedding 可獲取新節點 embedding。EGES [26] 和 ANRL [27] 是其中的兩個典型工作。其中，EGES 在 skip-gram 模型的輸入中引入屬性信息。ANRL 將 skip-gram 和 AE 相結合，僅使用屬性特徵作為節點表示，並將傳統 AE 中的 decoder 替換為 neighbor enhancement decoder，使節點和其上下文節點 ( 而非其自身 ) 更為相似。EGES 和 ANRL 主要用於屬性信息豐富的電商領域的圖嵌入，但在視頻推薦領域，除少量長視頻 ( 影視劇 )、演員等具有豐富的屬性外，大部分短、小視頻屬性較稀缺，無法直接複用。為解決該問題，我們提出多模態 ANRL，如圖 7 所示，將節點的屬性特徵和多種模態 ( 文本、封面圖、視頻 ) 的預訓練語義表示特徵一起用於表徵節點，作為模型輸入。對新節點，可直接基於訓練好的模型和節點自身內容 ( 即屬性和多模態特徵 ) 進行推理獲取， 基於多模態 ANRL embedding 的近鄰示例如圖 8 所示。此外，知識圖譜也可以視為一種豐富的 side information，可以嘗試通過引入外部先驗知識進一步學習更好的深度語義表示。</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ded769f8a94d4509a304db269e1efde3><p class=pgc-img-caption></p></div><p>圖 7 多模態 ANRL 模型結構</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dd6ba05d26464c578acd7c410112d9ed><p class=pgc-img-caption></p></div><p>圖 8 多模態 ANRL 近鄰結果示例 ( 左邊第一個為種子視頻，其他為近鄰視頻 )</p><p><strong>② 更先進的特徵抽取器</strong></p><p>傳統的圖嵌入模型通常是基於圖生成序列數據，並採用簡單的 skip-gram 模型學習節點 embedding， 模型過於簡單，特徵抽取能力較弱，僅能建模局部鄰居信息 ( 通常為一階或二階 )。圖神經網絡 ( GNN，Graph Neural Network ) 或圖卷積網絡 ( GCN，Graph Convolutional Network ) 可以直接基於圖結構和節點的多模態特徵，利用特徵抽取能力更強的多層圖卷積迭代的對節點的鄰域子圖進行卷積操作，聚合鄰居特徵 ( textual、visual 特徵等 )，生成節點的深度語義表示。借鑑業界經驗，我們復現了多種 GCN 模型，比如 PinSAGE [28] ( 如圖 9 所示 )，ClusterGCN [29] 等。此外，我們還使用了一個在大規模圖數據上非常快速和可擴展的圖嵌入算法 ProNE [30]。如圖 10 所示，ProNE 先將圖嵌入問題轉換為稀疏矩陣分解問題，高效獲得具有一階鄰居信息的特徵向量，作為節點的初始 embedding；然後再通過頻譜傳播，基於頻域上的 filter 對其進行過濾從而融合高階鄰居信息作為最終的節點深度語義表示，可同時將低階和高階鄰居信息融入節點語義表示。更重要的是，可將常見的網絡嵌入算法 ( 比如 Node2vec 等 ) 生成的 embedding 作為 ProNE 中第一步的節點初始 embedding，再進行頻譜傳播，效果平均會提升~10%。</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/0d75e37253f94d8aa07af2425b05d7ec><p class=pgc-img-caption></p></div><p>圖 9 PinSAGE 模型結構</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7980cba58d3a47d9b0e32ea87713fc9f><p class=pgc-img-caption></p></div><p>圖 10 ProNE 模型結構</p><p><strong>① 建模多元異構圖</strong></p><p>現有方法主要基於具有單一類型節點&邊的網絡圖 ( 同構圖 )，但現實世界中大部分圖都包含多種類型的節點和邊，不同類型的節點往往具有不同的屬性和多模態特徵。比如，在搜索場景中，最簡單的異構圖是用戶的搜索-點擊二部圖，具有兩種類型的節點：query 和視頻，視頻具有豐富的屬性和多模態特徵；而在推薦場景也包含大量異構圖，比如用戶-視頻、視頻-圈子-內容標籤、演員-角色-作品等。</p><p>傳統的 graph embedding 算法會忽略圖中邊的類型以及節點的特徵，比如 node2vec，metapath2vec，雖然 metapath2vec 可用於異構節點的表示學習，但仍然將節點視為 ID，忽略節點豐富的特徵。異構圖 ( HINE，Heterogenous Information Network Embedding ) 深度語義模型同時引入節點的多種模態特徵，和圖中節點和邊類型的多樣性，對不同類型的節點和邊分別建模，其中多元是指圖中具有多種類型的邊。</p><p>我們首先在搜索場景的語義相關性任務中進行了異構圖深度語義表示學習的初步嘗試。語義相關性在搜索中扮演重要角色，可用於搜索語義召回和語義相關性匹配。為衡量 query 和視頻標題的語義相關性，學習 query 和視頻在同一個空間的深度語義表徵，我們基於搜索查詢-點擊異構圖，通過組合 representation-based 和 interaction-based 兩者思想，學習 query 和視頻標題的語義相關性 embedding，模型結構如圖 11 所示, 左邊的 encoder 建模 query 或視頻標題的深度語義表示，用於學習文本的顯示語義相關性；decoder 引入行為相關性約束，用於建模隱式的語義相關性，比如 &lt;query:李菁菁，title:歡天喜地對親家>, 前者是後者的主要演員之一。右側用於建模 query 和視頻標題之間的多粒度交互語義。相比於 baseline，語義相關性準確率提升6%以上， 表 1給出了部分 query-title 語義相關性例子。除點擊類型外，目前還在嘗試引入收藏、評論、點贊等邊類型，和視頻類型 ( 比如長、短、小視頻, 專輯和播單等 )，以及在視頻側引入封面圖和視頻模態特徵，進行更為細緻的建模。</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bda5ac8d0c67401a8d1945a9e03e3574><p class=pgc-img-caption></p></div><p>圖 11 搜索 query-title 語義相關性 embedding 模型結構</p><p>目前也正在將該模型遷移到推薦場景中學習用戶和視頻、圈子以及標籤等之間的同空間語義相關性。此外，最近還引入了阿里在異構圖表示學習方面的工作 GATNE-I [31]，支持多源異構網絡的表示學習和以及具有強大特徵抽取器的 HGT ( Heterogeneous Graph Transformer ) [32] 網絡，並引入節點的多模態特徵，嘗試學習效果更好的節點深度語義表示。</p><div class=pgc-img><img alt=「語義搜索」愛奇藝深度語義表示學習的探索與實踐 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6b0b74781c5a46debcb9ae7c0c1d4d0f><p class=pgc-img-caption></p></div><p>表 1 搜索 query-title embedding 語義相關性例子</p><p><strong>06</strong></p><p><strong>後續優化</strong></p><p><strong>1. 視頻通用預訓練語義表示</strong></p><p>由於時間性能和視頻語義表示預訓練數據缺乏等因素，目前僅簡單的通過融合視頻關鍵幀序的圖像級別特徵得到視頻的語義特徵。後續將基於大量 video captioning 數據，借鑑 BERT 思想學習視頻預訓練語義模型 ( e.g.UniViLM [35] ) 抽取視頻的深度語義表徵。</p><p><strong>2. 融入知識圖譜先驗的深度語義表示學習</strong></p><p>視頻的文本和描述中往往包含一些實體 ( 比如標題"漫威英雄內戰，鋼鐵俠為隊友量身打造制服，美隊看傻了"中包含實體"漫威、鋼鐵俠" )，通過在文本表徵中引入圖譜中的實體，以及實體之間關係等先驗知識 ( "鋼鐵俠"和"復仇者聯盟" )，能夠進一步提升語義表徵的效果。後續將嘗試在 NLP 預訓練語言模型和推薦場景中引入知識圖譜，分別用於提升文本語義表徵效果 ( 比如 KEPLER [33] ) 和發現用戶深層次用戶興趣，提升推薦的準確性，多樣性和可解釋性 ( e.g. KGCN [34] )。</p><p><strong>3. 覆蓋更多的業務</strong></p><p>深度語義表示通常用於視頻智能分發場景，目前已經覆蓋愛奇藝的長&短&小視頻、直播、圖文、漫畫等推薦和搜索業務；後續將持續增加愛奇藝智能製作場景的支持，為各種業務場景提供深層次語義特徵。</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>語義</a></li><li><a>愛奇藝</a></li><li><a>學習</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/824bb18c.html alt=自然語言的語義表示學習方法與應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/521cf0b197594fb3957739696fc08bc7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/824bb18c.html title=自然語言的語義表示學習方法與應用>自然語言的語義表示學習方法與應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a895615c.html alt=語義分割的經典學習方法和深度學習方法綜述 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bb5a9fd3cedf4dc29238f1faf98d704f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a895615c.html title=語義分割的經典學習方法和深度學習方法綜述>語義分割的經典學習方法和深度學習方法綜述</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8fcabd80.html alt=亞馬遜的Alexa的語義分析性能得到大幅度提高 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RPsVq1PFCI1UDR style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8fcabd80.html title=亞馬遜的Alexa的語義分析性能得到大幅度提高>亞馬遜的Alexa的語義分析性能得到大幅度提高</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/16133b9e.html alt=深度語義模型以及在淘寶搜索中的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/48529b82dc7c4f048fc3e45d586717e6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/16133b9e.html title=深度語義模型以及在淘寶搜索中的應用>深度語義模型以及在淘寶搜索中的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f4c5c93c.html alt=谷歌推出自然語言框架語義解析器SLING，但沒說有沒有用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/46ea0001172cab9535dc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f4c5c93c.html title=谷歌推出自然語言框架語義解析器SLING，但沒說有沒有用>谷歌推出自然語言框架語義解析器SLING，但沒說有沒有用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html alt=直流鍋爐給水控制學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/eba10edcc8d14d9f8cde6fd5b212d90e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html title=直流鍋爐給水控制學習>直流鍋爐給水控制學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html alt=HTMLCSS學習筆記（六）——元素類型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb5988349894ce9bf568c6418f85b7d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html title=HTMLCSS學習筆記（六）——元素類型>HTMLCSS學習筆記（六）——元素類型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html alt="web前端（從零開始），每天更新學習筆記 HTML5元素分類" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/46d70004fcd55e1ddad3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html title="web前端（從零開始），每天更新學習筆記 HTML5元素分類">web前端（從零開始），每天更新學習筆記 HTML5元素分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html alt="MySQL 學習筆記" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html title="MySQL 學習筆記">MySQL 學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html alt=深入學習MySQL事務：ACID特性的實現原理「轉」 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/cdc702d66d6943499997d11e931425eb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html title=深入學習MySQL事務：ACID特性的實現原理「轉」>深入學習MySQL事務：ACID特性的實現原理「轉」</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html alt=如何學習模擬IC設計？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html title=如何學習模擬IC設計？>如何學習模擬IC設計？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html alt=小猿圈python學習-三大特性之多態 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ad0e8e3777854337abeb7c779ad79a04 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html title=小猿圈python學習-三大特性之多態>小猿圈python學習-三大特性之多態</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>