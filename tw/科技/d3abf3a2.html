<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能 | 极客快訊</title><meta property="og:title" content="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p9.pstatp.com/large/pgc-image/Rjm7XFREsRlxK"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d3abf3a2.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d3abf3a2.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d3abf3a2.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d3abf3a2.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d3abf3a2.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d3abf3a2.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d3abf3a2.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d3abf3a2.html><meta property="article:published_time" content="2020-11-14T21:02:25+08:00"><meta property="article:modified_time" content="2020-11-14T21:02:25+08:00"><meta name=Keywords content><meta name=description content="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/d3abf3a2.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p><strong>雷鋒網 AI 開發者按：</strong>一個月前，在「AICon 全球人工智能與機器學習技術大會」上，華為諾亞方舟實驗首席科學家劉群剛分享了新發布的中文預訓練語言模型 NEZHA（哪吒）；就在這兩天，NEZHA 已在 Github 上開源，同時開源的還有壓縮 BERT 模型「TinyBERT」，它在推理時大小可縮小 7.5 倍，並且速度加快 9.4 倍。</p><p>可以看到的是近兩年預訓練模型的發展非常快速，從 Word2Vec 到 ULMFiT、CoVe 再到 BERT、XLNET 等，都各有特點且在不斷完善中。聚焦於「多項中文 NLP 任務性能」的 NEZHA 也有亮眼的性能升級。在此，雷鋒網 AI 開發者將 NEZHA 詳細內容及 TinyBERT 相關地址整理如下。</p><img alt="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Rjm7XFREsRlxK><p></p><h3>NEZHA 開發背景</h3><p>預訓練語言模型本質上，就是神經網絡語言模型。它主要有兩個特點，即：可以使用大規模無標註純文本語料進行訓練，以及可以用於各類下游 NLP 任務，各項性能指標均獲得大幅度提高，並可以將各類下游任務的解決方案統一簡化為集中固定的 fine-tune 框架。</p><p>預訓練語言模型通常有兩個大類型。一類是 Encoder，用於自然語言理解，輸入整個文章，用於自然語言理解；另一類是 Decoder，是解碼式的，用於自然語言生成，只能來看到已經生成的內容，看不到沒有生成的內容，這兩類模型有所區別。</p><p>更直觀來看，github 上來自清華大學的兩位同學——王曉智和張正彥（在讀本科生）整理的一份關於預訓練模型的關係圖，則可以從功能方面更簡單明瞭的幫我們理解該類模型類別。</p><img alt="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rjm7XFzAAWM8Yq><p>預訓練模型的關係圖</p><blockquote><p>更多詳細內容，可參見 PLM 論文整理 Github 項目地址：</p><p>https://github.com/thunlp/PLMpapers</p></blockquote><p>圖中列出了 BERT、GPT、XLNet、ERNIE 等模型以及它們之間的關係，並擬出了一份相關的論文列表。列表把預訓練模型主要分為了三個部分，包括：模型、知識蒸餾與模型壓縮。按照這樣的分類，TinyBERT 模型則可以歸類為「知識蒸餾與模型壓縮」部分；NEZHA 則歸為「模型」部分。</p><p>而根據研究結果顯示，近年來的模型大多將重心落到了數據與算力部分。與早期的 ResNet（視覺模型）模型參數相比，數據顯示 GPT1 為 100M，BERT large 為 340M，GPT2 為 1.5BN，GPT-2 8B 為 8.3BN。</p><img alt="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rjm7XGTEPwOVl2><p>預訓練語言模型研究結果</p><p>因此，在預訓練模型開發過程中，華為諾亞方舟研究者提出了一種為基於 transformer 的模型設計的知識蒸餾方法——壓縮 BERT 模型 TinyBERT；該模型大小不到 BERT 的 1/7，但速度是 BERT 的 9 倍多。</p><p>而在模型方面，他們選擇在內部重現了 Google Bert-base 和 Bert-large 的實驗；利用 BERT 的代碼，實現了 OpenAI GPT-2 模型；實現基於 GPU 多卡多機並行訓練，並且對訓練過程進行了優化，提高訓練效率，最終得到了「多中文 NLP 任務」預訓練模型 NEZHA。</p><p></p><h3>三頭六臂 NEZHA（哪吒）</h3><p>儘管這一預訓練模型 NEZHA 的名稱聽起來有些匪夷所思，但它的開發者們將其視為「無所不能，可以解決不同任務」的寓意。在這個模型中，除了之前提到的重現、多卡多機並行訓練之外，主要有兩項改進，即：函數式相對位置編碼與全詞覆蓋的實現。</p><img alt="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rjm7XGu4nx7rkO><p><strong>一、函數式相對位置編碼</strong></p><p>位置編碼有函數式和參數式兩種，函數式通過定義函數直接計算就可以了。參數式中位置編碼涉及兩個概念，一個是距離；二是維度。其中，Word Embedding 一般有幾百維，每一維各有一個值，一個位置編碼的值正是通過位置和維度兩個參數來確定。</p><p>NEZHA 預訓練模型則採用了函數式相對位置編碼，其輸出與注意力得分的計算涉及到他們相對位置的正弦函數，這一靈感正是來源於 Transformer 的絕對位置編碼，而相對位置編碼則解決了在 Transformer 中，每個詞之間因為互不知道相隔的距離引發的一系列資源佔用問題。</p><img alt="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/Rjm7XH46KuD6Ao><p>位置編碼模型</p><p>具體而言，Transformer 最早只考慮了絕對位置編碼，而且是函數式的；後來 BERT 的提出就使用了參數式，而參數式訓練則會受收到句子長度的影響，BERT 起初訓練的句子最長為 512，如果只訓練到 128 長度的句子，在 128~520 之間的位置參數就無法獲得，所以必須要訓練更長的語料來確定這一部分的參數。</p><p>而在 NEZHA 模型中，距離和維度都是由正弦函數導出的，並且在模型訓練期間是固定的。也就是說，位置編碼的每個維度對應一個正弦，不同維度的正弦函數具有不同的波長，而選擇固定正弦函數，則可以使該模型具有更強的擴展性；即當它遇到比訓練中序列長度更長的序列時，依然可以發揮作用。函數式相對位置編碼公式，如下圖所示：</p><img alt="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Rjm7XUb5zZXQgi><p><strong>二、全詞覆蓋</strong></p><p>現在的神經網絡模型無論是在語言模型還是機器翻譯任務中，都會用到一個詞表；而在 Softmax 時，每個詞都要嘗試比較一下。每次運算時，所有詞要都在詞表中對比一遍，往往一個詞表會包含幾萬個詞，而機器翻譯則經常達到六七萬個詞，因此，詞表是語言模型運算中較大的瓶頸。</p><p>而 NEZHA 預訓練模型，則採用了全詞覆蓋（WWM）策略，當一個漢字被覆蓋時，屬於同一個漢字的其他漢字都被一起覆蓋。該策略被證明比 BERT 中的隨機覆蓋訓練（即每個符號或漢字都被隨機屏蔽）更有效。</p><img alt="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rjm7XUzHmJca3i><p>BERT 中的隨機覆蓋</p><p>在 NEZHA 的 WWM 實現中，研究者使用了一個標記化工具 Jieba2 進行中文分詞（即尋找中文單詞的邊界）。在 WWM 訓練數據中，每個樣本包含多個覆蓋漢字，覆蓋漢字的總數約佔其長度的 12%，隨機替換的佔 1.5%，儘管這樣預測整個詞運算難度有所增加，但最終取得的效果更好。</p><p><strong>三、混合精度訓練及 LAMB 優化器</strong></p><p>在 NEZHA 模型的預訓練中，研究者採用了混合精度訓練技術。該技術可以使訓練速度提高 2-3 倍，同時也減少了模型的空間消耗，從而可以利用較大的批量。</p><p>傳統的深度神經網絡訓練使用 FP32（即單精度浮點格式）來表示訓練中涉及的所有變量（包括模型參數和梯度）；而混合精度訓練在訓練中採用了多精度。具體而言，它重點保證模型中權重的單精度副本（稱為主權重），即在每次訓練迭代中，將主權值舍入 FP16（即半精度浮點格式），並使用 FP16 格式存儲的權值、激活和梯度執行向前和向後傳遞；最後將梯度轉換為 FP32 格式，並使用 FP32 梯度更新主權重。</p><img alt="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rjm7XVIG7C8yol><p>LAMB 優化器則是為專為深度神經元網絡的大批量同步分佈訓練而設計。儘管大小批量 DNN 訓練是加快 DNN 訓練速度的有效方法，但是如果不仔細調整學習速率的調度，當批量處理的大小超過某個閾值時，模型的性能可能會受到很大影響。</p><p>LAMB 優化器則不需要手動調整學習速率，而是採用了一種通用的自適應策略。優化器通過使用非常大的批量處理大小（實驗中高達 30k 以上）來加速 BERT 的訓練，而不會導致性能損失，甚至在許多任務中獲得最先進的性能。值得注意的是，BERT 的訓練時間最終從 3 天顯著縮短到 76 分鐘。</p><img alt="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/Rjm7XXu8P4TLkx><p></p><h3>NEZHA 實驗結果</h3><p>實驗通過對各種自然語言理解（NLU）任務進行微調來測試預訓練模型的性能，並將 NEZHA 模型和最先進的漢語預訓練語言模型：谷歌 BERT（漢語版），BERT-WWM 以及 ERNIE 進行了對比，最終結果如下：</p><img alt="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Rjm7XYDE46gIFW><p>NEZHA 實驗結果</p><p>可以看到，NEZHA 在大部分情況下，都取得了相較更好的性能；尤其在 PD-NER 任務下，NEZHA 最高達到了 97.87 分。另一個表現較亮眼的模型還有 ERNIE Baidu 2.0，頗有超越 NEZHA 的趨勢。關於這個情況，論文中作者也解釋到，由於實驗設置或微調方法可能存在差異，比較可能不完全公平，之後其它模型新版發佈後，他們將在相同的設置下對其進行評估並更新此報告。</p><img alt="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/Rjm7XsW2Zx4Oy2><blockquote><p>更多詳情，可參見 NEZHA 論文地址：</p><p>https://arxiv.org/pdf/1909.00204.pdf</p><br><p>關於知識蒸餾模型 TinyBERT 詳細解讀，可參考往期內容：</p><p>https://mp.weixin.qq.com/s/f2vxlhaGW1wnu8UYrvh-tA</p><p>Github 開源地址（包含 NEZHA 與 TinyBERT ）：</p><p>https://github.com/huawei-noah/Pretrained-Language-Model</p></blockquote><p><strong>雷鋒網年度評選——</strong><strong>尋找19大行業的最佳AI落地實踐</strong></p><p>創立於2017年的「AI最佳掘金案例年度榜單」，是業內首個人工智能商業案例評選活動。雷鋒網從商用維度出發，尋找人工智能在各個行業的最佳落地實踐。</p><p>第三屆評選已正式啟動，關注微信公眾號“雷鋒網”，回覆關鍵詞“榜單”參與報名。詳情可諮詢微信號：xqxq_xq</p><img alt="華為開源預訓練語言模型「哪吒」：編碼、掩碼升級，提升多項中文 NLP 任務性能" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RhmqHzzGzZcQi7></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>華為</a></li><li><a>開源</a></li><li><a>預訓練</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/47ab8d9.html alt=華為正式開源AI框架MindSpore，及業界最快自動網絡架構搜索技術 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/599bb1683f684fde91cb6e7b2d5278a9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/47ab8d9.html title=華為正式開源AI框架MindSpore，及業界最快自動網絡架構搜索技術>華為正式開源AI框架MindSpore，及業界最快自動網絡架構搜索技術</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a2871b0c.html alt="可用於企業的 7 個最佳開源 Web 服務器！" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1531874286737ba179830a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a2871b0c.html title="可用於企業的 7 個最佳開源 Web 服務器！">可用於企業的 7 個最佳開源 Web 服務器！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7716935e.html alt="華為 FreeBuds Pro 體驗" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/d875ba12de174368bd8d1546d44dc699 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7716935e.html title="華為 FreeBuds Pro 體驗">華為 FreeBuds Pro 體驗</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/cd4cfe46.html alt=華為P40手機5200萬像素像素，能打敗小米1億像素鏡頭嗎？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/fc9919cf92c34784beed41e142f20503 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/cd4cfe46.html title=華為P40手機5200萬像素像素，能打敗小米1億像素鏡頭嗎？>華為P40手機5200萬像素像素，能打敗小米1億像素鏡頭嗎？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/51c6dcb7.html alt=經典分享：華為模擬集成電路設計講義 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ReTMiwsAymTebE style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/51c6dcb7.html title=經典分享：華為模擬集成電路設計講義>經典分享：華為模擬集成電路設計講義</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b690d039.html alt=「專利解密」“多穿一堵牆”——華為Wifi6+如何提升功率譜密度 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4ae57316ed024517a9bf8396c80de9f0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b690d039.html title=「專利解密」“多穿一堵牆”——華為Wifi6+如何提升功率譜密度>「專利解密」“多穿一堵牆”——華為Wifi6+如何提升功率譜密度</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/5cd7abc7.html alt=分享一組開源的匹配中國大陸手機號碼的正則表達式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ef9e7cc39f8b46b6a66d5b3de5c68e5c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/5cd7abc7.html title=分享一組開源的匹配中國大陸手機號碼的正則表達式>分享一組開源的匹配中國大陸手機號碼的正則表達式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2dc2b98b.html alt=和華為合作有望落地，股價又將翻倍的風華高科，要注意些什麼？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/c3776da5ae4a453eacb21a7b4ac70cb3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2dc2b98b.html title=和華為合作有望落地，股價又將翻倍的風華高科，要注意些什麼？>和華為合作有望落地，股價又將翻倍的風華高科，要注意些什麼？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d21477ec.html alt="Huawei/華為麥芒9新機發布，感覺很實在 參數配置指南" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/c28c82ca-8b9f-4b9a-bb4c-299630be7546 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d21477ec.html title="Huawei/華為麥芒9新機發布，感覺很實在 參數配置指南">Huawei/華為麥芒9新機發布，感覺很實在 參數配置指南</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ba7a4e76.html alt="華為P40系列旗艦影像實力的背後，XD Fusion圖像引擎功不可沒！" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/0cf20266b951477cb87fdb5a7ce20f32 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ba7a4e76.html title="華為P40系列旗艦影像實力的背後，XD Fusion圖像引擎功不可沒！">華為P40系列旗艦影像實力的背後，XD Fusion圖像引擎功不可沒！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fa0e7702.html alt=阿里巴巴開源的超輕量的跨平臺圖形渲染引擎——GCanvas class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/549e2d3e22db49d4a5e13667f7bdd3e3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fa0e7702.html title=阿里巴巴開源的超輕量的跨平臺圖形渲染引擎——GCanvas>阿里巴巴開源的超輕量的跨平臺圖形渲染引擎——GCanvas</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c6b2a07d.html alt=華為Phoenix引擎：定義圖形之美 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/0999d0958a034618a5ebb844544e3c5b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c6b2a07d.html title=華為Phoenix引擎：定義圖形之美>華為Phoenix引擎：定義圖形之美</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9ef2b040.html alt=開源的的二維繪圖引擎，EChart在用的圖形渲染器——ZRender class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/c2d8dcb7ff7945758fe1df812324bd89 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9ef2b040.html title=開源的的二維繪圖引擎，EChart在用的圖形渲染器——ZRender>開源的的二維繪圖引擎，EChart在用的圖形渲染器——ZRender</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9856fe2d.html alt=華為推進CloudOptiX技術創新，引領全光網新時代 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/15299385055354e5a2775aa style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9856fe2d.html title=華為推進CloudOptiX技術創新，引領全光網新時代>華為推進CloudOptiX技術創新，引領全光網新時代</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fed0e97d.html alt=華為、騰訊、阿里被評亞洲品牌前10強；聯通電信將聯手共建5G網絡 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/f254f371b68a426eb3b1dadaa6992676 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fed0e97d.html title=華為、騰訊、阿里被評亞洲品牌前10強；聯通電信將聯手共建5G網絡>華為、騰訊、阿里被評亞洲品牌前10強；聯通電信將聯手共建5G網絡</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>