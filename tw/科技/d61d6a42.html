<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>支持向量機（第八章） | 极客快訊</title><meta property="og:title" content="支持向量機（第八章） - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/153985032133768ee02ea64"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d61d6a42.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d61d6a42.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d61d6a42.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d61d6a42.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d61d6a42.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d61d6a42.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/d61d6a42.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/d61d6a42.html><meta property="article:published_time" content="2020-11-14T21:05:18+08:00"><meta property="article:modified_time" content="2020-11-14T21:05:18+08:00"><meta name=Keywords content><meta name=description content="支持向量機（第八章）"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/d61d6a42.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>支持向量機（第八章）</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><h1>第八章 多類SVMs</h1><p>吳天暉/譯</p><p>SVMs可以生成二分分類器。然而，我們經常面對的數據集多於兩類。舉例，原葡萄酒數據集實際上包含三種不同的產品。有很多種方法可以讓SVMs在多類下工作。在這章裡，我們將介紹一些非常流行的多類分類方法和討論它們的細節。</p><p>這一章裡的所有代碼，我們將用代碼表41產生的數據集(如圖51所示)。</p><pre>代碼表41import numpy as np def load_X():return np.array([[1, 6], [1, 7], [2, 5], [2, 8],[4, 2], [4, 3], [5, 1], [5, 2],[5, 3], [6, 1], [6, 2], [9, 4],[9, 7], [10, 5], [10, 6], [11, 6],[5, 9], [5, 10], [5, 11], [6, 9],[6, 10], [7, 10], [8, 11]])def load_y():return np.array([1, 1, 1, 1,2, 2, 2, 2, 2, 2, 2,3, 3, 3, 3, 3,4, 4, 4, 4, 4, 4, 4])</pre><div class=pgc-img><img alt=支持向量機（第八章） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/153985032133768ee02ea64><p class=pgc-img-caption>圖51:4個類的分類問題</p></div><p><strong>解多類二分問題</strong></p><p><strong>一對多（One-against-all ）</strong></p><p>也叫一對所有（one-versus-the-rest），這可能是最簡單的方法。</p><p>為了分類K個類，我們構造K個不同的二分分類器。對於一個給出的類，這個類中的所有示例點都為正，不在這個類裡的所有示例點都為負（代碼表42)。</p><p>代碼表42</p><pre>import numpy as npfrom sklearn import svm# Create a simple datasetX = load_X()y = load_y()# Transform the 4 classes problem# in 4 binary classes problems.y_1 = np.where(y == 1, 1, -1)y_2 = np.where(y == 2, 1, -1)y_3 = np.where(y == 3, 1, -1)y_4 = np.where(y == 4, 1, -1)</pre><p>所有的問題我們用二分分類器來訓練（代碼表43）。結果，我們每一個分類器都得到一個決策邊緣（如圖52）。</p><pre>代碼表43# Train one binary classifier on each problem.y_list = [y_1, y_2, y_3, y_4]classifiers = []for y_i in y_list:clf = svm.SVC(kernel='linear', C=1000)clf.fit(X, y_i)classifiers.append(clf)</pre><div class=pgc-img><img alt=支持向量機（第八章） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1539850321389bdd266e7d0><p class=pgc-img-caption>圖52:一對多方法為每一個類建一個分類器</p></div><p>為了做一個新的預測，我們用分類器去預測分類器中返回正值的類（代碼表44)。然而，這可能會給出不一致的結果，因為一個標量可能同時分配給多類或為空(Bishop, 2006)。圖53顯示了這個問題；一對多分類器不能預測分類分佈在每一角落的藍色區域中的數據,因為兩個分類器都同時給出正值預測。這將返回一個示例數據同時屬於兩個類。同樣的問題會發生在中間，因為每一個分類器都給出正值。結果，在這個區域沒有類可以用來分配。</p><pre>代碼表44def predict_class(X, classifiers):predictions = np.zeros((X.shape[0], len(classifiers)))for idx, clf in enumerate(classifiers):predictions[:, idx] = clf.predict(X)# returns the class number if only one classifier predicted it 	# returns zero otherwise. 	return np.where((predictions == 1).sum(1) == 1,(predictions == 1).argmax(axis=1) + 1,0)</pre><div class=pgc-img><img alt=支持向量機（第八章） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15398503214764bb2b9ef14><p class=pgc-img-caption>圖53:一對多導致模糊的決策</p></div><p>作為一個替代方案，Vladimir Vapnik 建議哪一個決策函數返回的值最大分類器就將其分到哪個類（Vapnik V. N., 1998 ）。代碼表45演示了這個建議。注意這裡我們用decision_function 代替分類器中的predict方法。這個方法返回一個正的真實值當這個示例被分類器分類到正確的一邊時，如果在另一邊則返回負值。有趣的是注意到它用的是最大值，而不是最大的絕對值，如果所有分類器有爭執的情況下這個方法將選擇分給最接近的超平面的類。舉例，示例點（6,4)在圖中將被分配給這個藍星類。</p><pre>代碼表45def predict_class(X, classifiers):predictions = np.zeros((X.shape[0], len(classifiers)))for idx, clf in enumerate(classifiers):predictions[:, idx] = clf.decision_function(X)# return the argmax of the decision function as suggested by Vapnik.return np.argmax(predictions, axis=1) + 1</pre><p>如圖54所示，應用了這個啟發式讓我們的分類結果沒有模糊值。這個方法的主要缺點是不同的分類器要訓練不同的任務，所以不能但保decision_function 方法在相同變形下返回的質量。如果一個決策方法比其他方法返回大值多10次，這個類將對同一個示例數據做錯誤的分配。</p><div class=pgc-img><img alt=支持向量機（第八章） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1539850321561e84aa1484b><p class=pgc-img-caption>圖54:應用一個簡單的啟發式避免模糊決策問題</p></div><p>一對多方法另一個問題是訓練集是不均衡的(Bishop, 2006) 。如果一個問題有100類，每個類有10個示例數據，每一個分類器將被10個正值示例數據和990個負值示例數據訓練。因此，負值示例數據將大大影響決策邊緣。</p><p>儘管如此，一對多仍是一個流行的多類分類方法，因為它容易實現和理解。</p><p><strong>注意：</strong>"[…]實際上一對多方法被廣泛用盡管它有特製性和實際侷限。" (Bishop, 2006)</p><p>當我們使用的sklean,linearSVC自動機把一對多作為默認策略時。你也可以明確指定multi_class參數使用ovr(one-vs-the-rest),如代碼表46所示。</p><pre>代碼表46from sklearn.svm import LinearSVCimport numpy as npX = load_X()y = load_y()clf = LinearSVC(C=1000, random_state=88, multi_class='ovr')clf.fit(X,y)# Make predictions on two examples.X_to_predict = np.array([[5,5],[2,5]])print(clf.predict(X_to_predict)) # prints [2 1]</pre><p><strong>一對一(one-against-one)</strong></p><p>在這個方法中，我們試著去找區分一個類與另一個類來代替區分一個類與所有的類來代替。結果，我們用每對類去試驗一個分類器，會導致K個類有K(K-1)/2個分類器。每一個分類器訓練數據子集而產生它自己的決策邊緣（圖）。</p><p>預測使用一個簡單的投票策略。每一個希望預測的示例通過每一個分類器後，這個預測類被記錄下來。然後，這個具有最多的票數的類被分配給示例數據（代碼表47)。</p><pre>代碼表47from itertools import combinationsfrom scipy.stats import modefrom sklearn import svmimport numpy as np# Predict the class having the max number of votes.def predict_class(X, classifiers, class_pairs):predictions = np.zeros((X.shape[0], len(classifiers)))for idx, clf in enumerate(classifiers):class_pair = class_pairs[idx]prediction = clf.predict(X)predictions[:, idx] = np.where(prediction == 1,class_pair[0], class_pair[1])return mode(predictions, axis=1)[0].ravel().astype(int)X = load_X()y = load_y()# Create datasets.training_data = []class_pairs = list(combinations(set(y), 2))for class_pair in class_pairs:class_mask = np.where((y == class_pair[0]) | (y == class_pair[1]))y_i = np.where(y[class_mask] == class_pair[0], 1, -1)training_data.append((X[class_mask], y_i))# Train one classifier per class.classifiers = [] for data in training_data:clf = svm.SVC(kernel='linear', C=1000)clf.fit(data[0], data[1]) classifiers.append(clf)# Make predictions on two examples.X_to_predict = np.array([[5,5],[2,5]])print(predict_class(X_to_predict, classifiers, class_pairs))# prints [2 1]</pre><div class=pgc-img><img alt=支持向量機（第八章） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/153985032202089454562c0><p class=pgc-img-caption>圖55:一對一為所有分類中的每對類構造一個分類器</p></div><p>使用這個方法，我們仍然要面臨模糊分類的問題。如果兩個類有完全相同的票數，算法建議選擇序號小的可行的（不見得是最好的）策略（Hsu & Lin, A Comparison of Methods for Multi-class Support Vector Machines, 2002 ）。</p><div class=pgc-img><img alt=支持向量機（第八章） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153985032213918acad5888><p class=pgc-img-caption>圖56: 使用投票框架預測</p></div><p>上圖顯示一對一策略生成的決策區域與一對多所生成的不同。在圖57中，注意一對一分類器所生成的區域，區域的顏色變化僅在超平面內（注意黑色直線），這不同於一對多。</p><div class=pgc-img><img alt=支持向量機（第八章） onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15398503209016e5ae76a38><p class=pgc-img-caption>圖57:一對多（左）與一對一（右）的對比</p></div><p>一對多（左）與一對一（右）的對比一對一方法是sklearn默認的多類分類方法。作為取代代碼表47，在代碼表48中你將得到更精確的同樣的結果。</p><pre>代碼表48from sklearn import svmimport numpy as npX = load_X()y = load_y()# Train a multi-class classifier.clf = svm.SVC(kernel='linear', C=1000)clf.fit(X,y)# Make predictions on two examples.X_to_predict = np.array([[5,5],[2,5]])print(clf.predict(X_to_predict)) # prints [2 1]</pre><p>一對多方法的有一個主要缺點是分類器容易趨向過擬合。然而，大量的類會使分類器的數量超線性增長，所以這個方法在解決大問題時必然會變慢(Platt, Cristianini, & Shawe-Taylor, 2000 )。</p><p><strong>DAGSVM</strong></p><p>DAGSVM的名字為單向無環圖SVM（Directed Acyclic Graph SVM）的縮寫。由John Platt 等在2000年提出用來提升一對一方法(Platt, Cristianini, & Shawe-Taylor, 2000) 。</p><p>注意：John C. Platt 發明了SMO算法和普拉特校準算法（Platt Scaling)，還提出了DAGSVM。給SVMs作出巨大貢獻。</p><p>DAGSVM背後的目標是用和一對一相同的訓練方法，但是它用單向無環圖（DAG）來選擇可用分類器使檢測速度大幅提高。</p><p>如果我們有4個類A,B,C和D,則每對類組合需要六個分類器：(A, B); (A, C); (A, D); (B, C); (B, D); 和 (C, D)。我們先用分類器（A，D），它預測類A，也就是預測非D的數據，每二個分類器同樣預測A(非C）。它的意思是分類器(B, D), (B, C) 或者 (C, D)可以略過，因為我們已經知道數據不會在類C或D中。最後用到分類器（A，B），如果我們要預測B，我們分配數據點到類B。這個示例的技巧用紅色路徑標在圖58中。圖中的每一個節點表示一對類。</p><div class=pgc-img><img alt=支持向量機（第八章） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1539850321030f96097e118><p class=pgc-img-caption>圖58:預測路徑顯示為一個有向無環圖</p></div><p>四個類，我們用三個分類器做預測，代替一對一的六個。通常，平均K個類可用K-1個決策節點。</p><p>我們用代碼表49來取代代碼表47的predict_class 方法可得出同樣的結果，但是好處是少用了很多分類器。</p><p>在代碼表49中，我們用一個List實現了DAGSVM方法。我們從這個List中的可用類開始，經過每一次預測，我們移去一些不需要的。最後，留下一個類是我們要給它分配示例數據的。</p><p>注意代碼表49,這裡顯示的代碼不能用於生產環境，因為當數據集（X）特別大時它會變慢。</p><pre>代碼表49def predict_class(X, classifiers, distinct_classes, class_pairs):results = []for x_row in X:class_list = list(distinct_classes)# After each prediction, delete the rejected class# until there is only one class.while len(class_list) &gt; 1:# We start with the pair of the first and# last element in the list.class_pair = (class_list[0], class_list[-1])classifier_index = class_pairs.index(class_pair)y_pred = classifiers[classifier_index].predict(x_row)if y_pred == 1:class_to_delete = class_pair[1]else:class_to_delete = class_pair[0]class_list.remove(class_to_delete)results.append(class_list[0])return np.array(results)</pre><p><strong>注意：</strong>DAGSVM平均比投票方法快1.6到2.3倍。(Platt, Cristianini, & Shawe-Taylor, 2000) 。</p><p><strong>解單獨優化問題</strong></p><p>不是試著解多個二分問題，而是去解一個單獨優化問題，這個方法這些年來被很多人提出。</p><p><strong>Vapnik, Weston, 和 Watkins</strong></p><p>這個方法是用泛化的SVMs優化問題直接去解多類分類問題。它們被Vapnik (Vapnik V. N., 1998) 和 Weston & Watkins (Weston & Watkins, 1999)獨立發現。對每個類，就是增加約束的優化問題。結果，隨著類的增加問題的數量也成比例增加導致訓練越來越慢。</p><p><strong>Crammer 和 Singer</strong></p><p>Crammer 和 Singer (C&S) 提出另一可用的多類SVMs方法。如同Weston 和 Watkins，他們解單獨優化問題用到了太多的斯萊克(slack)變量(Crammer & Singer, 2001)。 這個方法優點是能減少內存的佔用和訓練時間，然而，比較這兩種算法，Hsu 和Lin發現當正交參數C的值較大時C&S 方法特別慢(Hsu & Lin, A Comparison of Methods for Multi-class Support Vector Machines, 2002) 。</p><p>在sklear裡，當用到LinearSVC 時你可以選擇C&S算法（如代碼表50所示）。在圖59中，我們能看到C&S預測不同於一對多和一對一方法。</p><pre>代碼表50from sklearn import svmimport numpy as npX = load_X()y = load_y()clf = svm.LinearSVC(C=1000, multi_class='crammer_singer')clf.fit(X,y)# Make predictions on two examples.X_to_predict = np.array([[5,5],[2,5]])print(clf.predict(X_to_predict)) # prints [4 1]</pre><div class=pgc-img><img alt=支持向量機（第八章） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1539850321144026aca8f86><p class=pgc-img-caption>圖59: Crammer&Singer算法預測</p></div><p><strong>你將要用那一種方法呢？</strong></p><p>對於這麼多的可選項，選擇哪一個多類方法比較適合你的問題是因難的。</p><p>Hsu和Lin寫了一個不錯的文章介紹不同的SVMs的多類方法(Hsu & Lin, A Comparison of Methods for Multi-class Support Vector Machines, 2002)。它們判斷一對一和DAG方法比其他方法要適用和實用。增強的一對一方法在sklearn中可用，所以它可能是你默認的選擇。</p><p>請記住在linearSVC中默認使用一對多方法，也許用Crammer&Singer算法能更好的幫你達到目的。基於這個主題，Dogan等發現儘管比其他算法要快，但是一對多假設函數在統計學上精確度明顯更差(Dogan, Glasmachers, & Igel, 2011) 。表1展示了這一章中的方法的簡介希望能幫你做出選擇。</p><p>表1: 多類SVM方法簡介</p><div class=pgc-img><img alt=支持向量機（第八章） onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15398507268045b34b38b7d><p class=pgc-img-caption></p></div><p><strong>小結</strong></p><p>感謝這些年來眾多的改進，現在有大量的方法用來給SVMs做多類分類。每一個方法有優點和缺點，你使用類庫中的方法，它們絕大部份都可以在有效的時間結束。然而，有必要，你現在知道哪一個方法能更好的解決你的獨特問題。</p><p>多類SVMs的研究還沒有結束。最近的研究集中在分佈式訓練。例如，Han & Berg 發佈了一個新算法叫做多類分佈式統一優化SVM（Distributed Consensus Multiclass SVM） ，它是Crammer 和 Singer公式的變形用來統一優化。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>第八章</a></li><li><a>向量</a></li><li><a>支持</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E5%AD%B8/0ad6da61.html alt=一文解釋支持向量機 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/3d38e71602404ee1bf306ccc937d4289 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/0ad6da61.html title=一文解釋支持向量機>一文解釋支持向量機</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/58f11d6c.html alt=初識支持向量機 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/593c00005f9561757b88 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/58f11d6c.html title=初識支持向量機>初識支持向量機</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/7885fca7.html alt=一文看懂支持向量機 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/b14f689acfb94e968e379121dee31bbb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/7885fca7.html title=一文看懂支持向量機>一文看懂支持向量機</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/9816751.html alt=支持向量機（第四章） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/153914838549618d2616eb5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/9816751.html title=支持向量機（第四章）>支持向量機（第四章）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e1a84312.html alt=127：平面向量(基底法，容易掉坑裡) class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/798fe5150f7e41db847e88330e26f9d1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e1a84312.html title=127：平面向量(基底法，容易掉坑裡)>127：平面向量(基底法，容易掉坑裡)</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a6560bc0.html alt=“孃家”支持！他的創業路越走越穩！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a0c141c4347841cb8c41c9097bb352cc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a6560bc0.html title=“孃家”支持！他的創業路越走越穩！>“孃家”支持！他的創業路越走越穩！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/62900ef0.html alt=一文了解邏輯迴歸和支持向量機的異同 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1539671186198856310f156 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/62900ef0.html title=一文了解邏輯迴歸和支持向量機的異同>一文了解邏輯迴歸和支持向量機的異同</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4668fec0.html alt="第八章 充填採礦法" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/8ae5e09b-9d70-4e70-bff0-5be87fa12958 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4668fec0.html title="第八章 充填採礦法">第八章 充填採礦法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7c54da7d.html alt=關於向量你所不知道的那1、2、3、4件事 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/3afd00000309c31b9722 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7c54da7d.html title=關於向量你所不知道的那1、2、3、4件事>關於向量你所不知道的那1、2、3、4件事</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f6469ee4.html alt=高中數學必修四-平面向量突破點（二）平面向量的線性運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/46cb0002bbea1028e85d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f6469ee4.html title=高中數學必修四-平面向量突破點（二）平面向量的線性運算>高中數學必修四-平面向量突破點（二）平面向量的線性運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/d35eb131.html alt=向量組的線性相關性 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/42878bb02cfd49a5bff27706934f48f2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/d35eb131.html title=向量組的線性相關性>向量組的線性相關性</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/1f3617bb.html alt="GroC 組合式詞向量生成算法" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a720bf73a52548348f6095c3ce6bd920 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/1f3617bb.html title="GroC 組合式詞向量生成算法">GroC 組合式詞向量生成算法</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/98922e26.html alt="「曲線弧長，單位切向量，主單位法向量」圖解高等數學 下-07" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/4b000000f1313245125e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/98922e26.html title="「曲線弧長，單位切向量，主單位法向量」圖解高等數學 下-07">「曲線弧長，單位切向量，主單位法向量」圖解高等數學 下-07</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/33284c83.html alt=平面向量最值問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/08de805129af4d4795d0fa7a88eff4c1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/33284c83.html title=平面向量最值問題>平面向量最值問題</a></li><hr><li><a href=../../tw/%E7%A7%91%E5%AD%B8/de0a6ec1.html alt=流形中的向量（或者矢量）和向量場 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/0f47062d207448fbb9e761d3eeb9e052 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/de0a6ec1.html title=流形中的向量（或者矢量）和向量場>流形中的向量（或者矢量）和向量場</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>