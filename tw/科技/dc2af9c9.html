<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 | 极客快訊</title><meta property="og:title" content="機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/dc2af9c9.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/dc2af9c9.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/dc2af9c9.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/dc2af9c9.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/dc2af9c9.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/dc2af9c9.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/dc2af9c9.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/dc2af9c9.html><meta property="article:published_time" content="2020-11-14T21:08:22+08:00"><meta property="article:modified_time" content="2020-11-14T21:08:22+08:00"><meta name=Keywords content><meta name=description content="機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/dc2af9c9.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><h1 class=pgc-h-arrow-right>一、算法概述</h1><p>邏輯迴歸(Logistic)雖帶有迴歸二字，但它卻是一個經典的二分類算法，它適合處理一些二分類任務，例如疾病檢測、垃圾郵件檢測、用戶點擊率以及上文所涉及的正負情感分析等等。</p><p>首先了解一下何為迴歸？假設現在有一些數據點，我們利用一條直線對這些點進行擬合(該線稱為最佳擬合直線)，這個擬合的過程就稱作迴歸。利用邏輯迴歸進行分類的主要思想是：根據現有數據對分類邊界線建立迴歸公式，以此進行分類。</p><p>線性迴歸算法後面的筆記會介紹，這裡簡單對比一下兩者，邏輯迴歸和線性迴歸的本質相同，都意在擬合一條直線，但線性迴歸的目的是擬合<strong>輸入變量的分佈</strong>，儘可能讓所有樣本到該條直線的距離最短；而邏輯迴歸的目的是擬合<strong>決策邊界</strong>，使數據集中不同的樣本儘可能分開，所以兩個算法的目的是不同的，處理的問題也不同。</p><h1 class=pgc-h-arrow-right>二、Sigmoid函數與相關推導</h1><p>我們想要的函數應該是，能接受所有的輸入並且預測出類別，比如二分類中的0或者1、正或者負，這種性質的函數被稱為海維賽德階躍函數，圖像如下：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051><p class=pgc-img-caption></p></div><p>但這種函數的問題在於從0跳躍到1的過程非常難處理，比如我們常接觸的多次函數，可能在某種條件下需要求導解決問題；而<strong>Sigmoid</strong>函數也具有類似的性質，並且在數學上更容易處理，其公式如下：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ce0599fb4c8c478695af3b062381f2b5><p class=pgc-img-caption></p></div><p>下圖是<strong>Sigmoid</strong>函數在不同座標尺度下的兩條曲線圖。當x為0時，<strong>Sigmoid</strong>函數值為0.5，隨著x的增大，對應的Sigmoid值將逼近於1；而隨著x的減小，<strong>Sigmoid</strong>值將逼近於0。如果橫座標刻度足夠大，<strong>Sigmoid</strong>函數看起來就很像一個階躍函數。</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7147661fa0314b0f8a1a82fb890f8ee4><p class=pgc-img-caption></p></div><p>若我們將<strong>Sigmoid</strong>函數的輸入記作z，可推出下面公式：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b6708f9ca2d645f9b56b87d51d8f1e34><p class=pgc-img-caption></p></div><p>它表示將這兩個數值向量對應元素相乘然後全部相加起來得到z值，其中向量x是分類器的輸入數據，向量w就是我們要找到的能使分類器儘可能準確的最佳參數。</p><p>由上述公式就可得：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c1c7c20be795491194f3ce30afbecb4a><p class=pgc-img-caption></p></div><p>其中h_w(x)的作用就是給定輸入時，輸出分類為正向類(1)的可能性。例如，對於一個給定的x，h_w(x)的值為0.8，則說明有80%的可能輸出為正向類(1)，有20%的可能輸出為負向類(0)，二者成補集關係。</p><p>對於一個二分類問題而言，我們給定輸入，函數最終的輸出只能有兩類，0或者1，所以我們可以對其分類。</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a6ea21e24c90440ea32f0e8e2e1cd296><p class=pgc-img-caption></p></div><p>為了運算便捷，我們將其整合為一個公式，如下：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f82ceeefb01c40f49886bd23e5e09af4><p class=pgc-img-caption></p></div><p>由於乘法計算量過大，所以可以將乘法變加法，對上式求對數，如下：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3bec757f6192428bbdd8a85bb8ae89c0><p class=pgc-img-caption></p></div><p>可以看出當y=1時，加號後面式子的值為0；當y=0時，加號前面式子的值為0，這與上文分類式子達到的效果是一樣的。L(w)稱為似然函數，J(w)稱為對數似然函數，是依據最大似然函數推導而成。此時的應用是梯度上升求最大值，如果梯度下降求最小值，可在公式之前乘以-1/n。</p><p>為了學習嘛，這裡再介紹一下另一種方式，利用<strong>損失函數</strong>推導應用於梯度下降的公式；損失函數是衡量<strong>真實值與預測值</strong>之間差距的函數，所以損失函數值越小，對應模型的效果也越好，損失函數公式如下：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/261ae2e6279d44978f0e922238b54688><p class=pgc-img-caption></p></div><p>可能只看公式理解相對抽象，通過對應的函數圖像足以理解上式，如下：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c30ec56af2614a748cb2a2ee70a44f10><p class=pgc-img-caption></p></div><p>注意！！！<strong>公式後面的y*不代表縱座標</strong> ！！！</p><p>當類標籤y=1時，對應的-log(x)圖像越接近於1，其距離x軸越近，代表其損失越小；反之當類標籤y=0時，對應的-log(1-x)圖像越接近於0，其距離x軸越近，代表其損失越小，也就是預測值越接近於真實值。</p><p>將兩個損失函數綜合起來得：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4f0e9dc7339b413fb32ce95299893533><p class=pgc-img-caption></p></div><p>對於m個樣本，總損失函數為：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/91f13091da6c49ed8309073c1db57fad><p class=pgc-img-caption></p></div><p>其中m為樣本個數、yi為標籤，可取0或1、i為第i個樣本、p(x_i)為預測輸出。</p><h1 class=pgc-h-arrow-right>三、梯度</h1><h1 class=pgc-h-arrow-right>3.1梯度上升</h1><p>上面已經列出了一大堆的公式，難道這又要開始一連串的大公式？</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/dacc5717800043109d50578cbb8239af><p class=pgc-img-caption></p></div><p>心態放平，上文雖說公式有點多，但目的都是為了推出最後的對數似然函數或者總損失函數，掌握這兩個是關鍵，梯度上升和梯度下降也會利用上面的公式做推導，所以二者之間存在關聯。首先梯度上升你需要了解一下何為梯度？</p><p>如果將梯度記為▽，則函數f(x,y)的梯度可由下式表示：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/96dabd3f47fd43a1b5791a167653229f><p class=pgc-img-caption></p></div><p>通俗的說，即對多元函數的參數求偏導，並把求得的各個參數的偏導以向量的形式寫出來。或者你只要理解這個梯度要沿著x的方向移動delta f(x,y)/delta x，沿著y方向移動delta f(x,y)/delta y足以，但f(x,y)必須要在待計算的點上有定義且可微。</p><p>下圖為一個梯度上升的例子，梯度上升法在到達每一個點之後都會重新評估下一步將要移動的方向。從x0開始，在計算完該點的梯度，函數就會移動到下一個點x1。在x1點，梯度會重新計算，繼而移動到x2點。如此循環迭代此過程，直到滿足停止條件，每次迭代過程都是為了找出當前能選取到的最佳移動方向。</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/611645e9e2494e1c978d4b877c13a33d><p class=pgc-img-caption></p></div><p>之前一直在討論移動方向，而未提到過移動量的大小。該量值稱為步長，記作$\alpha$。那麼就可以得出梯度上升法的迭代公式：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ed228fc3c66a44b8b06d8a1ee734c2f9><p class=pgc-img-caption></p></div><p>所以對於上文所提及的對數似然函數J(w)，我們也可以利用上述迭代的方式，一步一步移動到目標值或者無限接近於目標值，J(w)求偏導的公式如下：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e82777b03b9549b787aedbda29a69a35><p class=pgc-img-caption></p></div><p>可能有的人看到這個偏導公式有點蒙，其實這裡面用到的三個函數公式都是上文所提及的，來回顧一下。</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/eeaa3112bc334decb5493ce6c16ebf7d><p class=pgc-img-caption></p></div><p>求偏導過程涉及到高數知識，即最外層函數對外層函數求偏導、外層函數對內層函數求偏導、內層函數對其元素求偏導，三者相乘可得出所需偏導。推導過程有些麻煩，這裡只給出推導結果，在後面運用時我們也只會運用到最終結果，如下：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/7e6b48fd1be14ff3947f1ec40cb9b899><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>3.2梯度下降</h1><p>如果利用將對數似然函數換成損失函數$J(\Theta)$，則得到的是有關計算梯度下降的公式，如下：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/344b06a5345e4ddda7e3f0441b68e031><p class=pgc-img-caption></p></div><p>兩個公式中的w和Theta的含義是一樣的，都代表我們所求的最佳迴歸係數，兩個公式對比可以看出梯度上升和梯度下降只有加減號區別之分。下面這個動圖就可以很好的展示梯度下降法的過程：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/d6d73b3169b94e279bd692a2a4d9b851><p class=pgc-img-caption></p></div><p>公式推導部分至此結束了，基礎偏好的夥伴可能一遍就懂了，但基礎偏弱理解起來比較困難，偶當時也是對著書、跟著視頻啃了好久，多啃幾遍終歸會理解的。</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ad8a721899314a41a24a545d4bc9c51f><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>四、算法應用</h1><h1 class=pgc-h-arrow-right>4.1數據概覽</h1><p>有這樣一份數據集，共100個樣本、兩個特徵(X1與X2)以及一個分類標籤，所繪製圖像如下：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/0361ebe35ff542ebafdde5842fd238a4><p class=pgc-img-caption></p></div><p>在此數據集上，我們將通過梯度下降法找到最佳迴歸係數，也就是擬合出Logistic迴歸模型的最佳參數。 該算法的偽代碼如下：</p><pre><code>每個迴歸係數初始化為1重複R次：    計算整個數據集的梯度    使用alpha*gradient更新迴歸係數的向量    返回迴歸係數</code></pre><h1 class=pgc-h-arrow-right>4.2加載數據集</h1><pre><code>def loadDataSet():    dataMat = []    # 創建數據列表    labelMat = []    # 創建標籤列表    fr = open('LRData.txt','r',encoding='utf-8')    #逐行讀取全部數據    for line in fr.readlines():        #將數據分割、存入列表        lineArr = line.strip().split()        #數據存入數據列表        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])        #標籤存入標籤列表        labelMat.append(int(lineArr[2]))    fr.close()    return dataMat, labelMat</code></pre><p><strong>loadDataSet</strong>函數的作用是打開存儲數據的文本文件並逐行讀取。每行的前兩個值分別對應X1和X2，第三個值是數據對應的類別標籤。為了方便計算，函數還在X1和X2之前添加了一個值為1.0的X1，X1可以理解為偏置，即下圖中的x0。</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b6708f9ca2d645f9b56b87d51d8f1e34><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>4.3訓練算法</h1><pre><code>#sigmoid函數def sigmoid(inX):    return 1.0 / (1 + np.exp(-inX))</code></pre><p><strong>sigmoid</strong>函數就是傳入一個參數(這裡是一個100x1的向量)，通過公式計算並返回值。</p><pre><code>def gradAscent(dataMatIn, classLabels):    # 將列表轉換成numpy的matrix(矩陣)    dataMatrix = np.mat(dataMatIn)    # 將列表轉換成numpy的mat,並進行轉置    labelMat = np.mat(classLabels).T    # 獲取dataMatrix的行數和列數。    m, n = np.shape(dataMatrix)    # 設置每次移動的步長    alpha = 0.001    # 設置最大迭代次數    maxCycles = 500    # 創建一個n行1列都為1的矩陣    weights = np.ones((n,1))    for k in range(maxCycles):        # 公式中hΘ(x)        h = sigmoid(dataMatrix * weights)        # 誤差，即公式中y-hΘ(x)        error = labelMat - h        # 套入整體公式        weights = weights + alpha * dataMatrix.T * error    return weights</code></pre><p>最後<strong>weights</strong>返回的是一個3x1的矩陣,運行截圖如下：</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/54fc89ca6eb445c6a28e18c3e185b4a7><p class=pgc-img-caption></p></div><p><strong>gradAscent</strong>傳入參數為<strong>loadDataSet</strong>的兩個返回值，然後通過<strong>numpy</strong>的<strong>mat</strong>方法將<strong>dataMatrix</strong>和<strong>labelMat</strong> 分為轉化為100x3和1x100的矩陣，但<strong>labelMat</strong> 經過T轉置後變成100x1的矩陣。然後初始化權重，利用的方法就是創建一個n行1列的矩陣。整個算法的關鍵處於for循環中，我們先回顧一下上文的兩個公式。</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c1c7c20be795491194f3ce30afbecb4a><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9ca329c5f6b0404da06fc51c68a5e719><p class=pgc-img-caption></p></div><p>其中h的計算結果即h_w(x)，權重<strong>weight</strong>為W向量，輸入矩陣<strong>dataMatrix</strong>為x向量。誤差<strong>error</strong>代表yi-h_w(xi)，有人可能會發現1/m沒有出現在代碼中，因為alpha和1/m都為常數，二者相乘也為常數，所以只需要用alpha代替即可。</p><p>公式中的加和部分又怎麼體現呢？如果學過線性代數或者瞭解<strong>numpy</strong>運算的夥伴應該都理解矩陣的乘法，不理解也沒有關係，看下圖這個例子，當兩個矩陣相乘時，對應元素之間會求和作為最終元素。</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d2a771c173874bb4a35575084cb9579c><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>4.4繪製決策邊界</h1><pre><code>def plotDataSet(weight):    #獲取權重數組    weight = weight.getA()    # 加載數據和標籤    dataMat, labelMat = loadDataSet()    # 將列表轉換成numpy的array數組    dataArr = np.array(dataMat)    #獲取樣本個數    n = np.shape(dataMat)[0]    #創建4個空列表，1代表正樣本、0代表負樣本    xcord1 = []; ycord1 = []    xcord0 = []; ycord0 = []    # 遍歷標籤列表，根據數據的標籤進行分類    for i in range(n):        if int(labelMat[i]) == 1:            # 如果標籤為1，將數據填入xcord1和ycord1            xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])        else:            # 如果標籤為0，將數據填入xcord0和ycord0            xcord0.append(dataArr[i,1]); ycord0.append(dataArr[i,2])    #繪製圖像    fig = plt.figure()    ax = fig.add_subplot(111)    ax.scatter(xcord1, ycord1, s = 20, c = 'red', marker = '*',label = 'Class1')    ax.scatter(xcord0, ycord0, s = 20, c = 'green',marker = 's',label = 'Class2')    #繪製直線，sigmoid設置為0    x = np.arange(-3.0, 3.0, 0.1)    y = (-weight[0] - weight[1] * x) / weight[2]    ax.plot(x, y)    #標題、x標籤、y標籤    plt.title('LRData')    plt.legend(loc='upper left')    plt.xlabel('X1'); plt.ylabel('X2')    plt.savefig("E:\machine_learning\LR03.jpg")    plt.show()</code></pre><p>這部分代碼唯一需要注意的是，將<strong>sigmoid</strong>的值設置為0，可以回憶一下文章剛開始時的<strong>Sigmoid</strong>函數圖像，0是兩個分類的分界處。因此，我們設定0=w_0x_0+w_1x_1+w_2x_2，x_0的值為1，所以已知迴歸係數，就可求得x_1和x_2的關係式，從而畫出決策邊界。</p><div class=pgc-img><img alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9b59d1afc5e6472193d907ad7f2e5476><p class=pgc-img-caption></p></div><p>上圖可以看出分類的效果還是不錯的，根據函數繪製出的直線已經很大程度上將兩類樣本分隔開，100個樣本中，只有五個樣本分類錯誤，其中有三個還是位於迴歸線上。</p><h1 class=pgc-h-arrow-right>五、文末總結</h1><p>本文所講的梯度上升公式，屬於批量梯度上升，此外還有隨機梯度上升、小批量梯度上升，而批量梯度上升每次計算都要計算所有的樣本，所以程序計算過程是十分複雜的，並且容易收斂到局部最優，而隨機梯度上升將會對算法進行調優，下一篇文章將會介紹隨機梯度上升，並分析兩者之間的區別。</p><blockquote><p>私信小編可獲取數據和源碼供參考，感謝閱讀。</p></blockquote></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>筆記</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E5%AD%B8/be7a95a.html alt=機器學習筆記(九)——手撕支持向量機SVM之間隔、對偶、KKT推導 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/098e7c3fc26e4c148cd535acf6e21f94 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/be7a95a.html title=機器學習筆記(九)——手撕支持向量機SVM之間隔、對偶、KKT推導>機器學習筆記(九)——手撕支持向量機SVM之間隔、對偶、KKT推導</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html alt=HTMLCSS學習筆記（六）——元素類型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb5988349894ce9bf568c6418f85b7d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html title=HTMLCSS學習筆記（六）——元素類型>HTMLCSS學習筆記（六）——元素類型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html alt="MySQL 學習筆記" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html title="MySQL 學習筆記">MySQL 學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/112d1b5f.html alt=一造學習筆記—管理篇（2）：工程造價管理的組織和內容 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/9e65b076-038f-4720-96ff-182898f42dee style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/112d1b5f.html title=一造學習筆記—管理篇（2）：工程造價管理的組織和內容>一造學習筆記—管理篇（2）：工程造價管理的組織和內容</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0a02618e.html alt=某教程學習筆記（一）：17、php漏洞 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/6d21bdb33b0a49e8b6eaa2c2a725a1d8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0a02618e.html title=某教程學習筆記（一）：17、php漏洞>某教程學習筆記（一）：17、php漏洞</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b37aac89.html alt=【學習筆記】Android開發之kotlin語言（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ee3f9c8348ae4de58a5f62922d2042e1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b37aac89.html title=【學習筆記】Android開發之kotlin語言（一）>【學習筆記】Android開發之kotlin語言（一）</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>