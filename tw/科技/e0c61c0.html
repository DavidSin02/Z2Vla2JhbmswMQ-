<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 | 极客快訊</title><meta property="og:title" content="PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/704bd6b96fe84583802e545aabd17adc"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e0c61c0.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e0c61c0.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e0c61c0.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e0c61c0.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e0c61c0.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e0c61c0.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e0c61c0.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e0c61c0.html><meta property="article:published_time" content="2020-10-29T21:01:05+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:05+08:00"><meta name=Keywords content><meta name=description content="PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/e0c61c0.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><p class=ql-align-justify>前不久，百度發佈了基於PaddlePaddle的深度強化學習框架PARL。</p><p class=ql-align-justify>作為一個強化學習小白，本人懷著學習的心態，安裝並運行了PARL裡的quick-start。不體驗不知道，一體驗嚇一跳，不愧是 NeurIPS 2018 冠軍團隊的傑作，代碼可讀性良好，函數功能非常清晰，模塊之間耦合度低、內聚性強。不僅僅適合零基礎的小白快速搭建DRL環境，也十分適合科研人員復現論文結果。</p><p class=ql-align-justify>廢話不多說，我們從強化學習最經典的例子——迷宮尋寶(俗稱格子世界GridWorld)開始，用策略梯度(Policy-Gradient)算法體驗一把PARL。</p><p class=ql-align-justify><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/704bd6b96fe84583802e545aabd17adc><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-center><strong>模擬環境</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>強化學習適合解決智能決策問題。如圖，給定如下迷宮，黑色方格代表牆，黃色代表寶藏，紅色代表機器人；一開始，機器人處於任意一個位置，由於走一步要耗電，撞牆後需要修理，所以我們需要訓練一個模型，來告訴機器人如何避免撞牆、並給出尋寶的最優路徑。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/703f9dd4b31c4a36aba31fd0f622e1d1><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p class=ql-align-justify>接下來，定義強化學習環境所需的各種要素：狀態state、動作action、獎勵reward等等。</p><p class=ql-align-justify>state就是機器人所處的位置，用(行、列)這個元組來表示，同時可以表示牆：</p><pre>self.wallList=[(2,0),(3,2),(1,3),(4,4)]self.start=(0,4)self.end=(4,0)</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>使用random-start策略實現reset功能，以增加初始狀態的隨機性：</p><pre>defreset(self): for _ in range(0,1024): i=np.random.randint(self.row) j=np.random.randint(self.col) if (i,j) not in self.wallList and (i,j)!=self.end: self.pos=(i,j) break return self.pos</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>定義動作action，很顯然，機器人可以走上下左右四個方向：</p><pre>action_dim=4dRow=[0,0,-1,1]dCol=[1,-1,0,0]</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>定義獎勵reward，到達終點獎勵為10，走其他格子需要耗電，獎勵為-1：</p><pre>def reward(self, s): if s == self.end: return 10.0 else: return -1.0</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>另外，越界、撞牆需要給較大懲罰：</p><pre>if not checkBounds(nextRow, nextCol) : #越界 return self.pos, -5.0, False, {'code':-1,'MSG':'OutOfBounds!'} nextPos=(nextRow,nextCol) if meetWall(self.wallList, nextPos): #撞牆 return self.pos, -10.0, False, {'code':-1,'MSG':'MeetWall!'}</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>至此，強化學習所需的狀態、動作、獎勵均定義完畢。接下來簡單推導一下策略梯度算法的原理。</p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/704bd6b96fe84583802e545aabd17adc><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-center><strong>策略梯度(Policy-Gradient)算法是什麼？</strong></p><p class=ql-align-justify>我們知道，強化學習的目標是給定一個馬爾可夫決策過程，尋找出最優策略。所謂策略是指狀態到動作的映射，常用符號π表示，它是指給定狀態s時，動作集上的一個分佈，即：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/95dbcda3c3b3469a81abe862309e00e4><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p class=ql-align-justify>策略梯度的做法十分直截了當，它直接對求解最優策略進行參數化建模，策略p(a|s)將從一個概率集合變成一個概率密度函數p(a|s,θ)，即：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/9b3c1c72166c4caeb4acd902e2152726><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p class=ql-align-justify>這個策略函數表示，在給定狀態s和參數θ的情況下，採取任何可能動作的概率，它是一個概率密度函數，在實際運用該策略的時候，是按照這個概率分佈進行動作action的採樣的，這個分佈可以是離散(如伯努利分佈)，也可以說是連續（如高斯分佈）。最直觀的方法，我們可以使用一個線性模型表示這個策略函數:</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1d23de83872548b19010a63e9c157030><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>其中，φ(s)表示對狀態s的特徵工程，θ是需要訓練的參數。這樣建模有什麼好處呢？其實最大的好處就是能時時刻刻學到一些隨機策略，增強探索性exploration。</p><p class=ql-align-justify><br></p><p class=ql-align-justify>為什麼可以增加探索性呢？</p><p class=ql-align-justify><br></p><p class=ql-align-justify>比如迷宮尋寶問題，假設一開始機器人在最左上角的位置，此時p(a|s,θ)可以初始化為[0.25,0.25,0.25,0.25]，表明機器人走上、下、左、右、的概率都是0.25。當模型訓練到一定程度的時候，p(a|s,θ)變成了[0.1,0.6,0.1,0.2]，此時，向下的概率最大，為0.6，機器人最有可能向下走，這一步表現為利用exploitation；但是，向右走其實也是最優策略，0.2也是可能被選擇的，這一步表現為探索exploration；相對0.6和0.2，向上、向左兩個動作的概率就小很多，但也是有可能被選擇的。如果模型繼續訓練下去，p(a|s,θ)很有可能收斂成[0.05,0.45,0.05,0.45]，此時，機器人基本上只走向下或者向右，選擇向上、向左的可能性就極小了。這是最左上角位置(狀態)的情況，其他狀態，隨著模型的訓練，也會收斂到最優解。</p><p class=ql-align-justify>有了模型，就想到求梯度，那麼，如何構建損失函數呢？標籤y-Target又是什麼?</p><p class=ql-align-justify>一個非常樸素的想法就是：如果一個動作獲得的reward多，那麼就使其出現的概率變大，否則減小，於是，可以構建一個有關狀態-動作的函數 f(s,a) 作為損失函數的權重，這個權重函數可以是長期回報G(t)，可以是狀態值函數V(s)，也可以是狀態-行為函數Q(s,a)，當然也可以是優勢函數A。但是，這個權重函數和參數θ無關，對θ的梯度為0，僅僅作為p(a|s,θ)的係數。</p><p class=ql-align-justify>現在考慮模型的輸出π(a|s,θ)，它表示動作的概率分佈，我們知道，智能體每執行完一輪episode，就會形成一個完整的軌跡Trajectory:</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c293a3084e564948930f84ddd4d31794><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>其中，狀態</p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7767ca7d071e4e22a23e01078dcd886b><p class=pgc-img-caption></p></div><p class=ql-align-justify>和參數θ無關，狀態轉移概率P(s'|s,a)是由環境所決定的，和參數θ也無關。所以，我們的目標簡化為：優化參數θ，使得每個動作概率的乘積</p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ee052037d2504c7098c6840c1236c502><p class=pgc-img-caption></p></div><p class=ql-align-justify>達到最大，即使得</p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b701463cd81a46679fcf1218a8dd762e><p class=pgc-img-caption></p></div><p class=ql-align-justify>這個累乘概率達到最大，可用如下公式表示：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8be9ecfdad814768804cf7cc13cc0447><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>這顯然是我們熟悉的極大似然估計問題，轉化為對數似然函數：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d1701610055b43edb4274ff65d299159><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>乘以權重 f(s,a)，構建如下目標函數，這個目標函數和我們平時見到的損失函數正好相反，它需要使用梯度上升的方法求一個極大值：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1bc5ff7519b645edb98a30d541ea0034><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>注意到，這裡的aTrue就是標籤y-Target，表示agent在狀態$s_{t}$時真實採取的動作，可以根據軌跡trajectory採樣得到。學過機器學習的同學都知道，一般用目標函數的均值代替求和，作為新的目標函數：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/da23ef2a4ca24f8ba6868acd93d5a1a0><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>均值，就是數學期望，所以目標函數也可以表示為：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8846e5b3be274e93b27f58f7041e6b78><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>有了目標函數，梯度就很容易計算了，由於</p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8ad09cbe329c49d796d0512ffcc1a503><p class=pgc-img-caption></p></div><p class=ql-align-justify>對於θ來說是係數，故梯度公式如下:</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1fb76b71d77e41a0b19a4ac2745b1ab3><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>那麼，策略π具體的表現形式如何？前文提到，策略可以是離散的，也可以是連續的，不妨考慮離散的策略。由於我們需要求解最大值問題，也就是梯度上升問題，自然而然就想到把梯度上升問題轉化為梯度下降問題，這樣才能使得目標函數的相反數達到最小，而什麼樣的函數可以將梯度下降和對數函數關聯起來呢？顯然是我們熟悉的交叉熵，所以最終的損失函數確定為：</p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b48df45e15944315af3f81eb4d6141b7><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>連續策略的推導與離散策略類似，有興趣的讀者可以參考相關文獻。</p><p class=ql-align-justify>自此，公式推導可以告一段落。策略梯度的基本算法就是Reinforce，也稱為蒙特卡洛策略梯度，簡稱MCPG，PARL的官方policy-gradient就是基於以下算法框架實現的：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7dd2579d6fc94c2ca0a2e42f78cda67d><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/704bd6b96fe84583802e545aabd17adc><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-center><strong>PARL源碼結構</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>在搭建模型之前，我們先分析一下PARL的主要模塊：</p><p class=ql-align-justify>1. env：環境，在這裡，我們的環境就是迷宮尋寶。</p><p class=ql-align-justify>2. model：模型，可以是簡單的線性模型，也可以是CNN、RNN等深度學習模型。</p><p class=ql-align-justify>3. algorithm：算法，對model層進行封裝，並利用模型進行predict(預測)，同時構建損失函數進行learn(學習)；具體實現形式可以是DQN、PG、DDPG等等。</p><p class=ql-align-justify>4. agent：智能體，對algorithm層進行封裝，一般也包含predict、learn兩個函數；同時，由於智能體要同時進行探索exploration-利用exploitation，還經常包含一個sample函數，用於決定到底是randomSelect(隨機選擇或者根據分佈函數選擇動作)，還是argmax(100%貪心，總是選擇可能性最大的動作)。</p><p class=ql-align-justify>5. train：訓練和測試，用於實現agent和環境的交互，當模型收斂後，可以測試智能體的準確性。</p><p class=ql-align-justify>6. utils：其他輔助功能。</p><p class=ql-align-justify>以下的架構示意圖，可以幫助我們更好的理解PARL：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/ffeb24d457ef41be8890f1f5164e8e13><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/704bd6b96fe84583802e545aabd17adc><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-center><strong>代碼實現&源碼解讀</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>在理解了框架的各個模塊之後，我們就可以按照模板填代碼了，學過MVC、ORM等框架的同學都知道，這是一件非常輕鬆愉快的事情。</p><p class=ql-align-justify>1、MazeEnv。迷宮環境，繼承自gym.Env，實現了reset、step、reward、render四個主要方法，這裡不再贅述。</p><p class=ql-align-justify>2、MazeModel。模型層，搭建如下全鏈接神經網絡，輸入是狀態state-input，輸出是策略函數action-out，由於策略函數是動作的概率分佈，所以選用softmax作為激活函數，中間還有若干隱藏層。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a00b424c96b94362a6d1658403097cfc><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p class=ql-align-justify>代碼實現非常的簡單，讓MazeModel繼承官方的Model類，然後照貓畫虎搭建模型即可：</p><pre>class MazeModel(Model): def__init__(self, act_dim): self.act_dim = act_dim hid1_size = 32 hid2_size = 32 self.fc1 = layers.fc(size=hid1_size, act='tanh') self.fc2 = layers.fc(size=hid2_size, act='tanh') self.fcOut = layers.fc(size=act_dim,act='softmax') defpolicy(self, obs): out = self.fc1(obs) out = self.fc2(out) out = self.fcOut(out) return out</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>3、policy_gradient。算法層；官方倉庫提供了大量的經典強化學習算法，我們無需自己重複寫，可以直接複用算法庫(parl.algorithms)裡邊的 PolicyGradient 算法！</p><p class=ql-align-justify>簡單分析一下policy_gradient的源碼實現。</p><p class=ql-align-justify>define_predict函數，接收狀態obs，調用model的policy方法，輸出狀態所對應的動作：</p><pre>def define_predict(self, obs): """ use policy model self.model to predict the actionprobability """ return self.model.policy(obs) </pre><p class=ql-align-justify><br></p><p class=ql-align-justify>define_learn函數，接收狀態obs、真實動作action、長期回報reward，首先調用model的pocliy方法，預測狀態obs所對應的動作概率分佈act_prob，然後使用交叉熵和reward的乘積構造損失函數cost，最後執行梯度下降法，優化器為Adam，完成學習功能：</p><pre>def define_learn(self, obs, action, reward): """ update policy model self.model with policy gradientalgorithm """ act_prob = self.model.policy(obs) log_prob = layers.cross_entropy(act_prob, action) cost = log_prob * reward cost = layers.reduce_mean(cost) optimizer = fluid.optimizer.Adam(self.lr) optimizer.minimize(cost) return cost</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>4、MazeAgent。智能體。其中，self.pred_program是對algorithm中define_predict的簡單封裝，self.train_program是對algorithm中define_learn的簡單封裝，我們可以參考官方的CartpoleAgent實現，按照框架模板填入相應的格式代碼。</p><p class=ql-align-justify>這裡，僅僅分析self.pred_program，self.train_program寫法類似：</p><pre>self.pred_program = fluid.Program()#固定寫法with fluid.program_guard(self.pred_program): obs= layers.data( name='obs',shape=[self.obs_dim], dtype='float32')#接收外界傳入的狀態obsself.act_prob = self.alg.define_predict(obs)#調用algorithm的define_predict,self.act_prob為動作的概率分佈</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>sample函數，注意這句話：</p><pre>act = np.random.choice(range(self.act_dim),p=act_prob)</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>這句話表示根據概率分佈隨機選出相應的動作；假設上、下、左、右的概率分別為[0.5,0.3,0.15,0.05]，那麼上被選擇的概率是最大的，右被選擇的概率是最小的，所以sample函數既能exploration，又能exploitation，體現了強化學習中的探索-利用的平衡。</p><p class=ql-align-justify>predict函數，和sample函數不同的是，它總是貪心的選擇可能性最大的動作，常常用於測試階段：</p><pre>act = np.argmax(act_prob)</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>learn函數，接收obs、action、reward，進行批量梯度下降，返回損失函數cost。</p><p class=ql-align-justify>5、TrainMaze。讓環境env和智能體agent進行交互，最主要的部分就是以下代碼，體現了MCPG過程：</p><pre>#迭代十萬個episodefor i in range(1,100001): #採樣  obs_list, action_list, reward_list = run_train_episode(env, agent)  #使用滑動平均的方式計算獎勵的期望  MeanReward=MeanReward+(sum(reward_list)-MeanReward)/i  batch_obs = np.array(obs_list)  batch_action = np.array(action_list)  #通過backup的方式計算G(t)，並進行歸一化處理  batch_reward = calc_discount_norm_reward(reward_list, GAMMA)  #學習  agent.learn(batch_obs, batch_action, batch_reward)</pre><p class=ql-align-justify><br></p><p class=ql-align-justify>其中，滑動平均可以選擇任意一個公式，無偏估計表示真實的均值，有偏估計更加接近收斂後的平均獎勵：</p><p class=ql-align-justify>無偏估計：</p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fe24f8e9b0db4a0da69bb9d06b1f89c0><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>有偏估計：</p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d53d26d12321404c8f276b8bd275ba81><p class=pgc-img-caption></p></div><p class=ql-align-justify>，α是學習率，取0.1、0.01等等</p><p class=ql-align-justify>其他代碼都是輔助功能，如記錄log、畫圖、渲染環境等等。</p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/704bd6b96fe84583802e545aabd17adc><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-center><strong>運行程序並觀察結果</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>運行TrainMaze，可以看到如下輸出。</p><p class=ql-align-justify>1、訓練之前，機器人並不知道如何尋寶，所以越界、撞牆次數非常多，也繞了很多彎路，平均獎勵比較低。</p><p class=ql-align-justify>ErrorCountBeforeTrain:25052 #越界+撞牆次數</p><p class=ql-align-justify>平均獎勵曲線：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7183572cbe2b44e4befd82aade9bb389><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p class=ql-align-justify>2、訓練模型。迭代十萬個episode，觀察如下學習曲線，縱軸表示平均獎勵，可以看到，模型已經收斂了：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/bc20e059b5b04991870661e34da25f28><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-justify><br></p><p class=ql-align-justify>3、測試模型的準確性。測試階段，我們迭代128輪，智能體幾乎沒有任何越界或者撞牆行為，由於是random-start，所以平均獎勵有少許波動，但穩定在5-7之間。</p><p class=ql-align-justify>ErrorCountAfterTrain:0 #沒有任何撞牆或者越界</p><p class=ql-align-justify>訓練後的平均獎勵：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/805ed7ce086c433ebc289239bb9979f1><p class=pgc-img-caption></p></div><p class=ql-align-justify><br></p><p class=ql-align-center><br></p><p class=ql-align-justify>源碼Git地址</p><p class=ql-align-justify>https://github.com/kosoraYintai/PARL-Sample</p><p class=ql-align-justify>參考文獻：</p><p class=ql-align-justify>· CS 294-112 at UC Berkeley，DeepReinforcement Learning.</p><p class=ql-align-justify>· Deepmind，Silver.D，Reinforcement Learning Open Class.</p><p class=ql-align-justify>· 馮超. 強化學習精要[M]. 北京：電子工業出版社，2018.</p><p class=ql-align-justify>· 郭憲，方勇純. 深入淺出強化學習[M]. 北京：電子工業出版社，2018.</p><p class=ql-align-justify></p><div class=pgc-img><img alt=PARL源碼走讀——使用策略梯度算法求解迷宮尋寶問題 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a478223dc63f447cb6a38e838fa412a8><p class=pgc-img-caption></p></div></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>PARL</a></li><li><a>源碼</a></li><li><a>迷宮</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/3c09a7c2.html alt=網站源碼，服務器，域名三者是什麼關係？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/S1xQDgS3bmZd8q style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3c09a7c2.html title=網站源碼，服務器，域名三者是什麼關係？>網站源碼，服務器，域名三者是什麼關係？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3a5fae7e.html alt=HashMap源碼分析 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/4852d5083ec340be9f50ded10636e4b0 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3a5fae7e.html title=HashMap源碼分析>HashMap源碼分析</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e590a5d4.html alt="thinkphp6學習教程與源碼 tp6開源CMS系統源碼研究" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5ab1d67934084ab6bd585f4c84094c76 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e590a5d4.html title="thinkphp6學習教程與源碼 tp6開源CMS系統源碼研究">thinkphp6學習教程與源碼 tp6開源CMS系統源碼研究</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dbf8b0cf.html alt=Tomcat源碼分析之整體架構 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1531465870453535efa0b79 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dbf8b0cf.html title=Tomcat源碼分析之整體架構>Tomcat源碼分析之整體架構</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/07f82832.html alt="SynchronousQueue 同步隊列入門使用&源碼詳解" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/f76c27ed738844d099b011022d2e054f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/07f82832.html title="SynchronousQueue 同步隊列入門使用&源碼詳解">SynchronousQueue 同步隊列入門使用&源碼詳解</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2bf9a44d.html alt=我是如何閱讀JDK源碼的？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/25b18fb619994fcbaa58c5e28b7a4e51 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2bf9a44d.html title=我是如何閱讀JDK源碼的？>我是如何閱讀JDK源碼的？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/59958e85.html alt=深入分析java集合框架Collection（源碼分析） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/43efff37d8ff43bc8553cc71e3b99b17 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/59958e85.html title=深入分析java集合框架Collection（源碼分析）>深入分析java集合框架Collection（源碼分析）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e275511c.html alt=源碼分析：Java集合類的AbstractCollection源碼解析 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a3aefa7e5d8c491cb86dc9c9e4390671 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e275511c.html title=源碼分析：Java集合類的AbstractCollection源碼解析>源碼分析：Java集合類的AbstractCollection源碼解析</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4d741bce.html alt=有一說一！SpringAOP+源碼解析，切就完事了 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/965c060c22b942c1a3a9f143215f114c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4d741bce.html title=有一說一！SpringAOP+源碼解析，切就完事了>有一說一！SpringAOP+源碼解析，切就完事了</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8ea98537.html alt=源碼：太陽能路燈如何接線 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/c5ed2247de6949d89b9052d3fddd4921 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8ea98537.html title=源碼：太陽能路燈如何接線>源碼：太陽能路燈如何接線</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/436e2022.html alt=「源碼」用ModelSim做FPGA的時序仿真，看這一篇就夠了 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/769d1d2c04b9410aa18cd0add525013f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/436e2022.html title=「源碼」用ModelSim做FPGA的時序仿真，看這一篇就夠了>「源碼」用ModelSim做FPGA的時序仿真，看這一篇就夠了</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/b5561fb3.html alt=hdfs源碼分析第一彈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/cf716b17efaf49ae85d773a3f0ac1959 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/b5561fb3.html title=hdfs源碼分析第一彈>hdfs源碼分析第一彈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/21ffd2dc.html alt="看vue3源碼可以學到什麼 : 四、代碼模版生成器" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/5fba6d3b32b5430d8caf459298bc5a06 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/21ffd2dc.html title="看vue3源碼可以學到什麼 : 四、代碼模版生成器">看vue3源碼可以學到什麼 : 四、代碼模版生成器</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/be1948c8.html alt=（深度解析）基於規則的中文複合事件抽取，附python源碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/97e3484c0f684ab1b639765267e34178 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/be1948c8.html title=（深度解析）基於規則的中文複合事件抽取，附python源碼>（深度解析）基於規則的中文複合事件抽取，附python源碼</a></li><hr><li><a href=../../tw/%E9%81%8A%E6%88%B2/85cdd9f9.html alt=【攻略】劍與遠征異界迷宮技巧，輕鬆過第三層，免費拿女妖羊奶！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/c93cb6867da44ca3839cd8a58cd59372 style=border-radius:25px></a>
<a href=../../tw/%E9%81%8A%E6%88%B2/85cdd9f9.html title=【攻略】劍與遠征異界迷宮技巧，輕鬆過第三層，免費拿女妖羊奶！>【攻略】劍與遠征異界迷宮技巧，輕鬆過第三層，免費拿女妖羊奶！</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>