<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>常見機器學習模型的假設 | 极客快訊</title><meta property="og:title" content="常見機器學習模型的假設 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/ebcb3db739e3497490f5d18f69b507c4"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e20ffc2.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e20ffc2.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e20ffc2.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e20ffc2.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e20ffc2.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e20ffc2.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e20ffc2.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e20ffc2.html><meta property="article:published_time" content="2020-10-29T21:01:05+08:00"><meta property="article:modified_time" content="2020-10-29T21:01:05+08:00"><meta name=Keywords content><meta name=description content="常見機器學習模型的假設"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/e20ffc2.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>常見機器學習模型的假設</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div class=pgc-img><img alt=常見機器學習模型的假設 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ebcb3db739e3497490f5d18f69b507c4><p class=pgc-img-caption>> Photo by Thought Catalog on Unsplash</p></div><p></p><p>暫時忘記深度學習和神經網絡。</p><p><br></p><div class=pgc-search data-content=機器學習></div><p><br></p><p>隨著越來越多的人開始進入數據科學領域，我認為重要的是不要忘記這一切的基礎。</p><p>統計。</p><p>如果您不熟悉分析領域，那就可以了！ 我們都是從某個地方開始的！</p><p>但是，重要的是要意識到我將在本文中分享的機器學習模型假設的存在。</p><p>很幸運，我在大學時代就已經研究了所有這些概念，所以我認為回到基礎知識並撰寫一些有關它們的知識會很令人振奮。</p><p>關懷就是共享。</p><p>讓我們開始！</p><h1 class=pgc-h-arrow-right>線性迴歸又稱普通最小二乘（OLS）迴歸</h1><p>OLS迴歸試圖解釋您的自變量（預測變量）和因變量（目標）之間是否存在關係。</p><p>它通過最小化殘差平方和來使數據擬合一條線。</p><p>殘差是觀測值與預測值之差。 殘差用於指示模型與數據的擬合程度。</p><p>但是，為了能夠信任結果並對結果充滿信心，在建模之前必須滿足一些假設。</p><p>滿足所有這些假設將使您能夠為模型創建最佳估計。</p><p>OLS迴歸模型有5個關鍵假設。</p><h1 class=pgc-h-arrow-right>假設1：自變量和因變量之間存在線性關係。</h1><p>當我第一次在統計課上聽說這個假設時，我就措手不及。</p><p>我記得當我複習考試成績後，這種感覺已經被欺騙和欺騙了，以至於它銘刻在我的記憶中。</p><p>值得深思。</p><p>這些方程中的哪一個滿足此假設？</p><p>Y ＝β1 +β1 X 1 +β2 X 2</p><p>Y =β₀+β₁X₁+β2 X 22</p><p>事實證明兩者都是線性的。</p><p>通常認為線性方程式有誤解。</p><p>線性方程=直線非線性方程=曲線這是錯誤的。</p><p>當統計學家說方程是線性的時，他們指的是參數中的線性，並且方程採用某種格式。</p><p>格式如下：</p><p>Y =常數+參數1 變量1 +參數2 變量2…</p><p>注意：</p><p>· 必須有一個常數</p><p>· 其他術語遵循"參數*變量"的模式，所有內容加在一起。</p><p>變量是否為非線性（即平方）並不重要，只要方程遵循此指定格式，它就是線性方程。 任何其他不遵循此格式的方程式都是非線性的。</p><p>這也意味著一些線性方程線在擬合時是彎曲的。</p><p>因此，從技術上講……僅使用散點圖並不能真正告訴您所看到的擬合曲線是否為線性。 您可能需要查看曲線方程。</p><h1 class=pgc-h-arrow-right>假設2：沒有多重共線性</h1><p>多重共線性是指自變量之間的高度相關性。</p><p>多重共線性是一個問題，因為它會創建多餘的信息，從而導致迴歸模型的結果不可靠。</p><p>為了避免此問題，您可以部署兩種技術：</p><p>· 對所有自變量進行關聯分析。</p><p>· 刪除具有高方差膨脹因子（VIF）*的自變量。 一般而言，VIF> 10是多重共線性的有力指示。</p><p>· VIF = 1÷（1-R²）</p><h1 class=pgc-h-arrow-right>假設3：無自相關</h1><p>自相關是指殘差彼此不獨立。 即以前的觀測殘差會導致您當前觀測到的殘差有系統地增加/減少。</p><p>結果，它會使您低估方差，這會影響置信區間或假設檢驗的結果。</p><p>要檢查自相關，可以部署Durbin-Watson'D'測試。 1.5 &lt;d &lt;2.5之間的任何值都滿足此假設。</p><p>否則，為了補救自相關，在計算標準誤差以校正自相關時，應應用"自相關-穩健的標準誤差（HAC）"公式。</p><p>注意：您可能會遇到" HAC"作為" Newey–West估計量"。</p><h1 class=pgc-h-arrow-right>假設4：殘留物應為同方的</h1><p>同方性是指您的殘差圖應在所有觀測值中顯示均勻且隨機的模式。</p><p>換句話說，殘差的方差在所有觀察結果中都應保持一致，並且不應遵循某種形式的系統模式。</p><p>在下圖中，第一幅圖顯示了殘差圖中的系統模式。 這也稱為異方差； 使假設無效。</p><p>它下面的圖顯示了同方差殘差圖的樣子。</p><div class=pgc-img><img alt=常見機器學習模型的假設 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a35440c32c544c468277a070636f4ea3><p class=pgc-img-caption>> Example of Homoskedasticity, 29 Jul 2010, by Protonk (CC3.0) (Source)</p></div><p></p><p>那麼異方差到底有什麼問題呢？</p><p>· 您的無偏估計將不再是最好的。</p><p>· 它會影響標準誤差的計算，而這會無意中影響任何假設檢驗的結果。</p><p>要解決第一個異方差問題，一個好方法是增加樣本量。</p><p>對於第二個問題，您應該應用"穩健標準誤差"公式來考慮異方差對誤差的影響。</p><p>注意："魯棒標準誤差"也稱為"異方差一致性標準誤差"（HC）。 編程時，您可能會遇到" HC"字樣。</p><h1 class=pgc-h-arrow-right>假設5：所有自變量都是正態分佈的</h1><p>就產生最佳無偏估計而言，此假設是可選的。</p><p>但是，如果要執行假設檢驗以產生置信區間或預測區間，則需要此方法。</p><p>注意：您可以在此處查看兩者之間的區別。</p><p>有兩種檢查正常性的方法：</p><p>· 為每個自變量創建直方圖。</p><div class=pgc-img><img alt=常見機器學習模型的假設 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e1a232bc17c54d2ebe5ab147bcdf024c><p class=pgc-img-caption>> Example of Histograms, 16 Mar 2009 by Gbdivers (CC2.0) (Source)</p></div><p></p><p>2.在殘差上運行Q-Q圖。 如果殘差正常，則所有觀察值均應沿一條直線。</p><div class=pgc-img><img alt=常見機器學習模型的假設 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6f312f0ffba8480491b3a32d95cb140b><p class=pgc-img-caption>> Example of QQ-Plot, 19 Oct 2009 by Skbkekas (CC3.0) (Source)</p></div><p></p><p>如果您需要滿足此假設，但變量不是正態分佈的，則可以轉換變量。</p><h1 class=pgc-h-arrow-right>邏輯迴歸</h1><p>邏輯迴歸假設與OLS迴歸的不同之處在於：</p><p>· 自變量和因變量之間不需要線性關係。</p><p>· 不需要殘差是正常的。</p><p>· 無需滿足同方差假設</p><p>那麼邏輯迴歸需要滿足哪些假設？</p><p>這是邏輯迴歸的5個關鍵假設。</p><h1 class=pgc-h-arrow-right>假設1：適當的因變量結構</h1><p>該假設僅說明二進制邏輯迴歸要求您的因變量是二分法，而有序邏輯迴歸要求它是有序的。</p><p>此外，因變量既不應該是區間標度，也不應該是比率標度。</p><h1 class=pgc-h-arrow-right>假設2：結果的對數與每個自變量之間存在線性關係。</h1><p>logit函數由以下方式提供：</p><p>logit（p）= log（p /（1-p）），其中p是結果的概率</p><p>要檢查此假設，可以通過在散點圖上繪製每個自變量和logit值來直觀地做到這一點。</p><div class=pgc-img><img alt=常見機器學習模型的假設 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f313203410344337b4cc698e7b8bd450><p class=pgc-img-caption>> Checking linearity assumption for logistic regression</p></div><p></p><p>在上圖中，Y軸是自變量，而X軸顯示對數值。 然後查看曲線的方程式，看它是否符合線性假設。</p><p>請記住，線性在參數中。 只要方程滿足上述線性方程形式，就可以滿足線性假設。</p><p>注意：我對x軸標籤弄錯了，應該是" Logit"而不是" Logit Probability"</p><h1 class=pgc-h-arrow-right>假設3：沒有多重共線性</h1><p>與OLS迴歸的假設一樣，這裡也可以這樣說。</p><p>（有關詳細信息，請參閱有關OLS迴歸的部分。）</p><h1 class=pgc-h-arrow-right>假設4：無影響異常值</h1><p>有影響的離群值是影響邏輯迴歸模型質量的極端數據點。</p><p>並非所有異常值都有影響力。</p><p>在刪除或轉換它們以進行分析之前，您需要檢查哪些點是有影響力的。</p><p>要檢查異常值，可以對數據值運行"庫克距離"。 較高的庫克距離值表示存在異常值。</p><p>找出有影響力的異常值的經驗法則是當Cook的距離> 1時。</p><h1 class=pgc-h-arrow-right>假設5：觀察獨立</h1><p>該假設要求邏輯迴歸觀察值彼此獨立。</p><p>也就是說，觀察結果不應來自重複的測量設計。</p><p>重複測量設計是指在不同的實驗條件下或跨時間對同一個人進行的同一變量的多個測量。</p><p>重複測量的一個很好的例子是縱向研究-跟蹤學科多年的進展。</p><h1 class=pgc-h-arrow-right>支持向量機（SVM）</h1><p>沒有模型假設可以驗證SVM。</p><h1 class=pgc-h-arrow-right>基於樹的模型</h1><p>對於基於樹的模型，例如決策樹，隨機森林和梯度增強，沒有模型假設可以驗證。</p><p>與OLS迴歸或邏輯迴歸不同，基於樹的模型對異常值具有魯棒性，不需要因變量滿足任何正態性假設。</p><p>為什麼基於樹的模型對異常值具有魯棒性？</p><p>單擊此處獲取Quora的詳細說明。</p><h1 class=pgc-h-arrow-right>尾註</h1><p>好吧，就是這樣！</p><p>我認為這裡的關鍵是要計劃使用迴歸或任何廣義線性模型（GLM），在構建模型之前必須驗證模型假設。</p><p>對於SVM或基於樹的模型，沒有任何模型假設可以驗證。</p><p>希望這篇文章對您有所幫助！</p><p>下篇再見！</p><h1 class=pgc-h-arrow-right>參考文獻</h1><p>· https://www.lexjansen.com/wuss/2018/130_Final_Paper_PDF.pdf</p><p>· https://www.statisticssolutions.com/assumptions-of-logistic-regression/</p><p>· http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/#logistic-regression-assumptions</p><p>· http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R5_Correlation-Regression/R5_Correlation-Regression4.html</p><p>· https://www.statisticssolutions.com/assumptions-of-linear-regression/</p><p>· https://www.quora.com/Why-are-tree-based-models-robust-to-outliers</p><p>(本文翻譯自Timothy Tan的文章《Back to Basics: Assumptions of Common Machine Learning Models》，參考：https://towardsdatascience.com/back-to-basics-assumptions-of-common-machine-learning-models-e43c02325535)</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>常見</a></li><li><a>機器</a></li><li><a>學習</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E5%AD%B8/c63ba92.html alt=常見的機器學習小問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/c63ba92.html title=常見的機器學習小問題>常見的機器學習小問題</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>