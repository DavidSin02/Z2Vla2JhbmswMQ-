<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 | 极客快訊</title><meta property="og:title" content="時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/d7a0cea6243e43c4bf1756bbc29f76c2"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e6e28dd8.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e6e28dd8.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e6e28dd8.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e6e28dd8.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e6e28dd8.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e6e28dd8.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e6e28dd8.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e6e28dd8.html><meta property="article:published_time" content="2020-11-14T21:03:07+08:00"><meta property="article:modified_time" content="2020-11-14T21:03:07+08:00"><meta name=Keywords content><meta name=description content="時域卷積網絡TCN詳解：使用卷積進行序列建模和預測"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/e6e28dd8.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>時域卷積網絡TCN詳解：使用卷積進行序列建模和預測</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p><br></p><p>CNN經過一些簡單的調整就可以成為序列建模和預測的強大工具</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/d7a0cea6243e43c4bf1756bbc29f76c2><p class=pgc-img-caption></p></div><p><br></p><p>儘管卷積神經網絡(CNNs)通常與圖像分類任務相關，但經過適當的修改，它已被證明是進行序列建模和預測的有價值的工具。在本文中，我們將詳細探討時域卷積網絡(TCN)所包含的基本構建塊，以及它們如何結合在一起創建一個強大的預測模型。使用我們的開源Darts TCN實現，我們展示了只用幾行代碼就可以在真實數據集上實現準確預測。</p><p>以下對時間卷積網絡的描述基於以下論文:https://arxiv.org/pdf/1803.01271.pdf。本文引用用(*)表示。</p><h1 class=pgc-h-arrow-right>動機</h1><p>到目前為止，深度學習背景下的序列建模主題主要與遞歸神經網絡架構(如LSTM和GRU)有關。S. Bai等人(*)認為，這種思維方式已經過時，在對序列數據進行建模時，應該將卷積網絡作為主要候選者之一加以考慮。他們能夠表明，在許多任務中，卷積網絡可以取得比RNNs更好的性能，同時避免了遞歸模型的常見缺陷，如梯度爆炸/消失問題或缺乏內存保留。此外，使用卷積網絡而不是遞歸網絡可以提高性能，因為它允許並行計算輸出。他們提出的架構稱為時間卷積網絡(TCN)，將在下面的部分中進行解釋。為了便於理解TCN體系結構及其Darts實現，本文將儘可能使用與庫中看到的相同的模型參數名稱。</p><h1 class=pgc-h-arrow-right>基本模型</h1><p><strong>概述</strong></p><p>TCN是時域卷積網絡（Temporal Convolutional Network）的簡稱，它由具有相同輸入和輸出長度的擴張的、因果的1D卷積層組成。下面幾節將詳細介紹這些術語的實際含義。</p><p><strong>一維卷積網絡</strong></p><p>一維卷積網絡以一個三維張量作為輸入，也輸出一個三維張量。我們的TCN實現的輸入張量具有形狀(batch<em>size、input</em>length、input<em>size)，輸出張量具有形狀(batch</em>size、input<em>length、output</em>size)。由於TCN中的每一層都有相同的輸入和輸出長度，所以只有輸入和輸出張量的第三維是不同的。在單變量情況下，input<em>size和output</em>size都等於1。在更一般的多變量情況下，input<em>size和output</em>size可能不同，因為我們可能不希望預測輸入序列的每個組件。</p><p>單個1D卷積層接收一個shape的輸入張量(batch<em>size, input</em>length, nr<em>input</em>channels)並輸出一個shape張量(batch<em>size, input</em>length, nr<em>output</em>channels)。為了瞭解單個層如何將其輸入轉換為輸出，讓我們看一下批處理的一個元素(對批處理中的每個元素都進行相同的處理)。讓我們從最簡單的例子開始，其中nr<em>input</em>channels和nr<em>output</em>channels都等於1。在這種情況下，我們看到的是一維輸入和輸出張量。下圖顯示了輸出張量的一個元素是如何計算的。</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0ea62f26afba426ab84d340dbbe561ec><p class=pgc-img-caption></p></div><p><br></p><p>我們可以看到，要計算輸出的一個元素，我們需要查看輸入的一系列長度為kernel<em>size的連續元素。在上面的例子中，我們選擇了一個3的kernel</em>size。為了得到輸出，我們取輸入的子序列和相同長度的已學習權值的核向量的點積。輸出的下一個元素,相同的應用程序,但kernel_size-sized窗口的輸入序列是由一個元素轉移到正確的(對於本預測模型,stride 總是設置為1)。請注意,相同的一組內核權重將被用來計算每輸出一個卷積層。下圖顯示了兩個連續的輸出元素及其各自的輸入子序列。</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/806810afc5e34d3bbda752414a2bef97><p class=pgc-img-caption></p></div><p><br></p><p>為了使可視化更簡單，與核向量的點積不再顯示，而是對每個具有相同核權重的輸出元素髮生。</p><p>為了確保輸出序列與輸入序列具有相同的長度，將應用一些零填充。這意味著在輸入張量的開始或結束處添加額外的零值項，以確保輸出具有所需的長度。後面的部分將詳細解釋如何做到這一點。</p><p>現在讓我們看看有多個輸入通道的情況，即nr<em>input</em>channels大於1。在本例中，上述過程對每個單獨的輸入通道都重複，但每次都使用不同的內核。這將導致nr<em>input</em>channels中間輸出向量和kernel<em>size * nr</em>input<em>channels的一些內核權重。然後將所有中間輸出向量相加，得到最終輸出向量。在某種意義上，這相當於與一個形狀的輸入張量(input</em>size, nr<em>input</em>channels)和一個形狀的內核張量(kernel<em>size, nr</em>input_channels)進行2D卷積，如下圖所示。它仍然是一維的因為窗口只沿著一個軸移動，但是我們在每一步都有一個二維卷積因為我們使用的是一個二維核矩陣。</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b563674888b44ab6b244de246f34e05b><p class=pgc-img-caption></p></div><p><br></p><p>對於本例，我們選擇nr<em>input</em>channels等於2。現在，我們使用nr<em>input</em>channels by kernel<em>size內核矩陣沿著nr</em>input<em>channels寬系列長度input</em>length來代替在一維輸入序列上滑動的核向量。</p><p>如果nr<em>input</em>channels和nr<em>output</em>channels都大於1，那麼對每個具有不同內核矩陣的輸出通道重複上述過程。然後將輸出向量堆疊在一起，得到一個形狀的輸出張量(input<em>length, nr</em>output<em>channels)。本例中的內核權重數等於kernel</em>size<em>nrinputchannels</em>nr<em>output</em>channels。</p><p>nr<em>input</em>channels和nr<em>output</em>channels這兩個變量取決於該層在網絡中的位置。第一層是nr<em>input</em>channels = input<em>size，最後一層是nr</em>output<em>channels = output</em>size。所有其他層將使用由num_filters提供的中間通道號。</p><p><strong>因果卷積</strong></p><p>對於因果關係，對於{0，…，input<em>length - 1}中的每一個i，輸出序列的第i個元素可能只依賴於索引為{0，…，i}的輸入序列中的元素。換句話說，輸出序列中的元素只能依賴於輸入序列中在它之前的元素。如前所述，為了確保一個輸出張量與輸入張量具有相同的長度，我們需要進行零填充。如果我們只在輸入張量的左側填充零，那麼就可以保證因果卷積。要理解這一點，請考慮最右邊的輸出元素。假設輸入序列的右邊沒有填充，它所依賴的最後一個元素就是輸入的最後一個元素。現在考慮輸出序列中倒數第二個輸出元素。與最後一個輸出元素相比，它的內核窗口向左移動了1，這意味著它在輸入序列中最右邊的依賴項是輸入序列中倒數第二個元素。根據歸納，對於輸出序列中的每個元素，其在輸入序列中的最新依賴項與其本身具有相同的索引。下圖展示了一個input</em>length為4,kernel_size為3的示例。</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/66d682c92ac24d8d9dc07b7bb0df1928><p class=pgc-img-caption></p></div><p><br></p><p>我們可以看到，在兩個條目的左填充為零的情況下，我們可以獲得相同的輸出長度，同時遵守因果關係規則。事實上，在沒有擴展的情況下，維持輸入長度所需的零填充條目的數量總是等於kernel_size - 1。</p><p><strong>擴張</strong></p><p>預測模型的一種理想質量是輸出中特定條目的值取決於輸入中所有先前的條目，即索引小於或等於其自身的所有條目。當接受野(指影響輸出的特定條目的原始輸入的一組條目)的大小為input<em>length時，就可以實現這一點。我們也稱其為"完整的歷史記錄"。正如我們以前看到的，一個傳統的卷積層在輸出中創建一個依賴於輸入的kernel</em>size項的條目，這些條目的索引小於或等於它自己。例如，如果我們的kernel<em>size為3，那麼輸出中的第5個元素將依賴於輸入中的元素3、4和5。當我們將多個層疊加在一起時，這個範圍就會擴大。在下面的圖中我們可以看到，通過kernel</em>size 3疊加兩層，我們得到的接受野大小為5。</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e5550620c6c04544ac8c542f3179e95e><p class=pgc-img-caption></p></div><p><br></p><p>一般而言，具有n層且kernel_size為k的一維卷積網絡的接收場r為</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9957f2a0b63c444ba1947b0e6f6e2b3a><p class=pgc-img-caption></p></div><p><br></p><p>為了知道需要多少層才能完全覆蓋，我們可以將接受野大小設為input_length l，然後求解層數n(非整數值需要進行四捨五入):</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/cc48b002d6fd47b0a6593cbff5809702><p class=pgc-img-caption></p></div><p><br></p><p>這意味著,kernel_size固定,完整的歷史覆蓋所需的層數是線性的輸入長度的張量,這將導致網絡變得非常深非常快,導致模型與大量的參數,需要更長的時間來訓練。此外，大量的層已被證明會導致與損失函數梯度相關的退化問題。在保持層數相對較小的情況下，增加感受野大小的一種方法是向卷積網絡引入膨脹概念。</p><p>卷積層上下文中的膨脹是指輸入序列的元素之間的距離，該元素用於計算輸出序列的一個條目。 因此，傳統的卷積層可以看作是dilated為1的擴散層，因為1個輸出值的輸入元素是相鄰的。 下圖顯示了一個dilated為2的擴散層的示例，其input<em>length為4，kernel</em>size為3。</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9611432043eb44e9b8f8bb9437360a87><p class=pgc-img-caption></p></div><p><br></p><p>與dilated-1擴散的情況相比，該層的接收場沿5而不是3的長度擴展。更普遍地，具有內核大小k的d擴散層的接收場沿1 + d的長度擴展。 *（k-1）。 如果d是固定的，那麼仍然需要輸入張量的長度為線性的數字才能實現完全的接收場覆蓋（我們只是減小了常數）。</p><p>這個問題可以通過在層中向上移動時d的值呈指數增加來解決。 為此，我們選擇一個常數dilation<em>base整數b，它將使我們根據其下的層數i來計算特定層的膨脹d，即d = b ** i。 下圖顯示了一個網絡，其中input</em>length為10，kernel<em>size為3，dilation</em>base為2，這將導致3個膨脹的卷積層完全覆蓋。</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/81360652806947aa8de3a77035991773><p class=pgc-img-caption></p></div><p><br></p><p>這裡我們只顯示影響輸出最後一個值的輸入的影響。同樣，只顯示最後一個輸出值所必需的補零項。顯然，最後的輸出值依賴於整個輸入覆蓋率。實際上，給定超參數，input_length最多可以使用15，同時保持完全的接收野覆蓋。一般來說，每增加一層，當前接受野寬度就增加一個d*(k-1)值，其中d計算為d=b**i, i表示新層下面的層數。因此，給出了基b指數膨脹時TCN的感受場寬度w、核大小k和層數n為</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ce494e600c5c4665a32d7d71324897b1><p class=pgc-img-caption></p></div><p><br></p><p>然而，根據b和k的值，這個接受野可能會有"洞"。考慮以下網絡，其dilation_base為3，內核大小為2:</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/0873efd21a85402fb22f97718d3c67e0><p class=pgc-img-caption></p></div><p><br></p><p>接受野的範圍確實大於輸入的大小(即15)。然而，接受野是有洞的;也就是說，在輸入序列中有輸出值不依賴的條目(如上面紅色所示)。為了解決這個問題，我們需要將內核大小增加到3，或者將膨脹基數減小到2。一般來說，對於沒有孔的感受野，核的大小k至少要與膨脹基b一樣大。</p><p>考慮到這些觀察結果，我們可以計算出我們的網絡需要多少層才能覆蓋整個歷史。給定核大小k，膨脹基b，其中k≥b，輸入長度l，為了實現全歷史覆蓋，必須滿足以下不等式:</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/8f1c45c56f9c4f6ba50606837aca4364><p class=pgc-img-caption></p></div><p><br></p><p>我們可以求解n，得到所需的最小層數</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/a0b6dfefe3c64b568e07161cccf3dde3><p class=pgc-img-caption></p></div><p><br></p><p>我們可以看到，在輸入長度方面，層數現在是對數的，而不是線性的。這是一個顯著的改進，可以在不犧牲接受野覆蓋率的情況下實現。</p><p>現在，唯一需要指定的是每一層所需的零填充項的數量。假設膨脹基為b，核大小為k，當前層以下有i個層，則當前層所需的補零項數p計算如下:</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/f93d006ce62d41b291617e934f63d0e9><p class=pgc-img-caption></p></div><p><br></p><p><strong>基本TCN概述</strong></p><p>給定input<em>length, kernel</em>size, dilation_base和覆蓋整個歷史所需的最小層數，基本的TCN網絡看起來像這樣:</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/23a359863a5c40b1ac0a02426f555e1c><p class=pgc-img-caption></p></div><p><br></p><h1 class=pgc-h-arrow-right>預測</h1><p>到目前為止，我們只討論了'輸入序列'和'輸出序列'，而沒有深入瞭解它們之間是如何相互關聯的。在預測方面，我們希望預測未來時間序列的下一個條目。為了訓練我們的TCN網絡進行預測，訓練集將由給定時間序列的等大小子序列對(輸入序列、目標序列)組成。目標序列將是相對於其各自的輸入序列向前移動一定數量output<em>length的序列。這意味著長度input</em>length的目標序列包含其各自輸入序列的最後(input<em>length - output</em>length)元素作為第一個元素，位於輸入序列最後一個條目之後的output<em>length元素作為它的最後一個元素。在預測方面，這意味著該模型所能預測的最大預測視界等於output</em>length。使用滑動窗口的方法，許多重疊的輸入和目標序列可以創建出一個時間序列。</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2cfc21d01f834c0c950f782348f55f4b><p class=pgc-img-caption></p></div><p><br></p><h1 class=pgc-h-arrow-right>模型的改進</h1><p>S. Bai等人(*)建議對基本的TCN體系結構進行一些添加，以提高本節將討論的性能，即殘差連接、正則化和激活函數。</p><p><strong>殘差塊</strong></p><p>我們對之前介紹的基本模型做的最大的修改是將模型的基本構建塊從簡單的一維因果卷積層改為由相同膨脹因子和殘差連接的2層組成的殘差塊。</p><p>讓我們從基本模型中考慮一個膨脹係數d為2、內核大小k為3的層，看看這是如何轉化為改進模型的剩餘塊的。</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/822ace57e86a4df881d2d68efac21c43><p class=pgc-img-caption></p></div><p><br></p><p>變為</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/98f18e7b5bb244beb7422c624923c499><p class=pgc-img-caption></p></div><p><br></p><p>這兩個卷積層的輸出將被添加到殘差塊的輸入中，從而產生下一個塊的輸入。對於網絡的所有內部塊，即除了第一個和最後一個之外的所有內部塊，輸入和輸出通道寬度是相同的，即num_filters。由於第一個殘塊的第一卷積層和最後一個殘塊的第二卷積層可能有不同的輸入和輸出通道寬度，所以可能需要調整殘差張量的寬度，這是通過1x1卷積來完成的</p><p>此更改會影響對完整覆蓋所需的最小層數的計算。現在我們必須考慮需要多少殘差塊才能實現接收域的完全覆蓋。在TCN中添加一個殘差塊所增加的接受野寬度是添加一個基本因果層時的兩倍，因為它包含兩個這樣的層。因此，擴張基為b的TCN的感受場總大小r、k≥b的核大小k和剩餘塊數n可計算為</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/a3159bfcd31d4bf99000fe3545e03e9f><p class=pgc-img-caption></p></div><p><br></p><p>這保證了最小的殘差塊數n為input_length l的完整歷史覆蓋</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/94e17d54acaa41f0848dcf878cf8fee1><p class=pgc-img-caption></p></div><p><br></p><p><strong>激活,規範化、正規化</strong></p><p>為了使我們的TCN不僅僅是一個過於複雜的線性迴歸模型，需要在卷積層的頂部添加激活函數來引入非線性。ReLU激活被添加到兩個卷積層之後的殘差塊中。</p><p>為了規範化隱含層的輸入(抵消了梯度爆發的問題)，權值規範化應用於每一個卷積層。</p><p>為了防止過擬合，在每個剩餘塊的每個卷積層之後通過dropout引入正則化。下圖顯示了最終的剩餘塊。</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/2916501fef1c403fa92ccf0f739af710><p class=pgc-img-caption></p></div><p><br></p><p>第二個ReLU單元中的星號表示該層存在於除最後一層之外的所有層中，因為我們希望最終輸出也能夠具有負值（這與本文中概述的體系結構不同）。</p><h1 class=pgc-h-arrow-right>模型</h1><p>下圖顯示了我們最終的TCN模型，其中l等於input<em>length，k等於kernel</em>size，b等於dilation_base，k≥b，並且對於完整的歷史覆蓋n為最小數量的殘差塊，其中n可以從其他值計算得出 如上所述。</p><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/cc4b6a1020de433aa374122c3c2f47af><p class=pgc-img-caption></p></div><p><br></p><h1 class=pgc-h-arrow-right>示例</h1><p>讓我們看一個示例，該示例說明如何使用Darts庫使用TCN架構預測時間序列。</p><p>首先，我們需要一個時間序列來訓練和評估我們的模型。 為此，我們使用了Kaggle數據集，其中包含來自西班牙的每小時能源生產數據。 更具體地說，我們選擇預測"河流上游水電"的產量。 此外，為了使問題的計算量減少，我們將每天的平均能源生產量取平均以獲得每日的時間序列。</p><pre><code>from darts import TimeSeriesfrom darts.dataprocessing.transformers import MissingValuesFillerimport pandas as pddf = pd.read_csv('energy_dataset.csv', delimiter=",")df['time'] = pd.to_datetime(df['time'], utc=True)df['time']= df.time.dt.tz_localize(None)df_day_avg = df.groupby(df['time'].astype(str).str.split(" ").str[0]).mean().reset_index()value_filler = MissingValuesFiller()series = value_filler.transform(TimeSeries.from_dataframe(df_day_avg, 'time', ['generation hydro run-of-river and poundage']))series.plot()</code></pre><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1e24cb97353e47318697c722c845a41c><p class=pgc-img-caption></p></div><p><br></p><p>我們可以看到，除了每年的季節性之外，每月還會定期出現能源生產中的"峰值"。 由於TCN模型支持多個輸入通道，因此我們可以在當前時間序列中添加其他時間序列分量，以對當月的當前日期進行編碼。 這可以幫助我們的TCN模型更快地收斂。</p><pre><code>series = series.add_datetime_attribute('day', one_hot=True)</code></pre><p>現在，我們將數據分為訓練和驗證組件並執行標準化。</p><pre><code>from darts.dataprocessing.transformers import Scalertrain, val = series.split_after(pd.Timestamp('20170901'))scaler = Scaler()train_transformed = scaler.fit_transform(train)val_transformed = scaler.transform(val)series_transformed = scaler.transform(series)</code></pre><p>現在是時候創建和訓練我們的TCN模型了。 注意，上面對體系結構的描述中出現的所有變量名都可以用作Darts TCN實現的構造函數的參數。 由於我們要執行每週預測，因此output<em>length參數設置為7。 在訓練模型時，我們僅將訓練系列的第一部分指定為target</em>series，因為我們不想預測我們之前添加的助手時間序列。 我們嘗試了幾種不同的超參數組合，但是大多數值是任意選擇的。</p><pre><code>from darts.models import TCNModelmodel = TCNModel(    input_size=train.width,    n_epochs=20,     input_length=365,    output_length=7,     dropout=0,     dilation_base=2,     weight_norm=True,    kernel_size=7,    num_filters=4,    random_state=0)model.fit(    training_series=train_transformed,    target_series=train_transformed['0'],    val_training_series=val_transformed,    val_target_series=val_transformed['0'], verbose=True)</code></pre><p>為了評估我們的模型，我們希望使用7天的預測範圍在驗證集中的許多不同時間點測試其性能。 為此，我們使用了Darts的歷史回測功能。 請注意，該模型為每個前提提供了新的輸入數據，但從未對其進行過重新訓練。 為了節省時間，我們將跨度設置為5。</p><pre><code>pred_series = model.backtest(    series_transformed,    target_series=series_transformed['0'],    start=pd.Timestamp('20170901'),     forecast_horizon=7,    stride=5,    retrain=False,    verbose=True,    use_full_output_length=True)</code></pre><p>讓我們根據地面真實數據點將TCN模型的歷史預測預測可視化，並計算R2得分。</p><pre><code>from darts.metrics import r2_scoreimport matplotlib.pyplot as pltseries_transformed[900:]['0'].plot(label='actual')pred_series.plot(label=('historic 7 day forecasts'))r2_score_value = r2_score(series_transformed['0'], pred_series)plt.title('R2:' + str(r2_score_value))plt.legend()</code></pre><div class=pgc-img><img alt=時域卷積網絡TCN詳解：使用卷積進行序列建模和預測 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d0ce974d722343d5b59ba30a5e1b3bc6><p class=pgc-img-caption></p></div><p><br></p><p>有關更多詳細信息和其他示例，請在GitHub上查看 https://github.com/unit8co/darts/blob/develop/examples/TCN-examples.ipynb</p><h1 class=pgc-h-arrow-right>結論</h1><p>在大多數情況下，序列建模中的深度學習仍與遞歸神經網絡架構廣泛相關。 但是研究表明，在預測性能和效率方面，TCN可以在許多任務中勝過這些類型的模型。 在本文中，我們探討了如何通過簡單的構建塊（例如一維卷積層，膨脹和殘差連接）理解這種有前途的模型，以及它們如何融合在一起。 此外，我們成功地應用了TCN體系結構的當前Darts實現來預測實際時間序列。</p><p><br></p><p>作者:Francesco Lässig</p><p>deephub翻譯組</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>卷積</a></li><li><a>時域</a></li><li><a>網絡</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E5%AD%B8/915b745.html alt=深度｜卷積神經網絡十五問：CNN與生物視覺系統的研究探索 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1528704399761722b85d065 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/915b745.html title=深度｜卷積神經網絡十五問：CNN與生物視覺系統的研究探索>深度｜卷積神經網絡十五問：CNN與生物視覺系統的研究探索</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ea1bb612.html alt=光纜——未來網絡主導 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/e75c1afe12354a93bad8495ad1057693 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ea1bb612.html title=光纜——未來網絡主導>光纜——未來網絡主導</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/bdc59733.html alt="網絡詞名場面是什麼意思 名場面是什麼梗" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bdc59733.html title="網絡詞名場面是什麼意思 名場面是什麼梗">網絡詞名場面是什麼意思 名場面是什麼梗</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a002ca18.html alt=王一博那句年度網絡流行語「不愧是我」的：正版英文翻譯 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/26add5cdc08e4214800b25e21b623eb1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a002ca18.html title=王一博那句年度網絡流行語「不愧是我」的：正版英文翻譯>王一博那句年度網絡流行語「不愧是我」的：正版英文翻譯</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html alt=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/65c4000bda98898dcdbb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ad6f0929.html title=谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼>谷歌大腦發佈神經網絡的「核磁共振」，並公開相關代碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8dce33e7.html alt=理解生成對抗網絡，一步一步推理得到GANs（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/bee194d6fbec4d6f82e82998def3f7a3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8dce33e7.html title=理解生成對抗網絡，一步一步推理得到GANs（一）>理解生成對抗網絡，一步一步推理得到GANs（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/1fe1c2dd.html alt=瞭解生成對抗網絡（GAN） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/634604de44ad4d17931ccc0bcf3e46ef style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/1fe1c2dd.html title=瞭解生成對抗網絡（GAN）>瞭解生成對抗網絡（GAN）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2a9a3956.html alt="100 個網絡基礎知識普及，看完成半個網絡高手！" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/955ea722-be77-4b2d-b87a-64a8a04c8ea8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2a9a3956.html title="100 個網絡基礎知識普及，看完成半個網絡高手！">100 個網絡基礎知識普及，看完成半個網絡高手！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/78c87167.html alt=奧迪A7車載電路與網絡連接之網絡連接 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d34213e3b03545bd8e87ab074b54ca0f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/78c87167.html title=奧迪A7車載電路與網絡連接之網絡連接>奧迪A7車載電路與網絡連接之網絡連接</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8a15db0b.html alt=網絡上共享跨平臺的點對點文件 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/666bf87a9e16425ea1c9adc4f2c10acd style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8a15db0b.html title=網絡上共享跨平臺的點對點文件>網絡上共享跨平臺的點對點文件</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2b6ecb90.html alt=網絡基礎知識術語你知道哪些？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2b6ecb90.html title=網絡基礎知識術語你知道哪些？>網絡基礎知識術語你知道哪些？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6474ccee.html alt=點對點網絡的基本知識分享 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8678a34470a144ad88fc0487f0d8da91 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6474ccee.html title=點對點網絡的基本知識分享>點對點網絡的基本知識分享</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a61e29b6.html alt="幫助信息網絡犯罪活動罪 | 廈門刑事律師--辦案資料" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a61e29b6.html title="幫助信息網絡犯罪活動罪 | 廈門刑事律師--辦案資料">幫助信息網絡犯罪活動罪 | 廈門刑事律師--辦案資料</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ded83afe.html alt=博士論文摘要｜馬下平：“陸態網絡”並址站歸心基線精密解算及GNSS基準站數據處理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/53350006726e50ef72f9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ded83afe.html title=博士論文摘要｜馬下平：“陸態網絡”並址站歸心基線精密解算及GNSS基準站數據處理>博士論文摘要｜馬下平：“陸態網絡”並址站歸心基線精密解算及GNSS基準站數據處理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/65c66d61.html alt=網絡分析儀校準很頭疼，三大要點得記住！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/41dd3f31e88f48688e75d64978b36acf style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/65c66d61.html title=網絡分析儀校準很頭疼，三大要點得記住！>網絡分析儀校準很頭疼，三大要點得記住！</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>