<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 | 极客快訊</title><meta property="og:title" content="【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/RPEy9qLGGIWQde"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e8e0e2a.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e8e0e2a.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e8e0e2a.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e8e0e2a.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e8e0e2a.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e8e0e2a.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/e8e0e2a.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/e8e0e2a.html><meta property="article:published_time" content="2020-10-29T20:58:14+08:00"><meta property="article:modified_time" content="2020-10-29T20:58:14+08:00"><meta name=Keywords content><meta name=description content="【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/e8e0e2a.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p><strong>摘要</strong></p><p><strong>不同機器學習模型對隨機數種子的敏感程度不同</strong></p><p>本文考察邏輯迴歸、XGBoost、隨機森林和全連接神經網絡四種機器學習算法在100組不同隨機數種子下的模型性能和單因子回測表現。結果表明，當隨機數種子變化時，邏輯迴歸的結果幾乎保持不變，對隨機數不敏感；全連接神經網絡的結果可能發生較大變化，對隨機數較敏感；XGBoost和隨機森林對隨機數的敏感程度介於上述兩者之間。機器學習模型看似“必然”的結果背後包含一定“偶然”因素，投資者應認識到機器學習選股模型可能存在的隨機數種子選擇偏差。</p><p><strong>機器學習多個環節涉及隨機數，目的在於增強模型的泛化能力</strong></p><p>機器學習多個環節涉及隨機數，例如訓練集、驗證集和測試集的隨機劃分，對神經網絡權值進行隨機初始化，利用隨機梯度下降法求損失函數最優解，隨機森林、XGBoost等決策樹集成模型的行列採樣，神經網絡訓練過程中使用Dropout技術等。引入這些隨機數具有重要意義，它們或是為了保證損失函數更易達到最優解，或是為了避免極端值對模型訓練造成不良影響，或是為了產生具有差異性的樣本以便進一步集成，最終目的都在於增強模型的泛化能力。</p><p><strong>使用Python常用機器學習包時可進行若干設置保證訓練結果可重複</strong></p><p>由於機器學習模型中隨機數的存在，為了保證結果的可重複性，需要對模型進行若干設置。我們測試了多種常用Python機器學習包隨機數種子設置方法，結果表明sklearn和xgboost包設置random_state超參數後就能保證結果可完全復現；當以tensorflow作為後端使用keras包時，如果不使用GPU，在單線程環境下同時固定numpy和tensorflow兩處隨機數種子就能確保全連接神經網絡模型得到可重複的結果。</p><p><strong>機器學習模型受隨機數影響程度與模型複雜度及隨機數作用方式有關</strong></p><p>邏輯迴歸本身比較簡單，在使用隨機梯度下降算法擬合參數時引入了隨機數，由於損失函數為凸函數，參數最終大概率收斂到理論最優參數附近，而較少受隨機數影響。神經網絡參數量大，在初始化網絡權重，利用優化算法最小化損失函數，前向傳播進行Dropout等環節均引入了隨機數，模型整體具有較高的複雜度，受隨機數影響較大。XGBoost和隨機森林模型複雜度也較高，行列採樣環節涉及隨機數，但是由於模型已經進行集成，最終結果的不確定性有所降低。</p><p>風險提示：機器學習選股方法是對歷史投資規律的挖掘，若未來市場投資環境發生變化導致基學習器失效，則該方法存在失效的可能。機器學習存在一定過擬合風險。當機器學習算法涉及隨機數時，不同隨機數種子可能得到不同結果。</p><p><strong>本文研究導讀</strong></p><p>世界上沒有完全相同的兩片葉子。諸多看似“必然”的現象背後，可能蘊藏著不為人熟知的“偶然”因素。人類的侷限性之一，就是我們往往熱衷於追逐確定性的結論，而忽視了同樣關鍵的“必然中的偶然”。</p><p>以機器學習算法為例，有一個問題常常困擾機器學習的使用者：使用相同的算法訓練相同的數據集為什麼會得到不同的結果？產生這種現象的主要原因是：機器學習的諸多環節都涉及隨機數。例如訓練集、驗證集和測試集的隨機劃分，對神經網絡的權值進行隨機初始化，利用隨機梯度下降法求損失函數最優解，隨機森林、XGBoost等決策樹集成模型的行列隨機採樣，訓練神經網絡時通過Dropout技術隨機刪除部分神經元等。引入這些隨機數具有重要意義，它們或是為了保證損失函數更易達到最優解，或是為了產生差異化的樣本以便進一步集成，或是為了避免極端值對模型造成不良影響，最終目標都是增強模型的泛化能力。</p><p>在每次訓練過程中，計算機產生的隨機數不同，因而造成訓練結果不能完全重複。為了確保研究的可重複性，我們通常會事先固定隨機數種子，最終也僅僅彙報該隨機數種子對應的結果。這就引入了新的問題——當投資者閱讀人工智能和機器學習相關研究報告時，可能對報告中算法的泛化能力產生懷疑：是否可能是作者運氣好，選擇了一個特殊的隨機數種子，得到看似完美的結果，但是更換隨機數種子後結論不再成立？</p><p>為了迴應投資者的上述質疑，全面展示機器學習“必然中的偶然”，本文將系統性地整理和分析機器學習選股模型涉及的隨機數，具體關注下列問題：</p><p>1. 首先梳理分析隨機數在機器學習中的用途。機器學習選股流程中哪些環節涉及隨機數？這些環節引入隨機數又是基於何種考慮？</p><p>2. 其次測試不同選股模型對隨機數種子的敏感程度。為了得到可重複的結果，針對特定算法，應固定哪幾處隨機數種子？當變更這幾處隨機數種子時，機器學習選股的訓練及回測表現會發生怎樣的變化？</p><p><strong>機器學習中的隨機數</strong></p><p>機器學習的多個環節涉及到隨機數，例如：訓練集、驗證集和測試集的隨機劃分，對神經網絡的權值進行隨機初始化，利用隨機梯度下降法求損失函數最優解，對隨機森林進行行列隨機採樣，訓練神經網絡時通過Dropout技術隨機刪除部分神經元等。引入這些隨機數對於構建機器學習模型有何幫助？在具體的算法實現層面，這些隨機數是如何起作用的？</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RPEy9qLGGIWQde><p><strong>從計算機中的隨機數生成談起</strong></p><p>構建機器學習模型所需的隨機數，是由計算機的隨機數生成器產生的。計算機是如何生成隨機數的呢？是在內部設置一個均分的輪盤，每次向輪盤撒黃豆；還是在內部產生一個虛擬的硬幣，反覆地拋擲硬幣？實際上，計算機無法產生絕對隨機的隨機數，計算機能產生的僅僅是“偽隨機數”，即相對的隨機數。需要強調的是，偽隨機數並不是假的，這裡的“偽”，表示有規律。換言之，計算機產生的隨機數既是隨機的，又是有規律的。</p><p>何謂“隨機且有規律”？例如，世界上沒有兩片完全相同的樹葉，這突出了事物的隨機性。但是每種樹的葉子都有相似的形狀和顏色，這就是規律性。計算機產生的隨機數是可預測、有周期的，是由某些公式和函數生成的。因此，對於同一隨機種子與函數，得到的隨機數列是一定的。</p><p>在計算機中，如果沒有設置隨機數種子，那麼將默認採用當前時鐘作為隨機數種子，代入生成函數，產生隨機數。偽隨機數生成函數雖然只是幾個簡單的函數，卻是科學家數十年研究的成果。圖靈獎首位亞裔獲得者姚期智教授的研究方向之一就是偽隨機數生成。目前常用的隨機數生成法有同餘法（congruential method）和梅森旋轉算法（Mersenne twister）。偽隨機數的產生機理，確保了使用相同隨機數種子產生的序列是完全相同的，從而保證使用者在固定隨機數種子後能得到可重複的確定性結果。</p><p><strong>數據集的隨機劃分</strong></p><p>瞭解計算機中隨機數的生成模式後，我們將進一步梳理機器學習算法每步可能用到的隨機數。在訓練機器學習模型前，不可缺少的一步是數據集的隨機劃分：將原始樣本按照一定方法，隨機劃分成訓練集（training set）、驗證集（validation set）和測試集（test set）。</p><p>訓練集的作用是訓練模型，形成模型的內部結構和參數估計。例如線性迴歸模型，每個自變量前的參數都是基於訓練集估計得到。驗證集的作用是模型選擇或超參數選擇。例如隨機森林中樹的棵數，每個基決策樹的特徵個數，內部節點再劃分需要的最小樣本數等，都是基於模型在驗證集的表現，通過不同模型或超參數的對比得到。測試集的作用是測試已經訓練完成模型的表現。測試集不參與模型的訓練和選擇，僅僅用以展示模型的性能，不為模型提供任何信息。</p><p>實踐中，多種方法可用於數據集的隨機劃分。以留出法（hold-out）為例，首先對原始數據進行一次或若干次混洗（shuffle）；其次按照混洗後的順序，取一定比例樣本作為總體訓練集，剩餘樣本作為測試集；隨後從總體訓練集中依照同樣方法，劃分出訓練集和驗證集。對於原始數據中的每條樣本，我們無法確定它應被分到哪類數據集中，但是它被分到每類數據集的概率應服從給定的樣本比例。單次劃分數據集得到的結果可能不穩定，一般需要進行多次隨機劃分，重複實驗後再考慮其平均結果。</p><p>將原始數據集進行隨機劃分，確保了劃分後每類數據集的內部結構（數據分佈）儘可能與原始數據集保持一致，避免因劃分過程引入額外偏差對最終結果造成的影響，使得基於訓練集得到的模型能夠適用於全體樣本。</p><p>需要說明的是，當原始數據集為時間序列時，隨機劃分可能破壞樣本的時序信息，更常用的方法是不進行混洗，直接按時序劃分為訓練集、驗證集和測試集。此處不涉及隨機數。</p><p><strong>優化算法中的隨機數</strong></p><p>劃分完數據集後，核心環節是在訓練集上利用特定學習算法擬合模型。諸多機器學習算法會用到各種形式的損失函數，如何快速有效地對損失函數求最小值，從而估計出模型的參數, 這就涉及到優化問題。各種優化方法也會用到隨機數，一方面用來“跳出”局部極值點，使得優化結果更接近全局最優解，另一方面用於消除極端值對優化結果的影響。</p><p><strong>賦予參數隨機初始值</strong></p><p>除少數簡單模型外，機器學習涉及到的優化問題通常無法直接給出顯式解，實踐中需藉助數值優化算法進行求解。數值優化算法通常從某個初始參數值出發，按照一定規則搜索並更新參數，直到優化目標函數變化小於容忍幅度，或者搜索次數達到最大迭代次數為止。</p><p>對於凸目標函數，優化問題在理論上存在唯一解，只要搜尋次數足夠長，總可以得到近似最優數值解。然而，無論採用何種參數搜索方法，如果優化在觸及最大迭代次數後停止，最終參數所能到達的位置會依賴於初始出發位置。換言之，如果賦予參數不同的初始值，我們可能得到不同的優化結果。</p><p>對於非凸目標函數，該問題仍然存在。更為嚴重的是，由於非凸目標存在局部極值，即使迭代次數足夠長，也無法確保從不同的初始參數出發最終能夠得到相近的結果。以經典的梯度下降搜尋算法為例，如果給定的初始值恰好落在某個非全局最優的局部極值點附近，由於該點附近梯度近似為0，參數更新極度緩慢，導致最終結果只能取到該點附近；而如果選擇另一個遠離該點的初始值，那麼最終結果有可能落在全局最優點。</p><p>為了解決上述問題，可以隨機賦予優化問題若干組不同的初始解，按照某種特定搜索算法求解模型後，將得到與每組初始值對應的一組“最優”參數及局部最優目標值。如果結果與初始值無關，那麼全部結果應較接近，任意一組解都可以作為備選最優解；如果結果與初始值相關，那麼取各組“局部”最優解中的“全局”最優解對應參數作為最終參數即可。</p><p>總之，在賦予參數隨機初始值的過程中，使用隨機數的作用是：防止1）迭代次數不足或者2）參數搜索“陷入”局部最優對優化結果的負面影響。</p><p><strong>隨機梯度下降</strong></p><p>梯度下降（gradient decent）是經典的數值優化搜索算法，我們曾在華泰金工《人工智能2：廣義線性模型》（20170622）中詳細介紹過該方法。梯度下降法通過計算損失函數的梯度，找到使損失函數下降最快的方向進行迭代搜索，最終找到最優值。</p><p>不妨把損失函數想象成一處山谷，如下圖所示，小球從四周某處自由滾下，那麼小球將落在谷底，即損失函數的極小值處。用數學的語言來說，損失函數的導數描述了山谷中局部的“形態”，而萬有引力定律則保證能夠牽引小球沿著山谷下降方向走。梯度下降法正是模擬小球在每一步都沿著山谷的下降方向（損失函數梯度的負方向）滾動。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RPEy9qZE2SclHT><p>然而在實際應用中，梯度下降法存在諸多缺陷。首先，為了計算損失函數的梯度，需要遍歷每條訓練樣本，當訓練樣本量較大時，會耗費大量內存，整個訓練過程變得較為緩慢。其次由於不同樣本間的梯度可能相互抵消，導致參數變化幅度過小，參數收斂速度隨之變慢。最後對於非凸的損失函數來說，參數值趨於局部極值點時，梯度將趨於零，參數值幾乎不再更新，最終損失函數只能優化到局部極值點。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RPEy9ql11X9TAN><p>針對以上不足，研究者提出隨機梯度下降法（stochastic gradient decent，SGD）。隨機梯度下降法的核心思想是每次隨機選取全部訓練樣本中的單個樣本計算其方向梯度，並據此立即更新模型參數。在具體的算法實現上，通常首先將所有訓練樣本進行混洗（shuffle）, 然後依次遍歷重排後的樣本，每次根據遍歷到的樣本計算梯度並立即更新參數。將全部打亂後的訓練樣本全部遍歷一次，稱為一輪（epoch）迭代。多輪迭代後，模型參數將可能收斂到全局最優值附近。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RPEy9qw9Tkahx6><p>隨機梯度下降法的優點在於，每次更新參數時，只需要檢驗單個樣本，無需遍歷所有樣本，因此適用於大規模數據的模型優化問題。由其算法實現過程可知，在同樣執行N輪迭代後，經典梯度下降方法參數只更新了N次，而隨機梯度下降方法參數更新了N乘以訓練樣本數量次，這意味著參數變動更為頻繁。當樣本量較大時，可能無需訓練完所有樣本，就能得到一個損失值在可接受範圍之內的模型。</p><p>另外，當損失函數為凸函數，無論是經典梯度下降法還是隨機梯度下降法，在初始幾步迭代過程中，損失函數下降速度較快。而當參數逼近最優值時，多數樣本對應的損失函數已經優化得足夠好，僅少數點尚未優化到位。為了繼續優化這些樣本點，梯度下降法至少需要一次迭代，遍歷全體樣本使參數更新到最優，儘管其中多數樣本對於總的梯度值已經沒有貢獻。隨機梯度下降法也至少需要再執行一次迭代，但是由於樣本是隨機打亂的，運氣足夠好的情形下，靠前的幾條樣本便是尚未優化的樣本，利用它們更新參數後就能提前結束本輪迭代，運氣最差的情形下需要遍歷全體樣本，平均而言只需要遍歷一半的樣本。</p><p>當損失函數為非凸函數，使用梯度下降法時，可能“陷入”到局部極值域中。但是對於隨機梯度下降法，即使參數在搜索中進入到局部極值域，由於下次取到的樣本是隨機的，所以仍有一定概率“跳出”該區域，從而進一步搜索到全局最優參數。</p><p>總的來看，隨機梯度下降法使用隨機數的作用是：避免樣本極端性對優化結果的影響；同時利用隨機性使參數搜索“跳出”梯度為0的區域，從而獲得更接近全局最優的結果。</p><p><strong>集成學習中的隨機數</strong></p><p>劃分訓練集、驗證集和測試集，利用隨機優化算法基於訓練集擬合出模型，再根據驗證集選擇超參數後，通常可得到弱學習器。單個弱學習器的預測能力有限，要想進一步增強模型的泛化能力，就需要進行集成學習。集成學習算法主要有兩大類：Bagging（並行）和Boosting（串行）。集成學習中隨機數的使用首先出現在Bagging，隨後擴展到Boosting。</p><p>Bagging（全稱bootstrap aggrating）是Bootstrap重採樣思想在機器學習中的應用。Bootstrap重採樣是指從數據集裡有放回地隨機抽取相同數量的樣本。一般而言，欲得到泛化性能強的集成模型，集成學習中的基學習器應儘可能保證互相獨立。實踐中，由於訓練都是基於同一數據集，無法嚴格保證獨立性，可行的辦法是保證弱學習器之間存在較大差異。如果弱學習器完全一致，集成過程就會失效。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RPEy9r83kBCHM6><p>Bagging 方法通過對數據集進行多次Bootstrap重採樣，產生具有差異的基學習器。如上圖所示，我們基於原始數據集生成N個Bootstrap數據集，對於每個Bootstrap數據集分別訓練單個弱分類器，最終用投票、取平均值等方法組合成強分類器。Bagging算法中，對包含M條樣本的訓練集做N次隨機重採樣，由於隨機性的存在，N個重採樣集各不相同。Bagging算法採用不同的重採樣集，訓練得到差異化的基學習器，故其泛化能力較強，模型的方差較低。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RPEyA0nG1ZRHhv><p>隨機森林是Bagging模型的一個擴展變體。隨機森林在以決策樹為基學習器構建Bagging集成的基礎上，進一步在基決策樹的訓練中引入隨機特徵選擇。具體而言，隨機森林根據以下兩步方法構建每棵決策樹。第一步稱為“行採樣”，從全體訓練樣本中有放回地抽樣，得到若干個Bootstrap數據集，並建立對應的弱學習器。第二步稱為“列採樣”，對於每一棵基決策樹，傳統決策樹算法在每一步選擇特徵劃分時，從全部d個特徵集合中選出一個最優特徵；隨機森林算法不考慮全部特徵，而是隨機抽取其中k個特徵，選擇其中的最優特徵進行劃分。列採樣保證了即使Bootstrap數據集完全相同，也可能生成不同的決策樹。這裡的k決定了引入隨機性的程度，當k = d時，隨機森林中基決策樹的構建與傳統決策樹相同。</p><p>決策樹的缺陷之一是易受訓練集中極端樣本的影響而導致過擬合，隨機森林能夠降低過擬合程度。極端樣本之所以極端，是由於其出現概率小。對於隨機森林，只有少數情況下極端樣本才會被抽樣進入Bootstrap數據集中，即使它被抽樣進入Bootstrap數據集中，也只有少數情況下才被選中參與學習，從而有效降低極端樣本對結果的影響。</p><p>隨著研究者對Bagging並行集成方法的日益認可，其核心的行列採樣思想也逐漸拓展到Boosting串行集成學習方法中。具有代表性的決策樹串行集成學習方法XGBoost包含行採樣超參數subsample和列採樣超參數系列colsample_by*，該方法同樣受到隨機數影響。</p><p>總的來看，在集成學習中，使用隨機數的作用是：產生差異性的樣本，進而基於這些樣本訓練出具有差異性的弱學習器，最終得到泛化能力更強的集成學習器。</p><p><strong>神經網絡中的隨機數</strong></p><p>過擬合是機器學習常被人詬病的問題之一。神經網絡由於參數數量眾多，過擬合問題尤為突出。如果模型存在過擬合，那麼其實際使用效果將大打折扣。上一節介紹的集成模型通過組合多個具有差異性的基模型，一定程度上緩解了過擬合。然而，集成模型需要訓練多個基模型，對於神經網絡模型而言，訓練單個基模型就已較為耗時，進行後端集成學習不現實。研究者如何緩解神經網絡的過擬合？</p><p>2012年，Hinton等人在論文《Improving neural networks by preventing co-adaptation of feature detectors》中提出Dropout技術。Dropout技術的核心思想是在神經網絡進行前向傳播時，令神經元的激活值以一定的概率p停止工作，使得模型不會過於依賴某些局部特徵，從而增強模型的泛化能力。</p><p>訓練神經網絡的常規流程是：每一輪迭代，首先將特徵X通過網絡前向傳播，隨後將誤差e反向傳播，以決定如何更新參數使得網絡進行學習。使用Dropout時，每一輪迭代的訓練流程為：</p><p>1. 基於當前參數，拷貝一份臨時神經網絡，從臨時網絡中隨機地刪除一部分隱藏神經元，注意需要保持輸入輸出神經元不變，如下圖。</p><p>2. 基於刪除一部分神經元的臨時網絡進行一次前向傳播，然後把得到的損失結果反向傳播。一小批訓練樣本執行完上述步驟後，對於原始網絡沒有被刪除的神經元，按照隨機梯度下降法更新對應臨時網絡位置的參數；對於原始網絡上被刪除的神經元，參數保持不變。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RPEyA11BCcfLmL><p>在實際訓練時，可設定每個隱藏層的Dropout比例，當Dropout比例為1時，相當於經典的訓練算法。直觀上Dropout不同的隱藏神經元類似於訓練不同的神經網絡（隨機刪掉部分隱藏神經元導致網絡結構發生改變)，整個Dropout過程相當於對多個不同的神經網絡取多次平均。不同的神經網絡可能產生不同程度的過擬合，一些“反向”的擬合互相抵消，能夠在整體上緩解過擬合。</p><p>總的來看，Dropout技術使用隨機數的作用是：隨機選取部分隱藏層神經元，每次基於不同的網絡結構訓練參數，從而緩解神經網絡中的過擬合問題。神經網絡的權值初始化和隨機梯度下降的環節也涉及隨機數，這裡不再贅述。</p><p><strong>Python環境下如何設置隨機數種子</strong></p><p>以上我們簡單梳理了機器學習各環節可能用到的隨機數。理論上，固定隨機數種子後，機器學習的結果應保持不變。然而在實踐過程中，不同機器學習方法固定隨機數種子的方法不盡相同，技術實現並非易事。本節我們將介紹Python環境下如何設置隨機數種子。</p><p>我們以邏輯迴歸，XGBoost，隨機森林和全連接神經網絡四種常用的機器學習方法為例。上述算法依賴sklearn、xgboost、keras和tensorflow等機器學習包實現，這些包提供的外部調用接口並不一致。使用sklearn和xgboost包時，構建學習模型這步可顯式地控制random_state參數，實現隨機數種子的設置。而keras並沒有提供這一選項，其官方的常見問題說明中指出，如果想得到完全可重複的結果，需要同時控制Python解釋器、Python標準庫隨機數模塊、numpy和tensorflow的全局隨機數種子，此外還需考慮GPU和多線程的使用。</p><p>我們針對多個不同的隨機數種子，測試比較每個種子的多次訓練結果。結果表明，當使用sklearn和xgboost包時，對於幾個常見學習模型，設置random_state參數就能保證結果可以完全重現；當以tensorflow作為後端使用keras包時，如果不使用GPU，且在單線程環境下，無需考慮Python標準庫隨機數模塊種子設置，同時固定住numpy和tensorflow的隨機數種子，即可保證全連接神經網絡模型得到可重複的結果。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RPEyA1IE7o7s9S><p><strong>機器學習選股模型隨機性的來源</strong></p><p>華泰金工《人工智能19：偶然中的必然：重採樣技術檢驗過擬合》（20190422）報告中，我們提出機器學習選股模型隨機性的三種來源：樣本內數據集中因子的隨機擾動，樣本外數據集因子的隨機擾動，回測時間段的選擇。針對上述三種隨機性的可能來源，我們認為可以採用Bootstrap重採樣技術模擬這些隨機性，通過Bootstrap樣本內數據集、Bootstrap樣本外數據集和Bootstrap回測時間考察不同環節隨機性對模型的影響。</p><p>本文是對上篇報告的進一步拓展，關注隨機性的第四種來源：算法本身包含的隨機數。考察方式和上篇報告稍有不同，我們將直接對隨機數種子點進行遍歷，分析100組不同隨機數種子下模型表現的分佈，詳細方法請見下一章節。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RPEyA1UIHP1o3E><p><strong>方法</strong></p><p><strong>人工智能選股模型測試流程</strong></p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RPEyA1j5JSJEsO><p>本文考察邏輯迴歸，XGBoost，隨機森林，全連接神經網絡四種機器學習模型（後文圖表中分別以Logit、XGBoost、RandomForest、ANN指代）在100組不同隨機數種子下的結果分佈。機器學習模型的測試流程包含如下步驟：</p><p>1. 數據獲取：</p><p>a) 股票池：全A股。剔除ST股票，剔除每個截面期下一交易日停牌的股票，剔除上市3個月內的股票，每隻股票視作一個樣本。</p><p>b) 回測區間：2011年1月31日至2019年1月31日。</p><p>2. 特徵和標籤提取：每個自然月的最後一個交易日，計算之前報告裡的70個因子暴露度，作為樣本的原始特徵，因子池如圖表13所示。計算下一整個自然月的個股超額收益（以滬深300指數為基準），在每個月末截面期，選取下月收益排名前30%的股票作為正例（y = 1），後30%的股票作為負例（y = 0），作為樣本的標籤。</p><p>3. 特徵預處理：</p><p>a) 中位數去極值：設第T期某因子在所有個股上的暴露度序列為 D_i ，D_M為該序列中位數，D_{M_1}為序列|D_i-D_M|的中位數，則將序列D_i中所有大於D_M+5 D_{M_1}的數重設為D_M+5 D_{M_1}，將序列中所有小於D_M-5 D_{M_1}的數重設為D_M-5 D_{M_1}；</p><p>b) 缺失值處理：得到新的因子暴露度序列後，將因子暴露度缺失的地方設為中信一級行業相同個股的平均值；</p><p>c) 行業市值中性化：將填充缺失值後的因子暴露度對行業啞變量和取對數後的市值做線性迴歸，取殘差作為新的因子暴露度；</p><p>d) 標準化：將中性化處理後的因子暴露度序列減去其現在的均值、除以其標準差，得到一個新的近似服從N(0, 1)分佈的序列</p><p>4. 滾動訓練集和驗證集的劃分：由於月度滾動訓練模型的時間開銷較大，本文采用年度滾動訓練方式，全體樣本內外數據共分為八個階段，如下圖所示。例如預測2011年時，將2005~2010年共72個月數據合併作為樣本內數據集；預測T年時，將T-6至T-1年的72個月合併作為樣本內數據。交叉驗證採用分組時序交叉驗證，折數為12。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RPEyADjCLl0tey><p>5. 交叉驗證調參：對於除全連接神經網絡模型外的模型，對全部超參數組合進行網格搜索，選擇驗證集平均AUC最高的一組超參數作為模型最終的超參數。全連接神經網絡模型不調參，使用固定超參數。最終使用的超參數如下表所示。</p><p>6. 100組隨機數種子的樣本內訓練：每次固定隨機數種子，使用機器學習算法對訓練集進行訓練，重複100次。</p><p>7. 100組隨機數種子的樣本外測試：每組隨機數種子下的模型訓練完成後，以T月末截面期所有樣本預處理後的特徵作為模型的輸入，得到每個樣本的預測值，重複100次。</p><p>8. 模型評價：a) 100組測試集正確率、AUC等衡量模型性能的指標；b) 以模型預測值作為單因子進行回測得到的100組統計指標和績效。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RPEyAEAIsE2Gv0><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RPEyAEO9N2bHqm><p><strong>全連接神經網絡模型參數設定</strong></p><p>1. 隱藏層：神經網絡理論上可以採用4層或者更多的層，但是過多的隱藏層將使計算量過大，且容易造成過擬合。考慮到以上因素，本研究採用含有2個隱藏層的全連接神經網絡。</p><p>2. 神經元：網絡輸入層神經元節點數是系統的因子（自變量）個數，輸出層神經元節點數是系統目標分類數。隱藏層節點選取按經驗選取，一般設為輸入層節點數的75%。系統進行訓練時，實際還要對不同的隱藏層節點數分別進行比較，最後確定出最合理的網絡結構。本研究網絡輸入層節點個數是70（70個因子），最終輸出層節點個數為分類數量，我們採用二分類（top 30%、bottom 30%）則輸出層節點個數為2，第1、2層隱藏層節點個數分別取為100、10，最終我們構建了一個70-100-10-2的全連接神經網絡模型。</p><p>3. 激活函數：激活函數可以為神經網絡加入非線性因素，以彌補線性模型的不足。考慮不同激活函數的特點，我們在隱藏層採用relu激活函數，在輸出層採用softmax激活函數。</p><p>4. Dropout：使用Dropout可以有效減小過擬合概率。本研究兩個隱藏層統一取0.2。</p><p>5. 學習速率：取0.0001。</p><p>6. 其它參數：取默認值。</p><p><strong>單因子測試</strong></p><p>使用機器學習模型進行選股，在每個月底可以產生對全部個股下月超額收益率的預測值。因此可以將機器學習模型看作一個因子合成模型，即在每個月底將因子池中所有因子合成為一個“因子”。隨後使用迴歸法、IC值分析法和分層測試法進行合成因子的單因子測試。</p><p><strong>迴歸法和IC值分析法</strong></p><p>測試模型構建方法如下：</p><p>1. 股票池：全A股，剔除ST股票，剔除每個截面期下一交易日停牌的股票，剔除上市3個月以內的股票。</p><p>2. 回測區間：2011-01-31至2019-01-31。</p><p>3. 截面期：每個月月末，用當前截面期因子值與當前截面期至下個截面期內的個股收益進行迴歸和計算Rank IC值。</p><p>4. 數據處理方法：對於分類模型，將模型對股票下期上漲概率的預測值視作單因子。對於迴歸模型，將回歸預測值視作單因子。因子值為空的股票不參與測試。</p><p>5. 迴歸測試中採用加權最小二乘迴歸（WLS），使用個股流通市值的平方根作為權重。IC測試時對單因子進行行業市值中性。</p><p><strong>分層回測法</strong></p><p>依照因子值對股票進行打分，構建投資組合回測，是最直觀的衡量因子優劣的手段。測試模型構建方法如下：</p><p>1. 股票池、回測區間、截面期均與迴歸法相同。</p><p>2. 換倉：在每個自然月最後一個交易日核算因子值，在下個自然月首個交易日按當日收盤價換倉，交易費用以雙邊千分之四計。</p><p>3. 分層方法：因子先用中位數法去極值，然後進行市值、行業中性化處理（方法論詳見上一小節），將股票池內所有個股按因子從大到小進行排序，等分N層，每層內部的個股等權配置。當個股總數目無法被N整除時採用任一種近似方法處理均可，實際上對分層組合的回測結果影響很小。</p><p>4. 多空組合收益計算方法：用Top組每天的收益減去Bottom組每天的收益，得到每日多空收益序列r_1,r_2,\ldots,r_n，則多空組合在第n天的淨值等於(1+r_1)(1+r_2)\ldots(1+r_n)。</p><p>5. 評價方法：全部N層組合年化收益率（觀察是否單調變化），多空組合的年化收益率、夏普比率、最大回撤等。</p><p><strong>結果</strong></p><p><strong>模型性能</strong></p><p>我們首先展示100組隨機數種子下，四種機器學習模型在全部96個樣本外測試月份的平均正確率和AUC分佈，如下圖所示。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/RPEyAEc2SZMgjM><p>我們同時展示2018年1~12月的12個月末截面期對應測試月份（2018年2月至2019年1月）的平均正確率和AUC分佈，如下圖所示。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RPEyAEolY5XGm><p>無論是全部樣本外測試月份還是2018年的12個樣本外測試月份，邏輯迴歸的正確率和AUC分佈相對較窄，全連接神經網絡模型分佈相對較寬，XGBoost和隨機森林分佈大致相當，介於邏輯迴歸和全連接神經網絡之間。上述結果表明，變更隨機數種子時，邏輯迴歸的性能幾乎不會發生變化，XGBoost和隨機森林的模型性能會在一定範圍內波動，全連接神經網絡的模型性能則可能發生較大變化，最差情況下可能產生一個百分點的變動。</p><p><strong>迴歸法和IC值分析法</strong></p><p>下面展示改變隨機數種子對於迴歸法和IC值分析法各項指標的影響。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RPEyAROM5JSft><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RPEyAReHXZTt6h><p>迴歸法和IC分析法的結果和模型性能結果基本一致，從分佈寬度看，邏輯迴歸相對較窄，XGBoost和隨機森林次之，全連接神經網絡相對較寬。這表明邏輯迴歸對隨機數種子相對不敏感，全連接神經網絡對隨機數種子相對敏感。另外，從因子收益率和Rank IC的分佈位置看，XGBoost和隨機森林靠右側，邏輯迴歸居中，全連接神經網絡靠左側。該結果表明XGBoost和隨機森林這兩種決策樹的集成模型表現相對較好，全連接神經網絡表現相對較差。</p><p>除了統計平均Rank IC外，我們還可以分析累積Rank IC隨時間的變化情況。下圖展示了每種模型100組結果的累積Rank IC均值和±1倍和±2倍標準差區域（計算每個交易日 100個累積Rank IC的標準差，下同）。邏輯迴歸累積Rank IC的波動較低，XGBoost和隨機森林次之，全連接神經網絡的波動較高。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RPEyARw43QT9IB><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RPEyAS9Mcus85><p><strong>分層測試法</strong></p><p>下面展示改變隨機數種子對於分層測試法各項指標的影響。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RPEyASMEUmsar9><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RPEyAcy5Th7lN><p>從回測表現看，當變更隨機數種子時，邏輯迴歸的結果幾乎不會發生變化，XGBoost和隨機森林的結果會在較窄的範圍內波動，而全連接神經網絡的結果面臨較大的不確定性，Top組合最差和最好情形下的年化收益率相差甚至超過1.5%，多空組合的最差和最好情形下的年化收益率相差超過2%。</p><p>除統計多空組合的年化收益率和夏普比率外，我們還可以分析多空組合淨值隨時間的變化情況。下圖展示了每種模型100組結果多空組合淨值的均值和±1倍和±2倍標準差區域。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RPEyAdC2DdSbnZ><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RPEyAdP3NBzHnS><p>多空組合淨值結果和累積Rank IC結果類似，當變更隨機數種子時，邏輯迴歸的結果幾乎不會發生變化，XGBoost和隨機森林的結果會在一定範圍內波動，而全連接神經網絡的結果面臨較大的不確定性。</p><p>在實踐中，我們不僅關注隨機數種子變化對結果的平均影響，還關心最差或者最優情形下的結果。當我們針對單個隨機數種子進行回測得到一條淨值曲線時，它可能是最差的結果，實踐中的結果大多數情形下都會更好；它也可能是最優的結果，實際幾乎不可能出現這種情況；它也可能只是一個普通的結果，實踐中有較大的概率能夠實現。以下給出了四種模型對應的100組結果中總收益最差和最優兩種極端情況下的淨值表現。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RPEyAdcIKJKiMA><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/RPEyAdp1Z8frkn><p>當變更隨機數種子時，邏輯迴歸的結果幾乎不會發生變化，最優情形和最差情形相當，與平均結果接近。對於全連接神經網絡模型，模型最終結果的最差情形和最優情形都偏離均值較遠。</p><p><strong>不同隨機性來源的橫向比較</strong></p><p>在華泰金工《人工智能19：偶然中的必然：重採樣技術檢驗過擬合》和本文中，我們提出了四種可能的機器學習選股模型隨機性來源：1）樣本內數據集中因子的隨機擾動，2）樣本外數據集因子的隨機擾動，3）回測時間段的選擇和4）算法本身包含的隨機數。如何衡量這四類隨機性來源對結果的相對影響程度？</p><p>我們以XGBoost模型為例，分別展示100次1）Bootstrap樣本內數據集，2）Bootstrap樣本外數據集，3）Bootstrap回測時間和4）遍歷隨機數種子點的單因子測試指標的均值、標準差和變異係數，如下表所示。變異係數（coefficient of variation）用標準差除以均值衡量，相當於標準化後的標準差，目的在於使不同量綱的標準差可比。其中前三種隨機性來源的結果取自《人工智能19》，第四種隨機性來源的結果取自本文。</p><p>比較四種隨機性來源的變異係數，可知回測時間的變異程度較高，說明回測時間選擇對模型表現的影響較大；樣本外數據的變異程度居中，說明樣本外數據的隨機擾動對模型表現的影響一般；樣本內數據和隨機數種子的變異程度較低，說明樣本內數據的隨機擾動以及算法本身包含的隨機數對模型表現的影響較小。</p><img alt=【華泰金工林曉明團隊】必然中的偶然：機器學習中的隨機數——華泰人工智能系列之二十 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/RPEyAomIw1zOD3><p>總的來看，儘管算法中的隨機數是機器學習選股模型隨機性的重要來源，它對XGBoost模型最終結果的影響程度並不高。我們建議對於簡單模型（如邏輯迴歸）或者已證實隨機數影響程度不高的模型（如XGBoost），策略開發過程中僅使用固定的單個隨機數種子即可。對於複雜模型或者隨機數影響程度較大的模型，可取的做法是綜合考慮多個隨機數種子下的結果。</p><p><strong>總結</strong></p><p>機器學習中引入隨機數具有重要意義，或是為了保證損失函數更易達到最優解，或是為了避免極端值對模型訓練造成的不良影響，或是為了產生具有差異性的樣本以便進一步集成等，最終目的都在於增強模型的泛化能力。實際訓練中固定隨機數種子的做法雖然保證了結果的可重複性，但是掩蓋了隨機數本身對模型的影響——即掩蓋了“必然中的偶然”。</p><p>本文測試分析了100組不同隨機數種子下邏輯迴歸、XGBoost、隨機森林和全連接神經網絡四種機器學習選股模型的性能和單因子回測表現，發現當隨機數種子變化時，邏輯迴歸的結果幾乎保持不變，而全連接神經網絡模型的結果可能會發生較大變化，XGBoost和隨機森林模型的結果在一定範圍內發生變化。</p><p>得到這樣的結果並不意外。邏輯迴歸在使用隨機梯度下降算法優化損失函數時引入了隨機數，由於優化目標是標準的凸函數，優化算法最終大概率會收斂到唯一的理論最優參數附近，因而結果幾乎不會發生變化。神經網絡模型涉及大量的參數，在初始化神經元權重，利用優化算法最小化損失函數，前向傳播進行Dropout等環節均引入了隨機數，模型整體 具有較大的不確定性。和神經網絡模型類似，XGBoost和隨機森林模型也具有較高的複雜度，行列採樣環節涉及隨機數，但是由於這兩種模型本身進行了集成，相當於對結果求平均，因此結果的不確定性有所降低。</p><p>本文的啟示在於：單個隨機數種子下邏輯迴歸的結果較為可靠；單個隨機數種子下全連接神經網絡的結果具有較大的不確定性，得到一個較差結果時不應輕易否定模型本身的價值，而得到一個較好結果時不應輕信模型的表現，可取的做法是綜合考慮多個隨機數種子下的結果；對於XGBoost和隨機森林模型，由隨機數種子造成的結果不確定性介於邏輯迴歸和全連接神經網絡之間，投資者應認識到結果本身可能存在的隨機數種子選擇偏差。</p><p><strong>風險提示</strong></p><p>機器學習選股方法是對歷史投資規律的挖掘，若未來市場投資環境發生變化導致基學習器失效，則該方法存在失效的可能。機器學習存在一定過擬合風險。當機器學習算法涉及隨機數時，不同隨機數種子可能得到不同結果。</p><p><strong>免責申明</strong></p><p>本公眾平臺不是華泰證券研究所官方訂閱平臺。相關觀點或信息請以華泰證券官方公眾平臺為準。根據《證券期貨投資者適當性管理辦法》的相關要求，本公眾號內容僅面向華泰證券客戶中的專業投資者，請勿對本公眾號內容進行任何形式的轉發。若您並非華泰證券客戶中的專業投資者，請取消關注本公眾號，不再訂閱、接收或使用本公眾號中的內容。因本公眾號難以設置訪問權限，若給您造成不便，煩請諒解！本公眾號旨在溝通研究信息，交流研究經驗，華泰證券不因任何訂閱本公眾號的行為而將訂閱者視為華泰證券的客戶。</p><p>本公眾號研究報告有關內容摘編自已經發布的研究報告的，若因對報告的摘編而產生歧義，應以報告發布當日的完整內容為準。如需瞭解詳細內容，請具體參見華泰證券所發佈的完整版報告。</p><p>本公眾號內容基於作者認為可靠的、已公開的信息編制，但作者對該等信息的準確性及完整性不作任何保證，也不對證券價格的漲跌或市場走勢作確定性判斷。本公眾號所載的意見、評估及預測僅反映發佈當日的觀點和判斷。在不同時期，華泰證券可能會發出與本公眾號所載意見、評估及預測不一致的研究報告。</p><p>在任何情況下，本公眾號中的信息或所表述的意見均不構成對客戶私人投資建議。訂閱人不應單獨依靠本訂閱號中的信息而取代自身獨立的判斷，應自主做出投資決策並自行承擔投資風險。普通投資者若使用本資料，有可能會因缺乏解讀服務而對內容產生理解上的歧義，進而造成投資損失。對依據或者使用本公眾號內容所造成的一切後果，華泰證券及作者均不承擔任何法律責任。</p><p>本公眾號版權僅為華泰證券股份有限公司所有，未經公司書面許可，任何機構或個人不得以翻版、複製、發表、引用或再次分發他人等任何形式侵犯本公眾號發佈的所有內容的版權。如因侵權行為給華泰證券造成任何直接或間接的損失，華泰證券保留追究一切法律責任的權利。本公司具有中國證監會核准的“證券投資諮詢”業務資格，經營許可證編號為：91320000704041011J。</p><p>林曉明</p><p>執業證書編號：S0570516010001</p><p><strong>華泰金工深度報告一覽</strong></p><p><strong>金融週期系列研究（資產配置）</strong></p><p>【華泰金工林曉明團隊】二十載昔日重現，三四年週期輪迴——2019年中國與全球市場量化資產配置年度觀點（下）</p><p>【華泰金工林曉明團隊】二十載昔日重現，三四年週期輪迴——2019年中國與全球市場量化資產配置年度觀點（上）</p><p>【華泰金工林曉明團隊】週期輪動下的BL資產配置策略</p><p>【華泰金工林曉明團隊】週期理論與機器學習資產收益預測——華泰金工市場週期與資產配置研究</p><p>【華泰金工林曉明團隊】市場拐點的判斷方法</p><p>【華泰金工林曉明團隊】2018中國與全球市場的機會、風險 · 年度策略報告（上）</p><p>【華泰金工林曉明團隊】基欽週期的量化測度與歷史規律 · 華泰金工週期系列研究</p><p>【華泰金工林曉明團隊】週期三因子定價與資產配置模型（四）——華泰金工週期系列研究</p><p>【華泰金工林曉明團隊】週期三因子定價與資產配置模型（三）——華泰金工週期系列研究</p><p>【華泰金工林曉明團隊】週期三因子定價與資產配置模型（二）——華泰金工週期系列研究</p><p>【華泰金工林曉明團隊】週期三因子定價與資產配置模型（一）——華泰金工週期系列研究</p><p>【華泰金工林曉明團隊】華泰金工週期研究系列 · 基於DDM模型的板塊輪動探索</p><p>【華泰金工林曉明團隊】市場週期的量化分解</p><p>【華泰金工林曉明團隊】週期研究對大類資產的預測觀點</p><p>【華泰金工林曉明團隊】金融經濟系統週期的確定（下）——華泰金工週期系列研究</p><p>【華泰金工林曉明團隊】金融經濟系統週期的確定（上）——華泰金工週期系列研究</p><p>【華泰金工林曉明團隊】全球多市場擇時配置初探——華泰週期擇時研究系列</p><p>行業指數頻譜分析及配置模型：市場的週期分析系列之三</p><p>【華泰金工林曉明團隊】市場的頻率——市場輪迴，週期重生</p><p>【華泰金工林曉明團隊】市場的輪迴——金融市場週期與經濟週期關係初探</p><p><strong>FOF與金融創新產品</strong></p><p>【華泰金工】生命週期基金Glide Path開發實例——華泰FOF與金融創新產品系列研究報告之一</p><p><strong>因子週期（因子擇時）</strong></p><p>【華泰金工林曉明團隊】市值因子收益與經濟結構的關係——華泰因子週期研究系列之三</p><p>【華泰金工林曉明團隊】週期視角下的因子投資時鐘--華泰因子週期研究系列之二</p><p>【華泰金工林曉明團隊】因子收益率的週期性研究初探</p><p><strong>擇時</strong></p><p>【華泰金工林曉明團隊】華泰風險收益一致性擇時模型</p><p>【華泰金工林曉明團隊】技術指標與週期量價擇時模型的結合</p><p>【華泰金工林曉明團隊】華泰價量擇時模型——市場週期在擇時領域的應用</p><p><strong>行業輪動</strong></p><p>【華泰金工林曉明團隊】行業輪動系列之六：“華泰週期輪動”基金組合構建20190312</p><p>【華泰金工林曉明團隊】估值因子在行業配置中的應用——華泰行業輪動系列報告之五</p><p>【華泰金工林曉明團隊】動量增強因子在行業配置中的應用--華泰行業輪動系列報告之四</p><p>【華泰金工林曉明團隊】財務質量因子在行業配置中的應用--華泰行業輪動系列報告之三</p><p>【華泰金工林曉明團隊】週期視角下的行業輪動實證分析·華泰行業輪動系列之二</p><p>【華泰金工林曉明團隊】基於通用迴歸模型的行業輪動策略 · 華泰行業輪動系列之一</p><p><strong>Smartbeta</strong></p><p>【華泰金工林曉明團隊】Smart Beta：乘風破浪趁此時——華泰Smart Beta系列之一</p><p>【華泰金工林曉明團隊】Smartbeta在資產配置中的優勢——華泰金工Smartbeta專題研究之一</p><p><strong>多因子選股</strong></p><p>【華泰金工林曉明團隊】因子合成方法實證分析 ——華泰多因子系列之十</p><p>【華泰金工林曉明團隊】華泰單因子測試之一致預期因子 ——華泰多因子系列之九</p><p>【華泰金工林曉明團隊】華泰單因子測試之財務質量因子——華泰多因子系列之八</p><p>【華泰金工林曉明團隊】華泰單因子測試之資金流向因子——華泰多因子系列之七</p><p>【華泰金工林曉明團隊】華泰單因子測試之波動率類因子——華泰多因子系列之六</p><p>【華泰金工林曉明團隊】華泰單因子測試之換手率類因子——華泰多因子系列之五</p><p>【華泰金工林曉明團隊】華泰單因子測試之動量類因子——華泰多因子系列之四</p><p>【華泰金工林曉明團隊】華泰單因子測試之成長類因子——華泰多因子系列之三</p><p>【華泰金工林曉明團隊】華泰單因子測試之估值類因子——華泰多因子系列之二</p><p>【華泰金工林曉明團隊】華泰多因子模型體系初探——華泰多因子系列之一</p><p>【華泰金工林曉明團隊】五因子模型A股實證研究</p><p>【華泰金工林曉明團隊】紅利因子的有效性研究——華泰紅利指數與紅利因子系列研究報告之二</p><p><strong>人工智能</strong></p><p>【華泰金工林曉明團隊】偶然中的必然：重採樣技術檢驗過擬合——華泰人工智能系列之十九</p><p>【華泰金工林曉明團隊】機器學習選股模型的調倉頻率實證——華泰人工智能系列之十八</p><p>【華泰金工林曉明團隊】人工智能選股之數據標註方法實證——華泰人工智能系列之十七</p><p>【華泰金工林曉明團隊】再論時序交叉驗證對抗過擬合——華泰人工智能系列之十六</p><p>【華泰金工林曉明團隊】人工智能選股之卷積神經網絡——華泰人工智能系列之十五</p><p>【華泰金工林曉明團隊】對抗過擬合：從時序交叉驗證談起</p><p>【華泰金工林曉明團隊】人工智能選股之損失函數的改進——華泰人工智能系列之十三</p><p>【華泰金工林曉明團隊】人工智能選股之特徵選擇——華泰人工智能系列之十二</p><p>【華泰金工林曉明團隊】人工智能選股之Stacking集成學習——華泰人工智能系列之十一</p><p>【華泰金工林曉明團隊】宏觀週期指標應用於隨機森林選股——華泰人工智能系列之十</p><p>【華泰金工林曉明團隊】人工智能選股之循環神經網絡——華泰人工智能系列之九</p><p>【華泰金工林曉明團隊】人工智能選股之全連接神經網絡——華泰人工智能系列之八</p><p>【華泰金工林曉明團隊】人工智能選股之Python實戰——華泰人工智能系列之七</p><p>【華泰金工林曉明團隊】人工智能選股之Boosting模型——華泰人工智能系列之六</p><p>【華泰金工林曉明團隊】人工智能選股之隨機森林模型——華泰人工智能系列之五</p><p>【華泰金工林曉明團隊】人工智能選股之樸素貝葉斯模型——華泰人工智能系列之四</p><p>【華泰金工林曉明團隊】人工智能選股之支持向量機模型— —華泰人工智能系列之三</p><p>【華泰金工林曉明團隊】人工智能選股之廣義線性模型——華泰人工智能系列之二</p><p><strong>指數增強基金分析</strong></p><p>【華泰金工林曉明團隊】再探迴歸法測算基金持股倉位——華泰基金倉位分析專題報告</p><p>【華泰金工林曉明團隊】酌古御今：指數增強基金收益分析</p><p>【華泰金工林曉明團隊】基於迴歸法的基金持股倉位測算</p><p>【華泰金工林曉明團隊】指數增強方法彙總及實例——量化多因子指數增強策略實證</p><p><strong>基本面選股</strong></p><p>【華泰金工林曉明團隊】華泰價值選股之相對市盈率港股模型——相對市盈率港股通模型實證研究</p><p>【華泰金工林曉明團隊】華泰價值選股之FFScore模型</p><p>【華泰金工林曉明團隊】相對市盈率選股模型A股市場實證研究</p><p>【華泰金工林曉明團隊】華泰價值選股之現金流因子研究——現金流因子選股策略實證研究</p><p>【華泰金工林曉明團隊】華泰基本面選股之低市收率模型——小費雪選股法 A 股實證研究</p><p>【華泰金工林曉明團隊】華泰基本面選股之高股息率模型之奧軒尼斯選股法A股實證研究</p><p><strong>基金定投</strong></p><p>【華泰金工林曉明團隊】大成旗下基金2018定投策略研究</p><p>【華泰金工林曉明團隊】布林帶與股息率擇時定投模型——基金定投系列專題研究報告之四</p><p>【華泰金工林曉明團隊】基金定投3—馬科維茨有效性檢驗</p><p>【華泰金工林曉明團隊】基金定投2—投資標的與時機的選擇方法</p><p>【華泰金工林曉明團隊】基金定投1—分析方法與理論基礎</p><p><strong>其它</strong></p><p>【華泰金工林曉明團隊】A股市場及行業的農曆月份效應——月份效應之二</p><p>A股市場及行業的月份效應——詳解歷史數據中的隱藏法則</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>華泰</a></li><li><a>林曉明</a></li><li><a>團隊</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E5%AD%B8/303c6a3.html alt=「華泰金工林曉明團隊」國內首隻科技ETF將正式上市——指數基金專題之華寶科技ETF class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RZ66r6i1RpHPZi style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/303c6a3.html title=「華泰金工林曉明團隊」國內首隻科技ETF將正式上市——指數基金專題之華寶科技ETF>「華泰金工林曉明團隊」國內首隻科技ETF將正式上市——指數基金專題之華寶科技ETF</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/17eee2b1.html alt="小蜜團隊萬字長文 | 講透對話管理模型最新研究進展" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/249d781057744bdcb94e007250ac41bc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/17eee2b1.html title="小蜜團隊萬字長文 | 講透對話管理模型最新研究進展">小蜜團隊萬字長文 | 講透對話管理模型最新研究進展</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ede0fdf0.html alt=丁古巧團隊Small：全面總結碳基量子點固態發光領域研究進展 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/42aadd2d629c4987b716246f7bfd067e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ede0fdf0.html title=丁古巧團隊Small：全面總結碳基量子點固態發光領域研究進展>丁古巧團隊Small：全面總結碳基量子點固態發光領域研究進展</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/427e9d90.html alt=北師大閆東鵬團隊在熱阻型室溫磷光材料研究取得重要進展 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d60cf4d7590c4f84ba0d2387fb000ec7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/427e9d90.html title=北師大閆東鵬團隊在熱阻型室溫磷光材料研究取得重要進展>北師大閆東鵬團隊在熱阻型室溫磷光材料研究取得重要進展</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e236ffbb.html alt=新高度！唐本忠院士團隊《AM》綜述：提出超越分子科學的“聚集態科學” class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/S6r7K8MDw0QB9M style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e236ffbb.html title=新高度！唐本忠院士團隊《AM》綜述：提出超越分子科學的“聚集態科學”>新高度！唐本忠院士團隊《AM》綜述：提出超越分子科學的“聚集態科學”</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ebaa71f3.html alt=支援武漢協和西院鍾南山院士團隊首例成功為新冠肺炎危重患者拔管 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/Rr5wAhbCE6lOa7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ebaa71f3.html title=支援武漢協和西院鍾南山院士團隊首例成功為新冠肺炎危重患者拔管>支援武漢協和西院鍾南山院士團隊首例成功為新冠肺炎危重患者拔管</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0538cd31.html alt="緊急! 鍾南山團隊建議嚴格防控至4月底! 開學再延遲, 網課將高中生殘酷分層!" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1c580d93b759491d92a59f42bc648ca4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0538cd31.html title="緊急! 鍾南山團隊建議嚴格防控至4月底! 開學再延遲, 網課將高中生殘酷分層!">緊急! 鍾南山團隊建議嚴格防控至4月底! 開學再延遲, 網課將高中生殘酷分層!</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e2ea9375.html alt=諾獎得主領導團隊研發新技術：5分鐘檢測新冠病毒 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e2ea9375.html title=諾獎得主領導團隊研發新技術：5分鐘檢測新冠病毒>諾獎得主領導團隊研發新技術：5分鐘檢測新冠病毒</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c080b7f2.html alt="上海不停電作業團隊為浙江賦能 長三角首個跨省不停電作業工程今天實施" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/1487769fba9042be9272a4cb90c11a71 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c080b7f2.html title="上海不停電作業團隊為浙江賦能 長三角首個跨省不停電作業工程今天實施">上海不停電作業團隊為浙江賦能 長三角首個跨省不停電作業工程今天實施</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/63d29ca7.html alt=孫世剛團隊：一種新型高效酸性氧還原M/MN4複合活性位的構建 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/2f64dcc59e4d45faa9c69b3b8f17a518 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/63d29ca7.html title=孫世剛團隊：一種新型高效酸性氧還原M/MN4複合活性位的構建>孫世剛團隊：一種新型高效酸性氧還原M/MN4複合活性位的構建</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/db2ac461.html alt=北化呂超團隊利用點擊化學大規模製備高效聚合物室溫磷光材料 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/f8055c3195334dc1b552ab29676cbd88 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/db2ac461.html title=北化呂超團隊利用點擊化學大規模製備高效聚合物室溫磷光材料>北化呂超團隊利用點擊化學大規模製備高效聚合物室溫磷光材料</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/22aae29e.html alt=徐華強等合作團隊解析新冠重要藥靶RNA複製酶結合瑞德西韋的結構 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1188d9bed0e247a6bcd50e1a8558416c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/22aae29e.html title=徐華強等合作團隊解析新冠重要藥靶RNA複製酶結合瑞德西韋的結構>徐華強等合作團隊解析新冠重要藥靶RNA複製酶結合瑞德西韋的結構</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6d8712de.html alt=蘭州大學研究團隊成功研發我國首套塑閃宇宙射線繆子成像系統 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/3caeaf98a77649c3a70a478b50a2dbb3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6d8712de.html title=蘭州大學研究團隊成功研發我國首套塑閃宇宙射線繆子成像系統>蘭州大學研究團隊成功研發我國首套塑閃宇宙射線繆子成像系統</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c3e37c59.html alt=江西880家旅行社即日起可恢復跨省團隊遊業務 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/347f4a7b5fc44471b0894b894dea0c00 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c3e37c59.html title=江西880家旅行社即日起可恢復跨省團隊遊業務>江西880家旅行社即日起可恢復跨省團隊遊業務</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6a77df59.html alt="江西跨省團隊旅遊重啟後價格普降 跟團遊客仍不多" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/adb83600f66c4a6f8d8dc1f20f6a358a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6a77df59.html title="江西跨省團隊旅遊重啟後價格普降 跟團遊客仍不多">江西跨省團隊旅遊重啟後價格普降 跟團遊客仍不多</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>