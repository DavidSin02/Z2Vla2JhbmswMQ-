<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>SysML 2019論文解讀：推理優化 | 极客快訊</title><meta property="og:title" content="SysML 2019論文解讀：推理優化 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/aec50a880ead4851ab2b50759d749720"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/efd74a95.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/efd74a95.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/efd74a95.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/efd74a95.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/efd74a95.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/efd74a95.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/efd74a95.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/efd74a95.html><meta property="article:published_time" content="2020-10-29T21:11:12+08:00"><meta property="article:modified_time" content="2020-10-29T21:11:12+08:00"><meta name=Keywords content><meta name=description content="SysML 2019論文解讀：推理優化"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/efd74a95.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>SysML 2019論文解讀：推理優化</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><blockquote>推理優化是機器學習領域的核心問題之一，本文將解讀 SysML 會議上兩篇與推理優化有關的論文，其中一篇提出了準確高效的二位量化神經網絡，另一篇則試圖用寬鬆化圖替代優化深度神經網絡的計算過程。</blockquote><p class=ql-align-justify><br></p><p class=ql-align-justify>聲明：本文的所有圖片和公式都來自於原論文。</p><ul><li class=ql-align-justify>論文1：Accurate and Efficient 2-Bit Quantized Neural Netowrks</li><li class=ql-align-justify>地址：https://www.sysml.cc/doc/2019/168.pdf</li></ul><p class=ql-align-justify><br></p><p class=ql-align-center><strong>引言</strong></p><p class=ql-align-justify>隨著機器學習和人工智能領域的持續發展，神經網絡及其代表性的算法通過提升計算成本而實現了越來越高的準確度。量化（quantization）是一種以準確度為代價旨在降低計算成本的方法。為了在儘可能小地損失準確度的同時儘可能多地減少計算，研究者們已經提出了多種不同的量化方案。</p><p class=ql-align-justify>通常來說，量化可用在兩個地方，即神經網絡的權重和激活。在學習機器學習時，這一類工作往往會被忽視，但如果考慮到實用性，這又非常重要。因此，我決定解讀這篇論文。</p><p class=ql-align-justify>為了單獨實現各個權重和激活的量化，進而得到整體的量化神經網絡（QNN），這篇論文提出了一些新技術。其中用於激活量化的技術為「PArameterized Clipping acTivation（PACT）」，用於權重量化的技術則為「Statistics-Aware Weight Binning（SAWB）」。</p><p class=ql-align-justify>這篇論文聲稱，組合使用 PACT 與 SAWB 可以得到一種二位量化神經網絡（2-bit QNN），其分類準確度在一些常見的模型和數據集上能達到當前最佳水平。</p><p class=ql-align-center><strong>量化研究</strong></p><p class=ql-align-justify>簡單來說，量化是指降低表示一個數值的位數的過程。在機器學習領域，之前研究主要使用的數值格式是 32 位浮點數，或根據 CPU 不同甚至還會使用 64 位（雖然實際中不會有人用這麼多 bits 來訓練網絡）。</p><p class=ql-align-justify>但是，為了實現機器學習模型的實際部署，有必要降低模型所需的帶寬和計算量。這有助於降低計算成本和功耗，並提升速度。很多研究結果已經表明，使用 8 位整型數表示權重和激活不會顯著影響準確度。</p><p class=ql-align-center><strong>參數化截略式激活（PACT）</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify><strong>激活量化</strong></p><p class=ql-align-justify>卷積神經網絡（CNN）常使用 ReLU 作為激活函數。ReLU 的困難之處在於其輸出沒有限界，這意味著量化需要較大的輸出範圍（即更大的位寬）。當目標位寬有限時，這就成問題了。圖 1 給出了使用 ReLU 的 ResNet10 在 CIFAR10 數據集上的訓練和驗證誤差；當 ReLU 激活被量化為 2 位時，準確度明顯下降。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/aec50a880ead4851ab2b50759d749720><p class=pgc-img-caption>圖 1：當使用 ReLU 或截略函數的 CIFAR10 ResNet10 的激活被量化到 2 位時，</p></div><p class=ql-align-justify>截略（clipping）是一種為解決大輸出範圍問題而開發的方法。其會在輸出激活的幅度上加一個上界。我們也可以在圖 1 中看到網絡使用了截略方法時的訓練和驗證誤差。這個誤差比沒使用截略時更低，但這是不可接受的。這就引出了這篇論文的貢獻。</p><p class=ql-align-justify>PACT 技術基於以上見解，這是一種使用了一個參數化元素的截略方法。現在，激活函數有一個參數化的截略水平 α。α 可在訓練階段通過隨機梯度下降（SGD）進行動態調整，以最小化準確度下降。PACT ReLU 輸出的形式化定義如下：</p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/af501ab1cb944d2989a42ae381079f33><p class=pgc-img-caption></p></div><p class=ql-align-justify>α 將激活函數的輸出範圍限制到了 [0, α]。本質上講，這只是為損失函數的優化引入一個新參數 α，並在反向傳播步驟中選出 α 的最優值。圖 2 給出了帶有參數化截略的 PACT 激活函數。隨著 α→∞，該函數會向 ReLU 函數收斂。因此，α 讓人可根據實際情況靈活地調整輸出範圍。</p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7ae6c83fc7854cdbaa09d0151002da75><p class=pgc-img-caption>圖 2：PACT 激活函數及其梯度</p></div><p class=ql-align-justify>如前所述，α 的值可通過 SGD 更新。設 L 為損失函數，y 為網絡的輸出。在每次迭代時，α 遵循以下更新規則：</p><ol><li class=ql-align-justify>如果 x ≤ α，則 PACT 的行為類似常規 ReLU，α 不更新。</li><li class=ql-align-justify>如果 x > α，則 α 通過以下公式使用 SGD 更新。</li></ol><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/184a620a76e44e1592868c6029ffa785><p class=pgc-img-caption></p></div><p class=ql-align-justify>其中 η 是學習率。∂L/∂α = ∂L/∂y 的原因是 ∂L/∂y = 1（參見圖 2）。</p><p class=ql-align-justify>注意，這裡一個網絡僅有一個 α。也就是說，PACT 會整體考慮所有輸出神經元並更改 α 參數。所得到的 PACT 的訓練和驗證誤差如圖 3 所示。可以看到，PACT 的誤差會收斂到使用常規 ReLU 的網絡的誤差。</p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/0a04c9bc33bd4c4299cf7e34ff8c801c><p class=pgc-img-caption>圖 3：使用 PACT 的 CIFAR10 ResNet20 的訓練誤差（a）和驗證誤差（b）。注意</p></div><p class=ql-align-justify><br></p><p class=ql-align-justify><strong>截略誤差與量化誤差的平衡</strong></p><p class=ql-align-justify>如前所見，參數化截略式 ReLU 函數的表現仍可比肩常規的不限界的 ReLU 函數。較大的 α 可得到較大的輸出範圍，但需要量化的比特也更多。因此，α 和量化位寬之間存在一個權衡。</p><p class=ql-align-justify>PACT 可以有效找到截略誤差和量化誤差之間的平衡點，其方法是基於輸出與目標的相近程度來調整輸出的範圍，即如果目標輸出有較大的幅度，PACT 會調整到更高的 α 值。截略誤差和量化誤差都會使輸出偏離目標，PACT 會在訓練期間增大或降低輸出範圍，以盡力最小化這兩個誤差。</p><p>在圖 4 中，(a) 當使用 2 位量化時，截略和量化相對於參數 α 的歸一化均方誤差（MSE）之間的權衡。(b) 展示了 PACT 為 QNN 找到截略和量化誤差的平衡的方式。在訓練中，CIFAR10 ResNet20 的截略式激活函數使用了 1 到 16 的不同截略水平。</p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b4d3aa59b19e410687fb7631651e8f90><p class=pgc-img-caption>圖 4</p></div><p class=ql-align-justify><em>(a) 截略和量化相對於 α 的歸一化 MSE。(b) 在一個 2 位量化的模型上，PACT 的最低驗證誤差和截略的驗證誤差在不同 α 上的比較。</em></p><p class=ql-align-justify>可以看到，當激活被量化為 2 位時，使用截略式激活函數的網絡的準確度會隨 α 增大而顯著下降。總結一下，PACT 的目標是選出最優的 α，從而同時最小化量化誤差和截略誤差，其結果是能實現相對較低的截略誤差和可能最低的量化誤差。</p><p class=ql-align-center><br></p><p class=ql-align-center><strong>統計感知式權重分箱（SAWB）</strong></p><p class=ql-align-justify>這篇論文的另一個貢獻是權重的量化。其主要思想是利用權重分佈的統計情況，即一階矩和二階矩。量化比例的確定方式是使權重的分散情況能在訓練過程中更好地得到。</p><p class=ql-align-justify>這種量化方法是均勻的和對稱的，因此也是對硬件友好的。此外，量化水平可由 α 確定。圖 5 給出了一個 2 位量化的示例，其量化水平參數為 α_w。權重的均值 E(|w|) 應該在 [-α_w, α_w] 範圍內。</p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1d6e455ac5f2460c97e185cb3d0441cb><p class=pgc-img-caption>圖 5：(a) 根據參數α_w 選出量化的 4 個點。(b) 根據 6 個不同的分佈執行最優比例的線</p></div><p class=ql-align-justify>圖 5(a) 中假設權重統計分佈近似一個高斯分佈，但通常觀察到的情況是：權重分佈的形狀會在訓練過程中隨反向傳播通路的集合而變化，從而變得不同於高斯分佈。這就使得整個基礎假設無效了，這又反過來會增大量化誤差。</p><p>這篇論文提出不僅要考慮 E(|w|)，而且還要考慮 E(w^2)。直觀而言，二階矩 E(w^2) 能體現分佈的整體形狀，而一階矩 E(|w|) 則會給出有代表性的值。研究者根據經驗推導了一個簡單公式，可為更多不同的分佈求取一個通用的最優比例 α_w*；該公式如下：</p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bedc9aa5b248404398c355c9b4973424><p class=pgc-img-caption></p></div><p class=ql-align-justify>係數 c1 和 c2 是根據在不同分佈（即高斯分佈、均勻分佈、拉普拉斯分佈等）上的線性迴歸而預先確定的。這可能看起來有些令人質疑，因此我會嘗試概括這裡的研究，並解釋每一個步驟。和這篇論文中所有研究一樣，其量化是 2 位式的（4 級）。</p><ol><li class=ql-align-justify>問題是這樣的：「我們如何設置權重值的 4 個量化層級，使得我們可將權重值映射到其最近的量化層級並且準確度的降低最小？」一種最簡單方法是設置量化層級時使得均值在其中心。（見圖 5(a)）</li><li class=ql-align-justify>不只是使用均值，我們還使用權重值的二階矩來提供有關分佈的整體形狀的見解。</li><li class=ql-align-justify>α_w* 是一階矩和二階矩的函數，因此作者研究了最優比例與一階矩和二階矩在不同相關分佈上的關係。圖 5(b) 給出了 α_w*/E(|w|)（X 軸）與 √ E(w2)/E(|w|)（Y 軸）在六種分佈（用不同顏色表示）上的相對情況。每個點都表示通過在 α_w 上掃描而得到的分佈的最優比例。</li><li class=ql-align-justify>執行簡單的線性迴歸以學習參數 c1 和 c2，本質上就是 6 個點（6 個不同分佈假設）的線性擬合，從而找到 c1 和 c2 的最優值。（見圖 5(b)）</li><li class=ql-align-justify>一旦確定了 c1 和 c2，在訓練新模型時就可直接使用 c1 和 c2 來計算 α_w*，無需再完整掃描整個 α_w。</li></ol><p class=ql-align-justify><br></p><p>表 1 給出了在使用 2 位權重量化時，CIFAR10 ResNet20 的第 11 層在不同 epoch 的最優平方誤差（SE）和 SAWB 估計的比例因子的平方誤差。</p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c2a682ff02eb4ef2901199102e923706><p class=pgc-img-caption>表 1：最優和 SAWB 的平方誤差（SE）</p></div><p class=ql-align-justify><br></p><p class=ql-align-justify>可以看到，SAWB 的誤差略高於最優 α_w* 的誤差。因此，這能激勵人們使用這個封閉解來計算 α_w，這會比在 α_w 上執行完整掃描快很多，而且準確度損失也很小。</p><p class=ql-align-center><strong>實驗</strong></p><p class=ql-align-justify><strong>CIFAR10 實驗</strong></p><p class=ql-align-justify>PACT 和 SAWB 都在 TensorFlow（Abadi et al., 2015）中用 Tensorpack (Zhou et al., 2016) 實現，並且論文中也研究了多種著名的 CNN：在 CIFAR10 數據集上的 ResNet20/32/44/56 (He et al., 2016b)。</p><p>該論文估計了帶有 (20,32,44,56) 層的 CIFAR10 ResNet 的激活和權重量化方案。表 3 總結了僅進行激活量化、僅進行權重量化、進行激活量化且權重量化後的準確度。</p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f016abebea004030bcb5ba83c117494f><p class=pgc-img-caption></p></div><p class=ql-align-justify><em>表 3：CIFAR10 ResNet20/32/44/56 僅進行激活量化、僅進行權重量化、進行激活量化且權重量化後的驗證誤差。小寫字母 s 和 p 分別表示 SAWB 和 PACT。這兩種量化方案僅會導致準確度少量下降。當同時部署了 PACT 和 SAWB 時，PACT-SAWB 能在各種變體的 CIFAR10 ResNet 上保證 3% 以內的準確度損失。</em></p><p class=ql-align-justify><strong>ImageNet 實驗</strong></p><p>研究者還在 ImageNet 數據集上執行了另一組測試 PACT 和 SAWB 的實驗。研究者也比較了 PACT-SAWB 與其它 2 位量化方案。結果見表 2。</p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3c80962a46f0425286b2e35b45f0d8a8><p class=pgc-img-caption>表 2：ImageNet：top-1 測試準確度（%）和準確度下降情況比較</p></div><p class=ql-align-justify>可以看到，對於在所有三個網絡（AlexNet、ResNet18 和 ResNet50）上的 2 位 QNN，PACT-SAWB 都實現了較高的準確度，準確度損失也都最低。注意，我們之前討論的 PACT-SAWB 和 PACT-SAWB-fpsc 之間有些許不同。PACT-SAWB-fpsc 是指帶有全精度快捷連接（full-precision shortcut connections）的 PACT-SAWB。</p><p class=ql-align-justify>不多說細節，快捷連接就是跳過一層或多層的路徑，對 ResNet 等一些神經網絡的架構而言很關鍵。有觀察發現，當使用快捷連接進行訓練時，2 位 QNN 的訓練誤差的下降速度更慢。因此，對於這些類型的網絡，快捷連接將保持不量化，也由此得名全精度快捷連接。這可被視為 PACT-SAWB 的一種變體，其中由於沒有量化，以計算成本為代價能實現準確度的提升。</p><p class=ql-align-center><strong>總結和討論</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>這篇論文介紹了兩種分別針對權重量化和激活量化的技術，並且進而能得到整體的量化神經網絡（QNN）。其中，激活量化技術是參數化截略式激活（PACT），這是一種在訓練期間使用 ReLU 函數的參數化截略來確定量化的輸出範圍的方案。權重量化方案名為統計感知式權重分箱（SAWB），可基於權重分佈的統計特性確定能最小化量化誤差的最優比例因子，無需執行窮舉搜索。</p><p class=ql-align-justify>在我看來，PACT 中的動態範圍自動調節是一個很有趣的概念，可以實現大規模量化神經網絡的穩健訓練。但是，我主要擔心的是僅使用二位量化能否有效得到大的輸出範圍。正如之前提到的那樣，量化是降低表示一個數值的位數的過程。這對降低計算成本而言很關鍵，尤其是對於當今的手持設備而言。</p><p class=ql-align-justify>就我所知，當前移動設備的最小位數是使用 8 位量化執行推理計算（我沒找到任何有關訓練的數據）。如果讀者感興趣，也許可以查找一下是否有任何產品在其機器學習框架中使用了低於 8 位量化的方案（不管是訓練還是推理）。</p><p class=ql-align-justify>至於權重量化方案，在我看來，為什麼通過不同分佈生成的「最佳」數據點的線性迴歸得到的係數 c1 和 c2 是一組優解，這一點沒能說清。我希望讀者也能思考一下這個問題，究竟為什麼我們不能把權重看作高斯分佈呢？</p><p class=ql-align-justify>我同意作者說的權重分佈的形狀會在訓練過程中隨反向傳播通路的集合而變化，從而變得不同於高斯分佈。但是，如果我們將每次反向傳播通路視為一個「樣本」並計算每個樣本的彙總統計信息，則可以預見這些彙總統計情況遵循正態分佈——即使你不知道你的樣本的分佈。這不過就是應用中心極限定理而已。</p><p class=ql-align-justify>因此，為什麼作者會無視這在準確度劣化方面所表現出的潛力呢？這個問題很有意思。我討論這個問題的另一個原因是想問讀者：預先確定 c1 和 c2 並在整個訓練階段使用它們的做法是否合理？使用固定參數似乎有礙「學習」的目的。</p><p class=ql-align-justify>總體而言，這兩項新提出的方案是很有趣的，也帶來了一些富有成效的討論以及一些不錯的結果。我認為這是一篇值得一讀的好論文，因為隨著手持式移動設備變得越來越重要，量化問題必然會成為一個至關重要的研究領域。</p><ul><li class=ql-align-justify><strong>論文2：Optimizing DNN Computation With Relaxed Graph Substitution</strong></li><li class=ql-align-justify>地址：https://www.sysml.cc/doc/2019/22.pdf</li></ul><p class=ql-align-justify><br></p><p class=ql-align-center><strong>引言</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>深度神經網絡（DNN）已在很多實際問題中取得了進展，比如圖像分類、機器翻譯和遊戲。這些進展往往伴隨著 DNN 越來越大、越來越深的代價，也由此帶來了更高的計算成本需求。因此，緩解越來越高的計算需求問題是一個亟需關注的領域。</p><p class=ql-align-justify>最簡單形式的 DNN 可被視為由（數學）算子組成的計算圖。TensorFlow、PyTorch 和 TVM 等已有的深度學習軟件全都會將計算表達為有狀態的數據流圖。這些圖會在訓練期間得到優化，並會在整個過程中變換。</p><p class=ql-align-justify>在每次迭代（或變換）時，新圖相比於迭代前的圖通常會有嚴格更好的運行時間性能。這種「嚴格更好」會得到深度學習框架的非常受限的搜索空間，也是高計算成本的一大原因。直觀地說，可以認為優化問題存在諸多約束。約束越多，算法得到解的時間就會越長。</p><p class=ql-align-justify>因此為了降低計算成本，這篇論文提出了一種寬鬆化的圖替代方法，可通過放鬆每個迭代約束的「嚴格更好」來實現複雜圖優化的探索。這能增大問題的可行空間，並能在每次迭代時更快找到解。此外，研究者還引入了回溯方法（backtracking），可搜索一組寬鬆化圖替代來尋找每次迭代的最優解（沒有嚴格更好的約束）。</p><p class=ql-align-center><strong>術語和符號</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>我們用 G 表示計算圖，I 表示輸入張量，O 表示輸出張量。三者之間的關係可寫為 O = G(I)。如果兩個圖 G 和 G' 在輸入一樣時得到的輸出也一樣，則認為這兩個圖等價，即 G(I) = G'(I)。尋找圖 G 的等價圖 G' 的過程在一個名為 MetaFlow 的系統中實現，其可被用於優化已有的深度學習框架的 DNN 計算圖，即 MetaFlow 是用於 DNN 的寬鬆化圖替代優化器。</p><p class=ql-align-justify>下一個術語是源圖（source graph）。源圖是指定義了可用作替代的可用子圖的結構的圖。</p><ul><li class=ql-align-justify>源圖中的每個節點都關聯了一種類型，並且僅可被映射到同種類型的算子，即卷積必須映射到卷積，僅有核、步幅、填充等參數的差異。</li><li class=ql-align-justify>源圖中的每條邊都描述了算子之間的數據依賴性。替代要求新圖的數據依賴性與源圖的一樣。</li></ul><p class=ql-align-justify><br></p><p>下一個術語是目標圖（target graph）。目標圖描述的是如何構建新的子圖來替換當前的子圖。圖 1 給出了一個示例，其中給出了一組等價的源圖和目標替換圖。</p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/52df5ef3616343df8d400883eb2140c2><p class=pgc-img-caption>圖 1：源圖和目標圖，兩者執行一樣的運算</p></div><p class=ql-align-justify>可以看到，目標圖是將兩個卷積「融合」為了一個運算，之後再分開。注意 op1 和 op2 有同樣的輸出 op1.out，通過拆分 conv3 的結果，我們可以得到 conv1.out 和 conv2.out，它們分別等價於 conv1 和 conv2 的輸出。</p><p class=ql-align-justify>在 conv1 和 conv2 上的約束確保它們僅可映射到同樣核大小、步幅和填充的卷積。此外，虛線被稱為外部邊（external edge）。外部邊只是表示算子的輸出可由外部算子讀取，並且必須被保留。</p><p class=ql-align-justify>所有這些可能看起來相當抽象，難以理解，但我認為其類似於以下直覺理解。如果 conv3 是一個 3×3 卷積，其核為</p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/79145317fce24dad87c05be34731e31c><p class=pgc-img-caption></p></div><p class=ql-align-justify>可分解為</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6de5eb6f00724426a1ed13ac425d59f0><p class=pgc-img-caption></p></div><p class=ql-align-justify>因此，我們可以不用 3×3 大小的核執行卷積（9 次乘法），而是使用兩個 1×3 核執行卷積（6 次乘法）。結果還是一樣，但從計算角度看，每次卷積的成本更低了。此外，通過將卷積分為兩個可以並行執行的更小卷積，執行整個卷積的速度也可能會更快。</p><p class=ql-align-justify>儘管圖 1 是用一個卷積替換兩個卷積，但這個示例是將一個卷積拆分為兩個。我認為這一思路兩個方向都有效，而且這正是圖替換思想背後的基本直覺。如果源圖和目標圖計算出的輸出在外部邊上是數學上等價的，則圖替代就是有效的。</p><p class=ql-align-justify>最後說明一點，寬鬆化的思路可按如下方式展示。考慮以下等價表達式以及從上面的表達式到下面的表達式所採取的步驟。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/3391b0a2946044b9864b54edbe618026><p class=pgc-img-caption></p></div><p class=ql-align-justify>最後的表達式僅有 3 個算子，說明實現了優化。但是，如果系統每次迭代時都有約束——新子圖必須嚴格優於當前子圖；則第二個表達式就不會被允許，因此也就無法得到最終的表達式。這就體現了放鬆約束條件（寬鬆化）的重要性。</p><p class=ql-align-center><strong>MetaFlow 搜索算法</strong></p><p class=ql-align-justify><br></p><p class=ql-align-justify>圖替代的難點在於在搜索空間中找到最優的圖。原因是搜索空間可能非常大，因此窮舉搜索空間中的所有圖是不可行的。MetaFlow 搜索算法的設計目的就是為替代在大搜索空間中有效找到經過優化的圖（但不必是最優的）。</p><p class=ql-align-justify><strong>成本函數</strong></p><p class=ql-align-justify>與任何優化一樣，首先必須定義一個成本。這個案例中的成本模型計算的是 FLOPs、內存使用量以及核啟動次數等指標。對於圖中的每個算子，都有針對這些指標的關聯成本，而且這些指標可以組合起來得到圖的總成本。</p><p class=ql-align-justify>一個關鍵的觀察是，大多數 DNN 算子都涉及到線性代數，如果給定一樣的參數，其在硬件上會有一致的且可預測的性能表現。這使得成本計算更容易，因為如果我們已經測量並保存了帶有特定參數的算子的執行時間，我們就可以為圖中其它部分的具有同樣參數的同樣算子使用該執行時間。</p><p class=ql-align-justify><strong>回溯搜索</strong></p><p class=ql-align-justify>這篇論文提出了一種回溯搜索方法，用於尋找一個成本模型下最優的計算圖。算法 1 給出了其偽代碼。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/9cba923d60fb40f29af93c7824fd6605><p class=pgc-img-caption></p></div><p class=ql-align-justify>可以看到，所有的潛在圖都排列成一個全局優先級隊列，並且按照它們的成本以遞增順序移出隊列。對於每個被移出隊列的圖 G，搜索算法都會通過在 G 上應用新的潛在圖替代來生成和排列新的圖。參數 α（算法 1 的第 13 行）被用於縮放當前最優 G 的損失。更近一步，我們可以直觀地理解三種情況中的 α：</p><ul><li class=ql-align-justify>(0&lt; α &lt;1)：這可以被認為是「鼓勵」算法保持當前的解。只有當新圖 G' 的成本有明顯改善時，算法才會更新當前最優的圖。這有希望實現算法加速。</li><li class=ql-align-justify>(α = 1)：這與實現一個貪婪算法一樣，且僅考慮嚴格降低成本的圖替代。</li><li class=ql-align-justify>(α > 1) ：這可被視為「擴展」搜索空間，因為條件更加寬鬆。這個算法有希望找到最終能得到更優解的中間解。</li></ul><p class=ql-align-justify><br></p><p class=ql-align-justify>回溯到思想出現在算法 1 的第 14 行。假設 α > 1，如果當前圖 G' 有更低的成本，這會被加回到隊列中。這是因為未來的迭代有可能將 G' 替換為另一個解 G''，而 G'' 有可能最終得到更差的解。因此，將 G' 留在隊列中能為算法提供追溯能力，從而防止當前路徑不佳的問題。</p><p class=ql-align-justify><strong>基於流的遞歸式圖拆分</strong></p><p class=ql-align-justify>很多當前最佳的 DNN 模型都太大了，難以直接使用回溯搜索來優化。但是，為了解決這個問題，可以使用一個有用的觀察。也即，圖替代可以在少量局部相連的算子上獨立地執行。</p><p class=ql-align-justify>因此，將計算圖拆分為更小的單個子圖並單個地檢查它們能讓問題變得更小，同時還能保證所得到的圖替代有效。基於這一見解，作者提出了一種基於流的圖拆分算法，以遞歸式地將計算圖分為更小的可使用回溯搜索的不相交子圖。</p><p class=ql-align-justify>當將一個圖拆分為兩個圖時，目標是這樣的兩個不相交的子圖會有最小數量的圖替代。對於每個算子，將其容量 Cap(o_i) 定義為至少映射到了算子 o_i 的一個內邊（in-edge）和一個外邊（out-edge）的圖替代的數量。</p><p class=ql-align-justify>直觀而言，每個算子的容量即是被替代的「潛力」。我們最終得到了一個由節點和邊構成的圖，每個邊都有一個描述其容量的權重。這個圖拆分問題就變成了最小頂點切割問題。</p><p class=ql-align-justify>從整數規劃（integer programming）的角度看，圖的最小頂點切割問題是尋找一個頂點的子集（稱為頂點切割），其中移除操作會將圖分成兩個組件。這通常使用標準的 max-flow 算法求解。</p><p class=ql-align-justify>可能大部分讀者並不瞭解 min-cut / max-flow 這些術語，但如果你有興趣，通常可在整數規劃教科書/課程中找到它們。我不會深入這些問題的細節，只說一下這些類型的問題的目標以及它們與當前問題的關聯。</p><p class=ql-align-justify>術語「切割（cut）」用於描述將圖分為兩個集合 (S, S_b) 的操作。切割的表示方法是兩個獨立的集合 (S, S_b)。因此，使用在計算圖上定義好的容量，目標是切割圖以使得被切掉的邊有最小的容量，進而將圖一分為二。</p><p class=ql-align-justify>總結一下，該算法首先是將圖分成單個的子圖，然後再運行回溯搜索算法來優化單個子圖，最後，MetaFlow 將優化後的子圖再組合到一起，組合得到整個計算圖。</p><p class=ql-align-center><strong>實驗結果</strong></p><p class=ql-align-justify>這篇論文使用了 MetaFlow 來測試比較一些著名的圖像分類神經網絡。這包括用於圖像分類的 Inception-v3 (Szegedy et al., 2016)、SqueezeNet (Iandola et al., 2016) 和 ResNet50 (He et al., 2016) CNN。這三種不同的神經網絡使用了不同的 DNN 模塊來實現優良的模型準確度，並展現出了不同的圖架構。</p><p class=ql-align-justify>除了圖像分類，研究者還評估了在文本分類和機器翻譯上的表現。RNNTC 和 NMT 是兩個來自（Lei et al., 2017）的分別用於文本分類和神經機器翻譯的模型。</p><p class=ql-align-justify>RNNTC 使用了一個嵌入層、一個隱藏大小為 1024 的循環層和一個 softmax 層。NMT 包括一個編碼器和一個解碼器，兩者都由一個嵌入層和兩個各有 1024 隱藏大小的循環層構成。表 1 提供了這些網絡的概況。</p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3dc53588bcfc450d9afc87658f3f284a><p class=pgc-img-caption>表 1：實驗所用的神經網絡</p></div><p class=ql-align-justify>這些實驗將 MetaFlow 加到了 TensorFlow、TensorFlow XLA 和 TensorRT 上。TensorFlow XLA 是一個用於 TensorFlow 圖的編譯器，可用於加速 TensorFlow ML 模型；TensorRT 是一個用於高性能深度學習推理的平臺，由英偉達開發。TensorRT 包含一個深度學習推理優化器和優化時間，能為深度學習推理應用提供低延遲和高吞吐量。</p><p class=ql-align-justify>在所有實驗中，所使用的成本模型都是最小化執行時間。此外，參數 α 設置為 1.05，作為回溯搜索算法的剪枝參數。</p><p class=ql-align-justify><strong>推理性能</strong></p><p class=ql-align-justify>第一個比較是端到端的性能。MetaFlow 會自動將優化過的計算圖變換成基準框架會接受的標準格式，因此可以與基準比較測試。圖 2 給出了比較的結果。藍線是沒使用 MetaFlow 的優化圖的三個基準框架得到的最佳結果，紅線是 MetaFlow 結果。</p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/abf9410543f246cc96d774b77a2a4f9a><p class=pgc-img-caption>圖 2：MetaFlow、TensorFlow、TensorFlow XLA 和 TensorRT</p></div><p class=ql-align-justify>可以看到，MetaFlow 速度更快，優於已有的深度學習推理引擎。每條紅線上的數字都是相對於最佳基準的相對提速。注意 MetaFlow 並不能唯一能執行圖替代的框架。事實上，所有已有的系統在執行計算圖之前都會根據自己的規則在內部執行圖變換。</p><p class=ql-align-justify>因此，性能表現也可能因為所執行的圖優化而不同。論文沒有明確說明 MetaFlow 是否在準確度表現上也優於其它框架，但我認為推理時間（和其它指標）的結果是在所有框架的準確度表現相對接近的前提下得到的。</p><p class=ql-align-justify><strong>額外的指標比較</strong></p><p class=ql-align-justify>除了端到端的推理時間外，作者還在成本模型中包含了不同的指標，其中包括內存訪問、啟動的核、FLOPs 數量和設備利用率。表 2 給出了 MetaFlow 和 TensorRT 在這些指標上的比較。</p><div class=pgc-img><img alt="SysML 2019論文解讀：推理優化" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/5fa65503f81b4eb9a8eead64bf5eef4c><p class=pgc-img-caption>表 2：MetaFlow 和 TensorRT 在不同指標上的性能比較</p></div><p class=ql-align-justify>相比於 TensorRT，MetaFlow 能夠減少整體的內存訪問量以及核啟動的數量。但是，在一個計算圖中，MetaFlow 實現的推理性能提升是以 FLOPs 增長為代價的。我相信 FLOPs 的增長可通過以下案例進行解釋。</p><p class=ql-align-justify>回想一下之前的章節，我們可用兩個 1×3 核替代一個 3×3 卷積核。反過來也一樣，如果我們有兩個 1×3 核，我們也許可將它們組合（擴大）成一個 3×3 核。這種更大的操作更加複雜，也可能會有更高的 FLOP 量，但可能會減少內存訪問和核啟動，我相信這能夠解釋表 2 中的數據。</p><p class=ql-align-justify><strong>其它一些說明</strong></p><p class=ql-align-justify>作者還在子圖性能等其它指標上進行了實驗。子圖性能指標是用於確定 MetaFlow 能否提升 DNN 中單個子圖的性能。實驗中測試比較了不同的設備，以確定給定一個輸入圖時，MetaFlow 能否在不同設備上發現不同的優化圖。此外，論文也檢查了搜索算法的性能。因為這些指標並不與推理優化直接相關，所以這篇解讀不會介紹這些內容。感興趣的讀者請參閱原論文了解全部細節。</p><p class=ql-align-center><strong>總結與結語</strong></p><p class=ql-align-justify>這篇論文介紹了寬鬆化的圖替代，能實現已有深度學習框架（使用的是貪婪方法）中未實現的複雜圖優化的探索。寬鬆化圖替代在數學形式上被構建成了一個基於成本的搜索算法，其涉及到一個或多個指標。回溯搜索的實現方式使得算法可在寬鬆化圖替代生成的搜索空間中自動找到經過優化的計算圖。</p><p class=ql-align-justify>最後，作者在 MetaFlow 中實現了該方法，這是一種用於 DNN 的寬鬆化圖替代優化器。通過與幾種標準 DNN 比較，研究表明 MetaFlow 相比於已有框架能在保證相近網絡準確度的同時改善運行時間性能——能實現高達 1.6 倍的提升。</p><p class=ql-align-justify>我選擇解讀這兩篇論文的原因是它們都涉及推理優化這一主題。這兩篇論文是從不同角度實現推理優化，而我認為這兩個角度都很重要。Choi et al. 的第一篇論文關注的是用量化來提升推理。這更多是一種面向硬件的方法。</p><p class=ql-align-justify>通過使用僅 2 位且準確度損失不太多的優良量化方法，網絡速度相比於 8 位的（當前移動設備常用的配置）能實現極大的提升。鑑於學術界的大多數研究都著眼於網絡準確度，所以這是一項很重要的研究。在現實世界應用中，硬件尺寸、功耗和速度等因素往往比準確度更重要，在探討機器學習時我們應始終記得這一點。</p><p class=ql-align-justify>第二篇論文來自 Jia et al.，是從算法角度求解推理優化問題。我很喜歡這個框架，因為其涉及到將該問題表達為一個整數規劃（min-cut / max-flow）問題。現如今，人們常常將機器學習作為單個領域來進行思考探索。</p><p class=ql-align-justify>但是，機器學習的核心是數學和優化，因此我認為在數學知識投入精力也是很重要的。擁有堅實的數學技能能幫助你更有效地分析機器學習問題，並也許能讓你有能力以可能有更優解決方案的不同數學方式表達你的問題。</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>文解</a></li><li><a>SysML</a></li><li><a>2019</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/21d2ba3e.html alt=2019年土木畢業生要知道的那些事 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/21d2ba3e.html title=2019年土木畢業生要知道的那些事>2019年土木畢業生要知道的那些事</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/171b22b4.html alt=2019年度《特種鑄造及有色合金》優秀論文結果公佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/137c70000b816835d80bf style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/171b22b4.html title=2019年度《特種鑄造及有色合金》優秀論文結果公佈>2019年度《特種鑄造及有色合金》優秀論文結果公佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/817e1015.html alt=2019年度《特種鑄造及有色合金》網絡評選結果出爐 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/66c26cbe459a4361b1d501e4bbac6c88 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/817e1015.html title=2019年度《特種鑄造及有色合金》網絡評選結果出爐>2019年度《特種鑄造及有色合金》網絡評選結果出爐</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/df18bcc1.html alt=金川集團公司2019年全國合金鑄造行業商洽會舉行 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/df18bcc1.html title=金川集團公司2019年全國合金鑄造行業商洽會舉行>金川集團公司2019年全國合金鑄造行業商洽會舉行</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3c5292d6.html alt="2019年度數字孿生城市之無人機 航攝應用及真三維建模技術培訓班" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3c5292d6.html title="2019年度數字孿生城市之無人機 航攝應用及真三維建模技術培訓班">2019年度數字孿生城市之無人機 航攝應用及真三維建模技術培訓班</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fcd2a59f.html alt=2019年最爆笑的120個名場面合集 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/dc6fcd05f35e499a9ef7a2d19ff9c66b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fcd2a59f.html title=2019年最爆笑的120個名場面合集>2019年最爆笑的120個名場面合集</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/715eacc1.html alt=《獅子王》2019：引入VR虛擬製作技術，顛覆動畫電影拍攝 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9a08cc2f25cb41c5be515de0d79879a9 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/715eacc1.html title=《獅子王》2019：引入VR虛擬製作技術，顛覆動畫電影拍攝>《獅子王》2019：引入VR虛擬製作技術，顛覆動畫電影拍攝</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e342e11c.html alt="2019掌上生活10元風暴怎麼玩攻略 快速獲取小招喵方法" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/Rk8ZSn39iz0Be8 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e342e11c.html title="2019掌上生活10元風暴怎麼玩攻略 快速獲取小招喵方法">2019掌上生活10元風暴怎麼玩攻略 快速獲取小招喵方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/67fef4c6.html alt=2019年高考，怎樣設置院校梯度才合理？衝、穩、保、墊，很關鍵！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/f8d17716-f3b8-4c6d-8eb7-f482334ad491 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/67fef4c6.html title=2019年高考，怎樣設置院校梯度才合理？衝、穩、保、墊，很關鍵！>2019年高考，怎樣設置院校梯度才合理？衝、穩、保、墊，很關鍵！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e18c5b5e.html alt=2019年正在流行的16個網頁設計趨勢 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RPgQdSQEjBTN6Z style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e18c5b5e.html title=2019年正在流行的16個網頁設計趨勢>2019年正在流行的16個網頁設計趨勢</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d72ed228.html alt=2019年中式烹調師（技師）安全生產模擬考試題庫及答案（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/fbcf9bc4d8224c309106305ad34adf9a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d72ed228.html title=2019年中式烹調師（技師）安全生產模擬考試題庫及答案（一）>2019年中式烹調師（技師）安全生產模擬考試題庫及答案（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d4044e1f.html alt=2019年中式烹調師（高級）安全生產模擬考試題庫及答案（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d4044e1f.html title=2019年中式烹調師（高級）安全生產模擬考試題庫及答案（一）>2019年中式烹調師（高級）安全生產模擬考試題庫及答案（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f855d5ae.html alt=2019年中式烹調師（高級）安全生產模擬考試題庫及答案（二） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9796cfd44f754f9ab22310d91088423e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f855d5ae.html title=2019年中式烹調師（高級）安全生產模擬考試題庫及答案（二）>2019年中式烹調師（高級）安全生產模擬考試題庫及答案（二）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9adee5f1.html alt=2019中式烹調師（技師）在線免費模擬考試系統及模擬題庫2 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d6a75905fb9346ccb6e4c8a304a79e63 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9adee5f1.html title=2019中式烹調師（技師）在線免費模擬考試系統及模擬題庫2>2019中式烹調師（技師）在線免費模擬考試系統及模擬題庫2</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/01e18262.html alt="2019級軍訓 | 蛻變中的成長，在西京學院我們一起經歷的軍訓" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/5e6d48b6fc5a49f480f9de7e22ab7958 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/01e18262.html title="2019級軍訓 | 蛻變中的成長，在西京學院我們一起經歷的軍訓">2019級軍訓 | 蛻變中的成長，在西京學院我們一起經歷的軍訓</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>