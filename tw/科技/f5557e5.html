<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer | 极客快訊</title><meta property="og:title" content="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p9.pstatp.com/large/pgc-image/49f9047ce18c4f829b25b952a3dedefa"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5557e5.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5557e5.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f5557e5.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5557e5.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5557e5.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f5557e5.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f5557e5.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5557e5.html><meta property="article:published_time" content="2020-10-29T21:05:26+08:00"><meta property="article:modified_time" content="2020-10-29T21:05:26+08:00"><meta name=Keywords content><meta name=description content="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/f5557e5.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p>機器之心原創</p><p style=text-align:justify><strong>編輯：陳萍</strong></p><blockquote class=pgc-blockquote-abstract><p style=text-align:justify>在進行 NLP 模型訓練前，請先選擇一個好的特徵提取器。</p></blockquote><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/49f9047ce18c4f829b25b952a3dedefa><p class=pgc-img-caption></p></div><p style=text-align:justify>在上一篇文章中我們介紹了自然語言處理的基礎問題——文本預處理的常用步驟。本文將進階講述特徵提取方面的相關算法。</p><p style=text-align:justify>如果用一句話總結目前 NLP 在特徵提取方面的發展趨勢，那就是「RNN 明日黃花，正如夕陽產業，慢慢淡出舞臺；CNN 老驥伏櫪，志在千里，如果繼續優化，還可能會大放異彩；Transformer 可謂如日中天，在特徵提取方面起著中流砥柱的作用」。至於將來，又會有什麼算法代替 Transformer，成為特徵提取界的新晉寵兒。我想一時半會兒可能不會那麼快，畢竟算法開發可是個很漫長的過程。</p><p style=text-align:justify>現在我們就來探究一下，在 NLP 特徵提取方面，算法是怎樣做到一浪更比一浪強的。</p><p style=text-align:justify><strong>RNN（循環神經網絡）</strong></p><p style=text-align:justify>RNN 與 CNN（卷積神經網絡）的關鍵區別在於，它是個序列的神經網絡，即前一時刻的輸入和後一時刻的輸入是有關係的。</p><p style=text-align:justify><strong>RNN 結構</strong></p><p style=text-align:justify>下圖是一個簡單的循環神經網絡，它由輸入層、隱藏層和輸出層組成。</p><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/f43c4f0ac99140ab85807054c81b53bf><p class=pgc-img-caption></p></div><p style=text-align:justify>RNN 的主要特點在於 w 帶藍色箭頭的部分。輸入層為 x，隱藏層為 s，輸出層為 o。U 是輸入層到隱藏層的權重，V 是隱藏層到輸出層的權重。隱藏層的值 s 不僅取決於當前時刻的輸入 x，還取決於上一時刻的輸入。權重矩陣 w 就是隱藏層上一次的值作為這一次的輸入的權重。下圖為具有多個輸入的循環神經網絡的示意圖：</p><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/9106f46159604d748a6e9c5422fa240b><p class=pgc-img-caption></p></div><p style=text-align:justify>從上圖可以看出，Sn 時刻的值和上一時刻 Sn-1 時刻的值相關。將 RNN 以時間序列展開，可得到下圖：</p><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7d5baca347334a268ba4acc125309e50><p class=pgc-img-caption></p></div><p style=text-align:justify>RNN 自引入 NLP 後，就被廣泛應用於多種任務。但是在實際應用中，RNN 常常出現各種各樣的問題。因為該算法是採用線性序列結構進行傳播的，這種方式給反向傳播優化帶來了困難，容易導致梯度消失以及梯度爆炸等問題。</p><p style=text-align:justify>此外，RNN 很難具備高效的並行計算能力，工程落地困難。因為 t 時刻的計算依賴 t-1 時刻的隱層計算結果，而 t-1 時刻的結果又依賴於 t-2 時刻的隱層計算結果……，因此用 RNN 進行自然語言處理時，只能逐詞進行，無法執行並行運算。</p><p style=text-align:justify>為了解決上述問題，後來研究人員引入了 LSTM 和 GRU，獲得了很好的效果。</p><p style=text-align:justify>而 CNN 和 Transformer 不存在這種序列依賴問題，作為後起之秀，它們在應用層面上彎道超車 RNN。</p><p style=text-align:justify><strong>CNN（卷積神經網絡）</strong></p><p style=text-align:justify>CNN 不僅在計算機視覺領域應用廣泛，在 NLP 領域也備受關注。</p><p style=text-align:justify>從數據結構上來看，CNN 輸入數據為文本序列，假設句子長度為 n，詞向量的維度為 d，那麼輸入就是一個 n×d 的矩陣。顯然，該矩陣的行列「像素」之間的相關性是不一樣的，矩陣的同一行為一個詞的向量表徵，而不同行表示不同的詞。</p><p style=text-align:justify>要讓卷積網絡能夠正常地「讀」出文本，我們就需要使用一維卷積。Kim 在 2014 年首次將 CNN 用於 NLP 任務中，其網絡結構如下圖所示：</p><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/e28de315353d4c95b2b34804b82213dc><p class=pgc-img-caption></p></div><p style=text-align:justify>從圖中可以看到，卷積核大小會對輸出值的長度有所影響。但經過池化之後，可映射到相同的特徵長度（如上圖中深紅色卷積核是 4 × 5，對於輸入值為 7 × 5 的輸入值，卷積之後的輸出值就是 4 × 1，最大池化之後就是 1 × 1；深綠色的卷積核是 3 × 5，卷積之後的輸出值是 5 × 1，最大池化之後就是 1 × 1）。之後將池化後的值進行組合，就得到 5 個池化後的特徵組合。</p><p style=text-align:justify>這樣做的優點是：無論輸入值的大小是否相同（由於文本的長度不同，輸入值不同），要用相同數量的卷積核進行卷積，經過池化後就會獲得相同長度的向量（向量的長度和卷積核的數量相等），這樣接下來就可以使用全連接層了（全連接層輸入值的向量大小必須一致）。</p><p style=text-align:justify><strong>特徵提取過程</strong></p><p style=text-align:justify>卷積的過程就是特徵提取的過程。一個完整的卷積神經網絡包括輸入層、卷積層、池化層、全連接層等，各層之間相互關聯。</p><p style=text-align:justify>而在卷積層中，卷積核具有非常重要的作用，CNN 捕獲到的特徵基本上都體現在卷積核裡了。卷積層包含多個卷積核，每個卷積核提取不同的特徵。以單個卷積核為例，假設卷積核的大小為 d×k，其中 k 是卷積核指定的窗口大小，d 是 Word Embedding 長度。卷積窗口依次通過每一個輸入，它捕獲到的是單詞的 k-gram 片段信息，這些 k-gram 片段就是 CNN 捕獲到的特徵，k 的大小決定了 CNN 能捕獲多遠距離的特徵。</p><p style=text-align:justify>卷積層之後是池化層，我們通常採用最大池化方法。如下圖所示，執行最大池化方法時，窗口的大小是 2×2，使用窗口滑動，在 2×2 的區域上保留數值最大的特徵，由此可以使用最大池化將一個 4×4 的特徵圖轉換為一個 2*2 的特徵圖。這裡我們可以看出，池化起到了降維的作用。</p><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6326b25233874a279d385e0fd01ec460><p class=pgc-img-caption></p></div><p style=text-align:justify>最後通過非線性變換，將輸入轉換為某個特定值。隨著卷積的不斷進行，產生特徵值，形成特徵向量。之後連接全連接層，得到最後的分類結果。</p><p style=text-align:justify>但 CNN 網絡也存在缺點，即網絡結構不深。它只有一個卷積層，無法捕獲長距離特徵，卷積層做到 2 至 3 層，就沒法繼續下去。再就是池化方法，文本特徵經過卷積層再經過池化層，會損失掉很多位置信息。而位置信息對文本處理來說非常重要，因此需要找到更好的文本特徵提取器。</p><p style=text-align:justify><strong>Transformer</strong></p><p style=text-align:justify>Transformer 是谷歌大腦 2017 年論文《Attentnion is all you need》中提出的 seq2seq 模型，現已獲得了大範圍擴展和應用。而應用的方式主要是：先預訓練語言模型，然後把預訓練模型適配給下游任務，以完成各種不同任務，如分類、生成、標記等。</p><p style=text-align:justify>Transformer 彌補了以上特徵提取器的缺點，主要表現在它改進了 RNN 訓練速度慢的致命問題，該算法採用 self-attention 機制實現快速並行；此外，Transformer 還可以加深網絡深度，不像 CNN 只能將模型添加到 2 至 3 層，這樣它能夠獲取更多全局信息，進而提升模型準確率。</p><p style=text-align:justify><strong>Transformer 結構</strong></p><p style=text-align:justify>首先，我們來看 Transformer 的整體結構，如下是用 Transformer 進行中英文翻譯的示例圖：</p><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2f25e69284624b7ca085594a8774dd5a><p class=pgc-img-caption></p></div><p style=text-align:justify>我們可以看到，Transformer 由兩大部分組成：編碼器（Encoder） 和解碼器（Decoder），每個模塊都包含 6 個 block。所有的編碼器在結構上都是相同的，負責把自然語言序列映射成為隱藏層，它們含有自然語言序列的表達式，但沒有共享參數。然後解碼器把隱藏層再映射為自然語言序列，從而解決各種 NLP 問題。</p><p style=text-align:justify>就上述示例而言，具體的實現可以分如下三步完成：</p><p style=text-align:justify><strong>第一步：</strong>獲取輸入單詞的詞向量 X，X 由詞嵌入和位置嵌入相加得到。其中詞嵌入可以採用 Word2Vec 或 Transformer 算法預訓練得到，也可以使用現有的 Tencent_AILab_ChineseEmbedding。</p><p style=text-align:justify>由於 Transformer 模型不具備循環神經網絡的迭代操作，所以我們需要向它提供每個詞的位置信息，以便識別語言中的順序關係，因此位置嵌入非常重要。模型的位置信息採用 sin 和 cos 的線性變換來表達：</p><p style=text-align:justify>PE（pos,2i）=sin(pos/100002i/d)</p><p style=text-align:justify>PE (pos,2i+1)=cos(pos/100002i/d)</p><p style=text-align:justify>其中，pos 表示單詞在句子中的位置，比如句子由 10 個詞組成，則 pos 表示 [0-9] 的任意位置，取值範圍是 [0,max sequence]；i 表示詞向量的維度，取值範圍 [0,embedding dimension]，例如某個詞向量是 256 維，則 i 的取值範圍是 [0-255]；d 表示 PE 的維度，也就是詞向量的維度，如上例中的 256 維；2i 表示偶數的維度（sin）；2i+1 表示奇數的維度（cos）。</p><p style=text-align:justify>以上 sin 和 cos 這組公式，分別對應 embedding dimension 維度一組奇數和偶數的序號的維度，例如，0,1 一組，2,3 一組。分別用上面的 sin 和 cos 函數做處理，從而產生不同的週期性變化，學到位置之間的依賴關係和自然語言的時序特性。</p><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/ba46ca18b19a442488da07a7b76425c3><p class=pgc-img-caption></p></div><p style=text-align:justify><strong>第二步：</strong>將第一步得到的向量矩陣傳入編碼器，編碼器包含 6 個 block ，輸出編碼後的信息矩陣 C。每一個編碼器輸出的 block 維度與輸入完全一致。</p><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/df66dd26849b46fcb54969b1dd6be879><p class=pgc-img-caption></p></div><p style=text-align:justify>第三步：將編碼器輸出的編碼信息矩陣 C 傳遞到解碼器中，解碼器會根據當前翻譯過的單詞 1~ i 依次翻譯下一個單詞 i+1，如下圖所示：</p><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/63f91c70869246a7a2ddf707d55e3c71><p class=pgc-img-caption></p></div><p style=text-align:justify><strong>Self-Attention 機制</strong></p><p style=text-align:justify>下圖展示了 Self-Attention 的結構。在計算時需要用到 Q(查詢), K(鍵值), V(值)。在實踐中，Self-Attention 接收的是輸入（單詞表示向量 x 組成的矩陣 X）或者上一個 Encoder block 的輸出。而 Q, K, V 正是基於 Self-Attention 的輸入進行線性變換得到的。</p><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/fd1471e42e8944359841af65d1827131><p class=pgc-img-caption></p></div><p style=text-align:justify>那麼 Self-Attention 如何實現呢？</p><p style=text-align:justify>讓我們來看一個具體的例子（以下示例圖片來自博客 https://jalammar.github.io/illustrated-transformer/）。</p><p style=text-align:justify>假如我們要翻譯一個詞組 Thinking Machines，其中 Thinking 的詞向量用 x1 表示，Machines 的詞向量用 x2 表示。</p><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/495c1e079f92430082ff91ab30344ae2><p class=pgc-img-caption></p></div><p style=text-align:justify>當處理 Thinking 這個詞時，需要計算它與所有詞的 attention Score，將當前詞作為 query，去和句子中所有詞的 key 匹配，得出相關度。用 q1 代表 Thinking 對應的 query vector，k1 及 k2 分別代表 Thinking 和 Machines 的 key vector。在計算 Thinking 的 attention score 時，需要先計算 q1 與 k1 及 k2 的點乘，同理在計算 Machines 的 attention score 時也需要計算 q_2 與 k1 及 k2 的點乘。如上圖得到了 q1 與 k1 及 k2 的點乘，然後進行尺度縮放與 softmax 歸一化，得到：</p><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/c21e661287874375a1482179e4307c96><p class=pgc-img-caption></p></div><p style=text-align:justify>顯然，當前單詞與其自身的 attention score 最大，其他單詞根據與當前單詞的重要程度得到相應的 score。然後再將這些 attention score 與 value vector 相乘，得到加權的向量。</p><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/56c48213f25a40ed9ffd28a2f03dc7bd><p class=pgc-img-caption></p></div><p style=text-align:justify>如果將輸入的所有向量合併為矩陣形式，則所有 query, key, value 向量也可以合併為矩陣形式表示。以上是一個單詞一個單詞的輸出，如果寫成矩陣形式就是 Q*K，經過矩陣歸一化直接得到權值。</p><div class=pgc-img><img alt="NLP三大特徵提取器全梳理：RNN vs CNN vs Transformer" onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/91a18f18d18a4db688d7f433183ddb19><p class=pgc-img-caption></p></div><p style=text-align:justify><strong>總結</strong></p><p style=text-align:justify>RNN 在並行計算方面存在嚴重缺陷，但其線性序列依賴性非常適合解決 NLP 任務，這也是為何 RNN 一引入 NLP 就很快流行起來的原因。但是也正是這一線性序列依賴特性，導致它在並行計算方面要想獲得質的飛躍，近乎是不可能完成的任務。而 CNN 網絡具有高並行計算能力，但結構不能做深，因而無法捕獲長距離特徵。現在看來，最好的特徵提取器是 Transformer，在並行計算能力和長距離特徵捕獲能力等方面都表現優異。</p><p style=text-align:justify>在之後的文章中，我們將繼續介紹 NLP 領域的相關內容，敬請期待。</p><p style=text-align:justify>參考鏈接：</p><p style=text-align:justify>http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</p><p style=text-align:justify>https://www.sohu.com/a/299824613_814235</p><p style=text-align:justify>https://zhuanlan.zhihu.com/p/30844905</p><p style=text-align:justify>https://baijiahao.baidu.com/s?id=1651219987457222196&wfr=spider&for=pc</p><p style=text-align:justify>https://www.cnblogs.com/sandwichnlp/p/11612596.html</p><p style=text-align:justify>https://zhuanlan.zhihu.com/p/54356280</p><p style=text-align:justify>https://www.jianshu.com/p/d2df894700bf</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>vs</a></li><li><a>NLP</a></li><li><a>三大特徵</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/bcfe9654.html alt="層高vs淨高  一字之差區別大" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/15236084099229f174eda83 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/bcfe9654.html title="層高vs淨高  一字之差區別大">層高vs淨高 一字之差區別大</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fd07a494.html alt=傳統彈簧vs彈性振動支撐，這些區別優勢你知道嗎？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/987773187efc4e049e033523176abc7f style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fd07a494.html title=傳統彈簧vs彈性振動支撐，這些區別優勢你知道嗎？>傳統彈簧vs彈性振動支撐，這些區別優勢你知道嗎？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/33d04d4d.html alt=2020年各大頂會NLP、ML優質論文分類整理分享 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/fb47112700b049aa88994c8949ec9403 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/33d04d4d.html title=2020年各大頂會NLP、ML優質論文分類整理分享>2020年各大頂會NLP、ML優質論文分類整理分享</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2c6eb5ec.html alt=輪胎式移動破vs履帶式移動破，多方面對比讓你看清差別！ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/af670edad6fc46659f5d4540593c2880 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2c6eb5ec.html title=輪胎式移動破vs履帶式移動破，多方面對比讓你看清差別！>輪胎式移動破vs履帶式移動破，多方面對比讓你看清差別！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/086ffc39.html alt="HDD vs SSD：存儲的未來將會如何？" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/6c3400031e91ca70f98d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/086ffc39.html title="HDD vs SSD：存儲的未來將會如何？">HDD vs SSD：存儲的未來將會如何？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/123fcc4d.html alt="北京方案 vs. PT-Cy方案-- 多中心巢式對照研究評估再障貧血最佳單倍型移植方案" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RaS0hgGESFatvk style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/123fcc4d.html title="北京方案 vs. PT-Cy方案-- 多中心巢式對照研究評估再障貧血最佳單倍型移植方案">北京方案 vs. PT-Cy方案-- 多中心巢式對照研究評估再障貧血最佳單倍型移植方案</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/34542ef0.html alt=五筆輸入法vs拼音輸入法，誰才是書記員心中的寵兒 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/221ba703-135f-4937-8a72-d62d4a4e77e7 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/34542ef0.html title=五筆輸入法vs拼音輸入法，誰才是書記員心中的寵兒>五筆輸入法vs拼音輸入法，誰才是書記員心中的寵兒</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7a11c4af.html alt=圖解BERT（NLP中的遷移學習） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/b29b82aef73748bd9fc0a049212fba09 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7a11c4af.html title=圖解BERT（NLP中的遷移學習）>圖解BERT（NLP中的遷移學習）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/30177cc2.html alt=NLP中的遷移學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1534906754727755a6a6964 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/30177cc2.html title=NLP中的遷移學習>NLP中的遷移學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/45cbe488.html alt=NLP基礎-通用句子向量漫談 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/db54188d47524055b7a45d90aed407ac style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/45cbe488.html title=NLP基礎-通用句子向量漫談>NLP基礎-通用句子向量漫談</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/760de6b8.html alt=NLP領域中的遷移學習現狀 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/RayMFst8jYuQgG style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/760de6b8.html title=NLP領域中的遷移學習現狀>NLP領域中的遷移學習現狀</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f34d1055.html alt="ACL 2019 | 南大NLP，知識庫問答中的表示映射學習" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RdGewJX7BKaFVS style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f34d1055.html title="ACL 2019 | 南大NLP，知識庫問答中的表示映射學習">ACL 2019 | 南大NLP，知識庫問答中的表示映射學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a73c5785.html alt="「NLP 必備知識點」自然語言理解 NLU（概念+應用+3種實現方式）" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/8cc2bb8a2c0f4d529fd624f5df2fc70c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a73c5785.html title="「NLP 必備知識點」自然語言理解 NLU（概念+應用+3種實現方式）">「NLP 必備知識點」自然語言理解 NLU（概念+應用+3種實現方式）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d3668904.html alt=自然語言處理（NLP）常用庫整理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/235e94cda81a4858a3000bb62b4f970d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d3668904.html title=自然語言處理（NLP）常用庫整理>自然語言處理（NLP）常用庫整理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ff8d4ec0.html alt=近53種NLP中文語料庫，你一定用得到 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4baf47697c7f46c3bd15d310e7b9005c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ff8d4ec0.html title=近53種NLP中文語料庫，你一定用得到>近53種NLP中文語料庫，你一定用得到</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>