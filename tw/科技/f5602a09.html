<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>1美元訓練BERT，教你如何薅谷歌TPU羊毛｜附Colab代碼 | 极客快訊</title><meta property="og:title" content="1美元訓練BERT，教你如何薅谷歌TPU羊毛｜附Colab代碼 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/7e7fa9a848e043528bbbdc2b5acd77ca"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5602a09.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5602a09.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f5602a09.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5602a09.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5602a09.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f5602a09.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f5602a09.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f5602a09.html><meta property="article:published_time" content="2020-10-29T21:12:41+08:00"><meta property="article:modified_time" content="2020-10-29T21:12:41+08:00"><meta name=Keywords content><meta name=description content="1美元訓練BERT，教你如何薅谷歌TPU羊毛｜附Colab代碼"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/f5602a09.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>1美元訓練BERT，教你如何薅谷歌TPU羊毛｜附Colab代碼</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><blockquote><p>曉查 發自 凹非寺</p><p>量子位 出品 | 公眾號 QbitAI</p></blockquote><p class=ql-align-center><br></p><div class=pgc-img><img alt=1美元訓練BERT，教你如何薅谷歌TPU羊毛｜附Colab代碼 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/7e7fa9a848e043528bbbdc2b5acd77ca><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>BERT是谷歌去年推出的NLP模型，一經推出就在各項測試中碾壓競爭對手，而且BERT是開源的。只可惜訓練BERT的價格實在太高，讓人望而卻步。</p><p>之前需要用64個TPU訓練4天才能完成，後來谷歌用並行計算優化了到只需一個多小時，但是需要的TPU數量陡增，達到了驚人的1024個。</p><p>那麼總共要多少錢呢？谷歌雲TPU的使用價格是每個每小時6.5美元，訓練完成訓練完整個模型需要近4萬美元，簡直就是天價。</p><p>現在，有個羊毛告訴你，在培養基上有人找到了薅谷歌羊毛的辦法，只需1美元就能訓練BERT，模型還能留存在你的谷歌雲盤中，留作以後使用。</p><p><strong>準備工作</strong></p><p>為了薅谷歌的羊毛，您需要一個Google雲存儲（Google Cloud Storage）空間。按照Google雲TPU快速入門指南，創建Google雲平臺（Google Cloud Platform）帳戶和Google雲存儲賬戶。新的谷歌雲平臺用戶可獲得300美元的免費贈送金額。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=1美元訓練BERT，教你如何薅谷歌TPU羊毛｜附Colab代碼 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ad9854f2e94c43b691b97771651cbe19><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>在TPUv2上預訓練BERT-Base模型大約需要54小時.Google Colab並非設計用於執行長時間運行的作業，它會每8小時左右中斷一次訓練過程。對於不間斷的訓練，請考慮使用付費的不間斷使用TPUv2的方法。</p><p>也就是說，使用Colab TPU，你可以在以1美元的價格在谷雲盤上存儲模型和數據，以幾乎可忽略成本從頭開始預訓練BERT模型。</p><p>以下是整個過程的代碼下面的代碼，可以在Colab Jupyter環境中運行。</p><h1><strong>設置訓練環境</strong></h1><p>首先，安裝訓練模型所需的包.Jupyter允許使用直接從筆記本執行的bash命令 '！'：</p><pre>!pip install sentencepiece!git clone https://github.com/google-research/bert</pre><p>導入包並在谷歌雲中授權：</p><pre>import osimport sysimport jsonimport nltkimport randomimport loggingimport tensorflow as tfimport sentencepiece as spmfrom glob import globfrom google.colab import auth, drivefrom tensorflow.keras.utils import Progbarsys.path.append("bert")from bert import modeling, optimization, tokenizationfrom bert.run_pretraining import input_fn_builder, model_fn_builderauth.authenticate_user()# configure logginglog = logging.getLogger('tensorflow')log.setLevel(logging.INFO)# create formatter and add it to the handlersformatter = logging.Formatter('%(asctime)s : %(message)s')sh = logging.StreamHandler()sh.setLevel(logging.INFO)sh.setFormatter(formatter)log.handlers = [sh]if 'COLAB_TPU_ADDR' in os.environ: log.info("Using TPU runtime") USE_TPU = True TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR'] with tf.Session(TPU_ADDRESS) as session: log.info('TPU address is ' + TPU_ADDRESS) # Upload credentials to TPU. with open('/content/adc.json', 'r') as f: auth_info = json.load(f) tf.contrib.cloud.configure_gcs(session, credentials=auth_info)else: log.warning('Not connected to TPU runtime') USE_TPU = False</pre><h1><strong>下載原始文本數據</strong></h1><p>接下來從網絡上獲取文本數據語料庫。在本次實驗中，我們使用OpenSubtitles數據集，該數據集包括65種語言。</p><p>與更常用的文本數據集（如維基百科）不同，它不需要任何複雜的預處理，提供預格式化，一行一個句子。</p><pre>AVAILABLE = {'af','ar','bg','bn','br','bs','ca','cs', 'da','de','el','en','eo','es','et','eu', 'fa','fi','fr','gl','he','hi','hr','hu', 'hy','id','is','it','ja','ka','kk','ko', 'lt','lv','mk','ml','ms','nl','no','pl', 'pt','pt_br','ro','ru','si','sk','sl','sq', 'sr','sv','ta','te','th','tl','tr','uk', 'ur','vi','ze_en','ze_zh','zh','zh_cn', 'zh_en','zh_tw','zh_zh'}LANG_CODE = "en" #@param {type:"string"}assert LANG_CODE in AVAILABLE, "Invalid language code selected"!wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/OpenSubtitles.raw.'$LANG_CODE'.gz -O dataset.txt.gz!gzip -d dataset.txt.gz!tail dataset.txt</pre><p>你可以通過設置代碼隨意選擇你需要的語言。出於演示目的，代碼只默認使用整個語料庫的一小部分。在實際訓練模型時，請務必取消選中DEMO_MODE複選框，使用大100倍的數據集。</p><p>當然，100M數據足以訓練出相當不錯的BERT基礎模型。</p><pre>DEMO_MODE = True #@param {type:"boolean"}if DEMO_MODE: CORPUS_SIZE = 1000000else: CORPUS_SIZE = 100000000 #@param {type: "integer"}!(head -n $CORPUS_SIZE dataset.txt) &gt; subdataset.txt!mv subdataset.txt dataset.txt</pre><h1><strong>預處理文本數據</strong></h1><p>我們下載的原始文本數據包含標點符號，大寫字母和非UTF符號，我們將在繼續下一步之前將其刪除。在推理期間，我們將對新數據應用相同的過程。</p><p>如果你需要不同的預處理方式（例如在推理期間預期會出現大寫字母或標點符號），請修改以下代碼以滿足你的需求。</p><pre>regex_tokenizer = nltk.RegexpTokenizer("\w+")def normalize_text(text): # lowercase text text = str(text).lower() # remove non-UTF text = text.encode("utf-8", "ignore").decode() # remove punktuation symbols text = " ".join(regex_tokenizer.tokenize(text)) return textdef count_lines(filename): count = 0 with open(filename) as fi: for line in fi: count += 1 return count</pre><p>現在讓我們預處理整個數據集：</p><pre>RAW_DATA_FPATH = "dataset.txt" #@param {type: "string"}PRC_DATA_FPATH = "proc_dataset.txt" #@param {type: "string"}# apply normalization to the dataset# this will take a minute or twototal_lines = count_lines(RAW_DATA_FPATH)bar = Progbar(total_lines)with open(RAW_DATA_FPATH,encoding="utf-8") as fi: with open(PRC_DATA_FPATH, "w",encoding="utf-8") as fo: for l in fi: fo.write(normalize_text(l)+"\n") bar.add(1)</pre><h1><strong>構建詞彙表</strong></h1><p>下一步，我們將訓練模型學習一個新的詞彙表，用於表示我們的數據集。</p><p>BERT文件使用WordPiece分詞器，在開源中不可用。我們將在單字模式下使用SentencePiece分詞器。雖然它與BERT不直接兼容，但是通過一個小的處理方法，可以使它工作。</p><p>SentencePiece需要相當多的運行內存，因此在Colab中的運行完整數據集會導致內核崩潰。</p><p>為避免這種情況，我們將隨機對數據集的一小部分進行子採樣，構建詞彙表。另一個選擇是使用更大內存的機器來執行此步驟。</p><p>此外，SentencePiece默認情況下將BOS和EOS控制符號添加到詞彙表中。我們通過將其索引設置為-1來禁用它們。</p><p>VOC_SIZE的典型值介於32000和128000之間。如果想要更新詞彙表，並在預訓練階段結束後對模型進行微調，我們會保留NUM_PLACEHOLDERS個令牌。</p><pre>MODEL_PREFIX = "tokenizer" #@param {type: "string"}VOC_SIZE = 32000 #@param {type:"integer"}SUBSAMPLE_SIZE = 12800000 #@param {type:"integer"}NUM_PLACEHOLDERS = 256 #@param {type:"integer"}SPM_COMMAND = ('--input={} --model_prefix={} ' '--vocab_size={} --input_sentence_size={} ' '--shuffle_input_sentence=true '  '--bos_id=-1 --eos_id=-1').format( PRC_DATA_FPATH, MODEL_PREFIX,  VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)spm.SentencePieceTrainer.Train(SPM_COMMAND)</pre><p>現在，讓我們看看如何讓SentencePiece在BERT模型上工作。</p><p>下面是使用來自官方的預訓練英語BERT基礎模型的WordPiece詞彙表標記的語句。</p><pre>&gt;&gt;&gt; wordpiece.tokenize("Colorless geothermal substations are generating furiously")['color', '##less', 'geo', '##thermal', 'sub', '##station', '##s', 'are', 'generating', 'furiously']</pre><p>WordPiece標記器在“##”的單詞中間預置了出現的子字。在單詞開頭出現的子詞不變。如果子詞出現在單詞的開頭和中間，則兩個版本（帶和不帶” ##'）都會添加到詞彙表中。</p><p>SentencePiece創建了兩個文件：tokenizer.model和tokenizer.vocab讓我們來看看它學到的詞彙：</p><pre>def read_sentencepiece_vocab(filepath): voc = [] with open(filepath, encoding='utf-8') as fi: for line in fi: voc.append(line.split("\t")[0]) # skip the first &lt;unk&gt; token voc = voc[1:] return vocsnt_vocab = read_sentencepiece_vocab("{}.vocab".format(MODEL_PREFIX))print("Learnt vocab size: {}".format(len(snt_vocab)))print("Sample tokens: {}".format(random.sample(snt_vocab, 10)))</pre><p>運行結果：</p><pre>Learnt vocab size: 31743 Sample tokens: ['▁cafe', '▁slippery', 'xious', '▁resonate', '▁terrier', '▁feat', '▁frequencies', 'ainty', '▁punning', 'modern']</pre><p>SentencePiece與WordPiece的運行結果完全相反從文檔中可以看出：SentencePiece首先使用元符號“_”將空格轉義為空格，如下所示：</p><ul><li><br></li></ul><pre>Hello_World。</pre><p>然後文本被分段為小塊：</p><ul><li><br></li></ul><pre>[Hello] [_Wor] [ld] [.]</pre><p>在空格之後出現的子詞（也是大多數詞開頭的子詞）前面加上“_”，而其他子詞不變。這排除了僅出現在句子開頭而不是其他地方的子詞。然而，這些案件應該非常罕見。</p><p>因此，為了獲得類似於WordPiece的詞彙表，我們需要執行一個簡單的轉換，從包含它的標記中刪除“_”，並將“##”添加到不包含它的標記中。</p><p>我們還添加了一些BERT架構所需的特殊控制符號。按照慣例，我們把它們放在詞彙的開頭。</p><p>另外，我們在詞彙表中添加了一些佔位符標記。</p><p>如果你希望使用新的用於特定任務的令牌來更新預先訓練的模型，那麼這些方法是很有用的。</p><p>在這種情況下，佔位符標記被替換為新的令牌，重新生成預訓練數據，並且對新數據進行微調。</p><pre>def parse_sentencepiece_token(token): if token.startswith("▁"): return token[1:] else: return "##" + tokenbert_vocab = list(map(parse_sentencepiece_token, snt_vocab))ctrl_symbols = ["[PAD]","[UNK]","[CLS]","[SEP]","[MASK]"]bert_vocab = ctrl_symbols + bert_vocabbert_vocab += ["[UNUSED_{}]".format(i) for i in range(VOC_SIZE - len(bert_vocab))]print(len(bert_vocab))</pre><p>最後，我們將獲得的詞彙表寫入文件。</p><pre>VOC_FNAME = "vocab.txt" #@param {type:"string"}with open(VOC_FNAME, "w") as fo: for token in bert_vocab: fo.write(token+"\n")</pre><p>現在，讓我們看看新詞彙在實踐中是如何運作的：</p><pre>&gt;&gt;&gt; testcase = "Colorless geothermal substations are generating furiously"&gt;&gt;&gt; bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)&gt;&gt;&gt; bert_tokenizer.tokenize(testcase)['color',  '##less',  'geo',  '##ther',  '##mal',  'sub',  '##station',  '##s',  'are',  'generat',  '##ing',  'furious',  '##ly']</pre><h1><strong>創建分片預訓練數據（生成預訓練數據）</strong></h1><p>通過手頭的詞彙表，我們可以為BERT模型生成預訓練數據。</p><p>由於我們的數據集可能非常大，我們將其拆分為碎片：</p><pre>mkdir ./shardssplit -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_</pre><p>現在，對於每個部分，我們需要從BERT倉庫調用create_pretraining_data.py腳本，需要使用xargs的命令。</p><p>在開始生成之前，我們需要設置一些參數傳遞給腳本。你可以從自述文件中找到有關它們含義的更多信息。</p><pre>MAX_SEQ_LENGTH = 128 #@param {type:"integer"}MASKED_LM_PROB = 0.15 #@paramMAX_PREDICTIONS = 20 #@param {type:"integer"}DO_LOWER_CASE = True #@param {type:"boolean"}PRETRAINING_DIR = "pretraining_data" #@param {type:"string"}# controls how many parallel processes xargs can createPROCESSES = 2 #@param {type:"integer"}</pre><p>運行此操作可能需要相當長的時間，具體取決於數據集的大小。</p><pre>XARGS_CMD = ("ls ./shards/ | " "xargs -n 1 -P {} -I{} " "python3 bert/create_pretraining_data.py " "--input_file=./shards/{} " "--output_file={}/{}.tfrecord " "--vocab_file={} " "--do_lower_case={} " "--max_predictions_per_seq={} " "--max_seq_length={} " "--masked_lm_prob={} " "--random_seed=34 " "--dupe_factor=5")XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}',  VOC_FNAME, DO_LOWER_CASE,  MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)tf.gfile.MkDir(PRETRAINING_DIR)!$XARGS_CMD</pre><h1><strong>為數據和模型設置GCS存儲，將數據和模型存儲到雲端</strong></h1><p>為了保留來之不易的訓練模型，我們會將其保留在谷歌雲存儲中。</p><p>在谷歌雲存儲中創建兩個目錄，一個用於數據，一個用於模型。在模型目錄中，我們將放置模型詞彙表和配置文件。</p><p>在繼續操作之前，請配置BUCKET_NAME變量，否則將無法訓練模型。</p><pre>BUCKET_NAME = "bert_resourses" #@param {type:"string"}MODEL_DIR = "bert_model" #@param {type:"string"}tf.gfile.MkDir(MODEL_DIR)if not BUCKET_NAME: log.warning("WARNING: BUCKET_NAME is not set. " "You will not be able to train the model.")</pre><p>下面是BERT基的超參數配置示例：</p><pre># use this for BERT-basebert_base_config = { "attention_probs_dropout_prob": 0.1,  "directionality": "bidi",  "hidden_act": "gelu",  "hidden_dropout_prob": 0.1,  "hidden_size": 768,  "initializer_range": 0.02,  "intermediate_size": 3072,  "max_position_embeddings": 512,  "num_attention_heads": 12,  "num_hidden_layers": 12,  "pooler_fc_size": 768,  "pooler_num_attention_heads": 12,  "pooler_num_fc_layers": 3,  "pooler_size_per_head": 128,  "pooler_type": "first_token_transform",  "type_vocab_size": 2,  "vocab_size": VOC_SIZE}with open("{}/bert_config.json".format(MODEL_DIR), "w") as fo: json.dump(bert_base_config, fo, indent=2)with open("{}/{}".format(MODEL_DIR, VOC_FNAME), "w") as fo: for token in bert_vocab: fo.write(token+"\n")</pre><p>現在，我們已準備好將模型和數據存儲到谷歌雲當中：</p><pre>if BUCKET_NAME: !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME</pre><h1><strong>在雲TPU上訓練模型</strong></h1><p>注意，之前步驟中的某些參數在此處不用改變。請確保在整個實驗中設置的參數完全相同。</p><pre>BUCKET_NAME = "bert_resourses" #@param {type:"string"}MODEL_DIR = "bert_model" #@param {type:"string"}PRETRAINING_DIR = "pretraining_data" #@param {type:"string"}VOC_FNAME = "vocab.txt" #@param {type:"string"}# Input data pipeline configTRAIN_BATCH_SIZE = 128 #@param {type:"integer"}MAX_PREDICTIONS = 20 #@param {type:"integer"}MAX_SEQ_LENGTH = 128 #@param {type:"integer"}MASKED_LM_PROB = 0.15 #@param# Training procedure configEVAL_BATCH_SIZE = 64LEARNING_RATE = 2e-5TRAIN_STEPS = 1000000 #@param {type:"integer"}SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:"integer"}NUM_TPU_CORES = 8if BUCKET_NAME: BUCKET_PATH = "gs://{}".format(BUCKET_NAME)else: BUCKET_PATH = "."BERT_GCS_DIR = "{}/{}".format(BUCKET_PATH, MODEL_DIR)DATA_GCS_DIR = "{}/{}".format(BUCKET_PATH, PRETRAINING_DIR)VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)CONFIG_FILE = os.path.join(BERT_GCS_DIR, "bert_config.json")INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))log.info("Using checkpoint: {}".format(INIT_CHECKPOINT))log.info("Using {} data shards".format(len(input_files)))</pre><p>準備訓練運行配置，建立評估器和輸入函數，啟動BERT！</p><pre>model_fn = model_fn_builder( bert_config=bert_config, init_checkpoint=INIT_CHECKPOINT, learning_rate=LEARNING_RATE, num_train_steps=TRAIN_STEPS, num_warmup_steps=10, use_tpu=USE_TPU, use_one_hot_embeddings=True)tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)run_config = tf.contrib.tpu.RunConfig( cluster=tpu_cluster_resolver, model_dir=BERT_GCS_DIR, save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS, tpu_config=tf.contrib.tpu.TPUConfig( iterations_per_loop=SAVE_CHECKPOINTS_STEPS, num_shards=NUM_TPU_CORES, per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))estimator = tf.contrib.tpu.TPUEstimator( use_tpu=USE_TPU, model_fn=model_fn, config=run_config, train_batch_size=TRAIN_BATCH_SIZE, eval_batch_size=EVAL_BATCH_SIZE)train_input_fn = input_fn_builder( input_files=input_files, max_seq_length=MAX_SEQ_LENGTH, max_predictions_per_seq=MAX_PREDICTIONS, is_training=True)</pre><p>執行！</p><pre>estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)</pre><p>最後，使用默認參數訓練模型需要100萬步，約54小時的運行時間。如果內核由於某種原因重新啟動，可以從斷點處繼續訓練。</p><p>以上就是是在雲TPU上從頭開始預訓練BERT的指南。</p><h1><strong>下一步</strong></h1><p>好的，我們已經訓練好了模型，接下來可以做什麼？</p><p>如圖1所示，使用預訓練的模型作為通用的自然語言理解模塊;</p><p>2，針對某些特定的分類任務微調模型;</p><p>3，使用BERT作為構建塊，去創建另一個深度學習模型。</p><h1><strong>傳送門</strong></h1><p>原文地址：</p><p>https ：//towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379</p><p>Colab代碼：</p><p>https ：//colab.research.google.com/drive/1nVn6AFpQSzXBt8_ywfx6XR8ZfQXlKGAz</p><p>— 完 —</p><p>誠摯招聘</p><p>量子位正在招募編輯/記者，工作地點在北京中關村。期待有才氣、有熱情的同學加入我們！相關細節，請在量子位公眾號(QbitAI)對話界面，回覆“招聘”兩個字。</p><p>量子位 QbitAI · 頭條號簽約作者</p><p>վ'ᴗ' ի 追蹤AI技術和產品新動態</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>訓練</a></li><li><a>BERT</a></li><li><a>TPU</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/6ff19cd8.html alt=中小學作文訓練之描寫專題（九）場面描寫 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/75eab302c96d43e8a680908253f96bcc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6ff19cd8.html title=中小學作文訓練之描寫專題（九）場面描寫>中小學作文訓練之描寫專題（九）場面描寫</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0926be43.html alt="筋膜放鬆練習 - 按摩滾輪訓練方法" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/d8d39047a8ef48fdbc8ca96ec24e1957 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0926be43.html title="筋膜放鬆練習 - 按摩滾輪訓練方法">筋膜放鬆練習 - 按摩滾輪訓練方法</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/12065250.html alt=TPU/EVA無滷阻燃電纜料 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/86add7f3948e464f89663e7caefdd334 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/12065250.html title=TPU/EVA無滷阻燃電纜料>TPU/EVA無滷阻燃電纜料</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/0ac4e6b0.html alt="“訓練中心”基本操作 新手入門可快速上手！" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/986b0004185ef13341b4 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/0ac4e6b0.html title="“訓練中心”基本操作 新手入門可快速上手！">“訓練中心”基本操作 新手入門可快速上手！</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/91e57ca2.html alt=關於TPU注塑工藝……你知道多少 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/dfic-imagehandler/d9a14480-8fd7-424f-8783-b3612a585088 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/91e57ca2.html title=關於TPU注塑工藝……你知道多少>關於TPU注塑工藝……你知道多少</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/df3191d7.html alt="熱身訓練，準備開始 ，一招一式喚醒身體" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/8341520cbad340d9aabf407c2800fd62 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/df3191d7.html title="熱身訓練，準備開始 ，一招一式喚醒身體">熱身訓練，準備開始 ，一招一式喚醒身體</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/29d7e5af.html alt=關於TPU注塑工藝……你知道多少？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7b4645e056d444a4bbc2d2796028cd61 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/29d7e5af.html title=關於TPU注塑工藝……你知道多少？>關於TPU注塑工藝……你知道多少？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/8e59046e.html alt=腦外傷病人的日常生活能力訓練：在床上怎麼向左右移動？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/4aed0000b4ee529b04aa style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/8e59046e.html title=腦外傷病人的日常生活能力訓練：在床上怎麼向左右移動？>腦外傷病人的日常生活能力訓練：在床上怎麼向左右移動？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dfb3b071.html alt="體能訓練高效燃脂計劃：9個動作快速提升燃脂質量 突破減脂瓶頸期" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/615a00057863e9e221d1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dfb3b071.html title="體能訓練高效燃脂計劃：9個動作快速提升燃脂質量 突破減脂瓶頸期">體能訓練高效燃脂計劃：9個動作快速提升燃脂質量 突破減脂瓶頸期</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7a11c4af.html alt=圖解BERT（NLP中的遷移學習） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/b29b82aef73748bd9fc0a049212fba09 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7a11c4af.html title=圖解BERT（NLP中的遷移學習）>圖解BERT（NLP中的遷移學習）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/9dab4f31.html alt=從三個維度訓練口才，就能快速提高你的語言表達能力 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/4934eb76-719f-4562-86e9-2377677bf1d5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/9dab4f31.html title=從三個維度訓練口才，就能快速提高你的語言表達能力>從三個維度訓練口才，就能快速提高你的語言表達能力</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3765806c.html alt=語言落後的小朋友，訓練3周後有哪些變化？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/11cb61802ed147b89b8ae2d1ec4f39e2 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3765806c.html title=語言落後的小朋友，訓練3周後有哪些變化？>語言落後的小朋友，訓練3周後有哪些變化？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/09fa199a.html alt=前交叉韌帶重建術後康復訓練 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/R6QlwLm392mk0I style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/09fa199a.html title=前交叉韌帶重建術後康復訓練>前交叉韌帶重建術後康復訓練</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7a405265.html alt=健身力量訓練能幫到我們減肥嗎？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1530020814773cb5fbf0aba style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7a405265.html title=健身力量訓練能幫到我們減肥嗎？>健身力量訓練能幫到我們減肥嗎？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f1f18f63.html alt=強化體能訓練，提升長跑能力 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/15277794267744db16eb29c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f1f18f63.html title=強化體能訓練，提升長跑能力>強化體能訓練，提升長跑能力</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>