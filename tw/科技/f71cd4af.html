<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習入門第2章：SVM（支持向量機）—編碼 | 极客快訊</title><meta property="og:title" content="機器學習入門第2章：SVM（支持向量機）—編碼 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f71cd4af.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f71cd4af.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f71cd4af.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f71cd4af.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f71cd4af.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f71cd4af.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/f71cd4af.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/f71cd4af.html><meta property="article:published_time" content="2020-11-14T21:06:24+08:00"><meta property="article:modified_time" content="2020-11-14T21:06:24+08:00"><meta name=Keywords content><meta name=description content="機器學習入門第2章：SVM（支持向量機）—編碼"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/f71cd4af.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習入門第2章：SVM（支持向量機）—編碼</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><p><br></p><div class=pgc-img><img alt=機器學習入門第2章：SVM（支持向量機）—編碼 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6><p class=pgc-img-caption>It's not a bug — it's an undocumented feature.</p></div><p>與樸素貝葉斯相比，支持向量機的性能如何？ 訓練慢嗎？ 讓我們探索一下此編碼練習中的所有此類問題。 這是第二章：支持向量機或支持向量分類器的第二部分。 如果您還沒有讀懂該理論（第一部分），我建議您在<a class=pgc-link data-content=mp href="https://www.toutiao.com/i6787361765123424771/?group_id=6787361765123424771" target=_blank>這裡</a>仔細閱讀。 強烈建議您瞭解SVM分類器的基本知識。</p><blockquote class=pgc-blockquote-abstract><p>通過閱讀您將對實現有足夠的瞭解，但我強烈建議您與教程一起打開編輯器和代碼。 我將為您提供更好的見識和長期的學習。</p></blockquote><h1 class=pgc-h-arrow-right><strong>0.我們要做什麼。</strong></h1><p>別忘了點擊❤。 :)</p><p>編碼練習是以前的Naive Bayes分類程序的擴展，該程序將電子郵件分為垃圾郵件和非垃圾郵件。 不用擔心，如果您還沒有完成過樸素貝葉斯課程（第1章）（儘管我建議您先完成）。 相同的代碼段也將在此處以抽象方式進行討論。</p><p>我們將嘗試通過將訓練數據集大小減少原始大小的10％來減少訓練時間。 然後，我們更改調整參數以提高準確性。 我們將看到變化的內核，C和伽瑪如何改變準確性和時序。</p><div class=pgc-img><img alt=機器學習入門第2章：SVM（支持向量機）—編碼 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/b639586ada474a4992dbadf5045223d9><p class=pgc-img-caption>The side effects!</p></div><h1 class=pgc-h-arrow-right><strong>1.下載</strong></h1><p>我已經為數據集和示例代碼創建了一個git存儲庫。 您可以從此處下載（使用第2章文件夾）。 萬一它失敗了，您可以使用/參考我的版本（第2章文件夾中的classifier.py）來了解其工作原理。 忽略plot.py文件。</p><h1 class=pgc-h-arrow-right><strong>2.關於數據清理的一點</strong></h1><blockquote class=pgc-blockquote-abstract><p>如果您已經完成了樸素貝葉斯的編碼部分，則可以跳過這一部分。（這是針對直接跳到此處的讀者的）。</p></blockquote><p>在應用sklearn分類器之前，我們必須清理數據。 清理包括刪除停用詞，從文本中提取最常用的詞等。在相關的代碼示例中，我們執行以下步驟：</p><p>要詳細瞭解，請再次參考此處的第1章編碼部分。</p><p>· 從訓練集中的電子郵件文檔構建單詞詞典。</p><p>· 考慮最常見的3000個單詞。</p><p>· 對於訓練集中的每個文檔，為字典中的這些單詞和相應的標籤創建一個頻率矩陣。 [垃圾郵件的文件名以" spmsg"開頭。</p><pre><code>The code snippet below does this:def make_Dictionary(root_dir):   all_words = []   emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]   for mail in emails:        with open(mail) as m:            for line in m:                words = line.split()                all_words += words   dictionary = Counter(all_words)   # if you have python version 3.x use commented version.   # list_to_remove = list(dictionary)   list_to_remove = dictionary.keys()for item in list_to_remove:       # remove if numerical.        if item.isalpha() == False:            del dictionary[item]        elif len(item) == 1:            del dictionary[item]    # consider only most 3000 common words in dictionary.dictionary = dictionary.most_common(3000)return dictionarydef extract_features(mail_dir):  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]  features_matrix = np.zeros((len(files),3000))  train_labels = np.zeros(len(files))  count = 0;  docID = 0;  for fil in files:    with open(fil) as fi:      for i,line in enumerate(fi):        if i == 2:          words = line.split()          for word in words:            wordID = 0            for i,d in enumerate(dictionary):              if d[0] == word:                wordID = i                features_matrix[docID,wordID] = words.count(word)      train_labels[docID] = 0;      filepathTokens = fil.split('/')      lastToken = filepathTokens[len(filepathTokens) - 1]      if lastToken.startswith("spmsg"):          train_labels[docID] = 1;          count = count + 1      docID = docID + 1  return features_matrix, train_labels</code></pre><h1 class=pgc-h-arrow-right><strong>3.進入SVC世界</strong></h1><p>使用svc的代碼與樸素貝葉斯的代碼相似。 我們首先從庫中導入svc。 接下來，我們提取訓練功能和標籤。 最後，我們要求模型預測測試集的標籤。 基本的代碼塊片段如下所示：</p><pre><code>from sklearn import svmfrom sklearn.metrics import accuracy_scoreTRAIN_DIR = "../train-mails"TEST_DIR = "../test-mails"dictionary = make_Dictionary(TRAIN_DIR)print "reading and processing emails from file."features_matrix, labels = extract_features(TRAIN_DIR)test_feature_matrix, test_labels = extract_features(TEST_DIR)model = svm.SVC()print "Training model."#train modelmodel.fit(features_matrix, labels)predicted_labels = model.predict(test_feature_matrix)print "FINISHED classifying. accuracy score : "print accuracy_score(test_labels, predicted_labels)</code></pre><p><br></p><p>結合在一起：</p><pre><code>import osimport numpy as npfrom collections import Counterfrom sklearn import svmfrom sklearn.metrics import accuracy_scoredef make_Dictionary(root_dir):    all_words = []    emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]    for mail in emails:        with open(mail) as m:            for line in m:                words = line.split()                all_words += words    dictionary = Counter(all_words)    list_to_remove = dictionary.keys()for item in list_to_remove:        if item.isalpha() == False:            del dictionary[item]        elif len(item) == 1:            del dictionary[item]    dictionary = dictionary.most_common(3000)return dictionarydef extract_features(mail_dir):    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]    features_matrix = np.zeros((len(files),3000))    train_labels = np.zeros(len(files))    count = 0;    docID = 0;    for fil in files:      with open(fil) as fi:        for i,line in enumerate(fi):          if i == 2:            words = line.split()            for word in words:              wordID = 0              for i,d in enumerate(dictionary):                if d[0] == word:                  wordID = i                  features_matrix[docID,wordID] = words.count(word)        train_labels[docID] = 0;        filepathTokens = fil.split('/')        lastToken = filepathTokens[len(filepathTokens) - 1]        if lastToken.startswith("spmsg"):            train_labels[docID] = 1;            count = count + 1        docID = docID + 1    return features_matrix, train_labelsTRAIN_DIR = "../train-mails"TEST_DIR = "../test-mails"dictionary = make_Dictionary(TRAIN_DIR)print "reading and processing emails from file."features_matrix, labels = extract_features(TRAIN_DIR)test_feature_matrix, test_labels = extract_features(TEST_DIR)model = svm.SVC()print "Training model."#train modelmodel.fit(features_matrix, labels)predicted_labels = model.predict(test_feature_matrix)print "FINISHED classifying. accuracy score : "print accuracy_score(test_labels, predicted_labels)</code></pre><p><br></p><p>這是非常基本的實現。 假定調整參數為默認值（kernel = linear, C = 1 and gamma = 1）</p><p>· 看看在這種情況下您得到什麼精度？</p><p>· 訓練時間如何？ 它比樸素貝葉斯快/慢嗎？</p><p>· 與樸素貝葉斯相比，準確性如何？</p><h1 class=pgc-h-arrow-right><strong>嗯...我們如何減少訓練時間？</strong></h1><p>一種方法是減少訓練集的大小。 我們將其減小為原始大小的1/10，然後檢查準確性。 當然它會減少。</p><p>在這裡，我們有702封電子郵件用於培訓。 1/10意味著70封電子郵件用於培訓，這是非常少的。 （儘管結帳使我們感到驚奇）。</p><p>在訓練模型之前，添加以下行。 （它將feature_matrix和標籤減少1/10）。</p><pre><code>features_matrix = features_matrix[:len(features_matrix)/10]labels = labels[:len(labels)/10]</code></pre><p><br></p><p>現在什麼是訓練時間和準確性？</p><h1 class=pgc-h-arrow-right><strong>參數調整</strong></h1><div class=pgc-img><img alt=機器學習入門第2章：SVM（支持向量機）—編碼 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/8cdc14fb13de4c1fb48cc2adaa475794><p class=pgc-img-caption></p></div><p>我想您會收到大約56％的準確性。 太低了</p><p>現在將訓練集保持為1/10，讓我們嘗試調整三個參數：kernel，C和gamma。</p><h1 class=pgc-h-arrow-right><strong>1.內核</strong></h1><p>將內核更改為rbf。 即在模型= SVC（）中添加內核參數</p><pre><code>model = svm.SVC(kernel="rbf", C = 1)</code></pre><p><br></p><h1 class=pgc-h-arrow-right><strong>2. C</strong></h1><p>接下來，將C（正則化參數）更改為10、100、1000、10000。確定精度是增加還是減少？</p><p>您會注意到，在C = 100時，準確度分數提高到85.38％，並且在此之上幾乎保持相同。</p><h1 class=pgc-h-arrow-right><strong>3.伽瑪</strong></h1><p>最後，讓我們玩伽瑪。 再增加一個參數gamma = 1.0</p><pre><code>model = svm.SVC(kernel="rbf", C=100, gamma=1)</code></pre><p>糟糕！ 準確性得分下降。 對？ 嘗試將gamma = 10設為更高的值。 嘗試減少。 使用值0.1、0.01、0.001。 現在的精度是多少？ 它在增加嗎？</p><p>您會注意到，在這種運動情況下，較低的伽瑪值使我們具有很高的準確性。 （直覺：這意味著數據點稀疏，離圖形圖中的決策邊界足夠遠）。</p><p>在這種情況下，我們注意到減小訓練集大小可以達到85.4的最佳效果。 （PS：樸素貝葉斯的準確度得分是多少？）</p><h1 class=pgc-h-arrow-right><strong>快速運行腳本[可選]</strong></h1><p>您可能已經注意到，每次腳本花費大量時間來清理和讀取電子郵件中的數據（功能和標籤）。 通過保存從首次運行中提取的數據，可以加快處理速度。</p><p>這將為您節省更多時間來學習調整參數。</p><p>使用以下代碼段保存和加載代碼。</p><pre><code>import cPickleimport gzipdef load(file_name):    # load the model    stream = gzip.open(file_name, "rb")    model = cPickle.load(stream)    stream.close()    return modeldef save(file_name, model):    # save the model    stream = gzip.open(file_name, "wb")    cPickle.dump(model, stream)    stream.close()#To savesave("/tmp/features_matrix", features_matrix)save("/tmp/labels", labels)save("/tmp/test_feature_matrix", test_feature_matrix)save("/tmp/test_labels", test_labels)#To loadfeatures_matrix = load("/tmp/features_matrix")labels = load("/tmp/labels")test_feature_matrix = load("/tmp/test_feature_matrix")test_labels = load("/tmp/test_labels")</code></pre><h1 class=pgc-h-arrow-right><strong>最後的想法</strong></h1><p>通常，SVC比樸素貝葉斯需要更多的訓練時間，但預測速度更快。 在上面的編碼練習中，樸素貝葉斯的表現優於SVC。 但是，這完全取決於性能最佳的方案和數據集。</p><p>即使將訓練數據集減少到1/10，也可以獲得更高的準確性得分。</p><h1 class=pgc-h-arrow-right><strong>但是，為什麼我們需要減少訓練量呢？</strong></h1><p>訓練時間更長，SVC的訓練時間通常是樸素貝葉斯的3倍。 在某些應用中，與準確度相比，我們需要更快的預測。</p><p>· 想想信用卡交易。 快速響應交易欺詐標記比99％的準確性更為重要。 在這裡可以容忍90％的精度。</p><p>· 另一方面，只有將電子郵件標記為垃圾郵件可以忍受延遲，我們將努力提高準確性。</p><h1 class=pgc-h-arrow-right><strong>我們是否需要始終調整參數？</strong></h1><p>並不是的。 sklearn工具套件中有內置功能可以為我們完成任務。 我們將在以後的文章中探討它們。</p><p>希望本教程為您提供有關SVC編碼的基本概念。 即使對於較小的數據集，我們如何調整參數並實現合理的精度。 （我們在培訓集中只有70封電子郵件，在針對350封電子郵件的測試中達到了85％的準確性）。</p><h1 class=pgc-h-arrow-right><strong>接下來是什麼？</strong></h1><p>在下一章中，我們將學習決策樹。 （快來了）</p><p><br></p><div class=pgc-img><img alt=機器學習入門第2章：SVM（支持向量機）—編碼 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/839932bff1634e7180704414e16c36ad><p class=pgc-img-caption></p></div><p>(本文翻譯自Savan Patel的文章《Chapter 2 : SVM (Support Vector Machine) — Coding》，參考：https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-coding-edd8f1cf8f2d)</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>入門</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E5%AD%B8/c75c54fc.html alt=機器學習入門第2章：SVM（支持向量機）—理論 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/db2b59aa64f64e189449ae9773356bed style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E5%AD%B8/c75c54fc.html title=機器學習入門第2章：SVM（支持向量機）—理論>機器學習入門第2章：SVM（支持向量機）—理論</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9b52b0.html alt=機器學習入門第1章：監督學習和樸素貝葉斯分類-第2部分（編碼） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/a6894d2d1ea64a8eb3bad2b892648639 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9b52b0.html title=機器學習入門第1章：監督學習和樸素貝葉斯分類-第2部分（編碼）>機器學習入門第1章：監督學習和樸素貝葉斯分類-第2部分（編碼）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4b5cbda.html alt=機器學習入門：偏差和方差 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/e4d7bca8189b4528b0f564ee473d2a68 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4b5cbda.html title=機器學習入門：偏差和方差>機器學習入門：偏差和方差</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2d4007c7.html alt=“黑客”入門學習之“Windows組策略” class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ea21244d5f5c420ebef29650f3fafd1c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2d4007c7.html title=“黑客”入門學習之“Windows組策略”>“黑客”入門學習之“Windows組策略”</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c914526c.html alt=新手入門PLC，掌握學習方法是關鍵 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/15355275026304e8d787f10 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c914526c.html title=新手入門PLC，掌握學習方法是關鍵>新手入門PLC，掌握學習方法是關鍵</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>