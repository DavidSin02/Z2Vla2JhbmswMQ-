<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習採樣方法大全 | 极客快訊</title><meta property="og:title" content="機器學習採樣方法大全 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/f7bf1ee2492a4730a0bc2f5ca117282b"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fb200b2f.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fb200b2f.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fb200b2f.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fb200b2f.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fb200b2f.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fb200b2f.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fb200b2f.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fb200b2f.html><meta property="article:published_time" content="2020-10-29T21:12:41+08:00"><meta property="article:modified_time" content="2020-10-29T21:12:41+08:00"><meta name=Keywords content><meta name=description content="機器學習採樣方法大全"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/fb200b2f.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習採樣方法大全</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/f7bf1ee2492a4730a0bc2f5ca117282b><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ae6b11e754834333a337e974ea4d4f2b><p class=pgc-img-caption></p></div><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/cfc5f87fac494bf1bf58fe22a9998563><p class=pgc-img-caption></p></div><p>文章發佈於公號【數智物語】 （ID：decision_engine），關注公號不錯過每一篇乾貨。</p><blockquote><p>來源 | SAMshare</p></blockquote><p><strong>01</strong></p><p><strong>Index</strong></p><p>數據採樣的原因</p><p>常見的採樣算法</p><p>失衡樣本的採樣</p><p><strong>02</strong></p><p><strong>數據採樣的原因</strong></p><p>其實我們在訓練模型的過程，都會經常進行數據採樣，為了就是讓我們的模型可以更好的去學習數據的特徵，從而讓效果更佳。但這是比較淺層的理解，更本質上，數據採樣就是對隨機現象的模擬，根據給定的概率分佈從而模擬一個隨機事件。另一說法就是用少量的樣本點去近似一個總體分佈，並刻畫總體分佈中的不確定性。</p><p>因為我們在現實生活中，大多數數據都是龐大的，所以總體分佈可能就包含了無數多的樣本點，模型是無法對這些海量的數據進行直接建模的（至少目前而言），而且從效率上也不推薦。</p><p>因此，我們一般會從總體樣本中抽取出一個子集來近似總體分佈，這個子集被稱為“訓練集”，然後模型訓練的目的就是最小化訓練集上的損失函數，訓練完成後，需要另一個數據集來評估模型，也被稱為“測試集”。</p><p>採樣的一些高級用法，比如對樣本進行多次重採樣，來估計統計量的偏差與方法，也可以對目標信息保留不變的情況下，不斷改變樣本的分佈來適應模型訓練與學習（經典的應用如解決樣本不均衡的問題）。</p><p><strong>03</strong></p><p><strong>常見的採樣算法</strong></p><p>採樣的原因在上面已經闡述了，現在我們來了解一下采樣的一些算法：</p><p>01</p><p>逆變換採樣</p><p>有的時候一些分佈不好直接採樣，可以用函數轉換法，如果存在隨機變量x和u的變換關係：u=ϕ(x)，則它們的概率密度函數如下所示：</p><p>p(u)|ϕ′(x)|=p(x)</p><p>因此，如果從目標分佈p(x)中不好採樣x，可以構造一個變換u=ϕ(x)，使得從變換後地分佈p(u)中採樣u比較容易，這樣可以通過對u進行採樣然後通過反函數來間接得到x。如果是高維空間地隨機變量,則ϕ′(x)對應Jacobian行列式。</p><p>而且，如果變換關係ϕ(·)是x的累積分佈函數的話，則就是我們說的 逆變換採樣（Inverse Transform Sampling）， 我們假設待採樣的目標分佈的概率密度函數為p(x), 它的累積分佈函數為：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/2a370ca119a34be8a195bd08276498cf><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>逆變換採樣法的過程：</p><p>1. 從均勻分佈U(0，1)產生一個隨機數 Ui</p><p>2. 計算逆函數</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/381426c1996d4f089fe0ae3b197aaf96><p class=pgc-img-caption></p></div><p>來間接得到x</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a1996ebb437d44ccba37af8a07198d4e><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>但並不是所有的目標分佈的累積分佈函數的逆函數都是可以求解的（or容易計算），這個時候逆變換採樣法就不太適用，可以考慮拒絕採樣（Rejection Sampling）和重要度採樣（Importance Sampling）。</p><p>02</p><p>拒絕採樣（Rejection Sampling）</p><p>拒絕採樣，也被稱為接受採樣（Accept Sampling），對於目標分佈p(x)，選取一個容易採樣的參考分佈q(x)，使得對於任意的x都有：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/bcfdec011214436082fa654c46589f53><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>其採樣過程如下：</p><p>1）從參考分佈q(x)中隨機抽取一個樣本xi</p><p>2）從均勻分佈U(0,1)產生一個隨機ui</p><p>3）如果</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c2538078f60e4c1baa8c7643d9c976e6><p class=pgc-img-caption></p></div><p>，則接受樣本xi，否則拒絕，一直重複1-3步驟，直到新產生的樣本量滿足要求。</p><p>其實，拒絕採樣的關鍵就是為我們的目標分佈p(x)選取一個合適的包絡函數</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c99683c5ea2f4f5d9d777d9fb9454b98><p class=pgc-img-caption></p></div><p>，如下圖所示的正態分佈的函數：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1ceecac7e273499d948a563a300e09fb><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>可以知道，包絡函數越“緊”，</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/3a0a140fbfa74a9c97ac81d3fc0c7a65><p class=pgc-img-caption></p></div><p>的大小越接近，那麼</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/98d7256979064172827a5826b31e3c58><p class=pgc-img-caption></p></div><p>就越接近1，那麼更容易接受採樣樣本，這樣子採樣的效率就越高。</p><p>除了上面的形式，還有一種叫自適應拒絕採樣（Adaptive Rejection Sampling），在目標分佈是對數凹函數時，用分段的線性函數來做包絡函數，如下圖所示：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/6176e6205c2e4e7d9f73a5b967023390><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>03</p><p>重要性採樣（Importance Sampling）</p><p>還有一種採樣方法，是計算函數f(x)在目標分佈p(x)上的積分（函數期望），即：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/ac114022708a46a58e68384fefa05a3b><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>我們先找一個比較容易抽樣的參考分佈q(x)，並令</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/b0ee81accb004054af524a9802df2485><p class=pgc-img-caption></p></div><p>則存在：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/90d8789aa9db4b2eac6727c638973ff3><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>這裡的w(x)我們可以理解為權重，我們就可以從參考分佈q(x)中抽取N個樣本xi，並且利用如下公式來估計E[f]：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/e7c0e3edb02645c4bd2462b0ea654711><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>下圖就是重要性採樣的示意圖：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a1440a30972d41a59541946a765a01b5><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>04</p><p>馬爾科夫蒙特卡洛採樣法</p><p>在高維空間中，拒絕採樣和重要性採樣很難尋找到合適參考分佈，而且採樣的效率是很低的，這個時候是可以考慮一下馬爾科夫蒙特卡洛（Markov Chain Monte Carlo，MCMC）採樣法。</p><p>可能有一些同學對這個名詞還是比較陌生，那麼先來講解一下MCMC。</p><p><strong>1. 主要思想</strong></p><blockquote><p>MCMC採樣法主要包括兩個MC，即Monte Carlo和Markov Chain。Monte Carlo是指基於採樣的數值型近似求解方法，Markov Chain則是用於採樣，MCMC的基本思想是：針對待採樣的目標分佈，構造一個馬爾科夫鏈，使得該馬爾科夫鏈的平穩分佈就是目標分佈，然後從任何一個初始狀態出發，沿著馬爾科夫鏈進行狀態轉移，最終得到的狀態轉移序列會收斂到目標分佈，由此得到目標分佈的一系列樣本。</p></blockquote><p>MCMC有著不同的馬爾科夫鏈（Markov Chain），不同的鏈對應不用的採樣法，常見的兩種就是Metropolis-Hastings採樣法和吉布斯採樣法。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/4ab89dcf65b942fda53be38f05f822f4><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p><strong>2. Metropolis-Hastings採樣法</strong></p><p>對於目標分佈p(x)，首先選擇一個容易採樣的參考條件分佈</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/ec059837621048f2bf7b88184dbe06b3><p class=pgc-img-caption></p></div><p>，並令</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/61abaecca99b42bca7e0a95eec409f50><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>然後根據如下過程進行採樣：</p><p>1）隨機選取一個初始樣本</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/7b226f9e51f34d9199623ff9b6e6f45a><p class=pgc-img-caption></p></div><p>2）For t =1, 2, 3, ...:</p><p>{ 根據參考條件分佈</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/973ce1a9897b4f9ebe473a53f9823ac1><p class=pgc-img-caption></p></div><p>抽取一個樣本</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/98101e074dbe421e8488f4924526a1d5><p class=pgc-img-caption></p></div><p>根據均勻分佈U(0,1)產生隨機數u</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/b4b68c1a699349bc8675fc40a9fc1452><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>上面的圖是Metropolis-Hastings的示意過程圖，其中紅線代表被拒絕的移動（維持舊樣本），綠線代表被接受的移動（採納新樣本）。</p><p><strong>3. 吉布斯採樣法</strong></p><p>吉布斯採樣法是Metropolis-Hastings的一個特例，其核心是每次只對樣本的一個維度進行採樣和更新，對於目標分佈p(x)，其中</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/3f5904ae27fe40eab1bdfe53f842824f><p class=pgc-img-caption></p></div><p>是多維向量，按如下的過程進行採樣：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c8c6f9708e2847ba8f721c96648d5e91><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>同樣的上述過程得到的樣本序列會收斂到目標分佈p(x)，另外步驟2中對樣本每個維度的抽樣和更新操作，不是必須要按照下標順序進行的，可以是隨機進行的。</p><p>在拒絕採樣中，如果在某一步得到的樣本被拒絕，則該步不會產生新樣本，需要重新進行採樣，如在MCMC中，每一步都是會產生一個樣本的，只是有的時候是保留舊樣本罷了，而且MCMC是會在不斷迭代過程中逐漸收斂到平穩分佈的。</p><p><strong>04</strong></p><p><strong>失衡樣本的採樣</strong></p><p>我們在實際的建模中總會遇到很多失衡的數據集，比如點擊率模型、營銷模型、反欺詐模型等等，往往壞樣本（or好樣本）的佔比才千分之幾。雖然目前有些機器學習算法會解決失衡問題，比如XGBoost，但是很多時候還是需要我們去根據業務實際情況，對數據進行採樣處理，主要還是分兩種方式：</p><p>過採樣（over-sampling）：從佔比較少的那一類樣本中重複隨機抽樣，使得最終樣本的目標類別不太失衡；</p><p>欠採樣（under-sampling）：從佔比較多的那一類樣本中隨機抽取部分樣本，使得最終樣本的目標類別不太失衡；</p><p>科學家們根據上述兩類，衍生出了很多方法，如下：</p><p>01</p><p>Over-Sampling類</p><p><strong>1. Random Oversampling</strong></p><p>也就是隨機過採樣，我們現在很少用它了，因為它是從樣本少的類別中隨機抽樣，再將抽樣得來的樣本添加到數據集中，從而達到類別平衡的目的，這樣子做很多時候會出現過擬合情況。</p><p><strong>2. SMOTE</strong></p><p>SMOTE，全稱是Synthetic Minority Oversampling Technique，其思想就是在少數類的樣本之間，進行插值操作來產生額外的樣本。對於一個少數類樣本，使用K-Mean法（K值需要人工確定）求出距離。距離最近的k個少數類樣本，其中距離定義為樣本之間n維特徵空間的歐式距離，然後從k個樣本點中隨機抽取一個，使用下面的公式生成新的樣本點：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/946db0403f4047ff9f198e00acf617c0><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>其中，</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/c7d6a11c0a4540cba4402dd2c6881adc><p class=pgc-img-caption></p></div><p>為選出的k近鄰點，</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/6a3f89e671a6440b9b9f2f80b61a1bd6><p class=pgc-img-caption></p></div><p>是一個隨機數。下圖就是一個SMOTE生成樣本的例子，使用的是3-近鄰，可以看出SMOTE生成的樣本一般就在</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/bf53b1d9861146cb951913e015b028ac><p class=pgc-img-caption></p></div><p>相連的直線上：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/8a8db31fd40e4bd6ab300592a065cc62><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>從圖中可以看出Xnew就是我們新生成樣本點，但是，SMOTE算法也是有缺點的：</p><p>（1）如果選取的少數類樣本週圍都是少數類樣本，那麼新合成的樣本可能不會提供太多有用的信息；</p><p>（2）如果選取的少數類樣本週圍都是多數類樣本，那麼這可能會是噪聲，也無法提升分類效果。</p><p>其實，最好的新樣本最好是在兩個類別的邊界附近，這樣子最有利於分類，所以下面介紹一個新算法——Border-Line SMOTE。</p><p><strong>3. Border-Line SMOTE</strong></p><p>這個算法一開始會先將少數類樣本分成3類，分別DANGER、SAFE、NOISE，如下圖：</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/41f29b1cacef4a2fa647bb77eb212b83><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p>而Border-line SMOTE算法只會在“DANGER”狀態的少數類樣本中去隨機選擇，然後利用SMOTE算法產生新樣本。</p><p><strong>4. ADASYN</strong></p><p>ADASYN名為自適應合成抽樣(Adaptive Synthetic Sampling)，其最大的特點是採用某種機制自動決定每個少數類樣本需要產生多少合成樣本，而不是像SMOTE那樣對每個少數類樣本合成同數量的樣本。ADASYN的缺點是易受離群點的影響，如果一個少數類樣本的K近鄰都是多數類樣本，則其權重會變得相當大，進而會在其周圍生成較多的樣本。</p><p>02</p><p>Under-Sampling類</p><p><strong>1. Random Undersampling</strong></p><p>這類也是比較簡單的，就是隨機從多數類中刪除一些樣本，這樣子的缺失也是很明顯，那就是造成部分信息丟失，整體模型分類效果不理想。</p><p><strong>2. EasyEnsemble 和 BalanceCascade</strong></p><p>這兩個算法放在一起的原因是因為都用到了集成思想來處理隨機欠採樣的信息丟失問題。</p><p>1. EasyEnsemble ：將多數類樣本隨機劃分成n份，每份的數據等於少數類樣本的數量，然後對這n份數據分別訓練模型，最後集成模型結果。</p><p>2. BalanceCascade：這類算法採用了有監督結合boosting的方式，在每一輪中，也是從多數類中抽取子集與少數類結合起來訓練模型，然後下一輪中丟棄此輪被正確分類的樣本，使得後續的基學習器能夠更加關注那些被分類錯誤的樣本。</p><p><strong>3. NearMiss</strong></p><p>NearMiss本質上是一種原型選擇(prototype selection)方法，即從多數類樣本中選取最具代表性的樣本用於訓練，主要是為了緩解隨機欠採樣中的信息丟失問題。NearMiss採用一些啟發式的規則來選擇樣本，根據規則的不同可分為3類：</p><p><strong>NearMiss-1：</strong>選擇到最近的K個少數類樣本平均距離最近的多數類樣本</p><p><strong>NearMiss-2：</strong>選擇到最遠的K個少數類樣本平均距離最近的多數類樣本</p><p><strong>NearMiss-3：</strong>對於每個少數類樣本選擇K個最近的多數類樣本，目的是保證每個少數類樣本都被多數類樣本包圍</p><p>NearMiss-1和NearMiss-2的計算開銷很大，因為需要計算每個多類別樣本的K近鄰點。另外，NearMiss-1易受離群點的影響，如下面第二幅圖中合理的情況是處於邊界附近的多數類樣本會被選中，然而由於右下方一些少數類離群點的存在，其附近的多數類樣本就被選擇了。相比之下NearMiss-2和NearMiss-3不易產生這方面的問題。</p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/23dacca4825842f0906bb1454880c582><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/40d08653f3ee4b86852fff96c5aee903><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/aa85513b725c413a98e35b6ec5d92c26><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center>星標我，每天多一點智慧</p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/2a43d27ec144426895b2ded71071154c><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p><p class=ql-align-center><br></p><div class=pgc-img><img alt=機器學習採樣方法大全 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/77d474ecfcaa455aab465c96ba6e20ab><p class=pgc-img-caption></p></div><p class=ql-align-center><br></p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>採樣</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html alt=機器學習之線性代數速查表 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fee3515e.html title=機器學習之線性代數速查表>機器學習之線性代數速查表</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3873d795.html alt=金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/694a9289cde541dca807f9a30d291d0d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3873d795.html title=金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例>金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>