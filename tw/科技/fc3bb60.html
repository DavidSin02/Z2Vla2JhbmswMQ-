<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>NLP：不要重新造輪子 | 极客快訊</title><meta property="og:title" content="NLP：不要重新造輪子 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p1.pstatp.com/large/pgc-image/5e9be4ab5e114ba9be5c40bab9b80878"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fc3bb60.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fc3bb60.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fc3bb60.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fc3bb60.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fc3bb60.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fc3bb60.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fc3bb60.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fc3bb60.html><meta property="article:published_time" content="2020-10-29T21:07:50+08:00"><meta property="article:modified_time" content="2020-10-29T21:07:50+08:00"><meta name=Keywords content><meta name=description content="NLP：不要重新造輪子"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/fc3bb60.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>NLP：不要重新造輪子</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div class=pgc-img><img alt=NLP：不要重新造輪子 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/5e9be4ab5e114ba9be5c40bab9b80878><p class=pgc-img-caption></p></div><h1 class=pgc-h-arrow-right>介紹</h1><p>自然語言處理（NLP）是一個令人生畏的領域名稱。從非結構化文本中生成有用的結論是很困難的，而且有無數的技術和算法，每一種都有自己的用例和複雜性。作為一個接觸NLP最少的開發人員，很難知道要使用哪些方法以及如何實現它們。</p><p>如果我以最小的努力提供儘量完美的結果。使用80/20原則，我將向你展示如何在不顯著犧牲結果（80%）的情況下快速（20%）交付解決方案。</p><blockquote><p>“80/20原則認為，少數的原因、投入或努力通常導致大多數結果、產出或回報”</p><p>-理查德·科赫，80/20原則的作者</p></blockquote><p>我們將如何實現這一目標？有一些很棒的Python庫！我們可能站在巨人的肩膀上，迅速創新，而不是重新發明輪子。通過預先測試的實現和預訓練的模型，我們將專注於應用這些方法並創造價值。</p><p>本文的目標讀者是希望將自然語言處理快速集成到他們的項目中的開發人員。在強調易用性和快速效果的同時，性能也會下降。根據我的經驗，80%的技術對於項目來說是足夠的，但是也可以從其他地方尋找相關方法</p><p>不用多說了，我們開始吧！</p><hr><h1 class=pgc-h-arrow-right>什麼是NLP？</h1><p>自然語言處理是語言學、計算機科學和人工智能的一個分支領域，允許通過軟件自動處理文本。NLP使機器能夠閱讀、理解和響應雜亂無章的非結構化文本。</p><p>人們通常將NLP視為機器學習的一個子集，但實際情況更為微妙。</p><div class=pgc-img><img alt=NLP：不要重新造輪子 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/d04d7c20f1084055b9cfd05e57e33225><p class=pgc-img-caption></p></div><p>有些NLP工具依賴於機器學習，有些甚至使用深度學習。然而，這些方法往往依賴於大數據集，並且難以實現。相反，我們將專注於更簡單、基於規則的方法來加快開發週期。</p><h1 class=pgc-h-arrow-right>術語</h1><p>從最小的數據單位開始，字符是單個字母、數字或標點符號。一個單詞是一個字符列表，一個句子是一個單詞列表。文檔是句子的列表，而語料庫是文檔的列表。</p><h1 class=pgc-h-arrow-right>預處理</h1><p>預處理可能是NLP項目中最重要的一步，它涉及到清理輸入，這樣模型就可以忽略噪聲，並將注意力集中在最重要的內容上。一個強大的預處理管道將提高所有模型的性能，所以必須強調它的價值。</p><p>以下是一些常見的預處理步驟：</p><ul><li><strong>分段</strong>：給定一長串字符，我們可以用空格分隔文檔，按句點分隔句子，按空格分隔單詞。實現細節將因數據集而異。</li><li><strong>使用小寫</strong>：大寫通常不會增加性能，並且會使字符串比較更加困難。所以把所有的東西都改成小寫。</li><li><strong>刪除標點</strong>：我們可能需要刪除逗號、引號和其他不增加意義的標點。</li><li><strong>刪除停用詞</strong>：停用詞是像“she”、“the”和“of”這樣的詞，它們不會增加文本的含義，並且分散對關鍵字的注意力。</li><li><strong>刪除其他不相關單詞</strong>：根據你的應用程序，你可能希望刪除某些不相關的單詞。例如，如果評估課程回顧，像“教授”和“課程”這樣的詞可能沒有用。</li><li><strong>詞幹/詞根化</strong>：詞幹分析和詞根化都會生成詞形變化單詞的詞根形式（例如：“running”到“run”）。詞幹提取速度更快，但不能保證詞根是英語單詞。詞根化使用語料庫來確保詞根是一個單詞，但代價是速度。</li><li><strong>詞性標註</strong>：詞性標註以詞性（名詞、動詞、介詞）為依據，根據詞義和語境來標記單詞。例如，我們可以專注於名詞進行關鍵字提取。</li></ul><p>這些步驟是成功的預處理的基礎。根據數據集和任務的不同，你可以跳過某些步驟或添加新步驟。通過預處理手動觀察數據，並在出現問題時進行更正。</p><hr><h1 class=pgc-h-arrow-right>Python庫</h1><div class=pgc-img><img alt=NLP：不要重新造輪子 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a20b63b31da14210be17beb2345271cd><p class=pgc-img-caption></p></div><p>讓我們來看看NLP的兩個主要Python庫。這些工具將在預處理期間，佔據非常大的作用</p><h1 class=pgc-h-arrow-right>NLTK</h1><p>自然語言工具包是Python中使用最廣泛的NLP庫。NLTK是UPenn為學術目的而開發的，它有大量的特徵和語料庫。NLTK非常適合處理數據和運行預處理：https://www.nltk.org/</p><p>NLTK是構建Python程序以處理人類語言數據的領先平臺。它提供了易於使用的API</p><pre><code>&gt;&gt;&gt; import nltk&gt;&gt;&gt; sentence = "At eight o'clock on Thursday morning Arthur didn't feel very good."&gt;&gt;&gt; tokens = nltk.word_tokenize(sentence)&gt;&gt;&gt; tokens['At', 'eight', "o'clock", 'on', 'Thursday', 'morning', 'Arthur', 'did', "n't", 'feel', 'very', 'good', '.']&gt;&gt;&gt; tagged = nltk.pos_tag(tokens)&gt;&gt;&gt; tagged[0:6][('At', 'IN'), ('eight', 'CD'), ("o'clock", 'JJ'), ('or', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN')]</code></pre><p>這是NLTK網站上的一個例子，它展示了標記句子和標記詞性是多麼簡單。</p><h1 class=pgc-h-arrow-right>SpaCy</h1><p>SpaCy是一個現代的的庫。雖然NLTK對每個特性都有多個實現，但是SpaCy保留性能最好的實現。</p><p>只需幾行代碼，我們就可以使用SpaCy執行命名實體識別。使用SpaCy api可以快速完成許多其他任務。</p><pre><code>import spacynlp = spacy.load("en_core_web_sm")text = ("When Sebastian Thrun started working on self-driving cars at "        "Google in 2007, few people outside of the company took him seriously")doc = nlp(text)for entity in doc.ents:    print(entity.text, entity.label_)# 輸出# Sebastian Thrun# 谷歌組織# 2007日期</code></pre><h1 class=pgc-h-arrow-right>GenSim</h1><p>與NLTK和SpaCy不同，GenSim專門解決信息檢索（IR）問題。GenSim的開發重點是內存管理，它包含許多文檔相似性模型，包括Latent Semantic Indexing、Word2Vec和FastText。</p><p>Gensim是一個Python庫，用於主題模型、文檔索引和大型語料庫的相似性檢索。</p><p>下面是一個預先訓練的GenSim Word2Vec模型的例子，它可以發現單詞的相似性。不用擔心那些雜亂無章的細節，我們可以很快得到結果。</p><pre><code>import gensim.downloader as apiwv = api.load("word2vec-google-news-300")pairs = [    ('car', 'minivan'),    # 小型貨車是一種汽車    ('car', 'bicycle'),    # 也是有輪子的交通工具    ('car', 'airplane'),   # 沒有輪子，但仍然是交通工具    ('car', 'cereal'),     # ... 等等    ('car', 'communism'),]for w1, w2 in pairs:    print('%r\t%r\t%.2f % (w1, w2, wv.similarity(w1, w2)))# 輸出# 'car'   'minivan'    0.69# 'car'   'bicycle'    0.54# 'car'   'airplane'   0.42# 'car'   'cereal'     0.14# 'car'   'communism'  0.06</code></pre><h1 class=pgc-h-arrow-right>還有更多…</h1><p>這個列表並不全面，但涵蓋了一些用例。</p><hr><h1 class=pgc-h-arrow-right>應用</h1><p>既然我們已經討論了預處理方法和Python庫，讓我們用幾個例子把它們放在一起。對於每種算法，我將介紹幾個NLP算法，根據我們的快速開發目標選擇一個，並使用其中一個庫創建一個簡單的實現。</p><h1 class=pgc-h-arrow-right>應用1：預處理</h1><p>預處理是任何NLP解決方案的關鍵部分，所以讓我們看看如何使用Python庫來加快處理速度。根據我的經驗，NLTK擁有我們所需的所有工具，並針對獨特的用例進行定製。讓我們加載一個樣本語料庫：</p><pre><code>import nltk# 加載brown語料庫corpus = nltk.corpus.brown# 訪問語料庫的文件print(corpus.fileids())# 輸出['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'c...</code></pre><p>按照上面定義的管道，我們可以使用NLTK來實現分段、刪除標點和停用詞、執行詞幹化等等。看看刪除停用詞是多麼容易：</p><pre><code>from nltk.corpus import stopwordssw = stopwords.words("english")sw += ""  # 空字符串def remove_sw(doc):    sentences = []    for sentence in doc:        sentence = [word for word in sentence if word not in sw]        sentences.append(sentence)    return sentencesprint("With Stopwords")print(doc1[1])print()doc1 = remove_sw(doc1)print("Without Stopwords")print(doc1[1])# 輸出# 有停用詞# ['the', 'jury', 'further', 'said', 'in', 'presentments', 'that', 'the', 'city', 'executive', 'committee', 'which', 'had',# 'charge', 'of', 'the', 'election', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'city', 'of', 'atlanta', 'for', # 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted']# 沒有停用詞# ['jury', 'said', 'presentments', 'city', 'executive', 'committee', 'charge', 'election', 'deserves', 'praise', 'thanks', 'city',# 'atlanta', 'manner', 'election', 'conducted']</code></pre><p>整個預處理管道佔用了我不到40行Python。請參閱此處的完整代碼。記住，這是一個通用的示例，你應該根據你的特定用例的需要修改流程。</p><h1 class=pgc-h-arrow-right>應用2：文檔聚類</h1><p>文檔聚類是自然語言處理中的一個常見任務，所以讓我們來討論一些方法。這裡的基本思想是為每個文檔分配一個表示所討論主題的向量：</p><div class=pgc-img><img alt=NLP：不要重新造輪子 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/a48a602465f544c399dfa6d610b65ed5><p class=pgc-img-caption></p></div><p>如果向量是二維的，我們可以像上面一樣可視化文檔。在這個例子中，我們看到文檔A和B是緊密相關的，而D和F是鬆散相關的。即使這些向量是3維、100維或1000維，使用距離度量的話，我們也可以計算相似性。</p><p>下一個問題是如何使用非結構化文本輸入為每個文檔構造這些向量。這裡有幾個選項，從最簡單到最複雜的：</p><ul><li>詞袋：為每個唯一的單詞分配一個索引。給定文檔的向量是每個單詞出現的頻率。</li><li>TF-IDF：根據單詞在其他文檔中的常見程度來加強表示。如果兩個文檔共享一個稀有單詞，則它們比共享一個公共單詞更相似。</li><li>潛在語義索引（LSI）：詞袋和TF-IDF可以創建高維向量，這使得距離測量的準確性降低。LSI將這些向量壓縮到更易於管理的大小，同時最大限度地減少信息損失。</li><li>Word2Vec：使用神經網絡，從大型文本語料庫中學習單詞的關聯關係。然後將每個單詞的向量相加得到一個文檔向量。</li><li>Doc2Vec：在Word2Vec的基礎上構建，但是使用更好的方法從單詞向量列表中近似文檔向量。</li></ul><p>Word2Vec和Doc2Vec非常複雜，需要大量的數據集來學習單詞嵌入。我們可以使用預訓練過的模型，但它們可能無法很好地適應領域內的任務。相反，我們將使用詞袋、TF-IDF和LSI。</p><p>現在選擇我們的庫。GenSim是專門為這個任務而構建的，它包含所有三種算法的簡單實現，所以讓我們使用GenSim。</p><p>對於這個例子，讓我們再次使用Brown語料庫。它有15個文本類別的文檔，如“冒險”、“編輯”、“新聞”等。在運行我們的NLTK預處理例程之後，我們可以開始應用GenSim模型。</p><p>首先，我們創建一個將標識映射到唯一索引的字典。</p><pre><code>from gensim import corpora, models, similaritiesdictionary = corpora.Dictionary(corpus)dictionary.filter_n_most_frequent(1)  # removes ""num_words = len(dictionary)print(dictionary)print()print("Most Frequent Words")top10 = sorted(dictionary.cfs.items(), key=lambda x: x[1], reverse=True)[:10]for i, (id, freq) in enumerate(top10):    print(i, freq, dictionary[id])# 輸出# Dictionary(33663 unique tokens: ['1', '10', '125', '15th', '16']...)# 頻率最高的詞# 0 3473 one# 1 2843 would# 2 2778 say# 3 2327 make# 4 1916 time# 5 1816 go# 6 1777 could# 7 1665 new# 8 1659 year# 9 1575 take</code></pre><p>接下來，我們迭代地應用詞袋、TF-IDF和潛在語義索引：</p><pre><code>corpus_bow = [dictionary.doc2bow(doc) for doc in corpus]print(len(corpus_bow[0]))print(corpus_bow[0][:20])# 輸出# 6106# [(0, 1), (1, 3), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 2), (13, 2), (14, 2), (15,# 1), (16, 2), (17, 2), (18, 3), (19, 1)]tfidf_model = models.TfidfModel(corpus_bow)corpus_tfidf = tfidf_model[corpus_bow]print(len(corpus_tfidf[0]))print(corpus_tfidf[0][:20])# 輸出# 5575# [(0, 0.001040495879718581), (1, 0.0011016669638018743), (2, 0.002351365659027428), (3, 0.002351365659027428), (4, # 0.0013108697793088472), (5, 0.005170600993729588), (6, 0.003391861538746009), (7, 0.004130105114011007), (8, # 0.003391861538746009), (9, 0.008260210228022013), (10, 0.004130105114011007), (11, 0.001955787484706956), (12, # 0.0015918258736505996), (13, 0.0015918258736505996), (14, 0.008260210228022013), (15, 0.0013108697793088472), (16, # 0.0011452524080876978), (17, 0.002080991759437162), (18, 0.004839366251287288), (19, 0.0013108697793088472)]lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=20)corpus_lsi = lsi_model[corpus_tfidf]print(len(corpus_lsi[0]))print(corpus_lsi[0])# 輸出# 15# [(0, 0.18682238167974372), (1, -0.4437583954806601), (2, 0.22275580411969662), (3, 0.06534575527078117), (4, # -0.10021080420155845), (5, 0.06653745783577146), (6, 0.05025291839076259), (7, 0.7117552624193217), (8, -0.3768886513901333), (9, # 0.1650380936828472), (10, 0.13664364557932132), (11, -0.03947144082104315), (12, -0.03177275640769521), (13, # -0.00890543444745628), (14, -0.009715808633565214)]</code></pre><p>在大約10行Python代碼中，我們處理了三個獨立的模型，併為文檔提取了向量表示。利用餘弦相似度進行向量比較，可以找到最相似的文檔。</p><pre><code>categories = ["adventure", "belles_lettres", "editorial", "fiction", "government",               "hobbies", "humor", "learned", "lore", "mystery", "news", "religion",              "reviews", "romance", "science_fiction"]num_categories = len(categories)for i in range(3):    print(categories[i])    sims = index[lsi_model[corpus_bow[i]]]    top3 = sorted(enumerate(sims), key=lambda x: x[1], reverse=True,)[1:4]    for j, score in top3:        print(score, categories[j])    print()# 輸出# adventure# 0.22929086 fiction# 0.20346783 romance# 0.19324714 mystery# belles_lettres# 0.3659389 editorial# 0.3413822 lore# 0.33065677 news# editorial# 0.45590898 news# 0.38146105 government# 0.2897901 belles_lettres</code></pre><p>就這樣，我們有結果了！冒險小說和浪漫小說最為相似，而社論則類似於新聞和政府。</p><h1 class=pgc-h-arrow-right>應用3：情感分析</h1><p>情感分析是將非結構化文本解釋為正面、負面或中性。情感分析是分析評論、衡量品牌、構建人工智能聊天機器人等的有用工具。</p><p>與文檔聚類不同，在情感分析中，我們不使用預處理。段落的標點符號、流程和上下文可以揭示很多關於情緒的信息，所以我們不想刪除它們。</p><p>為了簡單有效，我建議使用基於模式的情感分析。通過搜索特定的關鍵詞、句子結構和標點符號，這些模型測量文本的積極消極性。以下是兩個帶有內置情感分析器的庫：</p><h1 class=pgc-h-arrow-right><strong>VADER</strong> 情感分析：</h1><p><strong>VADER</strong> 是 Valence Aware Dictionary and sEntiment Recognizer的縮寫，是NLTK用於情感分析的擴展。它使用模式來計算情緒，尤其適用於表情符號和短信俚語。它也非常容易實現。</p><pre><code>from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzeranalyzer = SentimentIntensityAnalyzer()print(analyzer.polarity_scores("This class is my favorite!!!"))print(analyzer.polarity_scores("I hate this class :("))# 輸出# {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.5962}# {'neg': 0.688, 'neu': 0.312, 'pos': 0.0, 'compound': -0.765}</code></pre><h1 class=pgc-h-arrow-right>TextBlob情感分析：</h1><p>一個類似的工具是用於情感分析的TextBlob。TextBlob實際上是一個多功能的庫，類似於NLTK和SpaCy。在情感分析工具上，它與VADER在報告情感極性和主觀性方面都有所不同。從我個人的經驗來看，我更喜歡VADER，但每個人都有自己的長處和短處。TextBlob也非常容易實現：</p><pre><code>from textblob import TextBlobtestimonial = TextBlob("This class is my favorite!!!")print(testimonial.sentiment)testimonial = TextBlob("I hate this class :(")print(testimonial.sentiment)# 輸出# Sentiment(polarity=0.9765625, subjectivity=1.0)# Sentiment(polarity=-0.775, subjectivity=0.95)</code></pre><p>注意：基於模式的模型在上面的例子中不能很好地處理這樣的小文本。我建議對平均四句話的文本進行情感分析。</p><h1 class=pgc-h-arrow-right>其他應用</h1><p>這裡有幾個附加的主題和一些有用的算法和工具來加速你的開發。</p><ul><li><strong>關鍵詞提取</strong>：命名實體識別（NER）使用SpaCy，快速自動關鍵字提取（RAKE）使用ntlk-rake</li><li><strong>文本摘要</strong>：TextRank（類似於PageRank）使用PyTextRank SpaCy擴展，TF-IDF使用GenSim</li><li><strong>拼寫檢查</strong>：PyEnchant，SymSpell Python端口</li></ul><p>希望這些示例有助於演示Python中可用於自然語言處理的大量資源。不管問題是什麼，有人開發了一個庫來簡化流程。使用這些庫可以在短時間內產生很好的結果。</p><hr><h1 class=pgc-h-arrow-right>提示和技巧</h1><p>通過對NLP的介紹、Python庫的概述以及一些示例應用程序，你幾乎可以應對自己的挑戰了。最後，我有一些技巧和技巧來充分利用這些資源。</p><ul><li><strong>Python工具</strong>：我推薦Poetry 用於依賴關係管理，Jupyter Notebook用於測試新模型，Black和/或Flake8用於保持代碼風格，GitHub用於版本管理。</li><li><strong>保持條理</strong>：從一個庫跳到另一個庫，複製代碼到當前你編寫的代碼測試雖然很容易實現，但是不好。我建議採取你採取合適的更慎重的方法，因為你不想在匆忙中錯過一個好的解決方案。</li><li><strong>預處理</strong>：垃圾進，垃圾出。實現一個強大的預處理管道來清理輸入非常重要。目視檢查處理後的文本，以確保所有內容都按預期工作。</li><li><strong>展示結果</strong>：選擇如何展示你的結果會有很大的不同。如果輸出的文本看起來有點粗糙，可以考慮顯示聚合統計信息或數值結果。</li></ul></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>NLP</a></li><li><a>造輪子</a></li><li><a>重新</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/33d04d4d.html alt=2020年各大頂會NLP、ML優質論文分類整理分享 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/fb47112700b049aa88994c8949ec9403 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/33d04d4d.html title=2020年各大頂會NLP、ML優質論文分類整理分享>2020年各大頂會NLP、ML優質論文分類整理分享</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/7a11c4af.html alt=圖解BERT（NLP中的遷移學習） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/b29b82aef73748bd9fc0a049212fba09 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/7a11c4af.html title=圖解BERT（NLP中的遷移學習）>圖解BERT（NLP中的遷移學習）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/30177cc2.html alt=NLP中的遷移學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1534906754727755a6a6964 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/30177cc2.html title=NLP中的遷移學習>NLP中的遷移學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/45cbe488.html alt=NLP基礎-通用句子向量漫談 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/db54188d47524055b7a45d90aed407ac style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/45cbe488.html title=NLP基礎-通用句子向量漫談>NLP基礎-通用句子向量漫談</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/760de6b8.html alt=NLP領域中的遷移學習現狀 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/RayMFst8jYuQgG style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/760de6b8.html title=NLP領域中的遷移學習現狀>NLP領域中的遷移學習現狀</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f34d1055.html alt="ACL 2019 | 南大NLP，知識庫問答中的表示映射學習" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/RdGewJX7BKaFVS style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f34d1055.html title="ACL 2019 | 南大NLP，知識庫問答中的表示映射學習">ACL 2019 | 南大NLP，知識庫問答中的表示映射學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a73c5785.html alt="「NLP 必備知識點」自然語言理解 NLU（概念+應用+3種實現方式）" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/8cc2bb8a2c0f4d529fd624f5df2fc70c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a73c5785.html title="「NLP 必備知識點」自然語言理解 NLU（概念+應用+3種實現方式）">「NLP 必備知識點」自然語言理解 NLU（概念+應用+3種實現方式）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/d3668904.html alt=自然語言處理（NLP）常用庫整理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/235e94cda81a4858a3000bb62b4f970d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/d3668904.html title=自然語言處理（NLP）常用庫整理>自然語言處理（NLP）常用庫整理</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ff8d4ec0.html alt=近53種NLP中文語料庫，你一定用得到 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4baf47697c7f46c3bd15d310e7b9005c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ff8d4ec0.html title=近53種NLP中文語料庫，你一定用得到>近53種NLP中文語料庫，你一定用得到</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a33d5f8c.html alt=NLP最新科研福利！MSRA開源學術界最全面語義分析數據集 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/ccdfe908755e4dd99f59e788131374bb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a33d5f8c.html title=NLP最新科研福利！MSRA開源學術界最全面語義分析數據集>NLP最新科研福利！MSRA開源學術界最全面語義分析數據集</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/02a71e69.html alt=NLP任務中的文本預處理步驟、工具和示例 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/RqSgmP9HojtMNK style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/02a71e69.html title=NLP任務中的文本預處理步驟、工具和示例>NLP任務中的文本預處理步驟、工具和示例</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/2070e90b.html alt=一文看懂自然語言處理-NLP（4個典型應用+5個難點+6個實現步驟） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/d1504f3b2d614621bd4081a64ef145ca style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/2070e90b.html title=一文看懂自然語言處理-NLP（4個典型應用+5個難點+6個實現步驟）>一文看懂自然語言處理-NLP（4個典型應用+5個難點+6個實現步驟）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4829ec8a.html alt=一文讓你入門NLP自然語言處理，看不懂你來找我 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/dfic-imagehandler/e0409d62-8a85-4eee-848a-f939a843c1db style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4829ec8a.html title=一文讓你入門NLP自然語言處理，看不懂你來找我>一文讓你入門NLP自然語言處理，看不懂你來找我</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/77565851.html alt="機器不學習：NLP系列3 自然語言理解-意圖分類" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1534769000731dd06801b0b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/77565851.html title="機器不學習：NLP系列3 自然語言理解-意圖分類">機器不學習：NLP系列3 自然語言理解-意圖分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/475b62da.html alt="自然語言處理 NLP 發展簡史" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/9a09ec23681e48f5952e8b830fbca5bb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/475b62da.html title="自然語言處理 NLP 發展簡史">自然語言處理 NLP 發展簡史</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>