<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>論文解讀：解救深度學習過擬合的神器——非線性係數 | 极客快訊</title><meta property="og:title" content="論文解讀：解救深度學習過擬合的神器——非線性係數 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/153203036197243ebf2baa6"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fe08328.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fe08328.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fe08328.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fe08328.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fe08328.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fe08328.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fe08328.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fe08328.html><meta property="article:published_time" content="2020-10-29T20:58:13+08:00"><meta property="article:modified_time" content="2020-10-29T20:58:13+08:00"><meta name=Keywords content><meta name=description content="論文解讀：解救深度學習過擬合的神器——非線性係數"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/fe08328.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>論文解讀：解救深度學習過擬合的神器——非線性係數</h1></header><date class="post-meta meta-date">2020-10-29</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153203036197243ebf2baa6><p><strong>圖片來源：www.cs.uaf.deu</strong></p><p><strong>導語</strong></p><p>設計出高性能的神經網絡架構是AI從業者追求的目標，但是並沒有通用的設計準則。在不久前發表的一篇論文中，研究者提出一種指標——非線性係數，可以很好地度量深度神經網絡的過擬合程度。下面是對這篇論文的解讀。</p><blockquote><p>論文題目：</p><p>The Nonlinearity Coefficient - Predicting Overfitting in Deep Neural Networks</p><p>論文地址：</p><p>https://arxiv.org/abs/1806.00179</p></blockquote><p>如何判斷神經網絡的性能</p><p>在神經網絡在各個領域不斷攻城略地的今天，如何設計一個性能優良的網絡，和判斷網絡是否能夠能夠被訓練，成為了科學家和工程師們的主要問題。針對這個問題，本文介紹了一種叫做非線性係數（NLC）的度量方式，NLC是針對梯度爆炸和塌陷域的一種度量方法。</p><p>通過NLC，可以對一個神經網絡的單個梯度計算所獲得的信息量做出一個較為精確的估計。同時論文中通過大量實證研究，說明了NLC能夠較好的預測全連接網絡的測試誤差，這使得通過計算 NLC ，在一定程度上指導神經網絡的設計。</p><p>此外，本文還發現了避免過度偏激的神經元激活的必要性，以及殘差連接對於降低NLC值和提高網絡性能的重要性。在文章的第五部分，文章還介紹了NLC 與網絡中非線性化程度之間的一個有趣的關係，該關係可以用線性函數近似。</p><p>文章的最後一部分，作者討論了NLC的魯棒性，並且認為NLC是目前最好的通用網絡性能預測器，同時對過擬合有較好預測效果。</p><blockquote><p><strong>本文的工作在《The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions.》基礎上展開，下面將這篇文章表示為[1]</strong>，該文章提出了一個叫做梯度規模係數(GSC)的度量方式。GSC 是第一個關於梯度爆炸是否會產生的度量方式。提出了塌陷域(collapsing domain problem)問題，該問題是指在特定區域內，隨著深度的增長，更深處隱藏層的激活函數對不同的數據點的響應越來越相似。對NLC感興趣的讀者可以通過閱讀文章[1]獲得更多相關內容。</p></blockquote><p><strong>數學預警</strong></p><p>為了明確且無歧義的介紹NLC的定義以及介紹方式，接下來兩部分將先介紹本文使用的一些術語與數學標記，然後介紹NLC的數學定義。如果你對數學不感興趣，可以略過以下兩部分內容。</p><p><strong>數學準備——術語與標記</strong></p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/15320303620722e0f18d788><p>代表一個</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15320303620624ed8a4fd44><p>層的神經網絡，其中每一層用</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1532030362054b653c4fbde><p>來表示。</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1532030361990b5652135b0><p>為第</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1532030362273f2f0789f80><p>層的輸出維度，每層的輸出維度固定，不同層的輸出維度不一定相同。</p><p>輸入向量</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15320303622821b06b0686b><p>可被記做</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1532030362290a4ffc93ed4><p>，其維度為</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/153203036230795564ce5eb><p>。</p><p>數據標籤</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1532030362298736f37eaf1><p>與網絡輸出</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1532030362560f6ee623b67><p>的誤差記做</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1532030362579e1eff38bc8><p>，也被記做</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1532030362551b44ea2106e><p>，誤差層的維度</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1532030362589c01efa551a><p>固定為 1。</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1532030362571645c71ee8a><p>為關於參數</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1532030362806d266517ab2><p>， 第</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1532030362797361a9ff7a6><p>層對第</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153203036278761fe41b392><p>層的雅克比矩陣。在本文中，通常會省略</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/153203036282605e45737d7><p>直接記做</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1532030362815f07a762b25><p>，</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1532030363137079b7988d1><p>是</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/153203036311244f8835778><p>的簡寫。</p><p>變量</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15320303630907a8f6cb4da><p>的標準差記做</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15320303631016af3035422><p>，變量的二次期望</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/15320303631269d6fa92607><p>。</p><p><strong>NLC的數學本體</strong></p><p>本文的背景工作[1]中定義了 GSC，首先來回顧一下 GSC的定義。</p><p><strong>定義1</strong>：網絡的梯度規模係數(GSC，gradient scale coefficient）定義公式如下：</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1532030363501d2abe68ad8><p>網絡誤差的GSC如下：</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15320303634829f80cd11ab><p>對於上述公式，一個理解是，當 GSC 的值增加，說明計算單個梯度網絡所獲得的信息就會減少。</p><p>在介紹下一個度量方式DBC之前，需要一個關鍵的模型假設。 假設在 d 中 fl (x，y)的分佈是一個具有一個常數 l 的正態分佈協方差矩陣 li 和一個恆定矩陣 i。 在這種情況下，域的相對收縮僅僅是潛在表徵的大小。 根據經驗對這兩個數據進行估計，從而定義:</p><p><strong>定義2</strong>：網絡的域偏差係數(DBC, domain bias coefficient) 定義公式如下：</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/153203036346627fe290e08><p>現在，萬事俱備，接下來就是對於NLC的定義。</p><p><strong>定義3</strong>：網絡的非線性係數(NLC，nonlinearity coefficient) 定義公式如下：</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15320303635281c82bb2e88><p>文章認為 GSC 是一個更準確，儘管可能不那麼保守的，對於域的大小的估計。所以把</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15320303635090b6d8d81bf><p>近似為</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1532030363838e80d65afe8><p>，獲得下式：</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1532030363857335254912c><p>介紹完了相關的數學公式，下面的部分將通過圖表來介紹NLC的強大之處。</p><p><strong>實證研究</strong></p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15320303640689c3d25626a><p><strong>表2</strong></p><p>本文通過實驗驗證了 NLC 在實踐中能夠捕獲梯度信息量。 隨機生成了大量的神經網絡結構。在生成過程中，改變了網絡的深度，初始權重的大小，初始偏差的大小，非線性函數，歸一化的方法，殘差連接的存在，跳過連接的位置和跳過連接的強度。</p><p>實驗從8個非線性的集合(表2)中隨機挑選非線性函數，然後隨機擴張、橫向偏移和降低偏差。注意，這裡只考慮了全連接的前饋網絡，因為在深層網絡的分析研究中這是比較通用的例子。生成上述網絡後，接下來研究它們在以下三個數據集上的表現:</p><ul><li><p><strong>MNIST</strong></p></li><li><p><strong>CIFAR10</strong></p></li><li><p><strong>waveform-noise</strong></p><br></li></ul><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1532030363916d37942e8aa><p><strong>圖1</strong></p><p>實驗為每個數據集抽樣了250個架構，並計算了NLC (0,l)以及輸入空間中區域相對大小的中位數，在輸入空間中，局部線性逼近以隨機方向提供信息。結果見圖1。實際上，這兩個量之間存在著密切的關係。</p><p>然後用 SGD 訓練這750個神經網絡。 對每個架構進行了的獨立網格搜索，以確定最佳的步長，從而消除這個潛在的混淆。 這導致了總共超過15,000次全面訓練。 文[1]證明了為每個架構獨立設置步長的重要性。文章[1]證明了為每個架構獨立設置步長的重要性。</p><p><strong>模型設計準則</strong></p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1532030364015aadb0f8a5e><p><strong>圖2</strong></p><p>在圖2中，縱軸為測試誤差，橫軸為NLC(0,L)，每個點是一個神經網絡。 作者發現，對於三個數據集，測試錯誤都與 NLC 高度相關。 事實上，對於NLC的值在1~100之間(圖 b / d / f)的網絡來說，測試錯誤似乎被限制在 NLC 的線性函數之下。</p><p>對於所有三個數據集，只有當 NLC 在一個很窄的範圍內，大約在1到2之間時，才能獲得最佳性能。 作者還發現，大多數隨機結構都有一個大於最優值的 NLC。 換句話說，這些網絡非常容易過擬合。 因此，本文給出了神經結構設計的第一條準則：</p><p><strong>通過觀察NCL的值，來選擇擁有適當複雜度的神經網絡</strong></p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15320303643960446e39c17><p><strong>圖3</strong></p><p>請注意，NLC 很容易計算。在圖 3A 中，對比在CIFAR10數據集上，訓練前後的 NLC 值。 在絕大多數情況下，變化很小。 在圖2 a / c / e 中，我們注意到，在左上角有一些異常點，它們代表了不可訓練的架構。</p><p>文章對圖2中的點進行了顏色編碼，其中 a 點越紅，DBC(l) 越高。 實驗發現上述異常值與表現出最高偏差的網絡結構相對應。為了進一步研究這種效應，作者修改了在 CIFAR10 上訓練的神經網絡的結構，使它們可能產生更高的域偏置值。</p><p>對這些網絡進行訓練後，將 DBC(l) 與對應的測試錯誤呈現在圖3B中。雖然偏差在一定程度上似乎不會影響性能，但是所有的DBC 超過300的網絡都表現出隨機性。 因此，根據上述結果，給出了神經結構設計的第二條準則：</p><p><strong>通過觀察DBC(L)的值，來避免太高的偏差。</strong></p><p>文[1]詳細說明了 k 稀釋效應(e k-dilution effect)，這種效應導致在在反向傳播過程，殘差連接大幅度減少 GSC 的增長。 此外，他們還證明，ResNets 大約實現了一個正交的初始狀態，並且認為不僅減少了複雜度，而且提高了性能。</p><p>在圖2中，對點進行彩色處理，使得架構的殘差連接越強，點就越藍。沒有殘差連接的架構沒有藍色。 正如預期的那樣，本文發現帶有殘差連接的架構表現出較低的 NLC。 因此，制定了第三條準則：</p><p><strong>利用殘差連接來降低 NCL，提升網絡準確性。</strong></p><p><strong>NLC對非線性單元的解釋</strong></p><p>本部分將探究NLC和非線性函數的低級屬性之間的聯繫。給定一個一維非線性操作 τ 和激活前的數據分佈，分佈為標準差為σ，均值為0的高斯分佈。</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15320303644098536650a16><p>如果 <img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15320303644184c344ff75d>代表<img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15320303644366b67233a3a>的各個分量應用了<img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/153203036442757f0d30590>函數之後的結果，那麼<img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1532030364641b799e901d0></p><p><em>是根據均值為0和協方差為 σI 的高斯分佈</em>。則<img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/15320303646312c3ea73a16>。結果表明，如果batch normalization 在一個全連接層之後，則神經元激活後的 batchnorm 大約是單位高斯分佈。</p><p>因此，在一個包含batchnorm 和非線性層的 全連接網絡中，當 batchnorm 近似是線性時，我們期望某一層的 NLC (1)大約是</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15320303646813e52765de0><p>。</p><p>訓練一個包含 batch normalization、非線性函數的兩層網絡。在表2中，本文展示了網絡使用的8個非線性函數的 值，以 及10個隨機初始化的兩層網絡的 NLC(0,l)(中值)。</p><p>作者發現這兩行值之間有著密切的關聯。然後測量了49層 batchnorm 網絡中的 NLC (0，l) ，其中包含48個非線性運算。 8個非線性函數中的6個，NLC(0，l)與指數</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1532030364696581140cf8f><p>十分接近。</p><p>這強調了關於層級智能 GSC 指數複合的理論分析。 結果表明，網絡的 NLC 與非線性密切相關。 請注意，平方和奇數次方非線性的 NLC 值高深度偏離</p><img alt=論文解讀：解救深度學習過擬合的神器——非線性係數 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1532030364653a156208450><p>，是因為這些非線性放大大輸入的值，而縮小了小輸入的值。這導致了與不同輸入相對應的潛在表示長度與隨深度偏離，破壞了高斯性。</p><p>這似乎意味著，改變非線性的線性近似，NLC 會隨之改變。對於所有CIFAR10上訓練，NLC(0，l)大於10的網絡，用0.1(s) + 0.9s 代替了非線性激活函數（即用線性函數大量稀釋了非線性）。</p><p>對這些架構進行了訓練，在110個案例中有102個案例得到了改進。 這表明比起非線性的精確形狀，NLC 對網絡性能的度量更加重要。</p><p><strong>總結</strong></p><p>本文介紹了非線性係數，對全連接網絡來說，這是一個強大的預測器，與網絡梯度的信息性和個體非線性的線性近似性密切相關。 因此，NLC 代表了一個簡單而有力的假設，即網絡的複雜性及過擬合的可能性。</p><p><strong>非線性係數方法（NLC）的計算成本低廉，整個訓練過程穩定，具有重要的實用價值。</strong>通過一項大規模的實證研究核實了文章的結果，這項研究還表明，必須避免存在過大的偏差，可以通過殘差連接來減少 NLC，並取得進一步的性能收益。</p><p>參考文獻：</p><p>[1]:The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions.</p><blockquote><p>作者：彩雲小譯7號</p><p>審校：Sylvia 王貝貝</p><p>編輯：孟婕</p></blockquote><p>推薦課程</p></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>文解</a></li><li><a>學習</a></li><li><a>過擬合</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html alt=直流鍋爐給水控制學習 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/eba10edcc8d14d9f8cde6fd5b212d90e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ac12f3a1.html title=直流鍋爐給水控制學習>直流鍋爐給水控制學習</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html alt=HTMLCSS學習筆記（六）——元素類型 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/bdb5988349894ce9bf568c6418f85b7d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a1bc38f3.html title=HTMLCSS學習筆記（六）——元素類型>HTMLCSS學習筆記（六）——元素類型</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html alt="web前端（從零開始），每天更新學習筆記 HTML5元素分類" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/46d70004fcd55e1ddad3 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/97886d06.html title="web前端（從零開始），每天更新學習筆記 HTML5元素分類">web前端（從零開始），每天更新學習筆記 HTML5元素分類</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html alt="MySQL 學習筆記" class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c9091681.html title="MySQL 學習筆記">MySQL 學習筆記</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html alt=深入學習MySQL事務：ACID特性的實現原理「轉」 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p9.pstatp.com/large/pgc-image/cdc702d66d6943499997d11e931425eb style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/175f9730.html title=深入學習MySQL事務：ACID特性的實現原理「轉」>深入學習MySQL事務：ACID特性的實現原理「轉」</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html alt=如何學習模擬IC設計？ class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f6b2ef73.html title=如何學習模擬IC設計？>如何學習模擬IC設計？</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html alt=小猿圈python學習-三大特性之多態 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/ad0e8e3777854337abeb7c779ad79a04 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/c56ee116.html title=小猿圈python學習-三大特性之多態>小猿圈python學習-三大特性之多態</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/03a295fc.html alt=地理學習5——地球的運動（地球的公轉及其地理意義） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/7b2b74c871eb40beb8ee143627d29611 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/03a295fc.html title=地理學習5——地球的運動（地球的公轉及其地理意義）>地理學習5——地球的運動（地球的公轉及其地理意義）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ebad378f.html alt=繼續學習打卡，還真心學不會了，努力，堅持 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/f36d6d47a06840aaaf78138853b9d9d1 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ebad378f.html title=繼續學習打卡，還真心學不會了，努力，堅持>繼續學習打卡，還真心學不會了，努力，堅持</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/112d1b5f.html alt=一造學習筆記—管理篇（2）：工程造價管理的組織和內容 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/dfic-imagehandler/9e65b076-038f-4720-96ff-182898f42dee style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/112d1b5f.html title=一造學習筆記—管理篇（2）：工程造價管理的組織和內容>一造學習筆記—管理篇（2）：工程造價管理的組織和內容</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>