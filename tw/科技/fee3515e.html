<!doctype html><html lang=tw><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><link rel=dns-prefetch href=https://i.geekbank.cf/><title>機器學習之線性代數速查表 | 极客快訊</title><meta property="og:title" content="機器學習之線性代數速查表 - 极客快訊"><meta property="og:type" content="article"><meta property="og:locale" content="tw"><meta property="og:image" content="https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a"><link rel=alternate hreflang=x-default href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fee3515e.html><link rel=alternate hreflang=zh-tw href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fee3515e.html><link rel=alternate hreflang=zh-cn href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fee3515e.html><link rel=alternate hreflang=zh-hk href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fee3515e.html><link rel=alternate hreflang=zh-mo href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fee3515e.html><link rel=alternate hreflang=zh-my href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fee3515e.html><link rel=alternate hreflang=zh-sg href=https://geekbank.cf/cn/%e7%a7%91%e6%8a%80/fee3515e.html><link rel=canonical href=https://geekbank.cf/tw/%e7%a7%91%e6%8a%80/fee3515e.html><meta property="article:published_time" content="2020-11-14T21:05:17+08:00"><meta property="article:modified_time" content="2020-11-14T21:05:17+08:00"><meta name=Keywords content><meta name=description content="機器學習之線性代數速查表"><meta name=author content="极客快訊"><meta property="og:url" content="/tw/%E7%A7%91%E6%8A%80/fee3515e.html"><link rel=apple-touch-icon sizes=180x180 href=../../apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../../favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../../favicon-16x16.png><link rel=manifest href=../../site.webmanifest><link rel=mask-icon href=../../safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#ffc40d"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://geekbank.cf/css/normalize.css><link rel=stylesheet href=https://geekbank.cf/css/style.css><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js></script><script type=text/javascript src=https://geekbank.cf/js/jqthumb.min.js></script><script data-ad-client=ca-pub-3525055026201463 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></head><body><header id=header class=clearfix><div class=container><div class=col-group><div class=site-name><h1><a id=logo href>🤓 极客快訊 Geek Bank</a></h1><p class=description>為你帶來最全的科技知識 🧡</p></div><div><nav id=nav-menu class=clearfix><a class=current href>猜你喜歡</a>
<a href=../../tw/categories/%E7%A7%91%E6%8A%80.html title=科技>科技</a>
<a href=../../tw/categories/%E9%81%8A%E6%88%B2.html title=遊戲>遊戲</a>
<a href=../../tw/categories/%E7%A7%91%E5%AD%B8.html title=科學>科學</a></nav></div></div></div></header><div id=body><div class=container><div class=col-group><div class=col-8 id=main><div class=res-cons><article class=post><header><h1 class=post-title>機器學習之線性代數速查表</h1></header><date class="post-meta meta-date">2020-11-14</date><div class=post-meta><span>|</span>
<span class=meta-category><a href=tw/categories/%E7%A7%91%E6%8A%80.html>科技</a></span></div><div class=post-content><div><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153089168574158dba8fa5a><p class=pgc-img-caption></p></div><h1>1. 向量</h1><p>1.1 基本概念</p><p><strong>【向量（vector）】</strong>：一個同時具有大小和方向的幾何對象。</p><p><strong>【行向量（row vector）】</strong>：一個1×n的矩陣，即矩陣由一個含有n個元素的行所組成：</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530891612505deb83dfe62><p class=pgc-img-caption></p></div><p><strong>【列向量（column vector）】</strong>：一個m × 1的矩陣，即矩陣由一個包含m個元素的列組成：</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530891612502368cb1fdcd><p class=pgc-img-caption></p></div><p>行向量的轉置是一個列向量，反之亦然。</p><p><strong>【向量的模】</strong>：向量的長度叫做向量的模。假設向量 v = (v1, v2, …, vn), 則v的模。記作：</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15308916124862198dd6ee2><p class=pgc-img-caption></p></div><p><strong>【單位向量】</strong>：模為1的向量就是單位向量。</p><p><strong>【向量的基（也稱為基底）】</strong>：給定一個向量空間 V。 V的一組基B，是指V裡面的可線性生成V的一個線性無關子集。B的元素稱為基向量。</p><p>1.2 常見運算</p><p>向量常見的運算有：加法，減法，標量乘向量以及向量之間的乘法（叉乘、點乘）。</p><p>在機器學習中，我們需要重點看加法，標量乘向量和點乘。</p><p>設：存在兩個n維度向量a = (a1, a2, …, an) 和 b = (b1, b2, …, bn)</p><p><strong>1.2.1 向量加法</strong></p><p>a + b = (a1 + b1, a2 + b2, …, an + bn)</p><p><strong>1.2.2 向量乘以標量</strong></p><p>設標量為k, 則 ka = (ka1, ka2, …, kan)</p><p><strong>1.2.3 向量點乘</strong></p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p9.pstatp.com/large/pgc-image/1530891612462428407f7a1><p class=pgc-img-caption></p></div><p>1.3 向量性質</p><p><strong>1.3.1 線性相關（linearly dependent）</strong></p><p>假設V是在域K上的向量空間。V中的一組(m個)元素中，若有向量可用有限個其他向量的線性組合所表示，則稱為線性相關，反之稱為線性無關。</p><p>換言之，如果v1, v2, ..., vn 是V的向量，如果從域K 中有非全零的元素a1, a2, ..., an，適合 a1v1 + a2v2 + ... + anvn = 0, 則稱它們為<strong>線性相關</strong>。</p><p>如果K中不存在這樣的元素，那麼v1, v2, ..., vn是<strong>線性無關</strong>或<strong>線性獨立</strong>。</p><p><strong>1.3.2 線性相關的幾何意義</strong></p><p>說向量組v1, v2, ... vm 線性相關，則：</p><p>當m = 1時，若v1 = 0， 則只含有v1一個元素的向量組線性相關，否則，線性無關。</p><p>當m = 2時，如果 a1v1 + a2v2 = 0,則v1和v2線性相關，也就是說v1和v2的分量對應成比例，在幾何意義上，v1和v2共線。否則，二者線性無關。</p><p>當m =3時， v1,v2,v3線性相關的幾何意義是三者共面。</p><p><strong>1.3.3 正交</strong></p><p>若內積空間中兩向量的內積為0，則稱它們是<strong>正交</strong>的。正交是垂直這一直觀概念的推廣。</p><p><strong>1.3.4 正交 vs 線性無關</strong></p><p>正交的向量一定線性無關，線性無關的向量不一定正交。</p><h1>2. 線性變換與線性函數</h1><p>2.1 線性變換</p><p>在兩個向量空間之間的一種保持向量加法和標量乘法的特殊映射，稱為<strong>線性變換</strong>（或線性映射）。</p><p>2.2 線性函數</p><p>設 V 和 W 是在相同域 K 上的向量空間。法則 f : V → W 被稱為是線性映射，如果對於 V 中任何兩個向量 x 和 y 與 K 中任何標量 a，滿足下列兩個條件:</p><p><strong>(1) 可加性</strong>： f(x+y) = f(x) + f(y) <strong>(2) 齊次性</strong>： f(ax) = af(x)</p><p>即其維持向量加法與標量乘法。</p><p>上述等價於要求對於任何向量 x1, ..., xm 和標量 a1, ..., am，下面方程成立：</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153089161247560a9ed8bcc><p class=pgc-img-caption></p></div><p>當上述的法則 f : V → W為函數時，就是<strong>線性函數</strong>。</p><p>比較直觀的理解就是大部分一次函數，例如二維空間中的f(x)=ax+b，其中a,b為常數。</p><h1>3. 矩陣</h1><p>3.1 m x n 矩陣</p><p><strong>3.1.1 定義</strong></p><p>將一些元素排列成若干行，每行放上相同數量的元素，就是一個矩陣。</p><p>一個m×n的矩陣是一個由m行n列元素排列成的矩形陣列，矩陣裡的元素可以是數字、符號或數學式。</p><p><strong>3.1.2 矩陣的基本運算</strong></p><p>最基本運算包括矩陣加（減）法，數乘和轉置運算。</p><p><strong>【1】矩陣加法</strong>：m×n矩陣A和B的和（差）：A±B為一個m×n矩陣，其中每個元素是A和B相應元素的和（差）: (A ± B)i,j = Ai,j ± Bi,j，其中1 ≤ i ≤ m , 1 ≤ j ≤ n.</p><p><strong>【2】矩陣數乘</strong>：標量c與矩陣A的數乘：cA的每個元素是A的相應元素與c的乘積，(cA)i,j = cAi,j</p><p><strong>【3】矩陣轉置</strong>：m×n矩陣A的轉置是一個n×m的矩陣，記為AT（或A'），其中的第i個行向量是原矩陣A的第i個列向量；或者說，轉置矩陣AT第i行第j列的元素是原矩陣A第j行第i列的元素， (AT)i,j = Aj,i</p><p><strong>【4】矩陣的乘法</strong>：兩個矩陣的乘法僅當第一個矩陣A的列數和另一個矩陣B的行數相等時才能定義。如A是m×n矩陣和B是n×p矩陣，它們的乘積AB是一個m×p矩陣，它的一個元素</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/15308916124742e76b26ec1><p class=pgc-img-caption></p></div><p>其中1 ≤ i ≤ m, 1 ≤ j ≤ p</p><p><strong>3.1.3 矩陣運算的規律</strong></p><p><strong>[1] 矩陣的加法運算滿足交換律</strong>：</p><p>A + B = B + A。</p><p><strong>[2] 矩陣的轉置和數乘運算滿足分配律</strong>：</p><p>(A + B)T = AT + BT c(A + B) = cA + cB</p><p><strong>並滿足類似於結合律</strong>的規律： c(AT) = (cA)T.</p><p><strong>[3] 矩陣的乘法滿足結合律和對矩陣加法的分配律（左分配律和右分配律）</strong>：</p><p>• 結合律：(AB)C = A(BC), • 左分配律：(A + B)C = AC + BC, • 右分配律：C(A + B) = CA + CB.</p><p><strong>[4] 矩陣的乘法與數乘運算之間也滿足類似結合律的規律</strong>:</p><p>c(AB) = (cA)B = A(cB)</p><p><strong>[5] 矩陣的乘法與轉置之間則滿足倒置的分配律</strong>：</p><p>(AB)T = BTAT</p><p><strong>[6] 矩陣乘法*不*滿足交換律</strong>。</p><p>一般來說，矩陣A及B的乘積AB存在，但BA不一定存在，即使存在，大多數時候AB ≠ BA。</p><p><strong>3.1.4 矩陣與線性變換的關係</strong></p><p>矩陣是線性變換的便利表達法。</p><p>以R^n表示所有長度為n的行向量的集合。每個m×n的矩陣A都代表了一個從R^n射到R^m的線性變換。</p><p>也就是說，對每個線性變換f: R^n -> R^m，都存在唯一m×n矩陣A使得對所有R^n中的元素x，f(x) = Ax。</p><p><strong>3.1.5 相關基本概念</strong></p><p><strong>【矩陣的秩】</strong>： 用初等行變換將矩陣A化為階梯形矩陣, 則矩陣中非零行的個數就定義為這個矩陣的秩。</p><p><strong>【列秩】</strong>：一個矩陣A的列秩是A的線性獨立的縱列的最大數目。</p><p><strong>【行秩】</strong>：一個矩陣A的行秩是A的線性獨立的橫行的最大數目。</p><p>行秩和列秩的關係：矩陣的列秩和行秩總是相等的。因此它們可以簡單地稱作矩陣A的秩。通常表示為r(A)，rk(A)或rank A。</p><p><strong>【滿秩矩陣（non-singular matrix）】</strong>:若矩陣秩等於行數，稱為行滿秩；若矩陣秩等於列數，稱為列滿秩。既是行滿秩又是列滿秩則為n階矩陣即n階方陣。</p><p><strong>【子式】</strong>：設A為一個 m×n 的矩陣，k為一個介於1和m之間的整數，並且k≤n。A的一個k階子式是在A中選取k行k列之後所產生的k2個交點組成的方塊矩陣的行列式。</p><p><strong>【餘子式】</strong>：A的一個k階餘子式是A去掉了k行與k列之後得到的(m-k)×(n-k)矩陣的行列式。</p><p>NOTE: 在m=/=n的情況下，這樣的行列式如何計算是沒有定義的，僅僅在概念上存在。</p><p><strong>【零矩陣】</strong>：即所有元素皆為0的矩陣。</p><p>NOTE：對稱矩陣，對角矩陣，矩陣的對角化等都有針對mxn矩陣的一般定義，但是在應用的層面，我們不必進行這些一般性的討論，而只需要關注其針對nxn階方陣的情形即可，因此，大多數情況下，對於矩陣的性質和運算，我們集中關注方陣這一特例。</p><p>3.2 n x n方陣</p><p>方陣具備一些一般m x n矩陣(m =/= n) 所不具備的特徵和屬性，使得它們特別有用。而一些運算，如對角化等在方陣中比一般矩陣中多見而且更容易，因此，許多問題我們集中在方陣裡討論。</p><p><strong>3.2.1 基本概念</strong></p><p><strong>【方陣】</strong>：在所有矩陣中，行和列相等的那類稱為方陣。</p><p><strong>【行列式】</strong>：將一個nxn的方陣A映射到一個標量，記作|A|或det(A)。雖然記作|A|，但其實一個矩陣的行列式有可能是負數，這裡要注意和絕對值區別。</p><p>• 1階矩陣的行列式：就是它本身。</p><p>• 2階矩陣的行列式：</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15308916126317765e094aa><p class=pgc-img-caption></p></div><p>• 3階矩陣的行列式：</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530891612629045471338c><p class=pgc-img-caption></p></div><p><strong>【主子式】</strong>：設A是一個n階方陣，I和J是集合{1,...,n}的一個k元子集，那麼[A]I,J表示A的k階子式。其中抽取的k行的行標是I中所有元素，k列的列標是J中所有元素。</p><p>如果I=J，那麼稱[A]I,J是A的主子式。</p><p>如果I=J={1,...,k}（所取的是左起前k列和上起前k行），那麼相應的主子式被稱為順序主子式。一個n×n的方塊矩陣有n個順序主子式。</p><p><strong>【餘子式】</strong>：設A為一個 n階方陣， A關於一個k階子式的餘子式，是A去掉了這個k階子式所在的行與列之後得到的(n-k)×(n-k)矩陣的行列式，簡稱為A的k階餘子式。</p><p>A關於第i行第j列的餘子式Mij是指A中去掉第i行第j列後得到的n−1階子矩陣的行列式。有時可以簡稱為A的（i，j）餘子式。記作Mij。</p><p><strong>【餘子矩陣】</strong>: n階方陣A的餘子矩陣是指將A的(i, j)代數餘子式擺在第i行第j列所得到的矩陣，記為C。</p><p>Cij = (−1)^(i + j) Mij</p><p><strong>【伴隨矩陣】</strong>：上述餘子矩陣C的轉置矩陣，稱為n階方陣A的伴隨矩陣。記作A*。</p><p><strong>【單位矩陣】</strong>：單位矩陣（記作I）的對角線全是1而其他位置全是0。</p><p><strong>【置換矩陣】</strong>：是一種係數只由0和1組成的方塊矩陣。置換矩陣的每一行和每一列都恰好有一個1，其餘的係數都是0。</p><p><strong>3.2.2 逆矩陣，可逆矩陣，（非）奇異矩陣及可逆與其他概念的關係</strong></p><p><strong>【逆矩陣】</strong>：給定一個n階方陣A，若存在一n 階方陣B， 使得AB=BA=I，其中I 為n 階單位矩陣，則稱A 是可逆的，且B 是A 的逆陣，記作 A^(-1)。</p><p><strong>【可逆矩陣】</strong>：若n 階方陣A 的逆陣存在，則稱A 為非奇異方陣或可逆方陣。</p><p>可逆和滿秩的關係：對n階方陣而言，滿秩等價於可逆。</p><p>可逆和伴隨的關係：如果n階方陣A可逆，那麼它的逆矩陣和它的伴隨矩陣之間只差一個係數。</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/15308916126590d8757c794><p class=pgc-img-caption></p></div><p>然而，伴隨矩陣對不可逆的矩陣也有定義，並且不需要用到除法。</p><p><strong>【奇異方陣】</strong>：若方塊矩陣A滿足條件|A|=0，則稱A為奇異方陣，否則稱為非奇異方陣。</p><p>可逆和非奇異方陣的關係：對於n階方陣而言，非奇異等價於可逆矩陣。</p><p><strong>3.2.3 對稱矩陣、對角矩陣、可對角化和對角化</strong></p><p><strong>【對稱矩陣】</strong>：對稱矩陣是一個n階方陣，其轉置矩陣和自身相等：</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/153089161265033e3a609b7><p class=pgc-img-caption></p></div><p>對稱矩陣中的右上至左下方向元素以主對角線（左上至右下）為軸對稱。若將其寫作A=（aij），則：aij = aji</p><p>方陣與對稱的關係：對於任何方陣A，A + AT 都是對稱矩陣。</p><p><strong>【對角矩陣】</strong>: 是一個主對角線之外的元素皆為0的n階方陣。對角線上的元素可以為0或其他值。</p><p>對角與對稱的關係：對角矩陣都是對稱矩陣。</p><p><strong>【可對角化】</strong>：如果一個方塊矩陣 A 相似於對角矩陣，也就是說，如果存在一個可逆矩陣 P 使得 P −1AP 是對角矩陣，則它就被稱為可對角化的。</p><p><strong>方陣可對角化充要條件</strong>：n x n方陣可進行對角化的充分必要條件是：</p><p>(1) n階方陣存在n個線性無關的特徵向量。</p><p>(2) 如果n階方陣存在重複的特徵值，每個特徵值的線性無關的特徵向量的個數恰好等於該特徵值的重複次數</p><p><strong>【對角化】</strong>：將可對角化的方陣A通過與轉換矩陣P的運算，轉換為對角矩陣的過程叫做對角化。</p><p><strong>3.2.4 相似矩陣和相似變換</strong></p><p><strong>【相似矩陣】</strong>：兩個係數域為K的n階方陣A與B為域L上的相似矩陣當且僅當存在一個係數域為L的n×n的可逆矩陣P，使得:</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530891612667de9f83a77a><p class=pgc-img-caption></p></div><p>這時，稱矩陣A與B“相似”。</p><p><strong>【相似變換】</strong>： 相似變換是矩陣之間的一種等價關係。也就是說滿足：</p><ol><li><strong>反身性</strong>：任意矩陣都與其自身相似。</li><li><strong>對稱性</strong>：如果A和B相似，那麼B也和A相似。</li><li><strong>傳遞性</strong>：如果A和B相似，B和C相似，那麼A也和C相似。</li></ol><p><strong>3.2.5 正交矩陣和正交變換</strong></p><p><strong>【正交矩陣】</strong>：一個n階方陣Q，其元素為實數，而且行（列）向量為兩兩正交的單位向量，使得該矩陣的轉置矩陣為其逆矩陣。</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530891612665f68f5857c4><p class=pgc-img-caption></p></div><p>其中，I為單位矩陣。正交矩陣的行列式值必定為+1或-1</p><p><strong>【正交變換】</strong>：Q為正交矩陣，而v為向量，則Qv稱作正交變換。正交變換不改變向量的長度。</p><p><strong>3.2.6 用正交陣對對稱陣進行合同變換</strong></p><p>對於n階對稱陣A，必存在正交陣P，使得：</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p3.pstatp.com/large/pgc-image/1530891612837a60b0b9b7c><p class=pgc-img-caption></p></div><p>其中 Λ 為以A的n個特徵值為對角元的對角陣。這種變換叫做合同變換。A和 Λ 互為合同矩陣。</p><p>3.3 實對稱矩陣</p><p><strong>3.3.1 定義</strong></p><p>實對稱矩陣是一個n階方陣，其元素都為實數，且轉置矩陣和自身相等：</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530891612840624e7a5de5><p class=pgc-img-caption></p></div><p><strong>3.3.2 實對稱矩陣的性質</strong></p><p>（1）實對稱陣的特徵值為實數，其特徵向量可以取實向量。</p><p>（2）實對稱矩陣都能對角化，且可用正交矩陣對其進行對角化。</p><p>（3） 任意的 nxn 實對稱矩陣都有 n 個線性無關的特徵向量。並且這些特徵向量都可以正交單位化而得到一組正交且模為 1 的向量。</p><p>故實對稱矩陣 A 可被分解成:</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530891615479cbaece87ed><p class=pgc-img-caption></p></div><p>其中 Q 為 正交矩陣， Λ 為實對角矩陣。</p><p>（4）實對稱矩陣不同特徵值的特徵向量正交。</p><p><strong>3.3.3 正定、半正定、負定、半負定</strong></p><p>對於一個n×n的實對稱矩陣M, 當且僅當它對於所有非零實係數向量z都有：</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530891612911efa8bccef2><p class=pgc-img-caption></p></div><p>其中zT表示z的轉置。</p><p>NOTE: 對於複數對稱陣，也有同樣概念，但此處不考慮。</p><h1>4. 特徵值和特徵向量</h1><p>4.1 定義</p><p>對於n x n方陣A，若標量λ和n維非0列向量v滿足：</p><div class=pgc-img><img alt=機器學習之線性代數速查表 onerror=errorimg.call(this); src=https://p1.pstatp.com/large/pgc-image/1530891612876ff4217ea4d><p class=pgc-img-caption></p></div><p>那麼稱λ為A的特徵值，v稱為對應於特徵值λ的特徵向量。</p><p>4.2 幾何意義</p><p>λ反映的是：特徵向量v的長度在線性變換A下縮放的比例。</p><p>如果特徵值為正，則表示v在經過線性變換的作用後方向也不變；如果特徵值為負，說明方向會反轉；如果特徵值為0，則是表示縮回零點。但無論怎樣，仍在同一條直線上。</p><p>4.3 相關概念</p><p><strong>【特徵空間】</strong>：n階方陣A所有具有相同的特徵值λ的特徵向量和零向量一起，組成了一個向量空間，稱為A的一個特徵空間。</p><p><strong>【幾何重數】</strong>：這個特徵空間如果是有限維的，那麼它的維數叫做λ的幾何重數。</p><p><strong>【主特徵向量】</strong>： 模最大的特徵值對應的特徵向量是A的主特徵向量。</p><p><strong>【譜】</strong>：在有限維向量空間上，一個方陣A的其所有特徵值的集合就是A的譜。</p><p><strong>【標準正交基】</strong>：是元素兩兩正交的基。稱基中的元素為基向量。</p><p>4.4 特徵向量與係數方程</p><p>特徵向量也可以看作是關於係數λ的方程：T(x) = λx 的非零解。</p><p>4.5 特徵值的性質</p><p>n階方陣A=（aij）有n個特徵值（其中可能包括重複值）λ1， λ2， … λn，則有</p><p>（1）這n個特徵值的和為A對角線上各個數的和： λ1 + λ2 + … + λn = a11 + a22 + … + ann</p><p>（2）這n個特徵值的乘積為A的行列式：λ1λ2…λn = |A|</p><p>（3）不相等的特徵值所對應的特徵向量線性無關。</p><p>（4） 如果一個n階方陣有n個不同的特徵值，那麼矩陣必然存在相似矩陣。</p><hr><p>鳴宇淳</p><p>沙發</p></div></div><div class="post-meta meta-tags"><ul class=clearfix><li><a>機器</a></li><li><a>學習</a></li><li><a>之線</a></li></ul></div></article></div></div><div id=secondary><section class=widget><form id=search action=//www.google.com/search method=get accept-charset=utf-8 target=_blank _lpchecked=1><input type=text name=q maxlength=20 placeholder=搜索>
<input type=hidden name=sitesearch value=geekbank.cf>
<button type=submit>🔍</button></form></section><section class=widget><h3 class=widget-title>最新文章 ⚡</h3><ul class=widget-list><li><a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html alt=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/13adbab9c7f94c7fa81d49a98861b051 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dc2af9c9.html title=機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式>機器學習筆記(七)——初識邏輯迴歸、不同方法推導梯度公式</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html alt=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1540372101455de0fb74774 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e214e6d7.html title=深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開>深度學習/機器學習入門數學知識整理（二）梯度與導數，泰勒展開</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html alt=講透機器學習中的梯度下降 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/5c80301e53424671bc22755be2e4ee33 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f3767255.html title=講透機器學習中的梯度下降>講透機器學習中的梯度下降</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html alt=機器學習時代的哈希算法，將如何更高效地索引數據 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525617261534ad07c6455c style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/ce278cf4.html title=機器學習時代的哈希算法，將如何更高效地索引數據>機器學習時代的哈希算法，將如何更高效地索引數據</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html alt=淺談機器學習時代的哈希算法（一） class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/1525788510275af3193bcdc style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/346037ff.html title=淺談機器學習時代的哈希算法（一）>淺談機器學習時代的哈希算法（一）</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html alt=機器學習入門第2章：SVM（支持向量機）—編碼 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/28eb40e101e44cfb8b88aac745d012d6 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f71cd4af.html title=機器學習入門第2章：SVM（支持向量機）—編碼>機器學習入門第2章：SVM（支持向量機）—編碼</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html alt=機器學習總結（基礎）：偏差和方差、iid、分佈 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/a9681e5f716547e288303eae292c5b3e style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/a55cbbea.html title=機器學習總結（基礎）：偏差和方差、iid、分佈>機器學習總結（基礎）：偏差和方差、iid、分佈</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html alt=機器學習數學篇—基礎數學知識清單 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/59470000766ddb369113 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/41b2e34d.html title=機器學習數學篇—基礎數學知識清單>機器學習數學篇—基礎數學知識清單</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html alt=使用機器學習的手寫數字識別 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/1f78eeb0e00a46b789e4bcb4ad07d97b style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/6cc52e2b.html title=使用機器學習的手寫數字識別>使用機器學習的手寫數字識別</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html alt=專為機器學習打造的半導體器件：可進行任意邏輯運算 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/4654f6d6a6dd496ebbf6787bb43a7231 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/f575bcd3.html title=專為機器學習打造的半導體器件：可進行任意邏輯運算>專為機器學習打造的半導體器件：可進行任意邏輯運算</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html alt=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/ebc79c3aa76541b393374cc521297870 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/fc9d01d7.html title=機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用>機器學習降維技術（PCA，ICA和流形學習）及醫學中流形學習的應用</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html alt=基於機器學習在雙光子光刻過程中進行自動探測產品的質量 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/7550407fa66941b2991e53b5a9ec4071 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/dd736e73.html title=基於機器學習在雙光子光刻過程中進行自動探測產品的質量>基於機器學習在雙光子光刻過程中進行自動探測產品的質量</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html alt=機器學習中模型評估和選擇的一些問題 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/6c3b00005e98772353a5 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/e33110be.html title=機器學習中模型評估和選擇的一些問題>機器學習中模型評估和選擇的一些問題</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/3873d795.html alt=金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p3.pstatp.com/large/pgc-image/694a9289cde541dca807f9a30d291d0d style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/3873d795.html title=金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例>金融中的AI和機器學習：在銀行，保險，投資以及用戶體驗中的用例</a></li><hr><li><a href=../../tw/%E7%A7%91%E6%8A%80/4d0f33bb.html alt=機器學習：什麼是自然語言處理 class="image featured" style=display:block;margin-left:auto;margin-right:auto;width:100%><img src=https://p1.pstatp.com/large/pgc-image/734232eb22dd45cfa000a5ed20aa6c78 style=border-radius:25px></a>
<a href=../../tw/%E7%A7%91%E6%8A%80/4d0f33bb.html title=機器學習：什麼是自然語言處理>機器學習：什麼是自然語言處理</a></li><hr></ul></section><section class=widget><h3 class=widget-title>其他</h3><ul class=widget-list><li><a href=TOS.html>使用條款</a></li><li><a href=CommentPolicy.html>留言政策</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>DMCA</a></li><li><a href=mailto:gdnews@tuta.io rel=nofollow>聯絡我們</a></li></ul></section></div></div></div></div><footer id=footer><div class=container>&copy; 2020 <a href>极客快訊</a></div></footer><script type=text/javascript>window.MathJax={tex2jax:{inlineMath:[['$','$']],processEscapes:true}};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><a id=rocket href=#top></a><script type=text/javascript src=https://kknews.cf/js/totop.js async></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e508ed9e4e698bb"></script></body></html>